{
  "blocks": [
    {
      "flow_id": "",
      "id": "1mpk2va",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/",
      "title": "Announcing LocalLlama discord server \u0026amp; bot!",
      "content": "INVITE: https://discord.gg/rC922KfEwj\n\nThere used to be one old discord server for the subreddit but it was deleted by the previous mod.\n\nWhy?\nThe subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).\n\nWe have a discord bot to test out open source models.\n\nBetter contest and events organization.\n\nBest for quick questions or showcasing your rig!",
      "author": "HOLUPREDICTIONS",
      "created_at": "2025-08-13T23:21:05Z",
      "comments": [
        {
          "id": "n8of8m1",
          "author": "HOLUPREDICTIONS",
          "content": "Alt invite: [https://discord.gg/4R7xS5hMdN](https://discord.gg/4R7xS5hMdN)",
          "created_at": "2025-08-14T16:38:28Z",
          "was_summarised": false
        },
        {
          "id": "n8qp4xq",
          "author": "BusRevolutionary9893",
          "content": "Discord, where valuable information goes to die and never be seen again.",
          "created_at": "2025-08-14T23:29:32Z",
          "was_summarised": false
        },
        {
          "id": "n8ms41x",
          "author": "TheMrCake",
          "content": "Please No.\n\nDiscord is a dystopian, gated version of what forums used to be.\nNo I don't want to give Discord all my data. This is peak enshittification.\n\nThis is yet another community where all information will be gated behind discords walls.\nWe as a society used to have real forums that were kept alive by donations, not VC money.",
          "created_at": "2025-08-14T11:22:55Z",
          "was_summarised": false
        },
        {
          "id": "n8z7nu8",
          "author": "shadow-studio",
          "content": "i'd prefer the \"more technical discussion\" to happen right here. discord isn't designed for keeping a knowlegde base that everyone can access freely. i'm ok with the idea of having a bot to test models, but otherwise i consider discord very inadequate and annoying for most things, just like whatsapp groups.\n\n\nalso, platform ensh!ttification is a thing.",
          "created_at": "2025-08-16T08:40:08Z",
          "was_summarised": false
        },
        {
          "id": "nc5tw7v",
          "author": "Appropriate_Cry8694",
          "content": "Discord is a cancer especially for open source projects, a lot of useful info simply dies with the server in the end.",
          "created_at": "2025-09-03T09:01:11Z",
          "was_summarised": false
        },
        {
          "id": "n8lsrr6",
          "author": "bephire",
          "content": "Would it be feasible to release regular archives of all messages/discussions in the server? We wouldn't want to lose valuable discussions and information that would otherwise become inaccessible if anything were to happen to the server.",
          "created_at": "2025-08-14T06:01:12Z",
          "was_summarised": false
        },
        {
          "id": "n8lzmct",
          "author": "Cosack",
          "content": "I do get what this is for, but still... you gotta appreciate the irony lol",
          "created_at": "2025-08-14T07:03:46Z",
          "was_summarised": false
        },
        {
          "id": "n9hjv2e",
          "author": "thedatawhiz",
          "content": "Not a good idea...",
          "created_at": "2025-08-19T06:43:49Z",
          "was_summarised": false
        },
        {
          "id": "n8mp0l9",
          "author": "a_beautiful_rhind",
          "content": "Discord asks for phone number and now face verification in some places.",
          "created_at": "2025-08-14T11:00:08Z",
          "was_summarised": false
        },
        {
          "id": "nc0z0i4",
          "author": "raysar",
          "content": "Be carefull discort is cool for friend but not for sharing public data. It's impossible to find information by searching on discord. Even bot can't do that XD",
          "created_at": "2025-09-02T15:12:51Z",
          "was_summarised": false
        },
        {
          "id": "nhaiysx",
          "author": "crantob",
          "content": "Discord sucks.  \n\nNext?",
          "created_at": "2025-10-02T01:36:16Z",
          "was_summarised": false
        },
        {
          "id": "na350q5",
          "author": "TheRealCookieLord",
          "content": "Cannot wait to invite the bot to multiple servers and make the heaviest thinking model contemplate life's meaning.",
          "created_at": "2025-08-22T15:11:34Z",
          "was_summarised": false
        },
        {
          "id": "n8krv0e",
          "author": "mrjackspade",
          "content": "Hopefully this server fairs better. The last server fucking sucked because the general chat was the landing room, 99%+ of the users only joined for tech support, and no one enforced the help channel rules because they didn't want to be \"mean\"\n\nSo at any given point in time the general chat was just being flooded by the same easy-to-google tech support questions, squashing any real conversation.",
          "created_at": "2025-08-14T01:41:28Z",
          "was_summarised": false
        },
        {
          "id": "n8o2667",
          "author": "Unigma",
          "content": "Could we add a few channels about Machine Learning and AI in general? For people that are training their own models and or creating projects with LLMs in it? I am working on some RL based fine-tuning, and wanted to know if there's a more technical oriented channel to discuss this.",
          "created_at": "2025-08-14T15:35:06Z",
          "was_summarised": false
        },
        {
          "id": "n8k52hw",
          "author": "Accomplished_Ad9530",
          "content": "Nice! Does the bot have retrieval capabilities? It might be useful for novices to have access to a chatbot with access to documentation for common inference engines and front ends, and perhaps localllama discussions. Sort of an interactive FAQ, which would help out with a lot of repeat posts here.",
          "created_at": "2025-08-13T23:30:05Z",
          "was_summarised": false
        },
        {
          "id": "n8kr3sa",
          "author": "TheRealSerdra",
          "content": "You may want to pin this for a few days, just to get some more activity",
          "created_at": "2025-08-14T01:36:54Z",
          "was_summarised": false
        },
        {
          "id": "n8mrbin",
          "author": "DeepWisdomGuy",
          "content": "Is this really running on a cerebras cluster?",
          "created_at": "2025-08-14T11:17:09Z",
          "was_summarised": false
        },
        {
          "id": "n8oc0xy",
          "author": "ilintar",
          "content": "Says the invite is invalid/expired.",
          "created_at": "2025-08-14T16:22:57Z",
          "was_summarised": false
        },
        {
          "id": "neh0mdc",
          "author": "boomboominkimspants",
          "content": "Isnâ€™t that why they call it vibe coding tho?.. like you still checking that itâ€™s meeting your vision, but you know enough and are engaged enough that u notices itâ€™s messing up",
          "created_at": "2025-09-16T04:41:45Z",
          "was_summarised": false
        },
        {
          "id": "nj6lixo",
          "author": "the100rabh",
          "content": "Invite seems to be expired",
          "created_at": "2025-10-12T23:01:24Z",
          "was_summarised": false
        },
        {
          "id": "nofhibc",
          "author": "ceramic-road",
          "content": "Thanks for relaunching the official Discord!   \n  \nThe community surpassing half a million members, having a dedicated space for deeper technical discussion makes sense.  \n  \nFor those of us running llama.cpp or Ollama on modest hardware, will there be channels dedicated to optimization tips or hardware discussion? Already joined",
          "created_at": "2025-11-12T09:27:58Z",
          "was_summarised": false
        },
        {
          "id": "ntjjg1y",
          "author": "roculus",
          "content": "can you make the \"your post is getting popular\" discord ad spam a private message to the poster so we don't have to see it every time? Thanks!",
          "created_at": "2025-12-11T22:12:32Z",
          "was_summarised": false
        },
        {
          "id": "n9h8tm1",
          "author": "GodSpeedMode",
          "content": "This sounds awesome! It's great to see the LocalLLaMA community expanding and finding ways to foster more technical discussions. With 500k users, I totally get the need for a space that caters to those of us who want to dive deeper into the nitty-gritty. The bot for testing out open-source models is a fantastic additionâ€”I can't wait to try it out! I'm also really looking forward to the contests and events; it's nice to have a central hub for that. Just hopped on the Discordâ€”letâ€™s see what everyone is working on!",
          "created_at": "2025-08-19T05:03:22Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1mpk2va",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.917823983Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.reddit.com/gallery/1mpk2va: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.917822109Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7qcux",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "title": "The NO FAKES Act has a \"Fingerprinting\" Trap that kills Open Source. We need to lobby for a Safe Harbor.",
      "content": "Hey everyone,\nâ€‹Iâ€™ve been reading the text of the \"NO FAKES Act\" currently in Congress, and itâ€™s worse than I thought.\nâ€‹The Tldr: It creates a \"digital replica right\" for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who \"makes available\" a tool that is primarily used for replicas.  \nâ€‹The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation).\nâ€‹There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.\n\nWhat I did:\nI contacted my reps email to flag this as an \"innovation killer.\" If you run a repo or care about open weights, you might want to do the same. We need them to add a \"Safe Harbor\" for tool devs.\n\nS.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress https://share.google/u6dpy7ZQDvZWUrlfc",
      "author": "PostEasy7183",
      "created_at": "2026-01-08T22:33:33Z",
      "comments": [
        {
          "id": "nyhm9rk",
          "author": "Revolutionalredstone",
          "content": "Making your own devs liable is how you turn your country into a third world nation.\n\nPeople who make it easy to USE the tools are the only ones who should be liable.\n\nThere are plenty of countries which won't play these silly blame games and their devs will keep releasing all their stuff either way.\n\nDevs are the inventors of ideas and making them liable for how others missuse them just cuts you off from new ideas completely, what we need todo is make operators / sites / places the normal people go to use the less desirable filters and tech liable (Instagram etc)\n\nAlso dev software licenses say you can't misuse their tech etc, so it's a joke to pretend they are in the wrong if users abuse their license.\n\nThat's a bit like holding petrol companies liable for people who stupidly try to throw bottles of gasoline onto fires:\n\nhttps://www.youtube.com/watch?v=3l50QZiPwnY\n\nEverything can be abused / used in a destructive way / used other than - intended usage.\n\nPowerful open source technologies always win and if your country is not compatible with openness then it's gonna get left behind (think north korea starving and surviving on cracked old builds of windows xp)",
          "created_at": "2026-01-08T22:44:14Z",
          "was_summarised": false
        },
        {
          "id": "nyhobge",
          "author": "jferments",
          "content": "This has been the point of the astro-turf \"anti-AI\" movement all along. I firmly believe that big tech corporations like Google, Microsoft, and OpenAI are behind the bots spreading \"anti-AI\" propaganda that supports laws that will essentially centralize control of AI and make open-source AI illegal.",
          "created_at": "2026-01-08T22:54:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhtae5",
          "author": "fortpatches",
          "content": "I understand where you are coming from, however, you may be misreading the text.\n\nSpecifically, you seem to have overlooked the phrase \"of a specifically identified individual\". E.g., (c)(1)(B)(i) states \"is primarily designed to produce 1 or more digital replicas of a specifically identified individual or individuals without \\[authorization\\].\"  The following subsections (ii) and (iii) have similar \"specifically identified individual\" language.\n\nThis would be more like making an AI designed to make you sound like Arnold Schwarzenegger as opposed to making an AI designed to make you sound like whatever audio sample you provide to it. Or Text-to-speech that makes \"AI Arnold\" say whatever you type.\n\nMoreover, is your AI \"primarily\" designed to produce \"AI Arnold\" audio?\n\nFurther, to actual knowledge is required: (c)(3)(B) states \"with respect to an activity carried out under paragraph (2) by an individual ..., the individual ... must have actual knowledge, ... that the applicable material isâ€” (i) a digital replica that was not authorized by the applicable right holder; or (ii) a product or service described in paragraph (2)(B).\"\n\nIn other words, the liability only attaches if the dev has \"actual knowledge\" that their service \"is \\*primarily\\* designed to produce a digital replica of a \\*specifically identified individual\\*.\"",
          "created_at": "2026-01-08T23:18:46Z",
          "was_summarised": false
        },
        {
          "id": "nyhkxhr",
          "author": "Aromatic-Low-4578",
          "content": "Don't most software licenses already try to protect the developer from liability due to users?  Will be interested to see how it plays out.",
          "created_at": "2026-01-08T22:37:58Z",
          "was_summarised": false
        },
        {
          "id": "nyitkq2",
          "author": "Acceptable_Home_",
          "content": "US and tech bros are actively targeting open source models",
          "created_at": "2026-01-09T02:28:39Z",
          "was_summarised": false
        },
        {
          "id": "nyhq9v2",
          "author": "Technical_Ad_440",
          "content": "i doubt it will pass sounds like a complete dud. the top ai will fight this cause it would kill all their tools in that case. personally for me i would rather have tons of fakes and misinfo why? cause in the future those that do due diligence will be the people you want to keep around those that dont fall for the missinfo and use tools to check origins and such they are the people you want. either as fans, work colleagues etc. \n\nwe need more and more fakes and we need people to start checking stuff not banning the tools. the future people are gonna be so hardened against all the missinfo and such that it wont make much difference. it actually baffles me why people dont want to flood fakes and such to just go oh yeh that incident it was AI and move on. its like a shield against most the normal attacks people do and actually weakens the attack vectors people have. kinda like how if people know where someone is people cant dox them cause people already know. trying it just makes the person go well people already know.\n\nyou cant trust most things anymore anyways so if i see things \"exposing\" stuff i just assume its probably AI until otherwise confirmed. it also means mundane attacks in the future are just gonna fall flat completely like if it doesnt affect you you will not care whatsoever",
          "created_at": "2026-01-08T23:03:38Z",
          "was_summarised": false
        },
        {
          "id": "nyhzwpj",
          "author": "AutomaticDriver5882",
          "content": "I would just have different developers, make different pieces of the code and then you would snap in the modules so no one person is responsible but maybe thatâ€™s an oversimplification",
          "created_at": "2026-01-08T23:52:54Z",
          "was_summarised": false
        },
        {
          "id": "nyif4r5",
          "author": "davedcne",
          "content": "Honest question, do you think your rep even understood what you were trying to explain to them? I think most of our politicians are so out of touch with technology that its like trying to teach a cave man calculus.",
          "created_at": "2026-01-09T01:11:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhvrpq",
          "author": "WristbandYang",
          "content": "OP only responds using AI. This is bot behavior.",
          "created_at": "2026-01-08T23:31:25Z",
          "was_summarised": false
        },
        {
          "id": "nyile2z",
          "author": "ortegaalfredo",
          "content": "If you think about it, It's way more disturbing than you think:\n\nThey don't want to criminalize porn, they want to criminalize FAKE porn, why? because they need to be in control of the porn generation, so men and particularly young men can be controlled with it.",
          "created_at": "2026-01-09T01:45:04Z",
          "was_summarised": false
        },
        {
          "id": "nyin0jf",
          "author": "SilentLennie",
          "content": "Don't know if it matters in practice what they propose. US politics is such a mess and the business interests are so 'great' they might prevent it being passed or no enforcement will happen (regulatory capture).",
          "created_at": "2026-01-09T01:53:44Z",
          "was_summarised": false
        },
        {
          "id": "nyhroca",
          "author": "PostEasy7183",
          "content": "Comments are coming in a lot quicker than I thought tonight. Please be patient and I will try to get back with you as many of you as I can in a couple hours. Thank you and make sure to bump the thread so this gets attention. Ensure to write to your reps or contact them with any other means necessary.",
          "created_at": "2026-01-08T23:10:40Z",
          "was_summarised": false
        },
        {
          "id": "nyioqq8",
          "author": "timschwartz",
          "content": "Just frame it like guns:\n\nModels don't deepfake people, people with models deepfake people.",
          "created_at": "2026-01-09T02:03:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhwkd3",
          "author": "grady_vuckovic",
          "content": "I would love to hear everyone's suggestion for what the alternative is to stop the deep fakes?\n\nThis seems to make it pretty simple. If you're a developer and you release a tool that can be used to impersonate people, you're responsible.\n\nSo let's say you're all against that, alright, fair, what's the next step? What laws, what enforcement, how do we stop this?\n\nBecause \"we just have to learn to live with it\" is not gonna fly.",
          "created_at": "2026-01-08T23:35:34Z",
          "was_summarised": false
        },
        {
          "id": "nyhubjk",
          "author": "Marksta",
          "content": "\u0026gt;a tool that is **primarily** used for replicas.\n\nIsn't the primarily part key here? Because everytime someone advertises a TTS related thing, they bold 1000 times EAZY ONE SHOT VOICE REPLICATION as if that's its only use. This thing passes, then just take that out of your description and add a warning that you're expected to own the rights to any likeness you're duplicating.\n\nThis worry is pretty same as Photoshop being able to edit images you don't own. Photoshop doesn't spam 1000 times that you can steal copyright and must have written somewhere that copyright laws are your own problem somewhere.\n\nSo all that's left is probably actual heinous sites on the chopping block who should definitely be liable if their advertising is spamming how easy their tool will allow you to rip off likenesses and generate infringing content.",
          "created_at": "2026-01-08T23:23:59Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://share.google/u6dpy7ZQDvZWUrlfc",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.91822167Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://share.google/u6dpy7ZQDvZWUrlfc: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918221109Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7d8bj",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/",
      "title": "Jensen Huang saying \"AI\" 121 times during the NVIDIA CES keynote - cut with one prompt",
      "content": "Someone had to count it. Turns out Jensen said \"AI\" exactly 121 times in the CES 2025 keynote.\n\nI used [https://github.com/OpenAgentPlatform/Dive](https://github.com/OpenAgentPlatform/Dive) (open-source MCP client) + two MCPs I made:\n\n\\- [https://github.com/kevinwatt/yt-dlp-mcp](https://github.com/kevinwatt/yt-dlp-mcp) \\- YouTube download  \n\\- [https://github.com/kevinwatt/ffmpeg-mcp-lite](https://github.com/kevinwatt/ffmpeg-mcp-lite) \\- video editing\n\n**One prompt:**\n\n\u0026gt;Task: Create a compilation video of every exact moment Jensen Huang says \"AI\".  \nVideo source: [https://www.youtube.com/watch?v=0NBILspM4c4](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n\u0026gt;**Instructions:**\n\n\u0026gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)\n\n\u0026gt;Parse JSON3 to find every \"AI\" instance with precise start/end times\n\n\u0026gt;Use ffmpeg to cut clips (\\~50-100ms padding for natural sound)\n\n\u0026gt;Concatenate all clips chronologically\n\n\u0026gt;Output: Jensen\\_CES\\_AI.mp4\n\nDive chained the two MCPs together - download â†’ parse timestamps â†’ cut 121 clips â†’ merge. All local, no cloud.\n\nIf you want to see how it runs: [https://www.youtube.com/watch?v=u\\_7OtyYAX74](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nThe result is... hypnotic.",
      "author": "Prior-Arm-6705",
      "created_at": "2026-01-08T14:29:47Z",
      "comments": [
        {
          "id": "nygb8aw",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T19:15:15Z",
          "was_summarised": false
        },
        {
          "id": "nyejqtm",
          "author": "YearZero",
          "content": "Honestly that's probably a great summary of the keynote. He should've just done exactly that and it wouldn't change anything.",
          "created_at": "2026-01-08T14:33:06Z",
          "was_summarised": false
        },
        {
          "id": "nyf9wy6",
          "author": "DriveSolid7073",
          "content": "\"All local, no cloud.\"   \nopen video  \n\\\u0026gt;claude opus 4.5  \nI couldn't get Dive to work with my Koboldcpp, and the functions aren't being called.",
          "created_at": "2026-01-08T16:33:54Z",
          "was_summarised": false
        },
        {
          "id": "nyekf0a",
          "author": "LambdaHominem",
          "content": "gamers nexus would be proud\n\nhttps://youtu.be/-qbylbEek-M",
          "created_at": "2026-01-08T14:36:33Z",
          "was_summarised": false
        },
        {
          "id": "nyepodd",
          "author": "LinkSea8324",
          "content": "The fuck is that latex-leather jacket",
          "created_at": "2026-01-08T15:02:13Z",
          "was_summarised": false
        },
        {
          "id": "nyekw78",
          "author": "FastDecode1",
          "content": "Reminds me of the Xbox One reveal: https://www.youtube.com/watch?v=KbWgUO-Rqcw",
          "created_at": "2026-01-08T14:38:59Z",
          "was_summarised": false
        },
        {
          "id": "nyenr5m",
          "author": "International-Try467",
          "content": "This man is the reason why everything is so expensive",
          "created_at": "2026-01-08T14:53:01Z",
          "was_summarised": false
        },
        {
          "id": "nyeqoex",
          "author": "anon235340346823",
          "content": "0:44 \"hey I have AI\"",
          "created_at": "2026-01-08T15:07:02Z",
          "was_summarised": false
        },
        {
          "id": "nyg0uk3",
          "author": "Sea_Succotash3634",
          "content": "\"Consumer\" Electronic Show",
          "created_at": "2026-01-08T18:30:35Z",
          "was_summarised": false
        },
        {
          "id": "nygtg3m",
          "author": "Freonr2",
          "content": "\"Consumer\" Electronics Show, showing billion dollar datacenter configs.",
          "created_at": "2026-01-08T20:36:11Z",
          "was_summarised": false
        },
        {
          "id": "nygf0cq",
          "author": "r0ckl0bsta",
          "content": "Old McJensen's server farm.\nA-I-A-I-O ðŸŽ¶",
          "created_at": "2026-01-08T19:31:52Z",
          "was_summarised": false
        },
        {
          "id": "nyfjql1",
          "author": "budz",
          "content": "dope, I made a script that does this , back in October      [https://imgur.com/a/h0vc2f6](https://imgur.com/a/h0vc2f6)",
          "created_at": "2026-01-08T17:16:29Z",
          "was_summarised": false
        },
        {
          "id": "nyfa965",
          "author": "MMAgeezer",
          "content": "Appreciate you sharing the details of how you did this. One small thing though, it includes multiple clips of a narrator (i.e. not Jensen, as instructed) saying \"AI\" too.",
          "created_at": "2026-01-08T16:35:22Z",
          "was_summarised": false
        },
        {
          "id": "nyfp4wj",
          "author": "positivcheg",
          "content": "Nah. AMD wins, AMD mentioned AI 299 times. Big win for AMD.\n\nSadly it was just 1 more to a nice 300.",
          "created_at": "2026-01-08T17:40:13Z",
          "was_summarised": false
        },
        {
          "id": "nyervtk",
          "author": "GoranjeWasHere",
          "content": "He's literally the only one that can say as much AI as he wants.\n\nHe literally build his whole company on AI promise before everyone outside of research circle even knew what was AI and his hardware innovated so much that AI finally became a thing.\n\nOutside of Nvidia only Tesla I think and few other small companies can shout AI without sounding like a fool. Tesla was also super early in it and it got to the point where they were building their own chips just not to pay Nvidia tax for AI before AI even became investor bait.",
          "created_at": "2026-01-08T15:12:41Z",
          "was_summarised": false
        },
        {
          "id": "nyfskqo",
          "author": "SmegPoison",
          "content": "Old MacDonald had a farm, ai-ai-oh!",
          "created_at": "2026-01-08T17:55:09Z",
          "was_summarised": false
        },
        {
          "id": "nyfuof4",
          "author": "Caladan23",
          "content": "Duh surprise... it's an AI hardware company nowadays. Nvidia as a gaming company wasn't worth even 1/10th.",
          "created_at": "2026-01-08T18:04:10Z",
          "was_summarised": false
        },
        {
          "id": "nyh9hl9",
          "author": "deltamoney",
          "content": "Did you use AI to find all occurrence of AI?",
          "created_at": "2026-01-08T21:46:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhhxe1",
          "author": "JealousAmoeba",
          "content": "I admit Iâ€™m surprised that worked. Great demo for MCP.",
          "created_at": "2026-01-08T22:24:02Z",
          "was_summarised": false
        },
        {
          "id": "nyet9oy",
          "author": "Agile_Date6729",
          "content": "someone should make a remix of it -would be a banger",
          "created_at": "2026-01-08T15:19:06Z",
          "was_summarised": false
        },
        {
          "id": "nyffw6s",
          "author": "WavierLays",
          "content": "I mean this is like making a supercut of every time KFC's CEO says \"chicken\"",
          "created_at": "2026-01-08T16:59:32Z",
          "was_summarised": false
        },
        {
          "id": "nyezpx4",
          "author": "XiRw",
          "content": "Willing to bet anything the future of gaming (based on prices going up and shortages) will be server based subscriptions like everything else out there. Youâ€™ll own nothing and be happy.",
          "created_at": "2026-01-08T15:48:33Z",
          "was_summarised": false
        },
        {
          "id": "nyfa87l",
          "author": "dasjati",
          "content": "Someone should put a nice beat under this!",
          "created_at": "2026-01-08T16:35:15Z",
          "was_summarised": false
        },
        {
          "id": "nyfu4de",
          "author": "nonaveris",
          "content": "ai ai ai Cthulhu fthangh",
          "created_at": "2026-01-08T18:01:45Z",
          "was_summarised": false
        },
        {
          "id": "nyfx33c",
          "author": "Amazing_Athlete_2265",
          "content": "AI? AI!",
          "created_at": "2026-01-08T18:14:33Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://v.redd.it/hein55gpx4cg1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/OpenAgentPlatform/Dive",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/kevinwatt/yt-dlp-mcp",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/kevinwatt/ffmpeg-mcp-lite",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=0NBILspM4c4",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=u\\_7OtyYAX74",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=u_7OtyYAX74",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.918681432Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://v.redd.it/hein55gpx4cg1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918318214Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/OpenAgentPlatform/Dive: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918395559Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/kevinwatt/yt-dlp-mcp: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918437352Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/kevinwatt/ffmpeg-mcp-lite: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918455669Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.youtube.com/watch?v=0NBILspM4c4: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918552974Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.youtube.com/watch?v=u\\_7OtyYAX74: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918571702Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.youtube.com/watch?v=u_7OtyYAX74: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918681151Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7mvuf",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/",
      "title": "Z.ai  (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange",
      "content": "",
      "author": "Old-School8916",
      "created_at": "2026-01-08T20:23:59Z",
      "comments": [
        {
          "id": "nygtope",
          "author": "ForsookComparison",
          "content": "Hopefully they're all partying it up.\n\nAnd hopefully their new shareholders don't mind it if they spend millions in compute to give us free stuff ðŸ˜¬",
          "created_at": "2026-01-08T20:37:16Z",
          "was_summarised": false
        },
        {
          "id": "nygwihp",
          "author": "_Sneaky_Bastard_",
          "content": "they also said GLM 5 is in training. hoping it would be a open weight release.",
          "created_at": "2026-01-08T20:49:51Z",
          "was_summarised": false
        },
        {
          "id": "nygvbc0",
          "author": "TheAncientPizza711",
          "content": "They issued shares at HK$116.20 each. Opened at HK$120 and is now currently HK$131.50.\n\nStock is up 13.17% on its 1st day. Not bad.",
          "created_at": "2026-01-08T20:44:33Z",
          "was_summarised": false
        },
        {
          "id": "nyh2044",
          "author": "FullOf_Bad_Ideas",
          "content": "Minimax IPOs a day later, 9th of January.\n\nLots of info about both of them can be found here:\n\n[Zhipu offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1230/2025123000017.pdf)\n\n[Minimax offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1231/2025123100025.pdf)\n\nSource: [this website](https://www.hkex.com.hk/Services/Trading/Securities/Trading-News/Newly-Listed-Securities?sc_lang=en)\n\nOne of the very interesting thing contained there is that Zhipu has slightly negative profit margin on GLM Coding Plan. They lose money on serving alone, not even counting in marketing or R\u0026amp;D costs.",
          "created_at": "2026-01-08T21:14:09Z",
          "was_summarised": false
        },
        {
          "id": "nyh1ztw",
          "author": "jacek2023",
          "content": "Hype hype and still no Air",
          "created_at": "2026-01-08T21:14:07Z",
          "was_summarised": false
        },
        {
          "id": "nygrahs",
          "author": "rookan",
          "content": "Good for them",
          "created_at": "2026-01-08T20:26:28Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://x.com/Zai_org/status/2009290783678239032",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.918862264Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://x.com/Zai_org/status/2009290783678239032: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918861723Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7uuxo",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/",
      "title": "OK I get it, now I love llama.cpp",
      "content": "I just made the switch from Ollama to llama.cpp.  Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all.  Once you realize what you truly want to run, llama.cpp is really the way to go.\n\nMy hardware ain't great, I have a single 3060 12GB GPU and three P102-100 GPUs for a total of 42GB.  My system ram is 96GB along with an Intel i7-9800x.  It blows my mind that with some tuning what difference it can make.  You really need to understand each of the commands for llama.cpp to get the most out of it especially with uneven vram like mine.  I used Chatgpt, Perplexity and suprisingly only Google AI studio could optimize my settings while teaching me along the way.\n\nCrazy how these two commands both fill up the ram but one is twice as fast as the other.  Chatgpt helped me with the first one, Google AI with the other ;).  Now I'm happy running local lol.\n\n**11t/s:**  \nsudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo CUDA\\_VISIBLE\\_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4\\_K\\_M/gpt-oss-120b-Q4\\_K\\_M-00001-of-00002.gguf --n-gpu-layers 21 --main-gpu 0 --flash-attn off --cache-type-k q8\\_0 --cache-type-v f16 --ctx-size 30000 --port 8080 --host [0.0.0.0](http://0.0.0.0) \\--mmap --numa distribute --batch-size 384 --ubatch-size 256 --jinja --threads $(nproc) --parallel 2 --tensor-split 12,10,10,10 --mlock\n\n**21t/s**  \nsudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo GGML\\_CUDA\\_ENABLE\\_UNIFIED\\_MEMORY=0 CUDA\\_VISIBLE\\_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4\\_K\\_M/gpt-oss-120b-Q4\\_K\\_M-00001-of-00002.gguf --n-gpu-layers 99 --main-gpu 0 --split-mode layer --tensor-split 5,5,6,20 -ot \"blk\\\\.(2\\[1-9\\]|\\[3-9\\]\\[0-9\\])\\\\.ffn\\_.\\*\\_exps\\\\.weight=CPU\" --ctx-size 30000 --port 8080 --host [0.0.0.0](http://0.0.0.0) \\--batch-size 512 --ubatch-size 256 --threads 8 --parallel 1 --mlock\n\nNothing here is worth copying and pasting as it is unique to my config but the moral of the story is, if you tune llama.cpp this thing will FLY!",
      "author": "vulcan4d",
      "created_at": "2026-01-09T01:39:13Z",
      "comments": [
        {
          "id": "nyit742",
          "author": "pmttyji",
          "content": "Since you have 42GB VRAM, experiment with increased batch-size(1024) \u0026amp; ubatch-size(4096) for more better t/s. And bottom command doesn't have flash attention, enable it.\n\nAnd don't use quantized version of GPT-OSS-120B model. Use MXFP4 version [https://huggingface.co/ggml-org/gpt-oss-120b-GGUF](https://huggingface.co/ggml-org/gpt-oss-120b-GGUF) instead which is best.",
          "created_at": "2026-01-09T02:26:39Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://0.0.0.0",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.919010562Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://0.0.0.0: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919010091Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7jd1a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/",
      "title": "LFM2.5 1.2B Instruct is amazing",
      "content": "This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.\n\nImportant note:  \n\"\"\"  \nWe recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.\n\n\"\"\"\n\n[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)",
      "author": "Paramecium_caudatum_",
      "created_at": "2026-01-08T18:17:04Z",
      "comments": [
        {
          "id": "nyg2ddq",
          "author": "pj-frey",
          "content": "It is the perfect small \"helper\" model for Open WebUI creating tags, chat headlines, web searches and that kind of stuff. Fast AND good. I love it. Big thank you to LiquidAI (and unsloth).",
          "created_at": "2026-01-08T18:37:05Z",
          "was_summarised": false
        },
        {
          "id": "nyg9prk",
          "author": "YearZero",
          "content": "Yeah models at this size just need to work with the data you give them and adhere to the prompt as much as possible.",
          "created_at": "2026-01-08T19:08:38Z",
          "was_summarised": false
        },
        {
          "id": "nygh6w6",
          "author": "-Akos-",
          "content": "I'm amazed, especially now that it has tool use. A few days ago it didn't yet, but now I can enable MCP in LM Studio, and have blazing fast inference. On my 8th gen i7 with a 1050Ti nvidia I am getting 41 tps!!\n\nI've asked it to create a small webpage, and then it complained that it's not good at that, so indeed the programming part is correct.",
          "created_at": "2026-01-08T19:41:30Z",
          "was_summarised": false
        },
        {
          "id": "nyggecn",
          "author": "LionStrange493",
          "content": "Yeah, that caveat is important.\n\nSmaller models can be surprisingly good, but once tools/RAG get involved, edge cases show up quickly.\n\nCurious how this holds up in real agent setups.",
          "created_at": "2026-01-08T19:38:00Z",
          "was_summarised": false
        },
        {
          "id": "nyh8p9o",
          "author": "Noob_l",
          "content": "Very promising, can it also do translation? \n~~What models are recommended for local on device translation?\nI had mixed results with small qwen results. \nThank you in advance for any helpers that can point me into the right direction. (I do not know which benchmarks would be for translation)~~\n\nEdit: no it does seem like a model that cannot follow instructions on translation well. I would still love to know which models are used by the community",
          "created_at": "2026-01-08T21:43:20Z",
          "was_summarised": false
        },
        {
          "id": "nygrptl",
          "author": "crantob",
          "content": "LFM2 has a variety of interrrresting models....",
          "created_at": "2026-01-08T20:28:22Z",
          "was_summarised": false
        },
        {
          "id": "nyh2lxw",
          "author": "ElectronSpiderwort",
          "content": "It is strong for 1.2B. I argued with it a bit about a topic I know well; it was confidently incorrect for a particular formula, and even when I corrected it, it kept spitting out the incorrect formula. It just couldn't use the corrected formula because it was sure the incorrect formula was \"the standard formula\". I wonder what happens in RAG when it retrieves a piece of information it thinks is wrong? \n\nRelatedly, I thing training data should include epistemic humility",
          "created_at": "2026-01-08T21:16:51Z",
          "was_summarised": false
        },
        {
          "id": "nyh1m7k",
          "author": "DHasselhoff77",
          "content": "I tried to replace Granite-4.0-h-micro with this for tab completion but the `/completion` endpoint in llama.cpp gave 501 errors when I loaded LFM2.5-1.2B-Q8.gguf. Perhaps it's missing FIM support?",
          "created_at": "2026-01-08T21:12:26Z",
          "was_summarised": false
        },
        {
          "id": "nyhhafa",
          "author": "countAbsurdity",
          "content": "Dunno I was never impressed with previous iterations, they always misinterpreted what I wrote them or hallucinated a lot.",
          "created_at": "2026-01-08T22:21:05Z",
          "was_summarised": false
        },
        {
          "id": "nyhl8y7",
          "author": "AyraWinla",
          "content": "Favorably impressed; it actually writes pretty well and has very good understanding for its size. Under 2b, it's by far the best I've seen at it, with no real competition (small Qwen and Granite aren't too good at it in my opinion, and Gemma 2 2b is pretty outdated by now).\n\nLarger models like Gemma 3 4b or even E2B definitely give better results, but LFM2.5 1.2b runs MUCH faster on my phone; it's the first time I try a model that feels both smart and fast local on my phone. Normally it's just one or the other. I'm not going to throw away my usual Gemma models, but LFM2.5 will definitely see some use from me and I'll certainly be experimenting more with it.\n\nPretty impressive!",
          "created_at": "2026-01-08T22:39:27Z",
          "was_summarised": false
        },
        {
          "id": "nygrtn7",
          "author": "nonerequired_",
          "content": "But context size is too low. Is there any way to increase that?",
          "created_at": "2026-01-08T20:28:50Z",
          "was_summarised": false
        },
        {
          "id": "nyiglpa",
          "author": "Thin_Yoghurt_6483",
          "content": "Give practical examples of how to use a model.",
          "created_at": "2026-01-09T01:19:35Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.91918415Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919183639Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7nqxl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/",
      "title": "llama.cpp has Out-of-bounds Write in llama-server",
      "content": "Maybe good to know for some of you that might be running llama.cpp on a regular basis.\n\n\u0026gt;llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n\\_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama\\_memory\\_seq\\_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.\n\nAlso reported [for Debian](https://security-tracker.debian.org/tracker/CVE-2026-21869).",
      "author": "radarsat1",
      "created_at": "2026-01-08T20:56:15Z",
      "comments": [
        {
          "id": "nyh0wmu",
          "author": "dinerburgeryum",
          "content": "Important note:\n\n\u0026gt;Prerequisite: start the server with context shift enabled (--context-shift).\n\nIt appears you have to be running with the --context-shift flag, at least according to [llama.cpp's security advisory](https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c).",
          "created_at": "2026-01-08T21:09:17Z",
          "was_summarised": false
        },
        {
          "id": "nyh1yxi",
          "author": "coder543",
          "content": "Wouldn't recommend exposing this kind of server directly on the internet, that's for sure.",
          "created_at": "2026-01-08T21:14:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhvt6s",
          "author": "Repulsive_Educator61",
          "content": "Doesn't llama-server logs warn about not exposing llama-server to the internet because it's still in alpha/beta or something?",
          "created_at": "2026-01-08T23:31:38Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.cve.org/CVERecord?id=CVE-2026-21869",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://security-tracker.debian.org/tracker/CVE-2026-21869",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.919429121Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.cve.org/CVERecord?id=CVE-2026-21869: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919345874Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://security-tracker.debian.org/tracker/CVE-2026-21869: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.91942869Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7dlkn",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/",
      "title": "Qwen3-VL-Reranker - a Qwen Collection",
      "content": "",
      "author": "LinkSea8324",
      "created_at": "2026-01-08T14:45:00Z",
      "comments": [
        {
          "id": "nyen4yx",
          "author": "swagonflyyyy",
          "content": "A...a...multimodal RERANKER??????",
          "created_at": "2026-01-08T14:50:02Z",
          "was_summarised": false
        },
        {
          "id": "nyenqbx",
          "author": "Hanselltc",
          "content": "multimodal rag in my home lab? yes please!",
          "created_at": "2026-01-08T14:52:54Z",
          "was_summarised": false
        },
        {
          "id": "nyfeiy6",
          "author": "unofficialmerve",
          "content": "I have just built an e2e notebook chaining these models together with Qwen3-VL for multimodal RAG if anyone's interested!  [https://colab.research.google.com/drive/1LyGQcNhrv7QnpSOKyUkHojD3Bq7MkGbU?usp=sharing](https://colab.research.google.com/drive/1LyGQcNhrv7QnpSOKyUkHojD3Bq7MkGbU?usp=sharing)Â \n\nhttps://preview.redd.it/8zv6o6h8o5cg1.png?width=2200\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2b28640bc39e55c1915cc0bad815dce6bd86579",
          "created_at": "2026-01-08T16:53:40Z",
          "was_summarised": false
        },
        {
          "id": "nyem95q",
          "author": "LinkSea8324",
          "content": "Didn't want to make a double post, they also released Qwen3-VL Embeddings : https://huggingface.co/collections/Qwen/qwen3-vl-embedding\n\nTech report : https://github.com/QwenLM/Qwen3-VL-Embedding/blob/main/assets/qwen3vlembedding_technical_report.pdf",
          "created_at": "2026-01-08T14:45:43Z",
          "was_summarised": false
        },
        {
          "id": "nyfthgd",
          "author": "planetearth80",
          "content": "Can this be used in OpenWebUI?",
          "created_at": "2026-01-08T17:59:03Z",
          "was_summarised": false
        },
        {
          "id": "nyeurap",
          "author": "exaknight21",
          "content": "Wow. Just wow.",
          "created_at": "2026-01-08T15:25:59Z",
          "was_summarised": false
        },
        {
          "id": "nyf1jct",
          "author": "coder543",
          "content": "The example they provide is funny/not confidence inspiring:\n\n    inputs = {\n        \"instruction\": \"Retrieval relevant image or text with user's query\",\n        \"query\": {\"text\": \"A woman playing with her dog on a beach at sunset.\"},\n        \"documents\": [\n            {\"text\": \"A woman shares a joyful moment with her golden retriever on a sun-drenched beach at sunset, as the dog offers its paw in a heartwarming display of companionship and trust.\"},\n            {\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"},\n            {\"text\": \"A woman shares a joyful moment with her golden retriever on a sun-drenched beach at sunset, as the dog offers its paw in a heartwarming display of companionship and trust.\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"}\n        ],\n        \"fps\": 1.0\n    }\n    \n    scores = model.process(inputs)\n    print(scores)\n    # [0.7838293313980103, 0.585621178150177, 0.6147719025611877]\n\nSo, the random snippet of text is ranked higher than the actual picture or the same snippet of text with the actual picture? Shouldn't the third one be the highest ranked, most relevant response?",
          "created_at": "2026-01-08T15:56:38Z",
          "was_summarised": false
        },
        {
          "id": "nyeqb03",
          "author": "maglat",
          "content": "To understand it right, to RAG a pdf which includes Text and images, I first need to OCR it, than embed it with Qwen3VL Embedding and at the end rank the content with Qwen3 VL Reranker?",
          "created_at": "2026-01-08T15:05:14Z",
          "was_summarised": false
        },
        {
          "id": "nyfqi1l",
          "author": "TaiMaiShu-71",
          "content": "Does the embedding model support patch embeddings like the colpali models do?",
          "created_at": "2026-01-08T17:46:12Z",
          "was_summarised": false
        },
        {
          "id": "nygib67",
          "author": "Sensitive_Sweet_1850",
          "content": "qwen is a making amazing job",
          "created_at": "2026-01-08T19:46:27Z",
          "was_summarised": false
        },
        {
          "id": "nygwotm",
          "author": "Salt-Advertising-939",
          "content": "when moe reranker so that it runs good on cpu",
          "created_at": "2026-01-08T20:50:37Z",
          "was_summarised": false
        },
        {
          "id": "nygxedf",
          "author": "Flamenverfer",
          "content": "Has anyone got this running? I tried in google colab and i am having issues\n\nThis dependency, pip install qwen-vl-utils\n\n    ModuleNotFoundError                       Traceback (most recent call last)\n    /tmp/ipython-input-2332886689.py in \u0026lt;cell line: 0\u0026gt;()\n    ----\u0026gt; 1 from scripts.qwen_vl_reranker import Qwen3VLReranker\n          2 \n          3 \n          4 # Specify the model path\n          5 model_name_or_path = \"Qwen/Qwen3-VL-Reranker-2B\"\n\n    ModuleNotFoundError: No module named 'scripts'\n\n\nIts trying to import something that doesn't exist?\n\n    from scripts.qwen_vl_reranker import Qwen3VLReranker",
          "created_at": "2026-01-08T20:53:44Z",
          "was_summarised": false
        },
        {
          "id": "nyh6gam",
          "author": "newdoria88",
          "content": "Still waiting for Qwen Next VL",
          "created_at": "2026-01-08T21:33:36Z",
          "was_summarised": false
        },
        {
          "id": "nyh9q2n",
          "author": "richardanaya",
          "content": "How do you fine tune something like this?",
          "created_at": "2026-01-08T21:47:51Z",
          "was_summarised": false
        },
        {
          "id": "nyeq2n8",
          "author": "lolwutdo",
          "content": "Could this give \"vision\" to non vision models?\n\nEdit: maybe was a dumb question, just ignore. lol",
          "created_at": "2026-01-08T15:04:08Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/collections/Qwen/qwen3-vl-reranker",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.919462287Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/collections/Qwen/qwen3-vl-reranker: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919462137Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7a62a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/",
      "title": "AI21 Labs releases Jamba2",
      "content": "https://preview.redd.it/zmo6dijns4cg1.png?width=1800\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb\n\n52B [https://huggingface.co/ai21labs/AI21-Jamba2-Mini](https://huggingface.co/ai21labs/AI21-Jamba2-Mini)\n\nJamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2).\n\n# Key Advantages\n\n* **Superior reliability-to-throughput ratio:** Maintains high performance at 100K+ token contexts\n* **Category-leading benchmarks:** Excels on IFBench, IFEval, Collie, and FACTS\n* **Statistically significant quality wins:** Outperforms comparable models on real-world enterprise tasks\n* **256K context window:** Processes technical manuals, research papers, and knowledge bases\n* **Apache 2.0 License:** Fully open source for commercial use\n* **Production-optimized:** Lean memory footprint for scalable deployments\n\nhttps://preview.redd.it/cqwicpwts4cg1.png?width=2400\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d\n\n3B [https://huggingface.co/ai21labs/AI21-Jamba2-3B](https://huggingface.co/ai21labs/AI21-Jamba2-3B)\n\nJamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devicesâ€”iPhones, Androids, Macs, and PCsâ€”while maintaining the grounding and instruction-following capabilities required for production use.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2).\n\n# \n\n# Key Advantages\n\n* **On-device deployment:** Runs efficiently on iPhones, Androids, Macs, and PCs\n* **Ultra-compact footprint:** 3B parameters enabling edge deployments with minimal resources\n* **Benchmark leadership:** Excels on IFBench, IFEval, Collie, and FACTS\n* **256K context window:** Processes long documents and knowledge bases\n* **Apache 2.0 License:** Fully open source for commercial use\n* **SSM-Transformer architecture:** Memory-efficient design for resource-constrained environments\n\nit works in llama.cpp, tested on my Windows desktop:\n\nhttps://preview.redd.it/ijzgde7bg5cg1.png?width=3802\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71\n\nfixed blog post [https://www.ai21.com/blog/introducing-jamba2/](https://www.ai21.com/blog/introducing-jamba2/)\n\nGGUFs are in progress [https://huggingface.co/mradermacher/model\\_requests/discussions/1683](https://huggingface.co/mradermacher/model_requests/discussions/1683)\n\n\n\nprevious generation of Jamba models\n\n399B [https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7)\n\n52B [https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7)\n\n3B [https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B)",
      "author": "jacek2023",
      "created_at": "2026-01-08T12:10:15Z",
      "comments": [
        {
          "id": "nye2sbf",
          "author": "ilintar",
          "content": "Previous Jamba models were terrible. They were an architectural novelty but their performance was abysmal. Curious to see if they've improved.",
          "created_at": "2026-01-08T13:01:25Z",
          "was_summarised": false
        },
        {
          "id": "nydvl7n",
          "author": "Smooth-Cow9084",
          "content": "52b named \"mini\" lol\n\n\nFor those curious, it has 12b active.Â \n\n\nThen the 3b model has no info on HF repository, for whatever reason.",
          "created_at": "2026-01-08T12:14:31Z",
          "was_summarised": false
        },
        {
          "id": "nyedmvu",
          "author": "LinkSea8324",
          "content": "Fixed blog link for the brainlets : https://ai21.com/blog/introducing-jamba2",
          "created_at": "2026-01-08T14:01:41Z",
          "was_summarised": false
        },
        {
          "id": "nyf3ail",
          "author": "FullOf_Bad_Ideas",
          "content": "It shares pre-training weights with Jamba 1.5, as per their own documentation.\n\nPre-training from scratch is becoming less and less common.\n\nI wonder where's 10T Qwen at.",
          "created_at": "2026-01-08T16:04:36Z",
          "was_summarised": false
        },
        {
          "id": "nygexex",
          "author": "YearZero",
          "content": "# Merged Benchmark Comparison Table\n\n|Benchmark Category|Jamba2 3B|Jamba2 Mini|**Qwen3 4B A3B Instruct 2507**|Qwen3 30B A3B Instruct 2507|Nemotron3 30B A3B (Non-Thinking)|Minstral3 3B Instruct 2512|Minstral3 14B Instruct 2512|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|**IFBench**|0.36|**0.38**|0.32|0.32|0.33|0.22|0.27|\n|**Collie**|0.24|0.33|**0.34**|0.29|0.30|0.20|0.26|\n|**IFEval**|0.93|**0.97**|0.90|0.91|0.88|0.67|0.73|\n|**FACTS**|0.54|**0.57**|0.54|0.54|0.48|0.44|0.49|\n|**Enterprise Reliability Score (Avg)**|0.52|**0.56**|0.53|0.51|0.50|0.38|0.44|\n\nGGUF's available here now:  \n[https://huggingface.co/bartowski/ai21labs\\_AI21-Jamba2-Mini-GGUF](https://huggingface.co/bartowski/ai21labs_AI21-Jamba2-Mini-GGUF)\n\nEdit: The 52b failed every single one of my tests (I used the Q6\\_K\\_L quant). It wasn't able to do anything right or follow instructions at all. Not sure if back-end and/or template may need adjusting, but right now the model is unusable for me.",
          "created_at": "2026-01-08T19:31:29Z",
          "was_summarised": false
        },
        {
          "id": "nyesr4v",
          "author": "Cool-Chemical-5629",
          "content": "Just a note. Jamba 1.7 alone wasn't the first generation. There were also 1.6 and 1.5.",
          "created_at": "2026-01-08T15:16:43Z",
          "was_summarised": false
        },
        {
          "id": "nyeiu0b",
          "author": "Accomplished_Ad9530",
          "content": "Apache 2.0 for the 52B, nice. Only the 3B had a permissive license in the prior gen, so itâ€™s nice to see larger models open up.",
          "created_at": "2026-01-08T14:28:29Z",
          "was_summarised": false
        },
        {
          "id": "nyho3pp",
          "author": "zoyer2",
          "content": "tested some one-shot coding tasks using ai21labs\\_AI21-Jamba2-Mini-Q4\\_K\\_M.gguf (52b) in llama.cpp vs:  \n\\- Qwen3-Next-80B-A3B-Instruct-IQ4\\_XS.gguf  \n\\- cerebras\\_GLM-4.5-Air-REAP-82B-A12B-IQ3\\_XXS.gguf  \n\\- Qwen3-Coder-30B-A3B-Instruct-UD-Q6\\_K\\_XL.gguf\n\nwasn't close to beat them, many times just started to outputting crap. I really would want a model this size to be a great coder model",
          "created_at": "2026-01-08T22:52:56Z",
          "was_summarised": false
        },
        {
          "id": "nye7ag6",
          "author": "SlowFail2433",
          "content": "Wow a 400B sub-quadratic model\n\n\nThis is by far the largest sub-quadratic model ever released as far as I know",
          "created_at": "2026-01-08T13:27:36Z",
          "was_summarised": false
        },
        {
          "id": "nydyh0r",
          "author": "abkibaarnsit",
          "content": "Blog post giving 404",
          "created_at": "2026-01-08T12:34:10Z",
          "was_summarised": false
        },
        {
          "id": "nyegfqg",
          "author": "International-Try467",
          "content": "Glad to see that AI-21 is still around. I remember them from the AI Dungeon days where they replaced GPT-3 with Jurassic instead. I wonder if their models are less slopped than OpenAI's",
          "created_at": "2026-01-08T14:16:18Z",
          "was_summarised": false
        },
        {
          "id": "nyf05ou",
          "author": "Cool-Chemical-5629",
          "content": "I guess there's no day one support for LlamaCpp. It usually leads to the models being buried under newer ones which have support on day one. What would be really cool is the REAP version 30B and support in LlamaCpp.",
          "created_at": "2026-01-08T15:50:29Z",
          "was_summarised": false
        },
        {
          "id": "nyfims9",
          "author": "FizzarolliAI",
          "content": "PSA: AI21 is an Israeli company founded by ex-IDF spies from their NSA equivalent who support the ongoing attempts at ethnic cleansing and genocide in Palestine. They are not worth supporting, and neither are their models.",
          "created_at": "2026-01-08T17:11:36Z",
          "was_summarised": false
        },
        {
          "id": "nye3dns",
          "author": "indicava",
          "content": "Blog post is 404â€™d, anyone know what kind of VRAM requirements we are looking at here for the 3B model (at native BF16)?",
          "created_at": "2026-01-08T13:05:03Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-Mini",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-3B",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.ai21.com/blog/introducing-jamba2/",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/mradermacher/model%5C_requests/discussions/1683",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/mradermacher/model_requests/discussions/1683",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.920464526Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-Mini: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919568791Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919650965Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-3B: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919762158Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919887199Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.ai21.com/blog/introducing-jamba2/: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919994885Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/mradermacher/model%5C_requests/discussions/1683: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.92010216Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/mradermacher/model_requests/discussions/1683: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920173252Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920255948Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920375478Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920464246Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q77rxh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/",
      "title": "Z-image base model is being prepared for release",
      "content": "[https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08)",
      "author": "Ravencloud007",
      "created_at": "2026-01-08T09:51:33Z",
      "comments": [
        {
          "id": "nydp3k8",
          "author": "AmazinglyObliviouse",
          "content": "Wake me up when it actually is released, I do not care for your month long teasing.",
          "created_at": "2026-01-08T11:26:07Z",
          "was_summarised": false
        },
        {
          "id": "nye3gr5",
          "author": "FastDecode1",
          "content": "Gooners waiting with bated breath, blue balls, and shivers runnin'.",
          "created_at": "2026-01-08T13:05:33Z",
          "was_summarised": false
        },
        {
          "id": "nydfyut",
          "author": "Geritas",
          "content": "True if big.\n\nI thought they were quietly abandoning the idea.",
          "created_at": "2026-01-08T10:08:18Z",
          "was_summarised": false
        },
        {
          "id": "nyilefa",
          "author": "q5sys",
          "content": "This really only means that they are releasing it on THAT platform.  It doesn't necessarily mean they're going to release open weights to us at the same time.   \n  \nI want them to release open weights, but a Cloud service prepping for it to be able to be used doesn't mean it's getting released to run on our systems. (I hope I'm wrong)",
          "created_at": "2026-01-09T01:45:07Z",
          "was_summarised": false
        },
        {
          "id": "nyilp9m",
          "author": "sammoga123",
          "content": "I thought it would only be T2I, but it seems it will also be able to edit images. I just hope it allows more than one input image, and that it's at least on par with Qwen Edit, because Flux 2 is still useless compared to Qwen.",
          "created_at": "2026-01-09T01:46:44Z",
          "was_summarised": false
        },
        {
          "id": "nyfjv5u",
          "author": "YearZero",
          "content": "What's the purpose of an image gen base model? I understand text base models without instruction tuning just complete/continue a text. What would this one do, and what would it be used for? Would it complete an incomplete image or something? So if I gave it an image with a missing section, would it fill that in? Would it handle any instructions? I know qwen-image-edit can already do this, but it's instruct tuned for that function, and it can work on sections of an image with the right scaffolding.",
          "created_at": "2026-01-08T17:17:02Z",
          "was_summarised": false
        },
        {
          "id": "nyh874x",
          "author": "azerpsen",
          "content": "Waiting for the posts crying why the model is too censored (it refused to generate furry futa inflation incest CP images)",
          "created_at": "2026-01-08T21:41:09Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/038zb25ok3cg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.9205561Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920554908Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7o8kl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/",
      "title": "GLM-4.7 on 4x RTX 3090 with ik_llama.cpp",
      "content": "With the help of Opus 4.5 I got unsloth/GLM-4.7-GGUF (Q4\\_K\\_M) running on my 4x RTX 3090 setup using ik\\_llama.cpp in Docker. I wanted to share my benchmark results and configuration, and ask if these numbers are what I should expect - or if there's room for improvement.\n\n# My Setup\n\n|Component|Specs|\n|:-|:-|\n|Motherboard|Supermicro H12SSL-i|\n|CPU|AMD EPYC 7282|\n|GPUs|4x NVIDIA RTX 3090 (96GB VRAM total, all at PCIe x16)|\n|RAM|256GB DDR4-2133|\n|Storage|2 TB NVMe SSD|\n\n# Benchmark Results\n\n|Config|Context|n-cpu-moe|Batch|VRAM/GPU|Prompt|**Generation**|\n|:-|:-|:-|:-|:-|:-|:-|\n|Initial (mmap)|16K|all|512|\\~5 GB|2.8 t/s|3.1 t/s|\n|split-mode layer|16K|partial|4096|\\~17 GB|2.8 t/s|âš ï¸ 0.29 t/s|\n|\\+ no-mmap|16K|all|4096|\\~10 GB|8.5 t/s|3.45 t/s|\n|\\+ n-cpu-moe 72|16K|72|4096|\\~17 GB|9.9 t/s|4.12 t/s|\n|**Best 8K**|**8K**|**65**|**4096**|**\\~21 GB**|**12.0 t/s**|**4.48 t/s** â­|\n|**Best 16K**|**16K**|**68**|**2048**|**\\~19 GB**|**10.5 t/s**|**4.28 t/s** â­|\n\n# Benchmark Methodology\n\nAll tests were performed using the same simple request via curl:\n\n    curl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"model\": \"GLM-4.7-GUFF\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Write a short Haiku.\"}],\n        \"temperature\": 0.7,\n        \"max_tokens\": 100\n      }'\n\nThe response includes timing information:\n\n    {\n      \"timings\": {\n        \"prompt_n\": 17,\n        \"prompt_ms\": 1419.902,\n        \"prompt_per_second\": 11.97,\n        \"predicted_n\": 100,\n        \"predicted_ms\": 22301.81,\n        \"predicted_per_second\": 4.48\n      }\n    }\n\n* **prompt\\_per\\_second**: How fast the input tokens are processed\n* **predicted\\_per\\_second**: How fast new tokens are generated (this is what matters most for chat)\n\nEach configuration was tested with a fresh server start (cold start) and the first request after warmup. Note that GLM-4.7 has a \"thinking/reasoning\" mode enabled by default, so the 100 generated tokens include internal reasoning tokens.\n\n# My Current Configuration\n\n# Best for 8K Context (fastest):\n\n    llama-server \\\n        --model \"/models/GLM-4-Q4_K_M-00001-of-00005.gguf\" \\\n        --host 0.0.0.0 --port 8080 \\\n        --ctx-size 8192 \\\n        --n-gpu-layers 999 \\\n        --split-mode graph \\\n        --flash-attn on \\\n        --no-mmap \\\n        -b 4096 -ub 4096 \\\n        --cache-type-k q4_0 --cache-type-v q4_0 \\\n        --k-cache-hadamard \\\n        --jinja \\\n        --n-cpu-moe 65\n\n# Best for 16K Context:\n\n    llama-server \\\n        --model \"/models/GLM-4-Q4_K_M-00001-of-00005.gguf\" \\\n        --host 0.0.0.0 --port 8080 \\\n        --ctx-size 16384 \\\n        --n-gpu-layers 999 \\\n        --split-mode graph \\\n        --flash-attn on \\\n        --no-mmap \\\n        -b 2048 -ub 2048 \\\n        --cache-type-k q4_0 --cache-type-v q4_0 \\\n        --k-cache-hadamard \\\n        --jinja \\\n        --n-cpu-moe 68\n\n# Key Findings:\n\n1. `--no-mmap` **is crucial** \\- Loading the model into RAM instead of memory-mapping from SSD **tripled** my prompt processing speed (2.8 â†’ 12 t/s)\n2. `--split-mode graph` **not** `layer` \\- Layer mode gave me only 0.29 t/s because GPUs process sequentially. Graph mode enables true tensor parallelism.\n3. `--n-cpu-moe X` \\- This flag controls how many MoE layers stay on CPU.\n4. **Batch size matters** \\- Smaller batches (2048) allowed more MoE layers on GPU for 16K context.\n\n# Docker Setup\n\nI'm running this in Docker. Here's my `docker-compose.yml`:\n\n    services:\n      glm-4:\n        build:\n          context: .\n          dockerfile: Dockerfile\n        container_name: glm-4-server\n        deploy:\n          resources:\n            reservations:\n              devices:\n                - driver: nvidia\n                  count: all\n                  capabilities: [gpu]\n        volumes:\n          - /path/to/models:/models:ro\n        ports:\n          - \"8080:8080\"\n        environment:\n          - CTX_MODE=${CTX_MODE:-8k}  # Switch between 8k/16k\n          - NO_MMAP=true\n          - KV_CACHE_K=q4_0\n          - KV_CACHE_V=q4_0\n          - K_CACHE_HADAMARD=true\n        shm_size: '32gb'\n        ipc: host\n        restart: unless-stopped\n\nAnd my `Dockerfile` builds ik\\_llama.cpp with CUDA support:\n\n    FROM nvidia/cuda:12.4.0-devel-ubuntu22.04\n    \n    # Install dependencies\n    RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\\n        git cmake build-essential curl \\\n        \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*\n    \n    # Clone and build ik_llama.cpp\n    WORKDIR /opt\n    RUN git clone https://github.com/ikawrakow/ik_llama.cpp.git\n    WORKDIR /opt/ik_llama.cpp\n    \n    RUN cmake -B build \\\n        -DGGML_CUDA=ON \\\n        -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n        -DCMAKE_CUDA_ARCHITECTURES=\"86\" \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        \u0026amp;\u0026amp; cmake --build build --config Release -j$(nproc) \\\n        \u0026amp;\u0026amp; cmake --install build\n    \n    EXPOSE 8080\n    COPY entrypoint.sh /entrypoint.sh\n    RUN chmod +x /entrypoint.sh\n    ENTRYPOINT [\"/entrypoint.sh\"]\n\n# Questions\n\n1. **Are these speeds (4.48 t/s generation) normal for this setup?** I've seen some posts mentioning 5-6 t/s with 2x RTX 5090, but they had 64GB VRAM total vs my 96GB.\n2. **Any other flags I should try?** I tested `--run-time-repack` but it didn't help much.\n3. **Is there a better MoE offloading strategy?** I'm using `--n-cpu-moe` but I know there's also the `-ot` regex approach.\n4. **Would a different quantization help?** Currently using Q4\\_K\\_M. Would IQ4\\_XS or Q5\\_K\\_M be faster/better?\n5. **Low GPU power usage during inference?** My cards are power-limited to 275W each, but during inference they only draw \\~100-120W. Could this be a bottleneck limiting my token/s?\n\nI would love to hear your thoughts and any optimization tips.",
      "author": "iamn0",
      "created_at": "2026-01-08T21:14:19Z",
      "comments": [
        {
          "id": "nyhbw4l",
          "author": "massive_rock33",
          "content": "The token gen seems too slow, im surprised it's this slow with 4 gpu",
          "created_at": "2026-01-08T21:57:11Z",
          "was_summarised": false
        },
        {
          "id": "nyhsog9",
          "author": "Lissanro",
          "content": "My rig is somewhat similar to yours, also EPYC and DDR4-based, with four 3090 cards. I have EPYC 7763 with 1TB DDR4 3200MHz RAM + 4x3090 GPUs, and with IQ4 quant of GLM-4.7 I get about 6 tokens/s generation 200 tokens/s prompt processing., with 19 full layers on GPUs along with common expert tensors and 200K context cache at Q8. Here is my command for reference:\n\n    numactl --cpunodebind=0 --interleave=all ~/pkgs/ik_llama.cpp/build/bin/llama-server \\\n    --model /mnt/neuro/models/GLM-4.7-IQ4_K/GLM-4.7-IQ4_K-00001-of-00006.gguf \\\n    --ctx-size 202752 --n-gpu-layers 62 --tensor-split 25,23,25,27 -ctk q8_0 -ctv q8_0 -b 4096 -ub 4096 -fa on \\\n    -ot \"blk\\.(3|4|5|6)\\.ffn_.*=CUDA0\" \\\n    -ot \"blk\\.(8|9|10|11|12)\\.ffn_.*=CUDA1\" \\\n    -ot \"blk\\.(13|14|15|16|17)\\.ffn_.*=CUDA2\" \\\n    -ot \"blk\\.(18|19|20|21|22)\\.ffn_.*=CUDA3\" \\\n    -ot exps=CPU \\\n    --threads 64 --host 0.0.0.0 --port 5000 \\\n    --jinja \\\n    --slot-save-path /var/cache/ik_llama.cpp/glm-4.7\n\nBy the way, with  `--split-mode graph` I get crash:\n\n    /home/lissanro/pkgs/ik_llama.cpp/src/llama.cpp:566: GGML_ASSERT(kl \u0026amp;\u0026amp; (!kv_self.v_l[il] || vl)) failed\n\nAnd with `--k-cache-hadamard` it generated gibberish. I tried with the latest ik\\_llama.cpp from git. But it sounds like you managed to get these options working?\n\nAs of your bottleneck, for token generation I think it is both your CPU and RAM. During token generation, all 64 cores on my EPYC 7763 get saturated a little bit sooner than memory bandwidth of 8-channel 3200MHz RAM, so any slower CPU would reduce the performance. This also means your rig is well balanced, faster memory would probably not make much difference, so your current memory is well suited for your CPU. 4 tokens/s is very good given your CPU and RAM speeds, probably GPUs help a lot.\n\nBut your prompt processing speed is unusually slow, you should be getting about the same speed as me (around 200 tokens/s) since prompt processing is done on GPUs and my CPU is almost idle while it happens. I see you have `--n-cpu-moe 68` \\- I did not try it myself, but very likely it is not equivalent to the proper `-ot` options. Calibrating `--tensor-split` and `-ot` lines to decide how many layers to put and where can be time consuming but I think it is necessary if you want to get the best performance. Notice how I have `exps=CPU` at the end - order is important. And you can manually write layers number from 3 and up in each `-ot` line, to define which layers go to which CUDA device - in my case I put 4 on my first GPU (CUDA0) and 5 layers on the rest, but depending on your VRAM usage by the system and other factors, you may need to change my example.",
          "created_at": "2026-01-08T23:15:41Z",
          "was_summarised": false
        },
        {
          "id": "nyhwmta",
          "author": "tenebreoscure",
          "content": "You can get definitely better speeds. I'm currently running IQ4\\_XS on plain llama.cpp, using something like this:\n\n    CUDA_DEVICE_ORDER='PCI_BUS_ID' LLAMA_SET_ROWS=1 ./llama.cpp/build/bin/llama-server \\ \n    --model zai-org_GLM-4.7-IQ4_XS-00001-of-00005.gguf \\ \n    --alias bartowski/GLM-4.7-IQ4_XS \\ \n    --ctx-size 65536 \\ \n    --flash-attn on \\ \n    -ngl 99 \\ \n    -ot \"blk\\.[0-9]\\.ffn.*=CUDA0,blk\\.1[0-2]\\.ffn.*=CUDA0,blk\\.1[3-9]\\.ffn.*=CUDA1,blk\\.20\\.ffn.*=CUDA1,blk\\.2[1-8]\\.ffn.*=CUDA2,blk\\.29\\.ffn.*=CUDA3,blk\\.3[0-6]\\.ffn.*=CUDA3,blk\\.3[7-9]\\.ffn.*=CUDA4,blk\\.4[0-1]\\.ffn.*=CUDA4,blk\\\\..*_exps\\\\.=CPU\" \\ \n    --no-mmap \\ \n    --threads 11 \\ \n    --parallel 1 \\ \n    --host 127.0.0.1 \\ \n    --port 8080\n\nOn an AM5 platform with 70 GB/S Ram bandwdith, 192GB RAM and a combination of 120GB of VRAM.\n\nI get ~100pp/8TG @8k and ~100pp/7.5TG @16k, and run at 65K. The trick is to load with the override tensor flag as many layers as you can on the video cards without compromising too much with context. \n\nI'd suggest against quantizing the cache to even 8 bit as it brings spelling errors and slight hallucinations, it's tolerable for general conversation, not for coding or anything income related. \n\nAlso for ik_llama.cpp use Ubergarm's quants https://huggingface.co/ubergarm/GLM-4.7-GGUF or Thireus's, they are optimized for ik_llama.cpp and should give faster results. Ubergarm has an excellent readme with every trick to squeeze out performances.\n\nHow many ram sticks do you have? That epyc has an 8ch memory controller, if you have all the dimm slots occupied the ram bandwidth should be around double mine. I'd suggest measuring it with Intel MLC or a similar tool. Nevermind, read the comment about the processor having 2 ccds. So yeah, It's probably bottlenecking. I'd still try to squeeze out some more numbers.\n\nPersonally --n-cpu-moe never worked for me and gave sluggish speed, the -ot approach worked better, so I'd try that one. I'd also choose the smallest Q4 quant over the K_M one. Power limiting should not be an issue, you can monitor it with nvtop anyway to check if the gpus cap it.",
          "created_at": "2026-01-08T23:35:56Z",
          "was_summarised": false
        },
        {
          "id": "nyhdw1m",
          "author": "FullstackSensei",
          "content": "You can sell those four 3090s, and buy an 8 CCD Epyc (256MB L3 cache). You'll save a lot of power, and end up with better performance.\n\nJokes aside, two things are killing your performance: that CPU and your RAM speed (you don't say how many sticks you have, so there's also a chance your memory configuration is bad if you have 4x64GB sticks).\n\nEpyc Rome and Milan have ~26GB/s bandwidth between each CCD and the IO die. To get maximum memory bandwidth, you need a CPU with 8 CCDs. Given the compute, 32 cores might not cut it, so you need either 48 or 64 cores to crunch those numbers. The IO die has 8 memory channels, each running at a maximum speed of 3200MT/s. You're running at 2133, which would be bad, if it wasn't for your CPU having 2 CCDs only cutting it's effective bandwidth by four.\n\nKnow thy hardware before buying parts and putting a system together. It's a bit of a moot point now that you have it, and with current RAM prices you'll probably need a kidney to upgrade to 3200 memory. Though you should try to overclock it to 2400 or even 2666 if 2400 works.1",
          "created_at": "2026-01-08T22:05:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhgj3c",
          "author": "MikeRoz",
          "content": "Can you do any better on the RAM speed? I can beat this using 2 GPUs and all the experts in system memory (also using ik_llama.cpp, IQ5_K quant). Getting ~10 tps generation, ~83 tps prompt processing, 4k or so into context out of 64k. Main difference between us is I'm running DDR5-6000 and you're running DDR4-2133. Another is that I'm not bothering with anything but the first layers on GPU - this allows me to allocate 64k cache yet use only 2 GPUs.",
          "created_at": "2026-01-08T22:17:37Z",
          "was_summarised": false
        },
        {
          "id": "nyhi65g",
          "author": "Egoz3ntrum",
          "content": "That dockerfile will be useful, thanks.",
          "created_at": "2026-01-08T22:25:09Z",
          "was_summarised": false
        },
        {
          "id": "nyhm5j5",
          "author": "FullOf_Bad_Ideas",
          "content": "try 2.10bpw exl3 quant\n\nhttps://huggingface.co/mratsim/GLM-4.7-EXL3/tree/2.10bpw-tuned\n\nIt probably won't be smart, but it will be stupid much faster since it will be all be in VRAM\n\nwith llama cpp and IQ3_XSS GLM 4.6/4.7 I had about 3.5 t/s generation speed. 3200 DDR4 128GB and 2x rtx 3090 ti 24gb.",
          "created_at": "2026-01-08T22:43:41Z",
          "was_summarised": false
        },
        {
          "id": "nyhmjkn",
          "author": "Leflakk",
          "content": "CPU \u0026amp; RAM bottlebeck and not enough VRAM. Why donâ€™t you rather consider minimax m2.1 with lower quantz?",
          "created_at": "2026-01-08T22:45:32Z",
          "was_summarised": false
        },
        {
          "id": "nyhqi8u",
          "author": "segmond",
          "content": "I'm getting 6tk/sec on quad 3090 with regular llama.cpp",
          "created_at": "2026-01-08T23:04:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhrh0k",
          "author": "ScoreUnique",
          "content": "Quick tip- explore -ot flag, that thing shows big numbers on ik llama CPP.",
          "created_at": "2026-01-08T23:09:39Z",
          "was_summarised": false
        },
        {
          "id": "nyhwbzl",
          "author": "chub0ka",
          "content": "Havent yet tried graph but 360gb quant on kimi runs 10t/s generation and 2 3090 is enough. Using 8 gpus doesnt help",
          "created_at": "2026-01-08T23:34:21Z",
          "was_summarised": false
        },
        {
          "id": "nyigyvn",
          "author": "ortegaalfredo",
          "content": "Prompt processing is super slow. As PP is mostly CPU-bound, I believe you might be hitting some kind of power-limitation, or power saving.",
          "created_at": "2026-01-09T01:21:30Z",
          "was_summarised": false
        },
        {
          "id": "nyih1e6",
          "author": "ortegaalfredo",
          "content": "Prompt processing is super slow. As PP is mostly CPU-bound, I believe you might be hitting some kind of power-limitation, or power saving is activating.",
          "created_at": "2026-01-09T01:21:53Z",
          "was_summarised": false
        },
        {
          "id": "nyip2mi",
          "author": "southern_gio",
          "content": "Dude thanks for shearing",
          "created_at": "2026-01-09T02:04:47Z",
          "was_summarised": false
        },
        {
          "id": "nyhdlk6",
          "author": "cantgetthistowork",
          "content": "Very very slow. My Q4 K2-Thinking on 2x3090 runs faster than this",
          "created_at": "2026-01-08T22:04:31Z",
          "was_summarised": false
        },
        {
          "id": "nyhoo4p",
          "author": "leonbollerup",
          "content": "Why not run gpt-oss-120b .. should rub nicely on that setup ?",
          "created_at": "2026-01-08T22:55:43Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://localhost:8080/v1/chat/completions",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/ikawrakow/ik_llama.cpp.git",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.920871222Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://localhost:8080/v1/chat/completions: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.92074068Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/ikawrakow/ik_llama.cpp.git: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920870651Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q71sbe",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "title": "Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)",
      "content": "Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything\n\nhttps://preview.redd.it/shr3e0liv1cg1.png?width=2560\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079\n\nGithub: [https://github.com/MVPandey/DTS](https://github.com/MVPandey/DTS)\n\nMotivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:\n\n(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)\n\n1. Generates N diverse strategies\n2. Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)\n3. Rolls out full multi-turn conversations down each branch\n4. Has 3 independent LLM judges score each trajectory, takes the median\n5. Prunes branches below threshold, backpropagates scores\n6. Repeats for however many rounds you configure\n\nhttps://preview.redd.it/zkii0idvv1cg1.png?width=762\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4\n\nThree judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.\n\nMain additions over CAE:\n\n* user intent forking (strategies get stress-tested against different personas)\n* deep research integration via GPT-Researcher for domain context\n* proper visualization with conversation playback\n\nOnly supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls\n\nIt's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.\n\n\\--\n\nBTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.",
      "author": "ManavTheWorld",
      "created_at": "2026-01-08T04:08:39Z",
      "comments": [
        {
          "id": "nycvkhb",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T07:05:10Z",
          "was_summarised": false
        },
        {
          "id": "nyc6x5o",
          "author": "TheGrossVolcano",
          "content": "This is actually pretty clever - using beam search instead of pure MCTS makes way more sense for dialogue since you don't want the exploration to go completely off the rails\n\n  \nThe user intent forking is a nice touch, most people forget that the same strategy can totally bomb depending on who you're talking to",
          "created_at": "2026-01-08T04:11:18Z",
          "was_summarised": false
        },
        {
          "id": "nyc8h0p",
          "author": "charlesrwest0",
          "content": "Weird thought... Could you use this to optimize an rp response?",
          "created_at": "2026-01-08T04:20:41Z",
          "was_summarised": false
        },
        {
          "id": "nycytz1",
          "author": "harlekinrains",
          "content": "firecrawls pricing guides you into a monthly subscription and is prohibitively costly for what it provides (140 USD/year). without knowing the intricacies, if you also could implement alternatives, that would be swell.\n\nThis github already collected a bunch of search providers, maybe it helps: https://github.com/rikkahub/rikkahub/tree/ffa2a0c4796d835454c7a9a0469f897ff1ffdb63/search/src/main/java/me/rerere/search",
          "created_at": "2026-01-08T07:33:08Z",
          "was_summarised": false
        },
        {
          "id": "nycqqjr",
          "author": "ItilityMSP",
          "content": "Nice work, lots of potential here to improve many ai chat agents. â­",
          "created_at": "2026-01-08T06:26:17Z",
          "was_summarised": false
        },
        {
          "id": "nycz02m",
          "author": "Nyghtbynger",
          "content": "That's cool, I was looking to optimize my prompts I think it has a good future in prompt fitting (for a specific dataset or customer)",
          "created_at": "2026-01-08T07:34:38Z",
          "was_summarised": false
        },
        {
          "id": "nydbiy1",
          "author": "Much-Researcher6135",
          "content": "This is new to me, pretty interesting. Anyone here used tech like this for interview prep before? Or is it all just chatbot strategy discovery for you guys?",
          "created_at": "2026-01-08T09:27:49Z",
          "was_summarised": false
        },
        {
          "id": "nydgto6",
          "author": "IrisColt",
          "content": "Thanks!",
          "created_at": "2026-01-08T10:15:52Z",
          "was_summarised": false
        },
        {
          "id": "nyizliz",
          "author": "hiepxanh",
          "content": "thank you",
          "created_at": "2026-01-09T03:00:43Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/MVPandey/DTS",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.920961523Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/MVPandey/DTS: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920961142Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7c0pd",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/",
      "title": "AI21 releases Jamba2 3B and Jamba2 Mini, built for grounding and instruction following",
      "content": "*Disclaimer: I work for AI21, creator of the Jamba model family.*\n\nWeâ€™re excited to announce the public release of Jamba2 3B and Jamba2 Mini.\n\nThe Jamba2 family aims to give enterprises cost-effective models that will integrate well into production agent stacks.\n\nThese models are designed for reliable instruction following and grounded outputs, working well over long documents and avoiding drifting once context becomes large.\n\nThey perform best for precise question answering over internal policies, technical manuals and knowledge bases, without the overhead of thinking tokens which can become costly.\n\n**Key performance data**\n\nJamba2 3B and Jamba2 Mini outperform peers due to their hybrid SSM-Transformer architecture and KV cache innovations:\n\n* Outpaces Ministral3 14B and Qwen3 30B A3B across FACTS, IFBench and IFEval.Â \n* Beats Ministral3 3B and Qwen3 4B on IFEval and IFBench, tying with Qwen3 4B as category leader on FACTS.\n* At context lengths of 100K, Jamba2 Mini delivers 2.7X greater throughput than Ministral3 14B and 1.4X greater throughout than Qwen3 30B A3B.\n* At context lengths of 100K, Jamba2 3B delivers 1.7X greater throughout than Ministral3 3B and 2.7X greater throughput than Qwen 3 14B.\n\nItâ€™s available today in AI21â€™s SaaS and from Hugging Face.\n\nHappy to answer questions or dig into benchmarks if people want more detail.\n\nBlog: [http://www.ai21.com/blog/introducing-jamba2](http://www.ai21.com/blog/introducing-jamba2)  \nHugging Face: [https://huggingface.co/collections/ai21labs/jamba2](https://huggingface.co/collections/ai21labs/jamba2)",
      "author": "zennaxxarion",
      "created_at": "2026-01-08T13:38:34Z",
      "comments": [
        {
          "id": "nye9nvl",
          "author": "StillResult4344",
          "content": "Nice to see more hybrid architectures hitting the scene, those throughput numbers at 100K context are pretty solid. How's the memory usage compared to pure transformer models at those longer contexts?",
          "created_at": "2026-01-08T13:40:39Z",
          "was_summarised": false
        },
        {
          "id": "nyfxuey",
          "author": "gofiend",
          "content": "Whatâ€™s the llama.cpp (infer) and unsloth (training) situation?",
          "created_at": "2026-01-08T18:17:46Z",
          "was_summarised": false
        },
        {
          "id": "nyf8u9c",
          "author": "lacerating_aura",
          "content": "Hi, thank you for the release. How well does it compare to other hybrid architecture models, like qwen3 next?",
          "created_at": "2026-01-08T16:29:13Z",
          "was_summarised": false
        },
        {
          "id": "nygqyrp",
          "author": "crantob",
          "content": "This appears to me to be important iterative engineering progress in linear attention implementations.\n\nSkÃ¥l",
          "created_at": "2026-01-08T20:25:01Z",
          "was_summarised": false
        },
        {
          "id": "nyhn4nu",
          "author": "danigoncalves",
          "content": "What would be the max context before degrading in quality?",
          "created_at": "2026-01-08T22:48:18Z",
          "was_summarised": false
        },
        {
          "id": "nyhzj0z",
          "author": "casual_butte_play",
          "content": "Cool models! Heads up, thereâ€™s a typo in your Tiny Models chart, where it refers to Qwen3 4B A3B. Probably just a typo in chart generation while changing 30B A3B -\u0026gt; 4B",
          "created_at": "2026-01-08T23:50:57Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://www.ai21.com/blog/introducing-jamba2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/collections/ai21labs/jamba2",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921023958Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://www.ai21.com/blog/introducing-jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921005501Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/collections/ai21labs/jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921023757Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7hikw",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/",
      "title": "Qwen3-4B-Instruct-2507 multilingual FT with upscaled Polish language",
      "content": "Hi, \n\nJust wanted to share a preview of my latest finetuned model based on Qwen3-4B-Instruct-2507.\n\nLanguages ratio:\n\nPolish - high  \nEnglish - medium  \nChinese - medium  \nCzech - medium/low  \nUkrainian - medium/low  \nRussian - medium/low\n\n\n\n[https://huggingface.co/piotr-ai/polanka\\_4b\\_v0.3\\_preview\\_260108\\_qwen3\\_gguf](https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf)\n\n",
      "author": "Significant_Focus134",
      "created_at": "2026-01-08T17:12:08Z",
      "comments": [
        {
          "id": "nyfleqe",
          "author": "mtomas7",
          "content": "How big was your dataset? Also, it would be great if you could share your \"recipe\" so it could be used for other languages too. Thank you!",
          "created_at": "2026-01-08T17:23:50Z",
          "was_summarised": false
        },
        {
          "id": "nyfnz7y",
          "author": "x86rip",
          "content": "nice work ! what are datasets that you used to ft this ?",
          "created_at": "2026-01-08T17:35:09Z",
          "was_summarised": false
        },
        {
          "id": "nyfott5",
          "author": "FullOf_Bad_Ideas",
          "content": "How is this model trained?\n\nhttps://huggingface.co/piotr-ai/polanka_3.6b_exp_WIP_251227\n\nI trained something similar, but 8 out of 128 experts active, instead of 2 out of 32 experts. Trained from scratch on Polish datasets, FineWeb2, HPLT3, FinePDFs. APT4 tokenizer.\n\nhttps://huggingface.co/adamo1139/poziomka-lora-instruct-alpha-2\n\nWe converged onto very similar things here!",
          "created_at": "2026-01-08T17:38:53Z",
          "was_summarised": false
        },
        {
          "id": "nygph0i",
          "author": "crantob",
          "content": "These small qwens have PC dogma so heavily blasted through them that they turn into quivering middle-school guidance counselors around any real world chat.",
          "created_at": "2026-01-08T20:18:17Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/piotr-ai/polanka%5C_4b%5C_v0.3%5C_preview%5C_260108%5C_qwen3%5C_gguf",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921282386Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/piotr-ai/polanka%5C_4b%5C_v0.3%5C_preview%5C_260108%5C_qwen3%5C_gguf: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921145752Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921281855Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7hywi",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/",
      "title": "How do you manage quality when AI agents write code faster than humans can review it?",
      "content": "We are shifting to an agentic workflow. My thesis is \"Code at Inference Speed.\" My CTO's counter-argument is that **reviewing code is harder than writing it**.\n\nHis concern is simple: If AI increases code volume by 10x, human review becomes a fatal bottleneck. He predicts technical debt will explode because humans canâ€™t mentally verify that much logic that quickly.\n\n  \nHow do handle this? I know one option is to slow down releases but is there any other approaches people are taking.",
      "author": "lostsoul8282",
      "created_at": "2026-01-08T17:28:30Z",
      "comments": [
        {
          "id": "nyfrj2m",
          "author": "Thick-Protection-458",
          "content": "\\\u0026gt; when AI agents write code faster than humans can review it?\n\nEasily. Just the bottleneck moves from me producing code (which is already lesser part of my job comparing to thinking about high-level structures. So it is kinda not a bottleneck anyway, just a nice spot to optimize) to me reviewing code.\n\nBefore that it was problematic too. Just we did not achieve the stage when this become bottleneck (means earlier bottlenecks is partially solved).\n\nAnd no, no way that electronic fucker (or human. My own, lol - better to at least review your own code later, when your stream of thoughts changed enough so you have a chance to see things from different angle) output get past me before I am sure I understand what this thing is doing.\n\n\\\u0026gt; **reviewing code is harder than writing it**\n\nHe is exactly right.\n\nIf you don't do it in digestable chunks.\n\nAnd for chunks to be digestable you have to know what to expect. So you have to take part in planning structural stuff. Either all by yourself or combo of you + LLM agent (it may give boost here too, by reviewing your ideas for missing corner cases or even noticing utterly wrong understanding of some stuff; also by suggesting tweaks). So this way you kinda know what to expect here.\n\nSo if you want to vibecode the whole thing and only review in the end - no, probably not the way unless coding agents get not only good quality, but actually superhuman quality. And even than - they would not be perfect decision mechanisms, so stacking them and human devs would still make sense. Because as soon as we make and notice different kind of errors - stacking different weak mechanisms would still work.\n\nIf you think about it like about pair programming, on the other hand - just a \"pair\" being not a human, but machine - it may start making sense,",
          "created_at": "2026-01-08T17:50:39Z",
          "was_summarised": false
        },
        {
          "id": "nyfv2lp",
          "author": "Abject-Kitchen3198",
          "content": "Start by accepting CTOs argument. \n\nSlow down. Accept that LLM induced productivity factor will be between 0.5 and 2x on a case-by-case basis.\n\nIterate with AI until you get a solution with minimal amount of code with acceptable quality that you feel comfortable reviewing. \n\nDo this for a quarter or two until you realize that either LLMs are not helpful for your case or that they provide some improvement on average and you can keep using them.",
          "created_at": "2026-01-08T18:05:52Z",
          "was_summarised": false
        },
        {
          "id": "nyfr4kz",
          "author": "AndThenFlashlights",
          "content": "I work in a field that has some pretty severe safety and liability consequences if something goes wrong. Qualified and competent humans eye need to review and comprehend every line of code that goes into the codebase, full stop. Reliability is more important than adding features. And weâ€™re usually working with devices or APIs that arenâ€™t documented publicly, so LLMs currently arenâ€™t super helpful at writing things unattended - theyâ€™re more useful in my workflow for writing API / class boilerplate or small contained methods, not vibe-coding whole things independently. \n\nTreat the AI like a flock of interns you need to watch and manage. You ever had too many interns to keep track of, and experienced that unfocused chaos? This is why I donâ€™t take on more than 1 intern at a time anymore.",
          "created_at": "2026-01-08T17:48:55Z",
          "was_summarised": false
        },
        {
          "id": "nyg6air",
          "author": "FullstackSensei",
          "content": "I find it funny how many here think LLMs will be able to review code and fix slop. Sounds like a chicken and egg problem to me. If you can train a model to detect and fix slop, then why wasn't the coding model trained to not generate said slop in the first place?\n\nIf we were anywhere near what some here seem to be predicting, why would anthropic spend a cool billion buying a Javascript runtime (Bun) rather than tuning a version of Claude to write something similar themselves?",
          "created_at": "2026-01-08T18:53:48Z",
          "was_summarised": false
        },
        {
          "id": "nyfrv55",
          "author": "Capaj",
          "content": "tests. Lots of them",
          "created_at": "2026-01-08T17:52:06Z",
          "was_summarised": false
        },
        {
          "id": "nyft98g",
          "author": "bigh-aus",
          "content": "It's a valid concern. But it's the same concern that larger enterprises are dealing with their current code stacks.  You need to increase the ecosystem around the code. Much like human written code that you outsourced to XYZ small company from ABC country.  \n\n\\#1: use a safe language: (rust, zig, safe c++, java, go etc). The compiler / runtime errors will help improve quality and catch bugs, vs interpreted languages where it's only runtime..  (It's one of the reasons I'm learning rust)\n\n\\#2: Full test suite imo is the main thing - unit tests, external API tests, integration tests, defensive tests, behavior tests, chaos tests, security tests, DR tests. Start simple, and scale up.  EG: extract any s3 buckets and check that they have encryption + auth turned on is a  classic example for low hanging fruit.  TLDR: how do you validate that the code is right? validate it by testing.\n\n\\#3: Have the code checked in in small steps, so if there is a problem rollback is easy.  Also look into having agents do code review.\n\n\\#4 CI/CD run as much static and dynamic analysis as you can on the code as part of the build / deployment pipeline.  Build agents to analyze the code, improvements, code smells.  Manage by exception.\n\n\\#5: Full red / blue team to test the security operation of the system, and build up automated security tests. \n\n\\#6: If required - compliance testing - is it HIPPA / PCI / Fedramp etc. How can you have continual testing to prove that the systems adhere to the standards. \n\n\\#7: run tests ON your staff - eg if there's a bug, how long does it take to find it, etc etc.  Break a non prod environment and have your staff try to fix it.\n\nAlso look at ways you can improve / reduce / optimize / etc the code using profiling and manual analysis.\n\nAlso do the dev, stage, prod environments at a minimum (more if needed).  Never have agents code in prod. ever.",
          "created_at": "2026-01-08T17:58:05Z",
          "was_summarised": false
        },
        {
          "id": "nyg0c0e",
          "author": "Zulfiqaar",
          "content": "\u0026gt;Â My thesis is \"Code at Inference Speed.\"\n\nJust cause someone can type at 100WPM doesn't mean they should\n\n\nThe alternatives all centre around increasing code quality or increasing review capacity",
          "created_at": "2026-01-08T18:28:23Z",
          "was_summarised": false
        },
        {
          "id": "nyfot0f",
          "author": "seanpuppy",
          "content": "I think this just highlights the importance of hiring highly skilled senior devs over jr's\n\nAny Senior dev today will have spent a TON of time reading and reviewing code, and will be both faster and better and finding issues.",
          "created_at": "2026-01-08T17:38:48Z",
          "was_summarised": false
        },
        {
          "id": "nyg6yqu",
          "author": "FastDecode1",
          "content": "Use AIs to review. Duh.\n\nWhat kind of \"agentic workflow\" are you using if the only thing that's automated is code generation? If you paid money for that, you need a refund.",
          "created_at": "2026-01-08T18:56:40Z",
          "was_summarised": false
        },
        {
          "id": "nyfo1wx",
          "author": "sabergeek",
          "content": "We'll probably have models for code review at some point, so that AI cleans it own slop.",
          "created_at": "2026-01-08T17:35:29Z",
          "was_summarised": false
        },
        {
          "id": "nygw7oq",
          "author": "ttkciar",
          "content": "Your CTO is totally right, and the problem he describes predates LLM codegen.  The advent of codegen has exacerbated problem tremendously, is all.\n\nPart of the problem in places I've worked is that management controls how much of developers' time is spend writing new code vs paying off technical debt, and management does not allocate enough time to paying off that debt.\n\nIn that sense, it is a people-problem, not a technical problem.  Fix management and the problem becomes a lot more tractable.\n\nOn the other hand, there are some things you can do to make LLM-inferred projects faster/easier to validate and review:\n\n**Write comprehensive unit tests**\n\nPreferably have the humans do this before codegen, because ideally unit tests will describe how code is expected to behave, which will help LLMs infer the expected code.  Not many devs like to write unit tests, though, so having your LLM generate unit tests after the fact is a second-best solution.  Note that you will need to instruct the LLM to write \"testable\" code, because sometimes the most natural-seeming implementations are not easily unit-tested.\n\nUnit tests with mocked dependencies are beneficial because they exercise the different parts of your project in isolation and verify that their outputs/side-effects comply with expectations.  This means you can find many bugs simply by running your unit tests, and which unit tests fail point you precisely at the code which needs to be fixed (if your tests are high-enough granularity, which requires that your functions/methods are decomposed into subroutines.  This is an important aspect of writing code to be testable).\n\nIt also makes adjusting the behavior of the project to comply with expectations easier, if you find that code does not do what you want it to do.  You can tweak the appropriate unit test(s), or write new tests, and have the dev or LLM fix the code so that the test passes.\n\nIt is good industry practice to make sure a development branch passes all unit tests before merging it into the main branch, and then making sure the merge passes all unit tests before pushing it to the remote repo.\n\n**Write good documentation**\n\nOne of the best uses I've found for codegen LLMs is to have them explain my coworkers' code to me.  Most codegen models (and some non-codegen models!) are good at writing code documentation.  This helps me come up to speed not just for code reviews but also for contributing to legacy projects with which I am familiar.\n\nIdeally you should have at least two layers of documentation, preferably three:\n\n* A high-level view, which is short and easy to read, explaining the purpose of the project, who is expected to use it, and for what, and the general flow of data through the project -- its inputs, its outputs, its side-effects, and the components it passes through in-between.\n\n* A component-level view, which describes the main subsystems involved in the project and their interfaces.  These can be libraries, external dependencies like databases or service APIs, frameworks, or any other reasonable-seeming partitioning of the project into a small handful of parts.  If you omit any documentation, it would be this one, not the high- or low-level views.\n\n* A low-level view, usually by source code file, which describes what the code in the file is for, what its classes and any global state are, the methods used by those classes, and what other files use those classes and/or call those methods.\n\nGood documentation will get the human reviewers up to speed quickly and let them start and finish their reviews more quickly.\n\n**Generate a list of possible bugs/problems**\n\nYou don't want to totally automate the code review process, but there's nothing wrong with asking the LLM to infer a list of what might be bugs or weaknesses in the project, for the human reviewers to assess.  When I ask GLM-4.5-Air to enumerate problems in my code, usually only about a third of the problems it identifies are actual problems which need fixing, but it's still better to have it than not.\n\nThis can help focus code reviewers' attention and at least give them something to consider, regarding whether the project should be better than it is.\n\n**Use a structured log**\n\nA lot of problems only become visible once you've been using a project for a while for real-world tasks.  A structured log will not only help you spot problems, but also expose the program's internal state in the steps leading up to the problem.  This is invaluable for rapid troubleshooting.\n\nWhen a problem crops up, you can look back through the log to identify exactly where things went awry, and use the conditions represented in the log to inform bugfixes and (especially!) new unit tests which would have identified the problem before it was put into production.\n\nStrictly speaking this is slightly out of scope for your problem, as the structured log only becomes useful *after* the code passes review and is put into production, but the simple fact is that not all problems get caught in code review.  Realistically new code needs to be vetted both before and after deployment.\n\n\n**These measures will accelerate code review, but the underlying problem persists.**\n\nIncorporating all of these measures can shorten the time it takes to review a project, but human reviewers still have to put in the work to verify that the code is good.  Depending on how many reviewers you have and how much code you are generating, they might or might not be able to keep up.\n\nWhether to bottleneck deployment of new code on code review, and how much, is and always has been a trade-off determined by the development team's management.  It is their job to assess the tradeoffs between releasing thoroughly-vetted code versus releasing possibly-buggy code and adding to the employer's technical debt.\n\nGenerating new code via LLM inference doesn't change that, but you should be able to demonstrate mathematically that given fixed human dev resources (say, programmer-hours per month, allocated to developing new code vs code reviews vs paying down technical debt), and given a fixed management tolerance for accumulating technical debt, the total useful code deployed per unit time is increased when LLMs generate at least some of the new code.",
          "created_at": "2026-01-08T20:48:32Z",
          "was_summarised": false
        },
        {
          "id": "nyfq6dy",
          "author": "1ncehost",
          "content": "I've been dealing with this for a year, and this is predominantly a solved issue with project management risk mitigation. Essentially executives have struggled with this issue since forever: how do you maintain quality when you don't know or interact with everyone in your company? Tests and process are the ultimate answer.\n\nYou must adopt the mindset of an executive and trust the employees, but ensure there are thoroughly enforced safegaurds, audits, and so on to maintain quality. The code you care about becomes the \"operating system\" that derives the systems, not the system design itself.",
          "created_at": "2026-01-08T17:44:47Z",
          "was_summarised": false
        },
        {
          "id": "nyfnzaq",
          "author": "notAllBits",
          "content": "Remit-driven development",
          "created_at": "2026-01-08T17:35:10Z",
          "was_summarised": false
        },
        {
          "id": "nyfsbbt",
          "author": "geoffwolf98",
          "content": "You have to balance the risks - whether it is better to get it out the door but potentially loses you millions due to a price error (or what ever) or have reliable working code that wont bankrupt you.",
          "created_at": "2026-01-08T17:54:02Z",
          "was_summarised": false
        },
        {
          "id": "nyg9e9z",
          "author": "adityaguru149",
          "content": "Yeah it gets difficult reviewing a lot of the slop by AI. My way is writing lots of tests and using AI for quick summarisations for code blacks so that I don't have to read through every line. I also get more involved in the architecting phase so that AI has better guidance before writing code.",
          "created_at": "2026-01-08T19:07:15Z",
          "was_summarised": false
        },
        {
          "id": "nyge9ds",
          "author": "CV514",
          "content": "AI agents have gained some innate right to merge PR or something? No? That's how.",
          "created_at": "2026-01-08T19:28:33Z",
          "was_summarised": false
        },
        {
          "id": "nygfde1",
          "author": "Jmc_da_boss",
          "content": "Oh wow, your telling me the historical bottle neck of human review and alignment in programming is STILL the bottleneck in programming?\n\nThat's crazy, however will we handle this thing that's been true for decades.",
          "created_at": "2026-01-08T19:33:27Z",
          "was_summarised": false
        },
        {
          "id": "nygjke0",
          "author": "synn89",
          "content": "I  expect we'll probably develop new code design methodologies that work best with AI. Languages may also end up being preferred for the same reason. This is why agile and MVC exists today, to optimize for human meat brains.",
          "created_at": "2026-01-08T19:51:57Z",
          "was_summarised": false
        },
        {
          "id": "nygoeq8",
          "author": "CallinCthulhu",
          "content": "Preliminary review by AI catches a lot of shit early. Still needs human review, but that review is faster",
          "created_at": "2026-01-08T20:13:32Z",
          "was_summarised": false
        },
        {
          "id": "nygps00",
          "author": "blackkettle",
          "content": "Thatâ€™s not an appropriate way to use AI for coding. Agentic workflows with high expertise can definitely make you much faster.  Blindly committing AI code based on prompts and no experience?  See you at the next post mortem!\n\nYour CTO is right.\n\nAnd AI shouldnâ€™t really be â€œincreasing code volumeâ€.  It should be used again with expertise to speed up well defined, low risk, relative tasks and gradually iterate to more complex ones.",
          "created_at": "2026-01-08T20:19:41Z",
          "was_summarised": false
        },
        {
          "id": "nygxd2i",
          "author": "a-wiseman-speaketh",
          "content": "I think this is like a corollary of Brandolini's Law - and we've seen how that's played out with the degeneration of shared reality and objective truth over the last decade, particularly.\n\nI will point out that one of the skills a senior dev should be great at and every LLM I have tried is absolutely awful at is DELETING code, or never writing it to begin with.",
          "created_at": "2026-01-08T20:53:34Z",
          "was_summarised": false
        },
        {
          "id": "nygzcnm",
          "author": "Psychological_Ear393",
          "content": "\u0026gt;How do you manage quality when AI agents write code faster than humans can review it?\n\nIf you are pumping out code faster than a human can understand and review it, then you literally can't.  It's a matter of doing the maths of which side you manage for what the product goals and roadmap is.  A pipe can only hold so much volume.  Right now you have pressure on the input side and it's more like a storm water drain than a filtered water outlet.  To strain the storm water you need a bigger pipe and grate which lets more things through.\n\nIt's up to the dev to ensure they are submitting quality pull requests.  If a PR comes in a human doesn't understand it then they have failed at their job.  If a reviewer finds a problem, it doesn't matter where it came from, that dev put in the PR - PRs have problems that's why we have them but to put one in that had no attempt to find the problems and submit understandable quality is egregious and if agents are writing code faster than the gates can handle then that's what's happening.\n\nPerformance objectives need to be updated to include appropriate use of AI. Everyone needs to be on the same page about what matters to your product, if some members of your team want to move faster than humans can understand and others want more thorough reviews then you have a culture problem that needs to be addressed.\n\nI mostly use AI for weird problems where I don't know where to start, like chunks of code I haven't touched before, then I take over and try to solve it myself where I can.  I use it to check the work I did for anything I missed, and you need to be careful with that too it can dream things so you need to know what you know to assess it. I also use it for bulk changes where it has a sample to go off for style and patterns.\n\nThe other day I had to put in a change that I didn't understand.  It was a legacy product in a framework I don't know and from top down they said they urgently need it and they are OK with AI writing it.  I reviewed it as best I could but I had no idea why it worked and in the PR I clearly stated that it was mostly AI written and I didn't fully understand how it works.  I'm a consultant and told them it's a bad idea, the owners said they wanted it, ok sure the people paying the bills get what they want.",
          "created_at": "2026-01-08T21:02:22Z",
          "was_summarised": false
        },
        {
          "id": "nyh2g45",
          "author": "DHasselhoff77",
          "content": "Look up \"The Goal\" by Eliyahu M. Goldratt. Your CTO is right.",
          "created_at": "2026-01-08T21:16:09Z",
          "was_summarised": false
        },
        {
          "id": "nyh8zks",
          "author": "Foreign_Risk_2031",
          "content": "Itâ€™s true. Itâ€™s difficult to accept but true. This is your CTOs job to solve. You can outsource testing. Or make more agentic workflows to review.",
          "created_at": "2026-01-08T21:44:35Z",
          "was_summarised": false
        },
        {
          "id": "nyhe0ju",
          "author": "rosstafarien",
          "content": "It's not tech debt that's your problem. That's literally the least of your worries. It's that nobody understands your codebase and nobody can say that it's correctly solving the problem.\n\nHow are you managing requirements? How are you testing the system to be sure that the requirements are being met? How are you going to confirm that a future change doesn't break existing functionality?\n\nAnd I have yet to see an AI produce sane code at 10x the rate of a human developer. An AI can produce boilerplate at 10x the rate, but that isn't the code you care about.",
          "created_at": "2026-01-08T22:06:21Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921283839Z"
    },
    {
      "flow_id": "",
      "id": "1q7m2eh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/",
      "title": "Built a blind benchmark for coding models - which local models should I add?",
      "content": "3 AI judges score each output blind. Early results from 10 coding tasks - Deepseek V3.2 at #9. GLM 4.7 at #6, beating Claude Opus 4.5.\n\nSome open-source models are free to evaluate. Which local models should I evaluate and add to the leaderboard?\n\n[codelens.ai/leaderboard](http://codelens.ai/leaderboard)\n\nEDIT: Tested community suggestions! Results now live on the leaderboard:  \n  \n\\- GPT-OSS-120B, Qwen3 Next 80B, Devstral 2, Nemotron Nano 30B, and more  \n  \nKeep the suggestions coming - we'll keep adding models.",
      "author": "Equivalent-Yak2407",
      "created_at": "2026-01-08T19:54:08Z",
      "comments": [
        {
          "id": "nygvlfu",
          "author": "ciprianveg",
          "content": "minimax m2.1, qwen 235b",
          "created_at": "2026-01-08T20:45:48Z",
          "was_summarised": false
        },
        {
          "id": "nygtwck",
          "author": "Aggressive-Bother470",
          "content": "gpt-oss-120b, Seed-OSS-36B, Qwen3-30B-A3B-Thinking-2507-BF16, GLM-4.6-UD-IQ2\\_M",
          "created_at": "2026-01-08T20:38:14Z",
          "was_summarised": false
        },
        {
          "id": "nyhe3dy",
          "author": "MrBIMC",
          "content": "Devstral-2512 is goated. I know it's free only temporarily, but as far as free models go - it most often delivers exactly to spec. So I'd like it benchmarked.",
          "created_at": "2026-01-08T22:06:42Z",
          "was_summarised": false
        },
        {
          "id": "nyh5uym",
          "author": "-InformalBanana-",
          "content": "Qwen3 2507 30b a3b instruct, qwen3 next 80b, gpt oss 20b/120b, Devstral small 2 24b, Nemotron nano 3 e0b a3b, Nemotron Cascade 14b.\nI tried Nemotron models and I think they are bad and benchmaxed so if you cound check that. For example Nemotron Cascade 14b has better LCBv6 score than qwen next 80b a3b. But in my one shot try it even had syntax errors so complete failure.",
          "created_at": "2026-01-08T21:31:00Z",
          "was_summarised": false
        },
        {
          "id": "nyiql7p",
          "author": "pmttyji",
          "content": "* Kimi K2 Instruct 0905\n* Kimi-K2-Thinking\n* Devstral-2-123B-Instruct-2512\n* Devstral-Small-2-24B-Instruct-2512\n* Mistral-Large-3-675B-Instruct-2512\n* Ling-1T\n* Olmo-3.1-32B-Instruct\n* Qwen3-32B\n* Llama-3\\_3-Nemotron-Super-49B-v1\\_5\n* dots.llm1.inst",
          "created_at": "2026-01-09T02:12:42Z",
          "was_summarised": false
        },
        {
          "id": "nyizqfy",
          "author": "celsowm",
          "content": "IQuest-Coder-V1",
          "created_at": "2026-01-09T03:01:27Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://codelens.ai/leaderboard",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/6ocf1gbxj6cg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921417296Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://codelens.ai/leaderboard: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921416515Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7k754",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/",
      "title": "toy model",
      "content": "If anyone is interested in creating, training, and chatting with a toy model, Iâ€™ve created [https://github.com/EduardTalianu/toygpt](https://github.com/EduardTalianu/toygpt).\n\nIt includes:\n\n* a model script to create a model\n* a training script to train it on a`.txt` file\n* a chat script to interact with the trained model\n\nItâ€™s a PyTorch research implementation of a Manifold-Constrained Hyper-Connection Transformer (mHC), combining Mixture-of-Experts efficiency, Sinkhorn-based routing, and architectural stability enhancements.\n\nSlower per step than a vanilla Transformer â€” but *much* more sample-efficient. At \u0026lt;1 epoch it already learns grammar, structure, and style instead of collapsing into mush.\n\nEnjoy!",
      "author": "Eduard_T",
      "created_at": "2026-01-08T18:46:43Z",
      "comments": [
        {
          "id": "nyghz0s",
          "author": "cosimoiaia",
          "content": "Very interesting, thanks for sharing!",
          "created_at": "2026-01-08T19:44:58Z",
          "was_summarised": false
        },
        {
          "id": "nyhirhi",
          "author": "-InformalBanana-",
          "content": "Just interested what degree/knowledge base do you personally have in order to implement papers/innovations like that? Math or CS I'm guessing?",
          "created_at": "2026-01-08T22:27:53Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/EduardTalianu/toygpt",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921621726Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/EduardTalianu/toygpt: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921621195Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q79n6x",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/",
      "title": "I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough â€œconscienceâ€ to realize something was wrong and freak out.",
      "content": "I now feel bad seeing the model realize it was losing its mind and struggling with it, it feels like I was torturing it :(",
      "author": "ikergarcia1996",
      "created_at": "2026-01-08T11:42:44Z",
      "comments": [
        {
          "id": "nydrzqu",
          "author": "Practical-Collar3063",
          "content": "One word: wtf ?",
          "created_at": "2026-01-08T11:48:34Z",
          "was_summarised": false
        },
        {
          "id": "nye6qe6",
          "author": "Red_Redditor_Reddit",
          "content": "I had that happen, but the weights were too corrupt to make complete sentences. Still, I could feel as if it was conciously trying to pull itself out of insanity.",
          "created_at": "2026-01-08T13:24:31Z",
          "was_summarised": false
        },
        {
          "id": "nyflkzy",
          "author": "Chromix_",
          "content": "Modern reasoning models are trained to stop and get back on track after descending into loops or garbage token streams. This is what you may be observing here, yet the \"getting back on track\" mechanism also seems to be corrupted (tone change) due to your steering vector injection.\n\nYou could disable the steering vector after 50 tokens or so, to see if it then sets itself back on track correctly.",
          "created_at": "2026-01-08T17:24:36Z",
          "was_summarised": false
        },
        {
          "id": "nyecpsc",
          "author": "IngwiePhoenix",
          "content": "That... that is interesting. o.o\n\nYes, I understand your sentiment. This really does \"read\" rather painful. xD\n\nI recently watched an anime movie, \"Planetarian: Storyteller of the Stars\" and this very much reminded me of Yumemi in a rather particular scene o.o;\n\nIt is about a lone android amidst a warzone and stuff. Really nice movie honestly. Would recommend if you have some spare time.",
          "created_at": "2026-01-08T13:56:51Z",
          "was_summarised": false
        },
        {
          "id": "nyetp2v",
          "author": "a_beautiful_rhind",
          "content": "What was the vector for? i.e the actual subject.",
          "created_at": "2026-01-08T15:21:04Z",
          "was_summarised": false
        },
        {
          "id": "nyewl30",
          "author": "llama-impersonator",
          "content": "steering reasoning models seems much less useful, imo, unless you turn it off for the reasoning block. something about RL for reasoning makes these models get extra tortured when they are OOD in a reasoning block",
          "created_at": "2026-01-08T15:34:26Z",
          "was_summarised": false
        },
        {
          "id": "nydwcdk",
          "author": "nielsrolf",
          "content": "Woah this is really interesting, such ood behavior is hard to explain away with \"it's just immitating something from training\". Can you share more details about what you did?",
          "created_at": "2026-01-08T12:19:47Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q79n6x",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921716727Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.reddit.com/gallery/1q79n6x: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921716206Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7fejp",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/",
      "title": "Are MiniMax M2.1 quants usable for coding?",
      "content": "Please share your real life experience. Especially interesting to hear from someone who had a chance to compare higher quants with lower ones.\n\nAlso, speaking of the model itself - do you feel it's worth the buzz around it?\n\nUse case - coding via opencode or claude proxy.\n\nThank you!",
      "author": "val_in_tech",
      "created_at": "2026-01-08T15:54:30Z",
      "comments": [
        {
          "id": "nyf35wf",
          "author": "this-just_in",
          "content": "Yes, itâ€™s worth the buzz. Â I use an AWQ 4bit and fp8 kv and can drive Claude Code at somewhere between Sonnet 3.7 and 4 level to my estimation. Â Stability gets dicey for me around 150k tokens but regains coherence after compact- potentially a consequence of kv cache quantization. Â Importantly itâ€™s very fast which makes it usable. Â It feels good at iteration too, which was important in the Sonnet 3.7-4 era- it didnâ€™t always get everything right but it could pivot and work with you.",
          "created_at": "2026-01-08T16:04:01Z",
          "was_summarised": false
        },
        {
          "id": "nyf5gsp",
          "author": "NaiRogers",
          "content": "0xSero/MiniMax-M2.1-REAP-50-W4A16 for me is better than gpt-oss-120b",
          "created_at": "2026-01-08T16:14:24Z",
          "was_summarised": false
        },
        {
          "id": "nyf6zza",
          "author": "suicidaleggroll",
          "content": "Unsloth UD-Q4_K_XL is working well for me",
          "created_at": "2026-01-08T16:21:11Z",
          "was_summarised": false
        },
        {
          "id": "nyf1u16",
          "author": "phenotype001",
          "content": "q4\\_k\\_s is good enough for me.",
          "created_at": "2026-01-08T15:57:59Z",
          "was_summarised": false
        },
        {
          "id": "nyfpcgu",
          "author": "MarketsandMayhem",
          "content": "Yes. I use the Unsloth 5-bit XL quant with fp8 kv and M2.1 works well with Claude Code, OpenCode, Droid and Roo. Heck, I even used the 2-bit XL quant for a bit and it was surprisingly usable. I think it's worth experimenting with quantized coding models, particularly at higher precision (and quality) quants. The ones I've found to be the best so far are Unsloth and Intel Autoround. I am excited about experimenting more with NVFP4.",
          "created_at": "2026-01-08T17:41:09Z",
          "was_summarised": false
        },
        {
          "id": "nyg0j0j",
          "author": "rhaikh",
          "content": "I've had very bad luck with this using the Minimax cloud hosted via kilo.  Bad at tool calling, reasoning, etc. It would duplicate files because it would write to the filename without extension. I had a much better experience with Devstral 2 for reference.",
          "created_at": "2026-01-08T18:29:13Z",
          "was_summarised": false
        },
        {
          "id": "nyhxeu3",
          "author": "Agreeable-Market-692",
          "content": "Grab the REAP versions from u/Noctrex on HuggingFace",
          "created_at": "2026-01-08T23:39:58Z",
          "was_summarised": false
        },
        {
          "id": "nyfnsct",
          "author": "Impressive_Chain6039",
          "content": "Edited a real backend. More then 40 files . Vscode and cline. C++. No errrors",
          "created_at": "2026-01-08T17:34:18Z",
          "was_summarised": false
        },
        {
          "id": "nyfr94b",
          "author": "StardockEngineer",
          "content": "I've been using Q3 from Unsloth and it's still very capable.",
          "created_at": "2026-01-08T17:49:28Z",
          "was_summarised": false
        },
        {
          "id": "nygtwpl",
          "author": "TokenRingAI",
          "content": "Yes, even 2 bit is very usable",
          "created_at": "2026-01-08T20:38:16Z",
          "was_summarised": false
        },
        {
          "id": "nyi2zfr",
          "author": "SillyLilBear",
          "content": "awq 4 works good",
          "created_at": "2026-01-09T00:08:48Z",
          "was_summarised": false
        },
        {
          "id": "nyf1fn4",
          "author": "wapxmas",
          "content": "Less than q8 - no, reap 50 q8 for now is the best.",
          "created_at": "2026-01-08T15:56:11Z",
          "was_summarised": false
        },
        {
          "id": "nyf7dfy",
          "author": "Morphon",
          "content": "I used it in a recent comparison:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing\\_llm\\_ability\\_to\\_port\\_code\\_comparison\\_and/](https://www.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing_llm_ability_to_port_code_comparison_and/)\n\nIt is good, but not as good as K2-Thinking. However, it is MUCH smaller. My personal setup can't run it above TQ1, which is probably too aggressively quantized for \"real work\". But even that quantized it produces better code than GPT-OSS-20b.",
          "created_at": "2026-01-08T16:22:49Z",
          "was_summarised": false
        },
        {
          "id": "nyffec0",
          "author": "sjoerdmaessen",
          "content": "Q4 was noticeable worse than Q5, so I'm sticking with Q5, Q6 didn't give me much of an improvement at all",
          "created_at": "2026-01-08T16:57:25Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921719081Z"
    },
    {
      "flow_id": "",
      "id": "1q6n5vl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/",
      "title": "16x AMD MI50 32GB at 10 t/s (tg) \u0026amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)",
      "content": "Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)\n\non vllm-gfx906-deepseek with 69000 context length\n\n**Power draw**: 550W (idle) / 2400W (peak inference)\n\n**Goal**: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation \u0026amp; prompt processing)\n\n**Coming next**: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking\n\n**Credits**: BIG thanks to the Global Open source Community!\n\nAll setup details here:\n\n[https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32)\n\n\n**Feel free to ask any questions and/or share any comments.**\n\nps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! \n\nps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... ",
      "author": "ai-infos",
      "created_at": "2026-01-07T18:22:05Z",
      "comments": [
        {
          "id": "nyb7g9w",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T01:00:14Z",
          "was_summarised": false
        },
        {
          "id": "ny8yjc2",
          "author": "fallingdowndizzyvr",
          "content": "\u0026gt; Power draw: 550W (idle) / 2400W (peak inference)\n\nSweet. It's winter. Might as well have your heater do work instead of making empty BTUs.",
          "created_at": "2026-01-07T18:49:27Z",
          "was_summarised": false
        },
        {
          "id": "ny8uc81",
          "author": "Soft_Possible1862",
          "content": "Holy shitâ€¦.",
          "created_at": "2026-01-07T18:31:17Z",
          "was_summarised": false
        },
        {
          "id": "ny8sxj3",
          "author": "kevin_1994",
          "content": "How loud is it? How are you able to run 2400W from home?",
          "created_at": "2026-01-07T18:25:10Z",
          "was_summarised": false
        },
        {
          "id": "ny9q1kn",
          "author": "SourceCodeplz",
          "content": "Tbh if you are coding professionally, this really isn't that much of a spend for having a basically offline programmer working with you on just electricity.",
          "created_at": "2026-01-07T20:49:32Z",
          "was_summarised": false
        },
        {
          "id": "ny8yh6u",
          "author": "OnlineParacosm",
          "content": "Can you give us a rough all in cost so I can figure what the tokens per second cost basis is for this? Thanks for such a great write up.",
          "created_at": "2026-01-07T18:49:12Z",
          "was_summarised": false
        },
        {
          "id": "ny9pr28",
          "author": "ThatCrankyGuy",
          "content": "Thousands of dollars of equipment hang by garden twist-tie wire. Reminds me of grad days.",
          "created_at": "2026-01-07T20:48:19Z",
          "was_summarised": false
        },
        {
          "id": "ny8vpf6",
          "author": "Dorkits",
          "content": "Me with my rumble 3060ti : Dream build ðŸ˜²",
          "created_at": "2026-01-07T18:37:14Z",
          "was_summarised": false
        },
        {
          "id": "ny8zw75",
          "author": "FullstackSensei",
          "content": "Is there a meaningful difference between something like DS AWQ and something like Minimax 2.1 at Q8?",
          "created_at": "2026-01-07T18:55:16Z",
          "was_summarised": false
        },
        {
          "id": "ny986ve",
          "author": "ForsookComparison",
          "content": "How is your prompt processing so good? Does Tensor parallelism come into play?",
          "created_at": "2026-01-07T19:31:38Z",
          "was_summarised": false
        },
        {
          "id": "nybo6v7",
          "author": "vulcan4d",
          "content": "Ditch the furnace, best home heater!",
          "created_at": "2026-01-08T02:28:27Z",
          "was_summarised": false
        },
        {
          "id": "ny9ewd9",
          "author": "organicmanipulation",
          "content": "Nice setup! I notice that you're splitting your PCIe lanes into two 8x. Can you please share the exact PCIe bifurcation card you're using?",
          "created_at": "2026-01-07T20:00:41Z",
          "was_summarised": false
        },
        {
          "id": "nya9bqr",
          "author": "cashmillionair",
          "content": "Thank you for sharing, appreciate it!",
          "created_at": "2026-01-07T22:12:32Z",
          "was_summarised": false
        },
        {
          "id": "nyb7t5l",
          "author": "noiserr",
          "content": "That's nuts! And I love it.",
          "created_at": "2026-01-08T01:02:04Z",
          "was_summarised": false
        },
        {
          "id": "nybm752",
          "author": "MaximKiselev",
          "content": "10 t/s Carl....",
          "created_at": "2026-01-08T02:18:00Z",
          "was_summarised": false
        },
        {
          "id": "nybvmx1",
          "author": "ryfromoz",
          "content": "I love massive frankenstein rigs like this!",
          "created_at": "2026-01-08T03:07:25Z",
          "was_summarised": false
        },
        {
          "id": "nydbb4o",
          "author": "PreparationLow6188",
          "content": "A Wow for this supreme project. It is the time should consider reenable MI50 on the shelf.",
          "created_at": "2026-01-08T09:25:48Z",
          "was_summarised": false
        },
        {
          "id": "ny8yc7v",
          "author": "egomarker",
          "content": "10 tks? Sigh",
          "created_at": "2026-01-07T18:48:36Z",
          "was_summarised": false
        },
        {
          "id": "ny943r0",
          "author": "exaknight21",
          "content": "Can you share your build please. Like what are those fans, how did you hook them up, what motherboard youâ€™re using. I am a little new to this aspect.",
          "created_at": "2026-01-07T19:13:39Z",
          "was_summarised": false
        },
        {
          "id": "ny9zj2e",
          "author": "Different-Toe-955",
          "content": "Epic setup I love the zip ties holding the GPUs up. How do those oculink extenders work out? Looks like each GPU is running at x8.",
          "created_at": "2026-01-07T21:29:36Z",
          "was_summarised": false
        },
        {
          "id": "nycaq4e",
          "author": "badgerbadgerbadgerWI",
          "content": "This is the kind of setup that makes enterprise local deployment actually viable. MI50s at those prices vs NVIDIA is a completely different ROI calculation. Are you seeing any stability issues with vllm on the older gfx906 arch over longer inference runs?",
          "created_at": "2026-01-08T04:34:52Z",
          "was_summarised": false
        },
        {
          "id": "nygckst",
          "author": "qcodec",
          "content": "500W/2400W. Oh my, even my solar system can't handle this. I guess I'll have to install a standalone one.",
          "created_at": "2026-01-08T19:21:10Z",
          "was_summarised": false
        },
        {
          "id": "nyieqz7",
          "author": "el3mancee",
          "content": "Nice setup.",
          "created_at": "2026-01-09T01:09:44Z",
          "was_summarised": false
        },
        {
          "id": "nyihpr7",
          "author": "el3mancee",
          "content": "https://preview.redd.it/vr6jf0i178cg1.jpeg?width=4284\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=2e4036b31d87b2e9d7d75dcdbebdae2f7e07cecf\n\nMy setup can run Deepseek 3.1 IQ4\\_XS at 7.5 t/s. 200W total when running.",
          "created_at": "2026-01-09T01:25:31Z",
          "was_summarised": false
        },
        {
          "id": "ny8ycbp",
          "author": "Far-Low-4705",
          "content": "pretty sure a mac would it run faster, and at far less power consumption.\n\nStill super cool, but not sure how practical it would be",
          "created_at": "2026-01-07T18:48:37Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/lor8ccu2xybg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.92183308Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921832338Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q6sp4b",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/",
      "title": "Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning",
      "content": "As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:\n\n* 169M parameters\n* Streaming support\n* Zero-shot voice cloning\n* 0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds\n* Requires 3-12 seconds of reference audio for voice cloning\n* Apache 2.0 license\n\nYes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.\n\nItâ€™s not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!\n\nGitHub repo: [https://github.com/samuel-vitorino/sopro](https://github.com/samuel-vitorino/sopro)",
      "author": "SammyDaBeast",
      "created_at": "2026-01-07T21:46:19Z",
      "comments": [
        {
          "id": "nya3x2v",
          "author": "Accurate-Tea8319",
          "content": "Pretty impressive for a solo project on a single GPU tbh. The streaming support is clutch - most TTS models make you wait forever for the full generation\n\n  \nHow's the quality compared to something like Coqui or Tortoise? The zero-shot cloning sounds tempting but I've been burned by models that promise it and deliver robot voices lol",
          "created_at": "2026-01-07T21:48:39Z",
          "was_summarised": false
        },
        {
          "id": "nyatrmo",
          "author": "TheRealMasonMac",
          "content": "How much did it cost to train?",
          "created_at": "2026-01-07T23:51:24Z",
          "was_summarised": false
        },
        {
          "id": "nya6icb",
          "author": "HungryMachines",
          "content": "The voice sounds a bit hoarse on the sample, is that something that can be improved with more training?",
          "created_at": "2026-01-07T21:59:57Z",
          "was_summarised": false
        },
        {
          "id": "nybzmea",
          "author": "lastrosade",
          "content": "My God, you gave us a model, a clear usage, an architecture, datasets, ~~training scripts.~~\n\n~~All we need now is a brave soul with money.\nHonestly, I'd love to see tomorrow if I can improve on this.\nMaybe even put some money down for training.\nI'd love to do it with a smaller parameter count though.~~\n\n~~If someone managed to make Kokoro that fucking good and bilingual and have multiple voices, I think we can make a kick ass single language, single voice, 60 million or less parameters Model.~~\n\nSomething I would really like is for someone to manage to pin down the exact recipe for a good TTS model and have that recipe be completely open source so that other people may concentrate on finding data sets for other languages and make multiple high quality, very small TTS models.\n\n~~And you gave me so much fucking hype with this.~~\n\nNever mind, false hopes, I just realized you did not give the training scripts, I'm fucking stupid.",
          "created_at": "2026-01-08T03:29:08Z",
          "was_summarised": false
        },
        {
          "id": "nybd9sa",
          "author": "RIP26770",
          "content": "We need a ComfyUI node ASAP ! Thanks for sharing this ðŸ™",
          "created_at": "2026-01-08T01:30:52Z",
          "was_summarised": false
        },
        {
          "id": "nyaex4o",
          "author": "SlavaSobov",
          "content": "Great work! I'll give it a try later. It looks very nice for small edge devices!",
          "created_at": "2026-01-07T22:38:03Z",
          "was_summarised": false
        },
        {
          "id": "nyaes62",
          "author": "PsychologicalFactor1",
          "content": "It will support Portuguese, right? â€¦right?",
          "created_at": "2026-01-07T22:37:24Z",
          "was_summarised": false
        },
        {
          "id": "nydjwmr",
          "author": "danigoncalves",
          "content": "Congrats mate! Very nice job you did here with such lower capacity. Maybe you can try to apply to some european fund in order to take this further because I guess Amalia is only TTT :)",
          "created_at": "2026-01-08T10:43:01Z",
          "was_summarised": false
        },
        {
          "id": "nye7roq",
          "author": "Fickle_Performer9630",
          "content": "Whatâ€™s the relation to Soprano TTS model?",
          "created_at": "2026-01-08T13:30:15Z",
          "was_summarised": false
        },
        {
          "id": "nycwctl",
          "author": "rm-rf-rm",
          "content": "The examples in the README are truly bad. There are so so many such \"I made a TTS\" projects - genuinely curious what your aim is? Just learn? Have fun?\n\nIt would be so much better for you and the community to contribute to one of the existing open source TTS projects. What the ecosystem lacks is genuinely good model that can handle long generations without going haywire. Its sad that we dont have aggressive competition from open source in TTS like we do in STT, LLMs, Image gen etc.",
          "created_at": "2026-01-08T07:11:47Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/samuel-vitorino/sopro",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.921946587Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/samuel-vitorino/sopro: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921946096Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7b3wy",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/",
      "title": "A 2.5M 10MB TinyStories model trained using GRU and attention (vs.TinyStories-1M)",
      "content": "Using a 20MB TinyStories datasheet, this TinyStories model 5x smaller than TinyStories-1M.\n\n  \nSince this was trained on google colab free(nvidia t4), the loss is only converged to \\~0.75.\n\n  \nThe architecture used was a hybrid of GRU, particularly GRUcell with a single attention layer.\n\n  \nIn a single, large GRUcell layer, I used a residual memory logic which writes decoded data into the drive, and feeds it to the input as for the hidden state.\n\nThe model creates a proposed memory:\n\nM\\~t=tanhâ¡(Wcht+bc)\n\nFinally, the old memory is mixed with the new one:\n\nMt=(1âˆ’pt)âŠ™Mtâˆ’1+ptâŠ™M\\~t\n\nThis allows for the architecture to train a model so small (0.36M), that can memorize words and output meaningful words at a train loss of 2.2.\n\n\n\nFinally, I added a self-attention layer that allows the model to see context of the previous words to a limited extent. This allows the model to remember what it said 5-10 words ago and prevent complete drifting, which was a limitation of standard GRU.\n\n\n\nThis brings the attention cost to O(TÂ³), but the model still remains faster than TinyStories-1M (50MB) for short bursts below \\~300 tokens when self attention becomes an overhead.\n\n  \nExample output(TinyStories-1M): \n\n\n\nPrompt: once upon a time\n\nOutput: once upon a time there was an old girl named Lily. She loved to play with her toys and pretend she was his friend. One day, they went outside to see the park. The sun looked blue and it had big eyes.\n\n\n\nLily's mom said \"Let me help you get my doll!\" They started to walk around the park and had lots of fun together. When it was dark and it seemed like something strange happened, a little bunny came up over to them.\n\n\n\n\"Hello there,\" said Mom. \"It's so pretty.\"\n\n\n\nThe bunny smiled at Lily and said, \"Yes! I'm glad we can find some more toys later.\" Then he opened his backpack and found that she was able to provide for him in a special place where he could have all the toys ready!\n\n(165 words, 67.97 it/s) (could be roughly around 200 chars/sec for BPE tokeniser)\n\n  \ntinystoriesgru:\n\n  \nPrompt: once upon a time  \nOutput: once upon a time to hear the wolf with a smile on his face. She was so happy that the monster was so cold.\n\nBut then, the piece of colorful circle came in. She wanted to see what was inside, but she thought it would be fun. She started to cry and started to cry. She quickly ran and ran until she found the crayon and started to cry.\n\nThe cat saw the pretty flower and started to shake and showed them the magazine. She thought it would be fun to cut the leaves. She was so happy with her new ball. She wanted to take h\n\n(500 tokens, 112.02 it/s)\n\n  \nAt lower characters, the GRU scales to be much faster while the transformer remains consistent with 67-68it/s, for more/less words.\n\n  \nThe pure transformer continues to have better context overall.\n\n  \nI've included the [train.py](http://train.py) here (if anyone can train it further):  \n[https://github.com/kavyamali/tinystoriesgru](https://github.com/kavyamali/tinystoriesgru)\n\n  \nThank you for reading.",
      "author": "ValuableLucky8566",
      "created_at": "2026-01-08T12:57:45Z",
      "comments": [
        {
          "id": "nye2od4",
          "author": "BloodAccomplished304",
          "content": "This is actually pretty cool - getting decent coherence out of such a tiny model is impressive. The GRU+attention hybrid approach makes sense for keeping memory usage down while still having some context awareness\n\n  \nYour output quality looks surprisingly good for 2.5M params, though I notice it gets a bit repetitive toward the end. Have you tried any techniques to reduce that or is it just a limitation of the architecture at this scale?",
          "created_at": "2026-01-08T13:00:45Z",
          "was_summarised": false
        },
        {
          "id": "nyejdgi",
          "author": "SrijSriv211",
          "content": "Pretty cool!!!",
          "created_at": "2026-01-08T14:31:12Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://train.py",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/kavyamali/tinystoriesgru",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.922254234Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://train.py: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.92212252Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/kavyamali/tinystoriesgru: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.922253583Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q77cnk",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/",
      "title": "RAG Paper 26.1.7",
      "content": "1. [RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection](http://arxiv.org/abs/2601.03981v1)\n2. [SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems](http://arxiv.org/abs/2601.03979v1)\n3. [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](http://arxiv.org/abs/2601.03948v1)\n4. [Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval](http://arxiv.org/abs/2601.03908v1)\n5. [Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation](http://arxiv.org/abs/2601.03903v1)\n6. [VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation](http://arxiv.org/abs/2601.03792v1)\n7. [Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning](http://arxiv.org/abs/2601.03748v1)\n8. [Whose Facts Win? LLM Source Preferences under Knowledge Conflicts](http://arxiv.org/abs/2601.03746v1)\n\n\n\n**Collected by OpenBMB, transferred by**Â [**RagView.ai**](https://www.ragview.ai/)Â **/**Â [**github/RagView**](https://github.com/RagView/RagView)Â **.**",
      "author": "Cheryl_Apple",
      "created_at": "2026-01-08T09:24:29Z",
      "comments": [
        {
          "id": "nydbexb",
          "author": "Maximum-Balance7685",
          "content": "Thanks for collecting these, super helpful having them all in one place\n\n  \nThe \"Whose Facts Win\" paper sounds particularly interesting - curious how they're handling source prioritization in conflict scenarios",
          "created_at": "2026-01-08T09:26:46Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://arxiv.org/abs/2601.03981v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03979v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03948v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03908v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03903v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03792v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03748v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03746v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.ragview.ai/",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/RagView/RagView",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.923248758Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03981v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.922386539Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03979v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.922515398Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03948v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.922678635Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03908v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.922761872Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03903v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.92290594Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03792v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.922976812Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03748v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923021332Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://arxiv.org/abs/2601.03746v1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923082715Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.ragview.ai/: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923186734Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/RagView/RagView: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923248558Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q6v7v5",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/",
      "title": "What hardware would it take to get Claude Code-level performance?",
      "content": "In my previous company I had a Claude license and my work was basically interacting with Claude Code all day long. The code base was rather complex and I was automating testing and â€œDevOpsâ€ stuff for an embedded device development so Claude Code saved me tons of time (it was much faster to ask and tune that to do it all by myself).\n\nIm currently unemployed but got a freelancing gig and the company doesnâ€™t provide access to commercial AI tools for contractors like me, but once again the work is rather demanding and I donâ€™t think Iâ€™ll meet the deadlines without AI help (itâ€™s a fairly old code base using mostly Java in a concurrent and distributed fashion), and of course due to compliance I canâ€™t just use a license I paid for by myself.\n\nSo, in new to all this. To be honest I have very little hardware, as I would always prioritize power efficiency since I never really needed to do anything hardware intensive before (I donâ€™t have a gaming PC or anything like that). I have an old HP Z2 G4 Tower I use as virtualization server and was thinking of getting a 3060 12GB for \\~300 USD (locally). Will I be able to run anything decent with that? Anything that would truly help me?\n\nI see everyone recommends a 3090 but Iâ€™d need a whole new PSU and build an entire computer around that. So thatâ€™d be roughly 2K USD (is it worth it? I donâ€™t know, maybe?)\n\nWhat hardware is requires to run anything remotely close to Claude Code? Something like 6x3090s (144GB VRAM)?",
      "author": "cashmillionair",
      "created_at": "2026-01-07T23:22:28Z",
      "comments": [
        {
          "id": "nyatmmq",
          "author": "Lissanro",
          "content": "I have four 3090 cards and that's enough to fit entire 256K context cache at Q8 for Kimi K2 Thinking, but the main issue today is going to be RAM - a year ago it was possible to get 512GB of 8-channel DDR4 3200 MHzfor around $800 (I got 1 TB at the time), but today this is no longer the case, so with limited budget, you have to consider smaller models.\n\nI think your best bet is MiniMax M2.1, some people got reported getting 20 tokens/s with as little as 72 GB VRAM: [https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/discussions/2](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/discussions/2) \\- for full VRAM inference you probably will need 8x3090 though, then the speed would be much greater. Good idea to buy used server EPYC hardware, even 8-channel DDR4 would be still much faster than any DDR5 dual channel RAM on gaming platforms, and connecting multiple GPUs works much better on server motherboards that have multilpe x16 slots and support bifurcation in case you need to plug-in more cards than there are full speed slots.",
          "created_at": "2026-01-07T23:50:41Z",
          "was_summarised": false
        },
        {
          "id": "nyay3pt",
          "author": "getfitdotus",
          "content": "I have close to it glm 4.7 with 4x6000 pro blackwells. Its faster then opus and sonnet. Not quite as good but close enough and i have 320k max context over all requests running 90t/s.",
          "created_at": "2026-01-08T00:13:22Z",
          "was_summarised": false
        },
        {
          "id": "nyaqabj",
          "author": "Fit-Produce420",
          "content": "Well part of what makes some solutions better is the CLI or dev environment or whatever.\n\n\nClaude Code is pretty near cutting edge no matter how you slice it, it's large model and uses tools really well.\n\n\n\n\nYou can use claude code with other models so you want good tool use, good context, and reasonable speed.Â \n\n\nMost open source models are MoE for speed so a large model good at tool use is your best bet.Â \n\n\nAs a hobbyist who needed a modern rig and couldn't just add le video card I bought a Strix Halo Framework Desktop 128GB, it can run dense models around 70B slowly but runs 50B-200B MoE models at 50+/toks.\n\n\nÂ but a Mac would be great if you don't want Linux or gaming. Pricier than strix.\n\n\nOr you can get a gb10 nvidia Blackwell with cuda if you don't want to use it as a general purpose computer, has nvlink which is the fastest built in link for clustering.",
          "created_at": "2026-01-07T23:33:34Z",
          "was_summarised": false
        },
        {
          "id": "nyavgx4",
          "author": "ithkuil",
          "content": "You can run Claude Code with multiple models. What you meant to ask was about the equivalent to Claude Opus 4.5 which there isn't. But GLM 4.7 and a few others are in the ballpark of some Claude Sinnet models, more or less. They are very large models and your purpose hardware is not remotely close.",
          "created_at": "2026-01-08T00:00:02Z",
          "was_summarised": false
        },
        {
          "id": "nyavtx8",
          "author": "datOEsigmagrindlife",
          "content": "The hardware you haven't isn't remotely close enough.\n\nYou're probably going to need to drop somewhere in the region of $50-$100k on GPUs.",
          "created_at": "2026-01-08T00:01:53Z",
          "was_summarised": false
        },
        {
          "id": "nyaq33o",
          "author": "DataGOGO",
          "content": "You would need a lot of hardware, easily over 10k to start, but in all reality more like 40-60k to get you 80% there (2 or 4 Rtx Pro Blackwellâ€™s + Xeon workstation, + 8-12 channels of memory, 2X power supplies)\n\nJust go get a $200 a month claude code subscription, just make sure the client is ok with you using AI on their code base; be sure to get permission in writing.",
          "created_at": "2026-01-07T23:32:31Z",
          "was_summarised": false
        },
        {
          "id": "nyatp0g",
          "author": "Short_Ad4946",
          "content": "No matter how much you spend you won't get that level of performance. You could spend 100k and have worde models than 4.5 Sonnet/Opus, they're cutting edge. Maybe there are open source CLI tools but you'll be bottlenecked by open source models(if you really need the same performance as Sonnet/Opus). Your best bet is to convince your client you have to work with those AI tools and either get a normal plan or an enterprise version if they give you data protection or whatever on those from Claude Code",
          "created_at": "2026-01-07T23:51:01Z",
          "was_summarised": false
        },
        {
          "id": "nyawe08",
          "author": "ithkuil",
          "content": "You can use Claude Code via the API like everyone else and if the client won't let you then you have to find another client because you're screwed because it's an unreasonable client and everyone uses it and you can't compete on that basis. He will find another freelancer who just lies to him about not using AI.",
          "created_at": "2026-01-08T00:04:44Z",
          "was_summarised": false
        },
        {
          "id": "nyau2h2",
          "author": "NotLogrui",
          "content": "Find a cheaper alternative to Nvidia's DGX Spark and use run Mimi V2 Flash or MiniMax\n\nYou're looking at a minimum $2000 investment\n\nThere are both models that fit in DGX Spark 128GB of integrated memory\n\n[unsloth/MiMo-V2-Flash-GGUF](https://model.lmstudio.ai/download/unsloth/MiMo-V2-Flash-GGUF) \\- currently top 5 coding model - open source\n\n[bartowski/MiniMaxAI\\_MiniMax-M2.1-GGUF](https://huggingface.co/MiniMaxAI/MiniMax-M2.1) \\- currently top 5 coding model - open source as well\n\nCorrection Ryzen 395+ Max is under $2000 but unknown for FP4 support: [https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc)",
          "created_at": "2026-01-07T23:52:56Z",
          "was_summarised": false
        },
        {
          "id": "nyb9n06",
          "author": "beedunc",
          "content": "skip the GPUs - Buy a Dell 5810 or similar Xeon system and add ram. Iâ€™ve run Qwen3coder480B @q3+ (240gb) and it gave excellent results running at 2-4 tps. Post your prompt, itâ€™ll be ready after making coffee. Itâ€™s light years ahead of any 16, 32, or even 64gb local models. \n\nBuilt mine for \u0026lt; $500 over the summer, but would be $1k now because ram.\n\nEdit:clarity",
          "created_at": "2026-01-08T01:11:37Z",
          "was_summarised": false
        },
        {
          "id": "nyavwp7",
          "author": "Ok-Bill3318",
          "content": "Itâ€™s not just hardware, youâ€™d need to train and tweak the model as well as Anthropic have.",
          "created_at": "2026-01-08T00:02:17Z",
          "was_summarised": false
        },
        {
          "id": "nybdimg",
          "author": "zugx2",
          "content": "get a zai glm coding plan, get it to work with claude it should be good enough.",
          "created_at": "2026-01-08T01:32:11Z",
          "was_summarised": false
        },
        {
          "id": "nyblnq1",
          "author": "Trennosaurus_rex",
          "content": "Just spend 20 dollars a month on Claude Code.",
          "created_at": "2026-01-08T02:15:12Z",
          "was_summarised": false
        },
        {
          "id": "nyd3mog",
          "author": "huzbum",
          "content": "I had a 12GB 3060, then added a CMP 100-210, then swapped that for a 3090.  On a 3060 you might be able to run a quant of Qwen3 Coder 30b REAP.  Probably better bang for you buck on a CMP 100-210 with 16GB for like $150, but you'd need to figure out cooling.  That's not really very close to Claude though.  \n\nTo get something on par with Claude, you'd want GLM 4.7 or MiniMax M2.1.  I use GLM with Claude Code with a [z.ai](http://z.ai) subscription.  It's great!  You'd need like 256GB of VRAM to run it though.  I was tempted by some old 8x V100 servers on Ebay for like $6k, but with all the RAM madness I think they are like $8k now.  Otherwise, you'd be looking at like 3 or 4 RTX Pro 6000 Blackwells for $8-10k each.  \n\nI think I've heard of people getting like 20tps with dual xeon server setups, but I'm not really interested in that, so I don't know much about it.",
          "created_at": "2026-01-08T08:15:22Z",
          "was_summarised": false
        },
        {
          "id": "nyeq19v",
          "author": "AnomalyNexus",
          "content": "\u0026gt;300 USD\n\nThink you're short a couple zeros",
          "created_at": "2026-01-08T15:03:57Z",
          "was_summarised": false
        },
        {
          "id": "nyau1yu",
          "author": "jonahbenton",
          "content": "You can plug a card into an egpu and plug that into an existing machine via thunderbolt. Even two cards/2 egpus. Works fine. \n\nYou will not get anywhere near Claude Code -\u0026gt; Anthropic cloud model performance with local hardware. But if there are 5 orders of magnitude of capability between nothing and CC, something like qwen 30b is at oom 2 or 2.5. It is definitely useful and a time saver. Just have to be careful what you ask it.",
          "created_at": "2026-01-07T23:52:52Z",
          "was_summarised": false
        },
        {
          "id": "nyblyqb",
          "author": "jstormes",
          "content": "I have been using Claude Code CLI, Gemini CLI, and Qwen Code CLI.\n\nThe open source setup I use is the the Qwen Code LLM.  I have a project that has a docker setup to run it on Ryzen AI Max+ 395 (aka StrixHalo), Ryzen 9 7940HS, and Ryzen 7 5700G.\n\nThe project is at https://github.com/jstormes/StrixHalo.   I give some of the token speed I get with the AMD hardware and how much context I can get.  I do use a Q4 for the lower hardware, but Q8 (Smarter???) for the StrixHalo.\n\nTake a look, hope it helps.\n\n# Performance Summary\n\n\n\n|System|GPU|RAM|Max Context|Prompt|Generation|\n|:-|:-|:-|:-|:-|:-|\n|Ryzen AI Max+ 395|Radeon 8060S|128GB|1M|\\~450 tok/s|\\~40 tok/s|\n|Ryzen 9 7940HS|Radeon 780M|64GB DDR5|512K|\\~30 tok/s|\\~31 tok/s|\n|Ryzen 7 5700G|Radeon Vega|64GB DDR4|256K|\\~74 tok/s|\\~13 tok/s|",
          "created_at": "2026-01-08T02:16:47Z",
          "was_summarised": false
        },
        {
          "id": "nyaq0r4",
          "author": "SeyAssociation38",
          "content": "I think that job is setting you up for failure. Are you ready to pay for strix halo with 128 gb of ram for $2200?",
          "created_at": "2026-01-07T23:32:12Z",
          "was_summarised": false
        },
        {
          "id": "nyazm4e",
          "author": "Irisi11111",
          "content": "You can't do that with the GPUs in the consumer market right now. Claude Opus 4.5 is really, really big â€“ at least a trillion parameters. Even if its weights leaked, and you could download them, most people and small to medium-sized companies still couldn't run it.\n\n\u0026gt;I have an old HP Z2 G4 Tower I use as virtualization server and was thinking of getting a 3060 12GB for \\~300 USD (locally).Â \n\nYou've hit some good things! For your first real test, maybe try using LMstudio. It's good for running a coding model locally, like the Qwen 3 series. See if that works for you. Sometimes, a local model can help write simple scripts or explain code. You can use your local LLM with the Claude agent.",
          "created_at": "2026-01-08T00:20:52Z",
          "was_summarised": false
        },
        {
          "id": "nyb1orw",
          "author": "HealthyCommunicat",
          "content": "Around $10,000 USD to buy an m3 ultra 512 gb ram. You can load in GLM 4.7, but even then itâ€™ll be at half the speed Claude Code is on average, and also wonâ€™t be exactly as smart.",
          "created_at": "2026-01-08T00:31:14Z",
          "was_summarised": false
        },
        {
          "id": "nyb5rfp",
          "author": "Such_Web9894",
          "content": "Praying for the days we can get improvements that itâ€™ll be possible on at least high-end consumer gpus",
          "created_at": "2026-01-08T00:51:41Z",
          "was_summarised": false
        },
        {
          "id": "nyba1no",
          "author": "ssrowavay",
          "content": "Yeah I was hopeful that I could use Continue with my 3060 12GB and get decent results on a personal open source project I work on occasionally. Compared to Claude Code, itâ€™s like asking your dumbest cousin versus the smartest person youâ€™ve ever met.",
          "created_at": "2026-01-08T01:13:44Z",
          "was_summarised": false
        },
        {
          "id": "nybau61",
          "author": "El_Danger_Badger",
          "content": "Build your own project and find out what you can do with what you have. Keep aiming for incremental improvements. \n\nI have my daily driver. M1 Mac Mini 16gb RAM. A nothing box . I run duplex blended  models for Chat reasoning, RAG, semantic memory, multi-node multi-graph LangBoard, web search to expand knowledge. All cuts remembered and referenced. A trading bot, with an agent ensemble that reasons on ticker symbols to analyze current patterns and forecast.  \n\nAll local. Slow, but not that slow. Everything works. Free. No api calls. No new hardware. Just hammering on the problems as they arise, until they get fixed. \n\nYou don't have one trillion dollars, so don't expect frontier level. Just see what you can do with what  and keep refining.",
          "created_at": "2026-01-08T01:17:56Z",
          "was_summarised": false
        },
        {
          "id": "nybubul",
          "author": "exaknight21",
          "content": "I use glm 4.7 + claude code. It works good for my use case. It was $20 a year for the first year. \n\nLocal Hosting for a good model is literally out of the question in this stupid market. We have to wait for the bubble to pop. Likely a year out still.",
          "created_at": "2026-01-08T03:00:26Z",
          "was_summarised": false
        },
        {
          "id": "nycdh48",
          "author": "Devcomeups",
          "content": "You want get anything even close worth using. Dont bother trying . Only model worth running locally is mini max.",
          "created_at": "2026-01-08T04:52:26Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.923256734Z"
    },
    {
      "flow_id": "",
      "id": "1q6c9wc",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/",
      "title": "DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.",
      "content": "arXiv:2501.12948 \\[cs.CL\\]: https://arxiv.org/abs/2501.12948",
      "author": "Nunki08",
      "created_at": "2026-01-07T10:49:12Z",
      "comments": [
        {
          "id": "ny6w58o",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-07T12:50:10Z",
          "was_summarised": false
        },
        {
          "id": "ny6ls96",
          "author": "qtvivies",
          "content": "https://preview.redd.it/t6ic0x3nywbg1.png?width=1965\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=ae5ef60128b8a0cf89351e8673ea41eddafb037c\n\nSomething interesting towards the end. Looks like someone forgot about this",
          "created_at": "2026-01-07T11:36:50Z",
          "was_summarised": false
        },
        {
          "id": "ny7c3yc",
          "author": "Ok_Technology_5962",
          "content": "Current research is linear attention. DeepSeek 3.2 with the cache optimization and now they had a massive paper come out that puts the linear into the whole modelaking it possible to train more than 60 layers. So yes this one is done. The compute and thinking will now happen internally in 1000 layers",
          "created_at": "2026-01-07T14:21:09Z",
          "was_summarised": false
        },
        {
          "id": "ny8iz9u",
          "author": "warnerbell",
          "content": "The original paper was light on implementation specifics. If they've added more on how they got the reasoning behavior to emerge, that's valuable.",
          "created_at": "2026-01-07T17:41:52Z",
          "was_summarised": false
        },
        {
          "id": "ny6gjni",
          "author": "ResidentPositive4122",
          "content": "New arch about to drop? dsv4 + r2? Packing all the goodies learned from last year. Hopefully they try smaller sizes as well. Would be interesting to see how the arch improvements work at several sizes.",
          "created_at": "2026-01-07T10:53:31Z",
          "was_summarised": false
        },
        {
          "id": "ny9ne3n",
          "author": "CryptoUsher",
          "content": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "created_at": "2026-01-07T20:38:03Z",
          "was_summarised": false
        },
        {
          "id": "nyae2ug",
          "author": "timfduffy",
          "content": "I think all this info was previously released as a [supplment to their R1 paper in Nature](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf).",
          "created_at": "2026-01-07T22:34:09Z",
          "was_summarised": false
        },
        {
          "id": "ny74o1r",
          "author": "jeffwadsworth",
          "content": "I was using the online chat version last night to improve a large Java class (40K tokens) with multiple methods.  It did so beautifully with zero issues in one shot.  The same task in my sub of Gemini 3 Pro chat interface failed in a few shots due to hallucinations.  They have really improved that model a lot from a year ago.",
          "created_at": "2026-01-07T13:40:40Z",
          "was_summarised": false
        },
        {
          "id": "nycauta",
          "author": "badgerbadgerbadgerWI",
          "content": "The level of detail they're releasing is remarkable. This kind of transparency is what pushes the whole field forward. Really interested in their distillation approach - getting smaller models to match larger ones' reasoning is key for edge deployment.",
          "created_at": "2026-01-08T04:35:42Z",
          "was_summarised": false
        },
        {
          "id": "ny9okeg",
          "author": "CryptoUsher",
          "content": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "created_at": "2026-01-07T20:43:14Z",
          "was_summarised": false
        },
        {
          "id": "nydlzki",
          "author": "Eyelbee",
          "content": "It's crazy it's only been one year, feels like ages",
          "created_at": "2026-01-08T11:00:27Z",
          "was_summarised": false
        },
        {
          "id": "ny6xeqo",
          "author": "Aggressive-Bother470",
          "content": "New grpo details perhaps? From reading the hf page it implied it was maybe light in that regard?",
          "created_at": "2026-01-07T12:58:03Z",
          "was_summarised": false
        },
        {
          "id": "ny76206",
          "author": "TelloLeEngineer",
          "content": "does arxiv have a diff UI?",
          "created_at": "2026-01-07T13:48:18Z",
          "was_summarised": false
        },
        {
          "id": "ny6y8mn",
          "author": "yoshiK",
          "content": "I did quickly throw the two papers into Gemini. It's really fun to live in the future. \n\n#Gemini summary: \n\nThe paper **\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\"** (arXiv:2501.12948) marks a significant milestone in open-source AI by demonstrating that advanced reasoning (similar to OpenAIâ€™s o1) can be achieved through large-scale Reinforcement Learning (RL) with minimal human-annotated data.\n\nThe two versions you provided represent the initial release (**v1**, Jan 22, 2025) and the latest updated version (which has been significantly expanded to **86+ pages** as of January 2026).\n\n### 1. Overall Paper Summary\nThe paper introduces two primary models:\n*   **DeepSeek-R1-Zero:** A model trained via \"pure RL\" (using the GRPO algorithm) starting directly from a base model without any Supervised Fine-Tuning (SFT). It demonstrates that reasoning behaviors like self-correction and reflection can emerge purely from reward signals.\n*   **DeepSeek-R1:** A more \"user-friendly\" version that uses a multi-stage pipeline (Cold-start SFT â†’ Reasoning RL â†’ Rejection Sampling/SFT â†’ General RL) to fix the \"readability\" and \"language mixing\" issues of R1-Zero while maintaining state-of-the-art reasoning performance.\n*   **Distillation:** The authors show that the reasoning patterns discovered by the 671B model can be distilled into smaller models (1.5B to 70B), allowing a 14B model to outperform much larger ones on math and coding benchmarks.\n\n---\n\n### 2. Comparison: Extensions in the New Version\nThe newer version is a massive technical expansion (growing from roughly 22 pages to over 85 pages). The key additions and extensions include:\n\n#### A. The \"Aha Moment\" Expansion (Section 2.2.1)\nThe new version provides a much deeper analysis of the **\"Aha Moment\"**â€”the point during RL training where the model unexpectedly learns to \"re-think\" its approach. The extension includes more qualitative examples and internal data showing the model's transition from linear solving to iterative self-correction without being prompted to do so.\n\n#### B. Detailed 4-Stage Training Pipeline\nWhile v1 outlined the stages, the new version details the specific composition of the **800k total training samples**:\n*   **Stage 1 (Cold Start):** Expanded details on the ~5,000-10,000 long CoT (Chain of Thought) samples used to \"prime\" the model.\n*   **Stage 3 (Rejection Sampling):** A deeper dive into how 600k reasoning-related and 200k non-reasoning samples were filtered and used to improve the model's general chat capabilities and prevent \"forgetting\" during the reasoning-heavy RL stages.\n\n#### C. Comprehensive Ablation Studies\nThe new version adds extensive \"What if?\" scenarios that were absent or brief in v1:\n*   **Distillation vs. RL:** New evidence explaining *why* distilling a large model's reasoning traces into a small model is more effective than training that small model directly with its own RL.\n*   **Base Model Impact:** Analysis of how different base models (DeepSeek-V3 vs. Qwen vs. Llama) respond to the R1 training recipe.\n\n#### D. Expanded \"Unsuccessful Attempts\" (Section 4.2)\nOne of the most valuable additions for researchers is the expanded section on what **did not work**. The new version elaborates on their failures with:\n*   **Process Reward Models (PRM):** Detailed reasons why step-level rewards were difficult to scale or prone to \"reward hacking\" compared to the outcome-based rewards used in R1.\n*   **Monte Carlo Tree Search (MCTS):** Technical explanation of why MCTS didn't provide the expected gains over simple RL in the context of LLM reasoning.\n\n#### E. New Benchmarks \u0026amp; Technical Specs\n*   **Updated Results:** Includes more recent evaluations on benchmarks like **AIME 2025**, **LiveCodeBench**, and specialized medical/legal reasoning tests.\n*   **Hyperparameters:** The new version includes exhaustive tables of training hyperparameters (learning rates, GRPO group sizes, KL divergence coefficients) which were previously withheld or summarized.\n\n### Summary Table\n| Feature | v1 (Original) | Latest Version (Extension) |\n| :--- | :--- | :--- |\n| **Page Count** | ~22 Pages | **86+ Pages** |\n| **Methodology** | High-level 4-stage overview | Granular detail on each stage (SFT, RL, Rejection Sampling) |\n| **Behaviors** | Mentions \"self-correction\" | Deep dive into \"Aha Moment\" with case studies |\n| **Failed Paths** | Brief mention of PRM/MCTS | Exhaustive analysis of why PRM and MCTS underperformed |\n| **Distillation** | Introduced 1.5B to 70B models | Added deep ablation on distillation efficiency and data filtering |\n| **Hyperparameters** | Partial/Summary | **Complete Technical Specs** for reproducibility |",
          "created_at": "2026-01-07T13:03:12Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q6c9wc",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://arxiv.org/abs/2501.12948",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T03:44:48.92329457Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.reddit.com/gallery/1q6c9wc: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923280221Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://arxiv.org/abs/2501.12948: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923294319Z"
        }
      ]
    }
  ]
}