{
  "blocks": [
    {
      "flow_id": "",
      "id": "1pwh0q9",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/",
      "title": "Best Local LLMs - 2025",
      "content": "***Year end thread for the best LLMs of 2025!***\n\n2025 is almost done! Its been **a wonderful year** for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!\n\n**The standard spiel:**\n\nShare what your favorite models are right now **and why.** Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.\n\n**Rules**\n\n1. Only open weights models\n\n\n\n*Please thread your responses in the top level comments for each Application below to enable readability*\n\n**Applications**\n\n1. **General**: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation\n2. **Agentic/Agentic Coding/Tool Use/Coding**\n3. **Creative Writing/RP**\n4. **Speciality**\n\nIf a category is missing, please create a top level comment under the Speciality comment\n\n\n\n**Notes**\n\nUseful breakdown of how folk are using LLMs: [https://preview.redd.it/i8td7u8vcewf1.png?width=1090\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d](https://preview.redd.it/i8td7u8vcewf1.png?width=1090\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d)  \n\n\nA good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)\n\n* Unlimited: \u0026gt;128GB VRAM \n* Medium: 8 to 128GB VRAM\n* Small: \u0026lt;8GB VRAM",
      "author": "rm-rf-rm",
      "created_at": "2025-12-26T22:31:28Z",
      "comments": [
        {
          "id": "nw3h6y7",
          "author": "rm-rf-rm",
          "content": "**GENERAL**",
          "created_at": "2025-12-26T22:31:44Z",
          "was_summarised": false
        },
        {
          "id": "nw61hxp",
          "author": "cibernox",
          "content": "I think having a single category from 8gb to 128gb is kind of bananas.",
          "created_at": "2025-12-27T09:41:22Z",
          "was_summarised": false
        },
        {
          "id": "nw5w4j6",
          "author": "GroundbreakingEmu450",
          "content": "How about RAG for technical documentation? Whats the best embedding/LLM models combo?",
          "created_at": "2025-12-27T08:48:38Z",
          "was_summarised": false
        },
        {
          "id": "nw42tlc",
          "author": "Amazing_Athlete_2265",
          "content": "My two favorite small models are Qwen3-4B-instruct and LFM2-8B-A1B. The LFM2 model in particular is surprisingly strong for general knowledge, and very quick. Qwen-4B-instruct is really good at tool-calling. Both suck at sycophancy.",
          "created_at": "2025-12-27T00:40:13Z",
          "was_summarised": false
        },
        {
          "id": "nw8na60",
          "author": "rainbyte",
          "content": "My favourite models for daily usage:\n\n- Up to 96Gb VRAM:\n  - GLM-4.5-Air:AWQ-FP16Mix (for difficult tasks)\n- Up to 48Gb VRAM:\n  - Qwen3-Coder-30B-A3B:Q8 (faster than GLM-4.5-Air)\n- Up to 24Gb VRAM:\n  - LFM2-8B-A1B:Q8 (crazy fast!)\n  - Qwen3-Coder-30B-A3B:Q4\n- Up to 8Gb VRAM:\n  - LFM2-2.6B-Exp:Q8\n  - Qwen3-4B-2507:Q8 (for real GPU, avoid on iGPU)\n- Laptop iGPU:\n  - LFM2-8B-A1B:Q8 (my choice when I'm outside without GPU)\n  - LFM2-2.6B-Exp:Q8 (better than 8B-A1B on some use cases)\n  - Granite4-350m-h:Q8\n- Edge \u0026amp; Mobile devices:\n  - LFM2-350M:Q8 (fast but limited)\n  - LFM2-700M:Q8 (fast and good enough)\n  - LFM2-1.2B:Q8 (a bit slow, but more smart)\n\nI recently tried these and they worked:\n\n- ERNIE-4.5-21B-A3B (good, but went back to Qwen3-Coder)\n- GLM-4.5-Air:REAP (dumber than GLM-4.5-Air)\n- GLM-4.6V:Q4 (good, but went back to GLM-4.5-Air)\n- GPT-OSS-20B (good, but need to test it more)\n- Hunyuan-A13B (I don't remember to much about this one)\n- Qwen3-32B (good, but slower than 30B-A3B)\n- Qwen3-235B-A22B (good, but slower and bigger than GLM-4.5-Air)\n- Qwen3-Next-80B-A3B (slower and dumber than GLM-4.5-Air)\n\nI tried these but didn't work for me:\n\n- Granite-7B-A3B (output nonsense)\n- Kimi-Linear-48B-A3B (couldn't make it work with vLLM)\n- LFM2-8B-A1B:Q4 (output nonsense)\n- Ling-mini (output nonsense)\n- OLMoE-1B-7B (output nonsense)\n- Ring-mini (output nonsense)\n\nTell me if you have some suggestion to try :)\n\nEDIT: I hope we get more A1B and A3B models in 2026 :P",
          "created_at": "2025-12-27T19:34:17Z",
          "was_summarised": false
        },
        {
          "id": "nw3hc33",
          "author": "rm-rf-rm",
          "content": "**Writing/Creative Writing/RP**",
          "created_at": "2025-12-26T22:32:33Z",
          "was_summarised": false
        },
        {
          "id": "nw4fsfh",
          "author": "Foreign-Beginning-49",
          "content": "Because I lived through the silly exciting wonder of teh tinyLlama hype I have fallen in with LFM2-1.2B-Tool gguf 4k quant at 750mb or so, this thing is like Einstein compared to tinlyllama, tool use and even complicated dialogue assistant possibilities and even basic screenplay generations it cooks on mid level phone hardware. So grateful to get to witness all this rapid change in first person view. Rad stuff. Our phones are talking back.¬†\n\n\nAlso wanna say thanks to qwen folks for all consumer gpu sized models like qwen 4b instruct and the 30b 3a variants including vl versions. Nemotron 30b 3a is still a little difficult to get a handle on but it showed me we are in a whole new era of micro scaled intelligence in little silicon boxes with it ability to 4x generation speed and huge context with llama.cpp on 8k quant cache settings omgg chefs kiss. Hopefully everyone is having fun and the builders are building and the tinkerers are tinkering and the roleplayers are going easy on their Ai S.O.'s Lol best of wishes",
          "created_at": "2025-12-27T02:01:38Z",
          "was_summarised": false
        },
        {
          "id": "nw3ha2d",
          "author": "rm-rf-rm",
          "content": "**Agentic/Agentic Coding/Tool Use/Coding**",
          "created_at": "2025-12-26T22:32:14Z",
          "was_summarised": false
        },
        {
          "id": "nw3qvbr",
          "author": "Don_Moahskarton",
          "content": "I'd suggest to change the small footprint category to 8GB of VRAM, to match many consumer level gaming GPU. 9 GB seems rather arbitrary.\nAlso the upper limit for the small category should match the lower limit for the medium category.",
          "created_at": "2025-12-26T23:28:22Z",
          "was_summarised": false
        },
        {
          "id": "nw7myme",
          "author": "OkFly3388",
          "content": "For whatewer reason, you set the average threshold at 128 GB, not 24 or 32 GB?   \n  \nIt's intuitive that smaller models work on mid-range hardware, medium on high-end hardware(4090/5090), and unlimited on specialized racks.",
          "created_at": "2025-12-27T16:30:11Z",
          "was_summarised": false
        },
        {
          "id": "nw3hcx4",
          "author": "rm-rf-rm",
          "content": "**Speciality**",
          "created_at": "2025-12-26T22:32:41Z",
          "was_summarised": false
        },
        {
          "id": "nwk6hcj",
          "author": "Aggressive-Bother470",
          "content": "Qwen3 2507 still probably the best at following instructions tbh.",
          "created_at": "2025-12-29T15:41:40Z",
          "was_summarised": false
        },
        {
          "id": "nw3hsae",
          "author": "MrMrsPotts",
          "content": "No math?",
          "created_at": "2025-12-26T22:35:07Z",
          "was_summarised": false
        },
        {
          "id": "nwvlxt1",
          "author": "Agreeable-Market-692",
          "content": "I'm not going to give vram or ram recommendations, that is going to differ based on your own hardware and choice of backend but a general rule of thumb is if it's f16 then it's twice the number of GB as it is parameters and if it's the Q8 then it's the same number of GB as it is parameters -- all of that matters less when you look at llamacpp or ik\\_llama as your backend.  \nAnd if it's less than Q8 then it's probably garbage at complex tasks like code generation or debugging.\n\nGLM 4.6V Flash is the best small model of the year, followed by Qwen3 Coder 30B A3B (there is a REAP version of this, check it out) and some of the Qwen3-VL releases but don't go lower than 14B if you're using screenshots from a headless browser to do any frontend stuff. The Nemotron releases this year were good but the datasets are more interesting. Seed OSS 36B was interesting.\n\nAll of the models from the REAP collection, Tesslate's T3 models are better than GPT-5 or Gemini3 for TailwindCSS, GPT-OSS 120B is decent at developer culture, the THRIFT version of MiniMaxM2 VibeStudio/MiniMax-M2-THRIFT is the best large MoE for code gen.\n\nQwen3 NEXT 80B A3B is pretty good but support is still maturing in llamacpp, althrough progress has accelerated in the last month.\n\nIBM Granite family was solid af this year. Docling is worth checking out too.\n\nKittenTTS is still incredible for being 25MB. I just shipped something with it for on device TTS. Soprano sounds pretty good for what it is. FasterWhisper is still the best STT I know of.\n\nQwen-Image, Qwen-Image-Edit, Qwen-Image-Layered are basically free Nano-Banana\n\nWan2.1 and 2.2 with LoRAs is comparable to Veo. If you add comfyui nodes you can get some crazy stuff out of them.\n\nZ-Image deserves a mention but I still favor Qwen-Image family.\n\nThey're not models, but they are model citizens of a sort... Noctrex and -p-e-w- deserve special recognition as two of the biggest most unsung heroes and contributors this year to the mission of LocalLLama.",
          "created_at": "2025-12-31T07:15:39Z",
          "was_summarised": false
        },
        {
          "id": "nwzs8d1",
          "author": "Illustrious_Big_2976",
          "content": "Honestly can't believe we went from \"maybe local models will be decent someday\" to debating if we've hit parity with GPT-4 in like 18 months\n\n  \nThe M2.1 hype is real though - been testing it against my usual benchmark of \"can it help me debug this cursed legacy codebase\" and it's actually holding its own. Wild times",
          "created_at": "2025-12-31T22:59:36Z",
          "was_summarised": false
        },
        {
          "id": "nx0uoxd",
          "author": "grepya",
          "content": "As someone with a  M1 Mac Studio with 32Gigs of RAM, can someone rate the best LLM's runnable on a reasonably spec'd  M series Mac?",
          "created_at": "2026-01-01T03:03:36Z",
          "was_summarised": false
        },
        {
          "id": "nw5rfns",
          "author": "NobleKale",
          "content": "\u0026gt; Useful breakdown of how folk are using LLMs: https://preview.redd.it/i8td7u8vcewf1.png?width=1090\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d\n\n'Games and Role Play'\n\n... cowards :D",
          "created_at": "2025-12-27T08:03:03Z",
          "images": [
            {
              "url": "https://preview.redd.it/i8td7u8vcewf1.png?width=1090\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nw7giwh",
          "author": "Lonhanha",
          "content": "Saw this thread, felt like it was a good place to ask and if anyone has a recommendation on a model to fine-tune using my groups chat data so that it learns the lingo and becomes an extra member of the group. What would you guys recommend?",
          "created_at": "2025-12-27T15:57:45Z",
          "was_summarised": false
        },
        {
          "id": "nwa86bp",
          "author": "Short-Shopping-1307",
          "content": "I want to use Claude as local LLM as we don‚Äôt have  better LLM then this for code",
          "created_at": "2025-12-28T00:46:31Z",
          "was_summarised": false
        },
        {
          "id": "nw4vip8",
          "author": "Short-Shopping-1307",
          "content": "How we can use Claude for coding in as local setup",
          "created_at": "2025-12-27T03:44:33Z",
          "was_summarised": false
        },
        {
          "id": "nw3ko6f",
          "author": "Busy_Page_4346",
          "content": "Trading",
          "created_at": "2025-12-26T22:51:34Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://preview.redd.it/i8td7u8vcewf1.png?width=1090\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:53:33.181287479Z"
    },
    {
      "flow_id": "",
      "id": "1mpk2va",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/",
      "title": "Announcing LocalLlama discord server \u0026amp; bot!",
      "content": "INVITE: https://discord.gg/rC922KfEwj\n\nThere used to be one old discord server for the subreddit but it was deleted by the previous mod.\n\nWhy?\nThe subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).\n\nWe have a discord bot to test out open source models.\n\nBetter contest and events organization.\n\nBest for quick questions or showcasing your rig!",
      "author": "HOLUPREDICTIONS",
      "created_at": "2025-08-13T23:21:05Z",
      "comments": [
        {
          "id": "n8of8m1",
          "author": "HOLUPREDICTIONS",
          "content": "Alt invite: [https://discord.gg/4R7xS5hMdN](https://discord.gg/4R7xS5hMdN)",
          "created_at": "2025-08-14T16:38:28Z",
          "urls": [
            {
              "url": "https://discord.gg/4R7xS5hMdN",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "n8qp4xq",
          "author": "BusRevolutionary9893",
          "content": "Discord, where valuable information goes to die and never be seen again.",
          "created_at": "2025-08-14T23:29:32Z",
          "was_summarised": false
        },
        {
          "id": "n8ms41x",
          "author": "TheMrCake",
          "content": "Please No.\n\nDiscord is a dystopian, gated version of what forums used to be.\nNo I don't want to give Discord all my data. This is peak enshittification.\n\nThis is yet another community where all information will be gated behind discords walls.\nWe as a society used to have real forums that were kept alive by donations, not VC money.",
          "created_at": "2025-08-14T11:22:55Z",
          "was_summarised": false
        },
        {
          "id": "n8z7nu8",
          "author": "shadow-studio",
          "content": "i'd prefer the \"more technical discussion\" to happen right here. discord isn't designed for keeping a knowlegde base that everyone can access freely. i'm ok with the idea of having a bot to test models, but otherwise i consider discord very inadequate and annoying for most things, just like whatsapp groups.\n\n\nalso, platform ensh!ttification is a thing.",
          "created_at": "2025-08-16T08:40:08Z",
          "was_summarised": false
        },
        {
          "id": "nc5tw7v",
          "author": "Appropriate_Cry8694",
          "content": "Discord is a cancer especially for open source projects, a lot of useful info simply dies with the server in the end.",
          "created_at": "2025-09-03T09:01:11Z",
          "was_summarised": false
        },
        {
          "id": "n8lsrr6",
          "author": "bephire",
          "content": "Would it be feasible to release regular archives of all messages/discussions in the server? We wouldn't want to lose valuable discussions and information that would otherwise become inaccessible if anything were to happen to the server.",
          "created_at": "2025-08-14T06:01:12Z",
          "was_summarised": false
        },
        {
          "id": "n8lzmct",
          "author": "Cosack",
          "content": "I do get what this is for, but still... you gotta appreciate the irony lol",
          "created_at": "2025-08-14T07:03:46Z",
          "was_summarised": false
        },
        {
          "id": "n9hjv2e",
          "author": "thedatawhiz",
          "content": "Not a good idea...",
          "created_at": "2025-08-19T06:43:49Z",
          "was_summarised": false
        },
        {
          "id": "n8mp0l9",
          "author": "a_beautiful_rhind",
          "content": "Discord asks for phone number and now face verification in some places.",
          "created_at": "2025-08-14T11:00:08Z",
          "was_summarised": false
        },
        {
          "id": "nc0z0i4",
          "author": "raysar",
          "content": "Be carefull discort is cool for friend but not for sharing public data. It's impossible to find information by searching on discord. Even bot can't do that XD",
          "created_at": "2025-09-02T15:12:51Z",
          "was_summarised": false
        },
        {
          "id": "nhaiysx",
          "author": "crantob",
          "content": "Discord sucks.  \n\nNext?",
          "created_at": "2025-10-02T01:36:16Z",
          "was_summarised": false
        },
        {
          "id": "na350q5",
          "author": "TheRealCookieLord",
          "content": "Cannot wait to invite the bot to multiple servers and make the heaviest thinking model contemplate life's meaning.",
          "created_at": "2025-08-22T15:11:34Z",
          "was_summarised": false
        },
        {
          "id": "n8krv0e",
          "author": "mrjackspade",
          "content": "Hopefully this server fairs better. The last server fucking sucked because the general chat was the landing room, 99%+ of the users only joined for tech support, and no one enforced the help channel rules because they didn't want to be \"mean\"\n\nSo at any given point in time the general chat was just being flooded by the same easy-to-google tech support questions, squashing any real conversation.",
          "created_at": "2025-08-14T01:41:28Z",
          "was_summarised": false
        },
        {
          "id": "n8o2667",
          "author": "Unigma",
          "content": "Could we add a few channels about Machine Learning and AI in general? For people that are training their own models and or creating projects with LLMs in it? I am working on some RL based fine-tuning, and wanted to know if there's a more technical oriented channel to discuss this.",
          "created_at": "2025-08-14T15:35:06Z",
          "was_summarised": false
        },
        {
          "id": "n8k52hw",
          "author": "Accomplished_Ad9530",
          "content": "Nice! Does the bot have retrieval capabilities? It might be useful for novices to have access to a chatbot with access to documentation for common inference engines and front ends, and perhaps localllama discussions. Sort of an interactive FAQ, which would help out with a lot of repeat posts here.",
          "created_at": "2025-08-13T23:30:05Z",
          "was_summarised": false
        },
        {
          "id": "n8kr3sa",
          "author": "TheRealSerdra",
          "content": "You may want to pin this for a few days, just to get some more activity",
          "created_at": "2025-08-14T01:36:54Z",
          "was_summarised": false
        },
        {
          "id": "n8mrbin",
          "author": "DeepWisdomGuy",
          "content": "Is this really running on a cerebras cluster?",
          "created_at": "2025-08-14T11:17:09Z",
          "was_summarised": false
        },
        {
          "id": "n8oc0xy",
          "author": "ilintar",
          "content": "Says the invite is invalid/expired.",
          "created_at": "2025-08-14T16:22:57Z",
          "was_summarised": false
        },
        {
          "id": "neh0mdc",
          "author": "boomboominkimspants",
          "content": "Isn‚Äôt that why they call it vibe coding tho?.. like you still checking that it‚Äôs meeting your vision, but you know enough and are engaged enough that u notices it‚Äôs messing up",
          "created_at": "2025-09-16T04:41:45Z",
          "was_summarised": false
        },
        {
          "id": "nj6lixo",
          "author": "the100rabh",
          "content": "Invite seems to be expired",
          "created_at": "2025-10-12T23:01:24Z",
          "was_summarised": false
        },
        {
          "id": "nofhibc",
          "author": "ceramic-road",
          "content": "Thanks for relaunching the official Discord!   \n  \nThe community surpassing half a million members, having a dedicated space for deeper technical discussion makes sense.  \n  \nFor those of us running llama.cpp or Ollama on modest hardware, will there be channels dedicated to optimization tips or hardware discussion? Already joined",
          "created_at": "2025-11-12T09:27:58Z",
          "was_summarised": false
        },
        {
          "id": "ntjjg1y",
          "author": "roculus",
          "content": "can you make the \"your post is getting popular\" discord ad spam a private message to the poster so we don't have to see it every time? Thanks!",
          "created_at": "2025-12-11T22:12:32Z",
          "was_summarised": false
        },
        {
          "id": "n9h8tm1",
          "author": "GodSpeedMode",
          "content": "This sounds awesome! It's great to see the LocalLLaMA community expanding and finding ways to foster more technical discussions. With 500k users, I totally get the need for a space that caters to those of us who want to dive deeper into the nitty-gritty. The bot for testing out open-source models is a fantastic addition‚ÄîI can't wait to try it out! I'm also really looking forward to the contests and events; it's nice to have a central hub for that. Just hopped on the Discord‚Äîlet‚Äôs see what everyone is working on!",
          "created_at": "2025-08-19T05:03:22Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1mpk2va",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://www.reddit.com/gallery/1mpk2va\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://discord.gg/rC922KfEwj",
          "was_fetched": true,
          "page": "Title: Join the LocalLLM Discord Server!\n\nURL Source: https://discord.gg/rC922KfEwj\n\nPublished Time: Tue, 06 Jan 2026 08:21:14 GMT\n\nWarning: This page maybe not yet fully loaded, consider explicitly specify a timeout.\n\nMarkdown Content:\nOpening Discord App.\n--------------------",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:53:49.565788618Z"
    },
    {
      "flow_id": "",
      "id": "1q5m2n6",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/",
      "title": "A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time",
      "content": "Hey r/LocalLLaMA,\n\nWe‚Äôre back with another **ShapeLearn** GGUF release ([Blog](https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/), [Models](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF)), this time for a model that *should not* feel this usable on small hardware‚Ä¶ and yet here we are:\n\n**Qwen3-30B-A3B-Instruct-2507** (device-optimized quant variants, llama.cpp-first).\n\nWe‚Äôre optimizing for TPS on a specific device without output quality falling off a cliff.\n\nInstead of treating ‚Äúsmaller‚Äù as the goal, we treat memory as a budget: Fit first, then optimize TPS vs quality.\n\nWhy? Because llama.cpp has a quirk: ‚ÄúFewer bits‚Äù does *not* automatically mean ‚Äúmore speed.‚Äù\n\nDifferent quant formats trigger different kernels + decode overheads, and on GPUs you can absolutely end up with **smaller and slower**.\n\n# TL;DR\n\n* Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieve **8.03 TPS** at 2.70 BPW, while retaining **94.18% of BF16 quality**.\n* Across devices, the pattern repeats: ShapeLearn tends to find better TPS/quality tradeoffs versus alternatives (we compare against Unsloth and MagicQuant as requested in our previous post).\n\n# What‚Äôs new/interesting in this one\n\n**1) CPU behavior is‚Ä¶ sane (mostly)**\n\nOn CPUs, once you‚Äôre past ‚Äúit fits,‚Äù **smaller tends to be faster** in a fairly monotonic way. The tradeoff curve behaves like you‚Äôd expect.\n\n**2) GPU behavior is‚Ä¶ quirky (kernel edition)**\n\nOn GPUs, performance depends as much on **kernel choice** as on memory footprint. So you often get **sweet spots** (especially around \\~4b) where the kernels are ‚Äúgolden path,‚Äù and pushing lower-bit can get weird.\n\n# Request to the community üôè\n\nWe‚Äôd *love* feedback and extra testing from folks here, especially if you can run:\n\n* different llama.cpp builds / CUDA backends,\n* weird batch sizes / context lengths,\n* real workloads (coding assistants, long-form, tool-ish prompts),\n* or non-NVIDIA setups (we‚Äôre aware this is where it gets spicy).\n\nAlso: we heard you on the previous Reddit post and are actively working to improve our evaluation and reporting. Evaluation is currently our bottleneck, not quantization, so if you have strong opinions on what benchmarks best match real usage, we‚Äôre all ears.",
      "author": "ali_byteshape",
      "created_at": "2026-01-06T15:45:12Z",
      "comments": [
        {
          "id": "ny41hn9",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-07T00:40:11Z",
          "urls": [
            {
              "url": "https://discord.gg/PgFhZ8cnWW",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny13qwu",
          "author": "Hot_Turnip_3309",
          "content": "You'll get double the tokens per second with Mamba2 hybrid transformers, aka nemotron-3-nano-30b-a3b",
          "created_at": "2026-01-06T16:24:09Z",
          "was_summarised": false
        },
        {
          "id": "ny1qvc1",
          "author": "florinandrei",
          "content": "nerds + AI = diagrams that are tacky as hell",
          "created_at": "2026-01-06T18:08:42Z",
          "was_summarised": false
        },
        {
          "id": "ny1189p",
          "author": "iKy1e",
          "content": "This sounds amazing! I‚Äôll have dig into the details later when I have more time, but really wanted to say this sort of low level optimism finding ways to squeeze more performance until smaller devices is amazing! I love reading about research like this.",
          "created_at": "2026-01-06T16:12:38Z",
          "was_summarised": false
        },
        {
          "id": "ny1vp6d",
          "author": "pgrijpink",
          "content": "Exciting and disappointing at the same time. If I‚Äôm not mistaken, your algorithm is not open source? So not as useful.",
          "created_at": "2026-01-06T18:30:02Z",
          "was_summarised": false
        },
        {
          "id": "ny1akwa",
          "author": "Odd-Ordinary-5922",
          "content": "can you guys try doing this with nemotron?",
          "created_at": "2026-01-06T16:55:12Z",
          "was_summarised": false
        },
        {
          "id": "ny17azk",
          "author": "bigh-aus",
          "content": "I wonder if this could be combined with an exo like solution to run on a cluster of pis...  I'm still pretty dumb with these models, but it makes me wonder if the MOE can be spread across pis.\n\n\u0026gt;Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieve¬†**8.03 TPS**¬†at 2.70 BPW, while retaining¬†**94.18% of BF16 quality**.\n\nI'm assuming that means it's a 4 bit quant...",
          "created_at": "2026-01-06T16:40:25Z",
          "was_summarised": false
        },
        {
          "id": "ny1j0za",
          "author": "frozen_tuna",
          "content": "That's actually insane. Well done!",
          "created_at": "2026-01-06T17:33:42Z",
          "was_summarised": false
        },
        {
          "id": "ny2lof4",
          "author": "Watchforbananas",
          "content": "I like the improved graphs on your blog that now shows the exact quant from unsloth and magicQuant on hover (and that you added comparisons with magicQuant in the first place). Much easier to pick an interesting quant based on what I've tried before. Could you perhaps do the same for your quants? Just so I don't have to search for the lookup table. \n\nI also appreciate the 4080 results.\n\nAny plans to update the graphs for the other models as well? I've never managed to quite figure out what unsloth quants your graphs for qwen3-4B-2507 referenced.",
          "created_at": "2026-01-06T20:28:31Z",
          "was_summarised": false
        },
        {
          "id": "ny1085p",
          "author": "dodiyeztr",
          "content": "Would this work on raspi 5 8GB?",
          "created_at": "2026-01-06T16:08:04Z",
          "was_summarised": false
        },
        {
          "id": "ny1gkmz",
          "author": "xandep",
          "content": "Already using the Instruct version and I liked. IQ-3 is about the same size / speed of a ptbr-REAP-16B of the original model that I use, and  initially it seems your model performs better.",
          "created_at": "2026-01-06T17:22:27Z",
          "was_summarised": false
        },
        {
          "id": "ny1rgw7",
          "author": "AvocadoArray",
          "content": "Just finished reading the blog post, nice work!\n\nWould love to see you do Seed-OSS 36B next.",
          "created_at": "2026-01-06T18:11:20Z",
          "was_summarised": false
        },
        {
          "id": "ny3s3pm",
          "author": "geerlingguy",
          "content": "Tested on my Pi 5, had to set context to `-c 4096` before it would run without segfaulting after model loading. But ran with:\n\n```\n./build/bin/llama-cli -m \"models/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf\" -c 4096 -e --no-mmap -t 4\n```\n\nAnd for a few prompts, it gave between 10-11 t/s prompt processing, and 4-8 t/s token generation (lower with much larger outputs, but on average it was around 7 t/s).\n\nImpressive! Qwen3 30B MoE is much more useful than like llama 3.2:3b on a Pi, though the 16GB Pi 5 is a bit more rare.",
          "created_at": "2026-01-06T23:51:50Z",
          "was_summarised": false
        },
        {
          "id": "ny1c2iv",
          "author": "Sensitive_Sweet_1850",
          "content": "wow. you should try nemotro too",
          "created_at": "2026-01-06T17:01:55Z",
          "was_summarised": false
        },
        {
          "id": "ny1zrry",
          "author": "pmttyji",
          "content": "Nice. I tried your Qwen3-4B-Q5\\_K\\_S which gave me 20 t/s same as what other provider's Q4 given me on CPU-Only performance.\n\nHope your backlog has 12-14-15B models which are better \u0026amp; useful for 8GB VRAM. Ex: Qwen3-14B's Q4\\_K\\_M(8.4GB) won't fit inside VRAM which gave me just 5 t/s. Then I picked IQ4\\_XS(7.5GB) which gave me 20+ t/s.",
          "created_at": "2026-01-06T18:48:21Z",
          "was_summarised": false
        },
        {
          "id": "ny3n0s3",
          "author": "Other_Hand_slap",
          "content": "i was able to run llama forninference on \n* 13th gen i3\n * 16g ram\n* nvidia 3060 ti with 8g\n\nwith a confident fair rate of 20 tokens/s",
          "created_at": "2026-01-06T23:25:10Z",
          "was_summarised": false
        },
        {
          "id": "ny0zs20",
          "author": "professormunchies",
          "content": "sounds promising and pretty cool. I'll give it a try today with cline and continue.dev.\n\nI've been running this on some smaller hardware: [https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit](https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit)\n\nI like that they specify which dataset was used for calibrating the quants. Would be nice if you guys divulged such information. As far as evals go, definitely checkout the nemotron collection, lots of good datasets: [https://huggingface.co/collections/nvidia/nemotron-post-training-v3](https://huggingface.co/collections/nvidia/nemotron-post-training-v3)",
          "created_at": "2026-01-06T16:05:59Z",
          "urls": [
            {
              "url": "https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://huggingface.co/collections/nvidia/nemotron-post-training-v3",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny12civ",
          "author": "_raydeStar",
          "content": "Would it work on a Pi 5 with 8GB?  Do I need that AI hat that they're offering?\n\nI'll give it a shot if I can, I love projects like this.  I'm REALLY interested in something like VL, to build a home automation system.",
          "created_at": "2026-01-06T16:17:45Z",
          "was_summarised": false
        },
        {
          "id": "ny1b0uq",
          "author": "xandep",
          "content": "Any plans on Thinking model?",
          "created_at": "2026-01-06T16:57:12Z",
          "was_summarised": false
        },
        {
          "id": "ny1lkgb",
          "author": "siegfried2p",
          "content": "which quant is better for moe model with expert to cpu scenario?",
          "created_at": "2026-01-06T17:45:20Z",
          "was_summarised": false
        },
        {
          "id": "ny25vpy",
          "author": "SlavaSobov",
          "content": "Bitchin' work would be cool to see if the 8GB Jetson Orin Nano could get some improvements. \n\nNVIDIA basically says here's some basic old models and lets it languish. It's Ampere so maybe could have some gains.\n\nI stuck to mostly 4B models but feels like there's more potential there. \n\nI'm just a dumb hobbyist who knows enough to break things though. üòÖ",
          "created_at": "2026-01-06T19:15:46Z",
          "was_summarised": false
        },
        {
          "id": "ny3sngb",
          "author": "nickgeorgiou",
          "content": "I thought this was Agario",
          "created_at": "2026-01-06T23:54:39Z",
          "was_summarised": false
        },
        {
          "id": "ny42z5f",
          "author": "DarkGeekYang",
          "content": "Nice effort. Will you consider adding more models like qwen3vl to your repo?",
          "created_at": "2026-01-07T00:47:54Z",
          "was_summarised": false
        },
        {
          "id": "ny2wsyh",
          "author": "Noiselexer",
          "content": "Time to first token: 2 minutes",
          "created_at": "2026-01-06T21:19:33Z",
          "was_summarised": false
        },
        {
          "id": "ny26hgr",
          "author": "Chromix_",
          "content": "In the Intel i7 section of the blog post the Unsloth Q5\\_K\\_M quant gets a better test score than the Q8\\_K\\_XL. So either that quant won the lottery or the benchmark results are more noisy than it looks like and don't really tell us that much with results being that close together. It'd be great to see more accurate results that prove that this method delivers smaller ( = faster) quants at the same quality level, but benchmarking this is difficult, [as written before](https://www.reddit.com/r/LocalLLaMA/comments/1pj7wjd/we_did_years_of_research_so_you_dont_have_to/ntbiyem/?context=3#ntbiyem). Maybe repeat each run a few times to get a better idea of the variance? Or check how many right/wrong answers flip between each run, to get an idea of the magnitude of randomness involved?",
          "created_at": "2026-01-06T19:18:32Z",
          "urls": [
            {
              "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pj7wjd/we_did_years_of_research_so_you_dont_have_to/ntbiyem/?context=3#ntbiyem",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/",
          "was_fetched": true,
          "page": "Title: A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time\n\nURL Source: https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/\n\nPublished Time: Tue, 06 Jan 2026 14:39:12 GMT\n\nMarkdown Content:\nA 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time\n===============\n\n[](https://byteshape.com/index.html#introduction)\n\n*   [Home](https://byteshape.com/index.html#introduction)\n*   [Technologies](https://byteshape.com/index.html#technologies)\n*   [Meet the Team](https://byteshape.com/index.html#team)\n*   [Blog](https://byteshape.com/blogs/)\n*   [Contact Us](https://byteshape.com/index.html#contact)\n\nA 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶\n\n and Runs in Real Time\n===================================================================\n\nPublished by ByteShape Team ‚Ä¢ January 5, 2026\n\nFor this release, we optimize for what people actually experience when they run a model: **fast, high-quality responses on a specific target device**.\n\nWe use Shapelearn, our bitlength learning method to choose weight datatypes for **Qwen3-30B-A3B-Instruct-2507** that maximize performance in terms of tokens per second (TPS) and output quality, with one practical constraint: the model must fit comfortably in the available memory. Once it fits, making the file smaller isn't a goal by itself. We only shrink further when it also improves the real tradeoff people care about: **speed vs. quality**.\n\nApproaching bitlength learning this way matters because in llama.cpp, \"fewer bits\" doesn't automatically mean \"more speed.\" Different quantization formats can trigger different kernels and overheads, and on some GPUs, **going lower-bit can even get slower**, despite using less memory.\n\n**Bottom line:** treat memory as a **budget** to meet, then optimize what matters most: **TPS and quality**.\n\n[Try Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF)\n\nTL;DR\n-----\n\nYes, this 30B Qwen3 runs on a Raspberry Pi. On a Pi 5 (16GB), \n```\nQ3_K_S-2.70bpw [KQ-2]\n```\n hits 8.03 TPS at 2.70 BPW and maintains 94.18% of BF16 quality. It genuinely feels real-time. More broadly, the same pattern shows up everywhere else: ByteShape models give you a better TPS/quality tradeoff than the alternatives (here we look at Unsloth and MagicQuant).\n\nCPUs\n----\n\nOn CPUs, the reducing footprint via shorter bitlengths affects the TPS and accuracy tradeoff as one would expect: once the model fits, reducing footprint tends to increase TPS in a fairly monotonic way. If datatypes are selected correctly, you can trade a bit of quality for speed predictably, which makes it much easier to pick a point on the curve that matches your constraints.\n\nWe'll start with the most memory-constrained CPU case (Raspberry Pi 5 16GB), where \"fits in RAM\" is the limiting factor, then move to an Intel i7 with 64GB, where everything fits.\n\n### Raspberry Pi 5\n\nThe figure below shows TPS vs. normalized accuracy for the models that fit in RAM on the Raspberry Pi 5 16GB.\n\n Raspberry Pi 5: Tokens per second vs quality (bubble size = model footprint) \n\n Raspberry Pi 5: Tokens per second vs quality (bubble size = model footprint) \n\nNotably, sustaining **8.5 TPS at 92%+ baseline accuracy** with a 30B model on a Raspberry Pi reshapes expectations for Pi-class systems. Overall, the trend shows that ShapeLearn consistently produces better models, with ByteShape trending up and to the right of Unsloth, achieving higher tokens per second at the same quality, or higher quality at the same throughput.\n\nWe highlight choices for two primary objectives: accuracy or response time.\n\n*   **Optimizing for response time while maintaining accuracy:** For interactive, on-device use, perceived responsiveness is driven by how quickly text appears, not peak throughput. In practice, generation feels real-time once it reaches roughly **8 TPS**, comfortably above typical reading speed. In this Raspberry Pi real-time regime, \n```\nQ3_K_S-2.70bpw [KQ-2]\n```\n (2.70 BPW, 8.03 TPS, 94.18% accuracy) is our go-to recommendation: it crosses the real-time threshold while maintaining high accuracy. Compared to Unsloth models at similar quality, ByteShape achieves real-time performance at lower BPW and higher TPS, making it the more efficient choice for interactive edge deployment. \n*   **Accuracy above all:** The table below lists the models that achieve the highest accuracy while still being able to run on a Raspberry Pi. Within this set, ByteShape models make the best use of the available resources to maximize accuracy, occupying the **lowest-error rows** (~1.1‚Äì1.3% relative error, ~98.8% accuracy), while the strongest Unsloth entries remain around 2.1‚Äì2.2% error (~97.9% accuracy). Compared to Unsloth's `UD-Q3_K_XL [8]`, ByteShape achieves up to a **1.87√ó lower error rate** while still operating at **~5‚Äì6 TPS**, comfortably within TPS-norms on Raspberry PI making it the better choice when accuracy is the priority. \n\n Even when prioritizing maximum speed with some reduction in accuracy, \n```\nQ3_K_S-3.25bpw [KQ-5]\n```\n offers a better tradeoff: **more accurate, smaller, and faster** than the fastest Unsloth model. \n\n| Model | Relative Error | BPW | TPS |\n| --- | --- | --- | --- |\n| ``` Q4_K_S-3.92bpw [KQ-7] ``` | 1.14% | 3.92 | 5.30 |\n| ``` Q4_K_S-3.61bpw [KQ-6] ``` | 1.25% | 3.61 | 5.94 |\n| ``` Q3_K_S-3.25bpw [KQ-5] ``` | 2.03% | 3.25 | 6.68 |\n| `UD-IQ3_XXS [6]` | 2.22% | 3.38 | 5.03 |\n| `UD-Q3_K_XL [8]` | 2.13% | 3.62 | 6.28 |\n\nMany other Unsloth and MagicQuant models (some of ours too!) are not in this chart. We compare them in other sections, but they're not applicable in the Raspberry Pi case. They simply don't fit!\n\n### Intel i7\n\nNext, we move to the Intel i7 with 64GB RAM. The figure below shows TPS vs normalized accuracy for all models.\n\n Intel i7: Tokens per second vs quality (bubble size = model footprint) \n\n Intel i7: Tokens per second vs quality (bubble size = model footprint) \n\nOverall, ByteShape models outperform both Unsloth and MagicQuant, delivering higher quality at comparable throughput using fewer bits per parameter. Only ByteShape offers models that run in the 26+ TPS range, extending performance well beyond the other methods.\n\n**Highlights:**\n\n*   **Quality-first:** At the high-accuracy end of the table, \n```\nIQ4_XS-4.67bpw [KQ-9]\n```\n achieves the lowest relative error (0.25%), outperforming the best-running Unsloth models (`Q6_K [20]` and `Q5_K_M [18]` whose relative errors are 0.36% and 0.44%). Compared directly, ByteShape delivers up to a 1.44√ó lower error rate with higher throughput than `Q6_K [20]`, and a 1.76√ó lower error rate at essentially the same speed as `Q5_K_M [18]`. MagicQuant `mxfp4 [3]` trails in this regime, with both higher error and lower TPS. \n*   **Balanced point:** In the mid-accuracy, high-throughput region, \n```\nQ3_K_S-3.25bpw [KQ-5]\n```\n combines ~98% accuracy with 23.1 TPS at just 3.25 BPW, offering the best overall balance in the table. Matching or exceeding this accuracy with Unsloth (`IQ4_XS [10]`) requires higher BPW and lower TPS, while choosing an Unsloth model closer in speed (`Q3_K_S [7]`) incurs a 1.73√ó higher error rate. MagicQuant does not offer a competitive model in this range; its fastest entry (`IQ4_NL [2]`) is behind both ByteShape and Unsloth in accuracy and throughput. \n\n**Takeaway:** Across both quality-first and balanced settings, ByteShape consistently converts the available bit budget into either higher accuracy or higher TPS, and is the only approach that simultaneously covers the high-quality and 26+ TPS balanced-performance regions in this comparison.\n\nGPUs: RTX5090/32GB and RTX4080/16GB\n-----------------------------------\n\nOn GPUs, performance depends as much on **kernel choice** as on raw memory footprint. For matmul/matvec, llama.cpp's quantization-specific GPU decode paths incur very different overheads, so fewer bits per weight do **not** reliably translate to higher TPS. Instead, TPS often peaks at quantization-specific sweet spots. Pushing BPW lower can even **increase VRAM traffic and instruction count**, hurting performance rather than improving it. We dig into this behavior in more detail right after the GPU results section, where the kernel-level tradeoffs become more apparent.\n\nWe evaluate on two GPUs: an **RTX 5090 (32 GB)**, which can run models **above 4 BPW** and typically reach the fastest sweet spots, and an **RTX 4080 (16 GB)**, where **\u003e4 BPW models do not fit**, forcing different trade-offs and making the device-optimized curve easier to see.\n\n### RTX 5090 (32GB of VRAM)\n\nLet's start with the 5090, which has enough VRAM to support all of the quantized models. The figure below shows TPS vs normalized accuracy.\n\n RTX 5090: Tokens per second vs quality (bubble size = model footprint) \n\n RTX 5090: Tokens per second vs quality (bubble size = model footprint) \n\nTwo things stand out immediately:\n\n First, this GPU shows a clear **~4-bit sweet spot**: several ~4b models cluster at very high TPS with nearly identical quality. Examples include `Unsloth Q4_0 [12]`, `Unsloth IQ4_XS [10]`, \n```\nIQ4_XS-3.87bpw [IQ-6]\n```\n , and MagicQuant `iq4_nl-EHQKOUD-IQ4NL [1]`, all running around ~302‚Äì303 TPS at ~98.4‚Äì98.9% accuracy. Within this tight cluster, Unsloth edges out slightly in throughput and quality.\n\nSecond, outside of that sweet spot, the tradeoff becomes much more uneven:\n\n*    Many other Unsloth and Magic Quant models show **significantly lower TPS**, regardless of whether they are quantized more or less aggressively. \n*    Past the ~4b region, only ByteShape continues to increase TPS with a more predictable reduction in quality. \n\n**Accuracy-critical workloads:** when output quality is paramount, ByteShape delivers the most accurate model on the 5090: \n```\nIQ4_XS-4.67bpw [IQ-8]\n```\n (4.67 BPW, 272.98 TPS, 99.75% accuracy). It surpasses `Unsloth Q6_K [20]` (6.57 BPW, 264.88 TPS, 99.64% accuracy) while using fewer bits and achieving slightly higher throughput, and it clearly outperforms MagicQuant `mxfp4_moe-H-B16-EUR-IQ4NL-KO-Q5K-QD-Q6K [3]` (5.46 BPW, 240.42 TPS, 99.32% accuracy) in both accuracy and speed, making it the strongest choice when accuracy is a task-critical deployment requirement.\n\n**Practical takeaway.** If your GPU has enough VRAM to run a strong ~4b model that already meets your speed and accuracy requirements, that cluster is an excellent default. The curve becomes more interesting when task-critical deployment constraints demand higher accuracy or smaller models as for example, under tighter memory budgets or constrained environments (as we'll see on the 4080).\n\n### RTX 4080 (16GB of VRAM)\n\nNext, let's move to a more accessible GPU, especially in these memory-challenged times. The biggest stumbling block for the 4080 is its 16GB of VRAM, which is not sufficient to support the \"magical\" ~4b quantizations for a 30B model. How convenient!This \"avoids\" the 5090's ~4b sweet spot and forces a more \"real-world\" comparison under a hard VRAM budget. The figure below shows TPS versus normalized accuracy for all models that fit on the 4080.\n\n RTX 4080: Tokens per second vs quality (bubble size = model footprint) \n\n RTX 4080: Tokens per second vs quality (bubble size = model footprint) \n\nOn the RTX 4080, ByteShape consistently outperforms Unsloth under the same 16 GB VRAM constraint, delivering a better TPS‚Äìquality tradeoff.\n\nIn particular, ByteShape's highest-quality model that fits, \n```\nIQ4_XS-3.87bpw [IQ-6]\n```\n (3.87 BPW, 214.81 TPS, 98.66% accuracy) delivers:\n\n*    a 1.59√ó lower error rate and 9.4% higher TPS vs. `Unsloth Q3_K_XL [8]` (3.62 BPW, 196.42 TPS, 97.87% accuracy). \n*    a 2.54√ó lower error rate at the same TPS vs. `Unsloth IQ2_M [2]` (2.84 BPW, 214.79 TPS, 96.59% accuracy). \n\nAs we move to higher throughput, ByteShape's maintains accuracy, while Unsloth's error rate experiences a cliff.\n\n### The Elephant in the Room: When 3-bits is not just 3-bits\n\nThere is an inconvenient truth hiding in these results. On several setups, around 4 bpw is already flying, and pushing quantization harder does not make things faster. It just manages to be smaller and slower at the same time.\n\nReducing the size of data doesn't automatically speed things up. While using fewer bits to store each number seems like it should reduce memory traffic and speed up computation, GPUs don't work that way. NVIDIA GPUs process work in fixed groups of 32 threads called \"warps,\" which move through instructions together in near lock-step. The GPU hardware is optimized for specific data formats, memory access patterns, and operations that the chip's circuits are physically designed to handle efficiently. When your workload matches these \"golden paths\", you get peak performance. Step outside them, and you hit slowdowns. This isn't a design flaw, it's a deliberate tradeoff. Supporting more flexibility would require additional circuitry: more wires, more transistors, more complexity. That extra hardware consumes more power and adds latency to every operation, whether a program needs that flexibility or not.\n\nHere a few examples of relevant hardware \"quirks\": VRAM is read in aligned 32-byte blocks, so reading one or 32 bytes consumes the same memory bandwidth. Both on-chip and off-chip memories can also suffer contention depending on how data is laid out, meaning that a warp's accesses may complete in a single step or, in the worst case, be serialized into 32 steps. And of course, decoding quantized values before computation can require extra instructions, with the cost depending on the quantization scheme.\n\nThis explains the behaviour we observe: 4-bit kernels use VRAM bandwidth more efficiently than 3- or 2-bit kernels and require fewer decode steps before computation. At the same time, 4-bit kernels exploit subword parallelism just as effectively as lower-bit kernels, and all rely primarily on dynamic caches rather than shared memory to take advantage of data reuse when possible.\n\nSo why llama.cpp hasn't been optimized to deliver peak speed for **every** bit-length? Our understanding is that llama.cpp prioritizes **portable, space-efficient quantization** that can run across a wide range of hardware. That design goal limits how aggressively backends can reshape data layouts or reorder computation in ways that might help one GPU or one bit-width.\n\nA key example is its choice to store quantized weights in fixed blocks of 256 values. Each block is self-contained (it carries everything needed to decode it) and sits at a simple, predictable offset in the tensor, which makes the format easy to implement and fast to locate.\n\nThe tradeoff is that GPUs often need to **decode many blocks in parallel** to keep their wide compute units busy. With many independent 256-value blocks, those parallel decodes can translate into more scattered or fragmented VRAM reads and extra decode overhead, reducing bandwidth efficiency, especially for some lower-bit formats.\n\n**Point for example on RTX 5090:** a matrix multiply [256, 768] √ó [768, 2048] takes **~54¬µs with**`iq4_xs`datatype, but **~62¬µs with**`iq3_xxs`(mul_mat_q()+mul_mat_q_stream_k_fixup()). In other words, **cutting nearly 1.2 bits per weight**(a reduction of more than 25% in weight footprint) leads to a **~13% slowdown**, directly hurting user experience.\n\nAn excellent reminder that bitlength learning matters: Heuristics can get us part of the way, but not all the way. ShapeLearn makes deliberate, per-tensor datatype choices that improve speed without sacrificing accuracy.\n\nMethodology (brief recap)\n-------------------------\n\nIf you're wondering how we are scoring these points, the full methodology is discussed in our previous [blog post](https://byteshape.com/blogs/Qwen3-4B-I-2507/). This post is intentionally focused on the curves and device tradeoffs, so here is the quick version.\n\nFor each quantized variant, we measure **throughput (TPS)** on the target device and compute a single **normalized quality** score relative to the **BF16 baseline**, using the same evaluation harness and prompts as the methodology post. The quality score aggregates standard benchmarks (MMLU, GSM8K, IFEval, LiveCodeBench V4) into one number so you can compare points directly. In other words, every dot in the plots answers two questions: how fast does it run on this device, and how much quality does it retain compared to BF16, with memory fit as the first constraint.\n\nWe also want to thank all for the many, excellent suggestions on our recent Reddit post for improving and extending this evaluation strategy, and we‚Äôre actively working through them. Right now, evaluation is the main bottleneck and not bitlength learning/quantization. Careful evaluation is essential to clearly communicate the strengths of each model.\n\nWrapping up\n-----------\n\n**First, thank you for your tenacity.** You made it through all of this without giving up. We are sincerely flattered!\n\n**The takeaway is simple:** treat **memory as a constraint, not a goal**. Once a model fits on your device, what matters is the tradeoff curve, **TPS versus quality**. Across CPUs and GPUs, **ByteShape** consistently lands on the better side of that curve, delivering either **more speed at the same quality** or **higher quality at the same speed**.\n\nIf you're deploying on a **Raspberry Pi 5 (16 GB)** and want a genuinely interactive experience, start with \n```\nQ3_K_S-2.70bpw [KQ-2]\n```\n . On larger CPUs or GPUs, you can move up the curve toward higher-quality points with little loss in throughput, the same rule applies: **fit first, then optimize the tradeoff**.\n\nWe'll keep releasing more device-targeted variants (and more plots). If your system can't run a 30B model smoothly, don't blame the model or the silicon. **Blame the datatypes.**\n\n[Hugging Face](https://huggingface.co/byteshape)[X](https://x.com/ByteShape)[LinkedIn](https://www.linkedin.com/company/byteshape)[Reddit](https://www.reddit.com/r/ByteShape/)\n\n¬© 2025 ByteShape. All rights reserved.\n\n√ó",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF",
          "was_fetched": true,
          "page": "Title: byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF\n\nMarkdown Content:\n[](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#qwen3-30b-a3b-instruct-2507-gguf-shapelearn-quantized) Qwen3-30B-A3B-Instruct-2507 GGUF (ShapeLearn Quantized)\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThis is a GGUF-quantized version of Qwen3-30B-A3B-Instruct-2507 produced with **ByteShape's ShapeLearn**, which learns the optimal datatype per tensor to maintain high quality even at very low bitlengths.\n\nTo learn more about ShapeLearn and to see detailed benchmarks across GPUs, CPUs, and even the Raspberry Pi, please visit our [blog](https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/).\n\nIf you have questions or want to share feedback, reach us on [Reddit](https://www.reddit.com/r/ByteShape/).\n\n[](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#how-to-pick-a-model) How to Pick a Model\n-------------------------------------------------------------------------------------------------------------\n\nWe provide **CPU and GPU optimized variants** for llama.cpp:\n\n*   **CPUs:** Models labeled as KQ, optimized for CPU inference with predominantly KQ quantization.\n*   **GPUs:** Models labeled as IQ, optimized for GPU inference with a hybrid approach combining KQ and IQ quantization for better throughput.\n\nEach hardware target includes a range of models covering different size and quality tradeoffs.\n\nThe charts below show **quality vs tokens per second** for each device, comparing ShapeLearn models with Unsloth or MagicQuant baselines.\n\n**Selection rule:** Choose the model with the highest quality at your target throughput or the fastest model that still meets your required quality.\n\n### [](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#cpu-models) CPU Models\n\n[](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/img/Intel.png)\n\n**Table sorted by model size** (match the chart numbers to model IDs):\n\n| Model ID | Bits/Weight | Model Size | Normalized Quality |\n| --- | --- | --- | --- |\n| [KQ-1](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.66bpw.gguf) | 2.66 | 10.2 GB | 92.84% |\n| [KQ-2](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf) | 2.70 | 10.3 GB | 94.18% |\n| [KQ-3](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.85bpw.gguf) | 2.85 | 10.9 GB | 95.49% |\n| [KQ-4](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.18bpw.gguf) | 3.18 | 12.1 GB | 96.97% |\n| [KQ-5](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf) | 3.25 | 12.4 GB | 97.97% |\n| [KQ-6](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.61bpw.gguf) | 3.61 | 13.8 GB | 98.75% |\n| [KQ-7](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.92bpw.gguf) | 3.92 | 14.9 GB | 98.86% |\n| [KQ-8](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.41bpw.gguf) | 4.41 | 16.9 GB | 99.34% |\n| [KQ-9](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf) | 4.67 | 17.8 GB | 99.75% |\n\n### [](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#gpu-models) GPU Models\n\n[](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/img/RTX5090.png)\n\n**Table sorted by model size** (match the chart numbers to model IDs):\n\n| Model ID | Bits/Weight | Model Size | Normalized Score |\n| --- | --- | --- | --- |\n| [IQ-1](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ3_S-2.69bpw.gguf) | 2.69 | 10.3 GB | 94.24% |\n| [IQ-2](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ3_S-2.75bpw.gguf) | 2.75 | 10.5 GB | 95.48% |\n| [IQ-3](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ3_S-3.02bpw.gguf) | 3.02 | 11.5 GB | 95.83% |\n| [IQ-4](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ3_S-3.29bpw.gguf) | 3.29 | 12.5 GB | 97.35% |\n| [IQ-5](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-3.63bpw.gguf) | 3.63 | 13.9 GB | 97.74% |\n| [IQ-6](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-3.87bpw.gguf) | 3.87 | 14.8 GB | 98.66% |\n| [IQ-7](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.41bpw.gguf) | 4.41 | 16.9 GB | 99.34% |\n| [IQ-8](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf) | 4.67 | 17.8 GB | 99.75% |\n\n[](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#notes-on-quantization-labels) Notes on quantization labels\n-------------------------------------------------------------------------------------------------------------------------------\n\nThe labels you see (for example `IQ4_XS`) are only there to make Hugging Face show our models in the GGUF table. We do not use the conventional quantization profiles as defined in llama.cpp. In our case, these labels indicate the primary quantization approach and average bit length. Note that both KQ and IQ models may use a mix of quantization techniques optimized for their target hardware, which is why several models can share the same tag.\n\n[](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#running-these-models-with-ollama) Running these models with Ollama\n---------------------------------------------------------------------------------------------------------------------------------------\n\nAll GGUF files in this repo can be used directly with Ollama.\n\nTo run a model with Ollama, use:\n\n```\nollama run hf.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF:FILE_NAME.gguf\n```\n\nReplace `FILE_NAME.gguf` with the GGUF filename you want. For example:\n\n```\nollama run hf.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF:Qwen3-30B-A3B-Instruct-2507-IQ4_XS-3.63bpw.gguf\n```",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/52juwyqq0rbg1.jpeg",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:53:55.873605758Z"
    },
    {
      "flow_id": "",
      "id": "1q61wpv",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/",
      "title": "NousResearch/NousCoder-14B ¬∑ Hugging Face",
      "content": "from NousResearch:\n\n\"We introduce *NousCoder-14B*, a competitive programming model post-trained on [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B) via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.\"",
      "author": "jacek2023",
      "created_at": "2026-01-07T01:37:25Z",
      "comments": [
        {
          "id": "ny4f9hx",
          "author": "Cool-Chemical-5629",
          "content": "It's happening. [https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm\\_source=share\u0026amp;utm\\_medium=web3x\u0026amp;utm\\_name=web3xcss\u0026amp;utm\\_term=1\u0026amp;utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button)",
          "created_at": "2026-01-07T01:54:22Z",
          "urls": [
            {
              "url": "https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm\\_source=share\u0026amp;utm\\_medium=web3x\u0026amp;utm\\_name=web3xcss\u0026amp;utm\\_term=1\u0026amp;utm\\_content=share\\_button",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny4hwdb",
          "author": "AvocadoArray",
          "content": "Maybe I'm missing something, but isn't this just a demonstration of overfitting a model to a test suite?",
          "created_at": "2026-01-07T02:08:35Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/NousResearch/NousCoder-14B",
          "was_fetched": true,
          "page": "Title: NousResearch/NousCoder-14B ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/NousResearch/NousCoder-14B\n\nMarkdown Content:\nNousResearch/NousCoder-14B ¬∑ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n[](https://huggingface.co/NousResearch)\n\n[NousResearch](https://huggingface.co/NousResearch)\n\n/\n\n[NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B)\n\nlike 14\n\nFollow\n\nNousResearch 2.9k\n=============================================================================================================================================================================================================\n\n[Text Generation](https://huggingface.co/models?pipeline_tag=text-generation)[Safetensors](https://huggingface.co/models?library=safetensors)\n\n4 datasets\n\n[qwen3](https://huggingface.co/models?other=qwen3)[conversational](https://huggingface.co/models?other=conversational)\n\nLicense:apache-2.0\n\n[Model card](https://huggingface.co/NousResearch/NousCoder-14B)[Files Files and versions xet](https://huggingface.co/NousResearch/NousCoder-14B/tree/main)[Community](https://huggingface.co/NousResearch/NousCoder-14B/discussions)\n\n*   [NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B#nouscoder-14b \"NousCoder-14B\")\n\n*   [Acknowledgements](https://huggingface.co/NousResearch/NousCoder-14B#acknowledgements \"Acknowledgements\")\n\n[](https://huggingface.co/NousResearch/NousCoder-14B#nouscoder-14b) NousCoder-14B\n=================================================================================\n\n[](https://x.com/NousResearch)[](https://www.apache.org/licenses/LICENSE-2.0)\n\nWe introduce _NousCoder-14B_, a competitive programming model post-trained on [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B) via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.\n\n[](https://huggingface.co/NousResearch/NousCoder-14B/blob/main/lcb_score_vs_step.png)[](https://huggingface.co/NousResearch/NousCoder-14B/blob/main/performance_params_ratio.png)\n\n[](https://huggingface.co/NousResearch/NousCoder-14B#acknowledgements) Acknowledgements\n=======================================================================================\n\nI would like to thank my mentor, Roger Jin, Dakota Mahan, Teknium, and others at the Nous Research team for their invaluable support throughout this project. I would also like to thank Together AI and Agentica for their immensely helpful blog posts on DeepCoder-14B. Finally, thank you to Modal and Lambda for their generous support by providing me with credits.\n\nDownloads last month- \n\nSafetensors[](https://huggingface.co/docs/safetensors)\n\nModel size\n\n15B params\n\nTensor type\n\nBF16 \n\n¬∑\n\nChat template\n\nFiles info\n\nInference Providers[NEW](https://huggingface.co/docs/inference-providers)\n\n[Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\n\nThis model isn't deployed by any Inference Provider.[üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=NousResearch/NousCoder-14B\u0026description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNousResearch%2FNousCoder-14B%5D(%2FNousResearch%2FNousCoder-14B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n\nModel tree for NousResearch/NousCoder-14B[](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\n----------------------------------------------------------------------------------------------------------------\n\nBase model\n\n[Qwen/Qwen3-14B-Base](https://huggingface.co/Qwen/Qwen3-14B-Base)\n\n Finetuned\n\n[Qwen/Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B)\n\n Finetuned\n\n([181](https://huggingface.co/models?other=base_model:finetune:Qwen/Qwen3-14B)) \n\nthis model \n\nQuantizations\n\n[1 model](https://huggingface.co/models?other=base_model:quantized:NousResearch/NousCoder-14B)\n\nDatasets used to train NousResearch/NousCoder-14B\n-------------------------------------------------\n\n[#### livecodebench/code_generation_lite Updated Jun 5, 2025‚Ä¢ 56.5k ‚Ä¢ 79](https://huggingface.co/datasets/livecodebench/code_generation_lite)[#### agentica-org/DeepCoder-Preview-Dataset Viewer ‚Ä¢ Updated Apr 9, 2025‚Ä¢ 25k‚Ä¢ 2.02k ‚Ä¢ 93](https://huggingface.co/datasets/agentica-org/DeepCoder-Preview-Dataset)[#### NousResearch/lcb_test Viewer ‚Ä¢ Updated 13 days ago‚Ä¢ 454‚Ä¢ 24](https://huggingface.co/datasets/NousResearch/lcb_test)\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)\n\nInference providers allow you to run inference using different serverless providers.",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/Qwen/Qwen3-14B",
          "was_fetched": true,
          "page": "Title: Qwen/Qwen3-14B ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/Qwen/Qwen3-14B\n\nMarkdown Content:\n[](https://chat.qwen.ai/)\n\n[](https://huggingface.co/Qwen/Qwen3-14B#qwen3-highlights) Qwen3 Highlights\n---------------------------------------------------------------------------\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n*   **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n*   **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n*   **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n*   **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n*   **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#model-overview) Model Overview\n-----------------------------------------------------------------------\n\n**Qwen3-14B** has the following features:\n\n*   Type: Causal Language Models\n*   Training Stage: Pretraining \u0026 Post-training\n*   Number of Parameters: 14.8B\n*   Number of Paramaters (Non-Embedding): 13.2B\n*   Number of Layers: 40\n*   Number of Attention Heads (GQA): 40 for Q and 8 for KV\n*   Context Length: 32,768 natively and [131,072 tokens with YaRN](https://huggingface.co/Qwen/Qwen3-14B#processing-long-texts).\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n[](https://huggingface.co/Qwen/Qwen3-14B#quickstart) Quickstart\n---------------------------------------------------------------\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers\u003c4.51.0`, you will encounter the following error:\n\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (\u003c/think\u003e)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang\u003e=0.4.6.post1` or `vllm\u003e=0.8.5` or to create an OpenAI-compatible API endpoint:\n\n*   SGLang:```\npython -m sglang.launch_server --model-path Qwen/Qwen3-14B --reasoning-parser qwen3\n``` \n*   vLLM:```\nvllm serve Qwen/Qwen3-14B --enable-reasoning --reasoning-parser deepseek_r1\n``` \n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#switching-between-thinking-and-non-thinking-mode) Switching Between Thinking and Non-Thinking Mode\n-------------------------------------------------------------------------------------------------------------------------------------------\n\n\u003e The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#enable_thinkingtrue)`enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `\u003cthink\u003e...\u003c/think\u003e` block, followed by the final response.\n\n\u003e For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](https://huggingface.co/Qwen/Qwen3-14B#best-practices) section.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#enable_thinkingfalse)`enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `\u003cthink\u003e...\u003c/think\u003e` block.\n\n\u003e For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](https://huggingface.co/Qwen/Qwen3-14B#best-practices) section.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#advanced-usage-switching-between-thinking-and-non-thinking-modes-via-user-input) Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-14B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n\u003e For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `\u003cthink\u003e...\u003c/think\u003e`. However, the content inside this block may be empty if thinking is disabled. When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `\u003cthink\u003e...\u003c/think\u003e` block.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#agentic-use) Agentic Use\n-----------------------------------------------------------------\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n\n```\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-14B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `\u003cthink\u003ethis is the thought\u003c/think\u003ethis is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n[](https://huggingface.co/Qwen/Qwen3-14B#processing-long-texts) Processing Long Texts\n-------------------------------------------------------------------------------------\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n*   Modifying the model files: In the `config.json` file, add the `rope_scaling` fields:\n\n```\n{\n    ...,\n    \"rope_scaling\": {\n        \"rope_type\": \"yarn\",\n        \"factor\": 4.0,\n        \"original_max_position_embeddings\": 32768\n    }\n}\n``` \nFor `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n*   Passing command line arguments:\n\nFor `vllm`, you can use\n\n```\nvllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\n``` \nFor `sglang`, you can use\n\n```\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n``` \nFor `llama-server` from `llama.cpp`, you can use\n\n```\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n``` \n\n\u003e If you encounter the following warning\n\u003e \n\u003e \n\u003e \n\u003e ```\n\u003e Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n\u003e ```\n\u003e \n\u003e \n\u003e please upgrade `transformers\u003e=4.51.0`.\n\n\u003e All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.** We advise adding the `rope_scaling` configuration only when processing long contexts is required. It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0.\n\n\u003e The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n\u003e The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#best-practices) Best Practices\n-----------------------------------------------------------------------\n\nTo achieve optimal performance, we recommend the following settings:\n\n1.   **Sampling Parameters**:\n\n    *   For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n    *   For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n    *   For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2.   **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3.   **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n\n    *   **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n    *   **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4.   **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#citation) Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:54:06.727457704Z"
    },
    {
      "flow_id": "",
      "id": "1q5dnyw",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/",
      "title": "Performance improvements in llama.cpp over time",
      "content": "",
      "author": "jacek2023",
      "created_at": "2026-01-06T09:03:03Z",
      "comments": [
        {
          "id": "nxzvomh",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-06T12:30:15Z",
          "urls": [
            {
              "url": "https://discord.gg/PgFhZ8cnWW",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxzom5l",
          "author": "Dr4x_",
          "content": "Is it merge already?",
          "created_at": "2026-01-06T11:37:28Z",
          "was_summarised": false
        },
        {
          "id": "nxz9nk6",
          "author": "ghost_ops_",
          "content": "these performance gains are only for nvidia gpus?",
          "created_at": "2026-01-06T09:24:02Z",
          "was_summarised": false
        },
        {
          "id": "nxzrawe",
          "author": "jacek2023",
          "content": "[https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/](https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/)\n\n  \nUpdates to llama.cpp include:\n\n* [GPU token sampling](https://github.com/ggml-org/llama.cpp/pull/17004): Offloads several sampling algorithms (TopK, TopP, Temperature, minK, minP, and multi-sequence sampling) to the GPU, improving quality, consistency, and accuracy of responses, while also increasing performance.\n* [Concurrency for QKV projections](https://github.com/ggml-org/llama.cpp/pull/16991): Support for running concurrent CUDA streams to speed up model inference. To use this feature, pass in the *‚ÄìCUDA\\_GRAPH\\_OPT=1* flag.\n* [MMVQ kernel optimizations](https://github.com/ggml-org/llama.cpp/pull/16847): Pre-loads data into registers and hides delays by increasing GPU utilization on other tasks, to speed up the kernel.\n* [Faster model loading time](https://github.com/ggml-org/llama.cpp/pull/18012): Up to 65% model load time improvements on DGX Spark, and 15% on RTX GPUs.\n* [Native MXFP4 support on NVIDIA Blackwell GPUs](https://github.com/ggml-org/llama.cpp/pull/17906/): Up to 25% faster prompt processing on LLMs using the hardware-level NVFP4 fifth-generation of Tensor Cores on the Blackwell GPUs.",
          "created_at": "2026-01-06T11:58:11Z",
          "urls": [
            {
              "url": "https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://github.com/ggml-org/llama.cpp/pull/17004",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://github.com/ggml-org/llama.cpp/pull/16991",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://github.com/ggml-org/llama.cpp/pull/16847",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://github.com/ggml-org/llama.cpp/pull/18012",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://github.com/ggml-org/llama.cpp/pull/17906/",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxzqdk2",
          "author": "Lissanro",
          "content": "Mainline llama.cpp in terms of token generation speed became quite good, getting very close to ik\\_llama.cpp. Prompt processing about twice as slow though, but still, it has been amazing progress, there have been so many optimizations and improvement in llama.cpp in the past year, and it has wider architecture support, making it sometimes the only choice. Nice to see they continue to improve token generation speeds. If prompt processing gets improved also in the future, it would be amazing.",
          "created_at": "2026-01-06T11:51:10Z",
          "was_summarised": false
        },
        {
          "id": "nxzdli6",
          "author": "No_Swimming6548",
          "content": "Time to update. Also, Nemotron 3 Nano optimization when?",
          "created_at": "2026-01-06T10:01:35Z",
          "was_summarised": false
        },
        {
          "id": "nxzinhx",
          "author": "horriblesmell420",
          "content": "Any modern performance comparisons to vLLM?",
          "created_at": "2026-01-06T10:47:14Z",
          "was_summarised": false
        },
        {
          "id": "nxzffab",
          "author": "pmttyji",
          "content": "In the right side chart(DGX Spark), GPT-OSS-20B Numbers seems low comparing to 120B model. (OR 120B performs well(giving 50% of what 20B gives) better than 20B). Possibly few optimizations pending for 20B.",
          "created_at": "2026-01-06T10:18:14Z",
          "was_summarised": false
        },
        {
          "id": "nxziw2o",
          "author": "am17an",
          "content": "They didn't put the PP results for these models, at least gpt-oss should have 30% gain in those as well due to the FP4 instructions on DGX spark. For TG it's mostly been a series of PRs for fusion with help from NVIDIA engineers. However the TG gains should be for AMD as well (at least I hope)",
          "created_at": "2026-01-06T10:49:17Z",
          "was_summarised": false
        },
        {
          "id": "ny2h6sz",
          "author": "cibernox",
          "content": "I‚Äôve also noticed performance gains over the last few months. I used to run 4B models in Q4 at 80tk/s last year and I‚Äôm consistently getting over 100tk/s now. In fact with some memory over clock I can run 8B dense models at 70tk/s now (when context is low). Thats quite amazing.",
          "created_at": "2026-01-06T20:07:36Z",
          "was_summarised": false
        },
        {
          "id": "ny35gjr",
          "author": "Firenze30",
          "content": "I didn't find any performance gain updating from 7394 (CUDA 12.4) to 7642 (CUDA 13.1). GPT-OSS-120B.",
          "created_at": "2026-01-06T21:59:16Z",
          "was_summarised": false
        },
        {
          "id": "ny4225w",
          "author": "HarambeTenSei",
          "content": "And still no audio support",
          "created_at": "2026-01-07T00:43:06Z",
          "was_summarised": false
        },
        {
          "id": "ny4i2uq",
          "author": "AdventurousGold672",
          "content": "Can we already see the benefit of it?",
          "created_at": "2026-01-07T02:09:34Z",
          "was_summarised": false
        },
        {
          "id": "ny4q394",
          "author": "suicidaleggroll",
          "content": "I really wish they would provide more info.\n\nhttps://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/\n\n\u0026gt; Jan‚Äô26 builds are run with the following environment variables and flags: GGML_CUDA_GRAPH_OPT=1, FA=ON, and ‚Äîbackend-sampling\n\nOk, are those compiler flags?  Runtime flags?  Arguments to llama.cpp?  Is this a CUDA improvement or llama.cpp improvement?  Which version of which one has these new commits?",
          "created_at": "2026-01-07T02:52:56Z",
          "urls": [
            {
              "url": "https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxzqxei",
          "author": "Ok_Warning2146",
          "content": "That's good news. From which release was this gain merged?",
          "created_at": "2026-01-06T11:55:21Z",
          "was_summarised": false
        },
        {
          "id": "nxzt3xk",
          "author": "__Maximum__",
          "content": "Is this merged into main of llama.cpp? What nvidia drivers? Any info at all?",
          "created_at": "2026-01-06T12:11:52Z",
          "was_summarised": false
        },
        {
          "id": "nxzm2i3",
          "author": "llama-impersonator",
          "content": "it's easy if you do it \"the amazon way\" by tanking the perf of recent builds so nvidia can come in and fix it",
          "created_at": "2026-01-06T11:16:34Z",
          "was_summarised": false
        },
        {
          "id": "ny0l13p",
          "author": "ZhiyongSong",
          "content": "These llama.cpp gains scream NVIDIA love: GPU sampling, concurrent QKV, MMVQ tweaks, faster loads‚ÄîBlackwell MXFP4 juices prefill. AMD/ROCm is in the works, but not 1:1. Want CUDA graphs? export GGML\\_CUDA\\_GRAPH\\_OPT=1. Prefill‚Äôs still laggy‚Äîprofile with Nsight; if prompt speeds up, mainline‚Äôs a beast.",
          "created_at": "2026-01-06T14:55:58Z",
          "was_summarised": false
        },
        {
          "id": "nxzzlc0",
          "author": "asraniel",
          "content": "How does this translate to ollama? I know, people hate ollama around here, but thats what i use.",
          "created_at": "2026-01-06T12:56:21Z",
          "was_summarised": false
        },
        {
          "id": "nxzob99",
          "author": "Niwa-kun",
          "content": "hope i can use more grok/gemini/chatgpt now. damn rate limits.",
          "created_at": "2026-01-06T11:35:04Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/lsqwma772pbg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:54:06.727761693Z"
    },
    {
      "flow_id": "",
      "id": "1q5vk9m",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/",
      "title": "200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring",
      "content": "This is the inference strategy:\n\n1. Embed your query using a dense embedding model into a 'standard' fp32 embedding\n2. Quantize the fp32 embedding to binary: 32x smaller\n3. Use an approximate (or exact) binary index to retrieve e.g. 40 documents (\\~20x faster than a fp32 index)\n4. Load int8 embeddings for the 40 top binary documents from disk.\n5. Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings\n6. Sort the 40 documents based on the new scores, grab the top 10\n7. Load the titles/texts of the top 10 documents\n\nThis requires:  \n\\- Embedding all of your documents once, and using those embeddings for:  \n\\- A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate  \n\\- A int8 \"view\", i.e. a way to load the int8 embeddings from disk efficiently given a document ID\n\nInstead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index.\n\nBy loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore \\~99% of the performance of the fp32 search, compared to \\~97% when using purely the binary index: [https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring)\n\nCheck out the demo that allows you to test this technique on 40 million texts from Wikipedia: [https://huggingface.co/spaces/sentence-transformers/quantized-retrieval](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval)\n\nIt would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'.\n\nIn short: your retrieval doesn't need to be so expensive!\n\nSources:  \n\\- [https://www.linkedin.com/posts/tomaarsen\\_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a](https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a)  \n\\- [https://huggingface.co/blog/embedding-quantization](https://huggingface.co/blog/embedding-quantization)  \n\\- [https://cohere.com/blog/int8-binary-embeddings](https://cohere.com/blog/int8-binary-embeddings)",
      "author": "-Cubie-",
      "created_at": "2026-01-06T21:24:42Z",
      "comments": [
        {
          "id": "ny3nimq",
          "author": "goldlord44",
          "content": "This looks like very interesting research! Thank you! \n\nI'd be keen to see the results for retrieval from clusters (i.e. a collection of chunks that are all from textbooks / docs talking about quantum mechanics).\n\nMy initial feeling and concern is that this method is very strong for semantically dissimilar databases, I.e. General rag on reddit comments from all communities, but for more targeted vector stores it will struggle significantly (by virtue of the cluster already causing all the chunks to effectively a dimension reduction to the small section in vector space representing the topic, thus the further reduction of to binary could misrepresent this space)\n\nMore than happy to be shown otherwise! Would save my company a lot of money!",
          "created_at": "2026-01-06T23:27:45Z",
          "was_summarised": false
        },
        {
          "id": "ny4ibo2",
          "author": "swagonflyyyy",
          "content": "This...is...actually a bigger deal than I thought!\n\nBut if I had to guess, it seems to be in prototype territory given all the steps you need to take. Do you think there's any way to remove some of those steps altogether?",
          "created_at": "2026-01-07T02:10:54Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/spaces/sentence-transformers/quantized-retrieval",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring",
          "was_fetched": true,
          "page": "Title: Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval\n\nURL Source: https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring\n\nMarkdown Content:\n[Back to Articles](https://huggingface.co/blog)\n\n[](https://huggingface.co/aamirshakir)\n\n[](https://huggingface.co/tomaarsen)\n\n[](https://huggingface.co/SeanLee97)\n\nWe introduce the concept of embedding quantization and showcase their impact on retrieval speed, memory usage, disk space, and cost. We'll discuss how embeddings can be quantized in theory and in practice, after which we introduce a [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showing a real-life retrieval scenario of 41 million Wikipedia texts.\n\n[](https://huggingface.co/blog/embedding-quantization#table-of-contents) Table of Contents\n------------------------------------------------------------------------------------------\n\n*   [Why Embeddings?](https://huggingface.co/blog/embedding-quantization#why-embeddings)\n    *   [Embeddings may struggle to scale](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale)\n\n*   [Improving scalability](https://huggingface.co/blog/embedding-quantization#improving-scalability)\n    *   [Binary Quantization](https://huggingface.co/blog/embedding-quantization#binary-quantization)\n        *   [Binary Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers)\n        *   [Binary Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases)\n\n    *   [Scalar (int8) Quantization](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization)\n        *   [Scalar Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers)\n        *   [Scalar Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases)\n\n    *   [Combining Binary and Scalar Quantization](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization)\n    *   [Quantization Experiments](https://huggingface.co/blog/embedding-quantization#quantization-experiments)\n    *   [Influence of Rescoring](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring)\n        *   [Binary Rescoring](https://huggingface.co/blog/embedding-quantization#binary-rescoring)\n        *   [Scalar (Int8) Rescoring](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring)\n        *   [Retrieval Speed](https://huggingface.co/blog/embedding-quantization#retrieval-speed)\n\n    *   [Performance Summarization](https://huggingface.co/blog/embedding-quantization#performance-summarization)\n    *   [Demo](https://huggingface.co/blog/embedding-quantization#demo)\n    *   [Try it yourself](https://huggingface.co/blog/embedding-quantization#try-it-yourself)\n    *   [Future work:](https://huggingface.co/blog/embedding-quantization#future-work)\n    *   [Acknowledgments](https://huggingface.co/blog/embedding-quantization#acknowledgments)\n    *   [Citation](https://huggingface.co/blog/embedding-quantization#citation)\n    *   [References](https://huggingface.co/blog/embedding-quantization#references)\n\n[](https://huggingface.co/blog/embedding-quantization#why-embeddings) Why Embeddings?\n-------------------------------------------------------------------------------------\n\nEmbeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects, like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors.\n\nAfter transforming the complex objects, you can determine their similarity by calculating the similarity of the respective embeddings! This is crucial for many use cases: it serves as the backbone for recommendation systems, retrieval, one-shot or few-shot learning, outlier detection, similarity search, paraphrase detection, clustering, classification, and much more.\n\n### [](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale) Embeddings may struggle to scale\n\nHowever, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in `float32`, i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory!\n\nThe table below gives an overview of different models, dimension size, memory requirement, and costs. Costs are computed at an estimated $3.8 per GB/mo with `x2gd` instances on AWS.\n\n| Embedding Dimension | Example Models | 100M Embeddings | 250M Embeddings | 1B Embeddings |\n| --- | --- | --- | --- | --- |\n| 384 | [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) | 143.05GB $543 / mo | 357.62GB $1,358 / mo | 1430.51GB $5,435 / mo |\n| 768 | [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) [jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1) | 286.10GB $1,087 / mo | 715.26GB $2,717 / mo | 2861.02GB $10,871 / mo |\n| 1024 | [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) | 381.46GB $1,449 / mo | 953.67GB $3,623 / mo | 3814.69GB $14,495 / mo |\n| 1536 | [OpenAI text-embedding-3-small](https://openai.com/blog/new-embedding-models-and-api-updates) | 572.20GB $2,174 / mo | 1430.51GB $5,435 / mo | 5722.04GB $21,743 / mo |\n| 3072 | [OpenAI text-embedding-3-large](https://openai.com/blog/new-embedding-models-and-api-updates) | 1144.40GB $4,348 / mo | 2861.02GB $10,871 / mo | 11444.09GB $43,487 / mo |\n\n[](https://huggingface.co/blog/embedding-quantization#improving-scalability) Improving scalability\n--------------------------------------------------------------------------------------------------\n\nThere are several ways to approach the challenges of scaling embeddings. The most common approach is dimensionality reduction, such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis). However, classic dimensionality reduction -- like PCA methods -- [tends to perform poorly when used with embeddings](https://arxiv.org/abs/2205.11498).\n\nIn recent news, [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) ([blogpost](https://huggingface.co/blog/matryoshka)) (MRL) as used by [OpenAI](https://openai.com/blog/new-embedding-models-and-api-updates) also allows for cheaper embeddings. With MRL, only the first `n` embedding dimensions are used. This approach has already been adopted by some open models like [nomic-ai/nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) and [mixedbread-ai/mxbai-embed-2d-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1) (For OpenAIs `text-embedding-3-large`, we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.).\n\nHowever, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: **Quantization**. Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs. Let's dive into it!\n\n### [](https://huggingface.co/blog/embedding-quantization#binary-quantization) Binary Quantization\n\nUnlike quantization in models where you reduce the precision of weights, quantization for embeddings refers to a post-processing step for the embeddings themselves. In particular, binary quantization refers to the conversion of the `float32` values in an embedding to 1-bit values, resulting in a 32x reduction in memory and storage usage.\n\nTo quantize `float32` embeddings to binary, we simply threshold normalized embeddings at 0:\n\nf(x)={0 if x‚â§0 1 if x\u003e0 f(x)= \\begin{cases} 0 \u0026 \\text{if } x\\leq 0\\\\ 1 \u0026 \\text{if } x \\gt 0 \\end{cases}\n\nWe can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.\n\n[Yamada et al. (2021)](https://arxiv.org/abs/2106.00882) introduced a rescore step, which they called _rerank_, to boost the performance. They proposed that the `float32` query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve `rescore_multiplier * top_k` results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the `float32` query embedding.\n\nBy applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers) Binary Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to binary would result in 1024 bits. In practice, it is much more common to store bits as bytes instead, so when we quantize to binary embeddings, we pack the bits into bytes using `np.packbits`.\n\nTherefore, quantizing a `float32` embedding with a dimensionality of 1024 yields an `int8` or `uint8` embedding with a dimensionality of 128. See two approaches of how you can produce quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2a. Encode some text using \"binary\" quantization\nbinary_embeddings = model.encode(\n    [\"I am driving to the lake.\", \"It is a beautiful day.\"],\n    precision=\"binary\",\n)\n```\n\nor\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2b. or, encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nbinary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere, you can see the differences between default `float32` embeddings and binary embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e binary_embeddings.shape\n(2, 128)\n\u003e\u003e\u003e binary_embeddings.nbytes\n256\n\u003e\u003e\u003e binary_embeddings.dtype\nint8\n```\n\nNote that you can also choose `\"ubinary\"` to quantize to binary using the unsigned `uint8` data format. This may be a requirement depending on your vector library/database.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases) Binary Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | [Yes](https://github.com/facebookresearch/faiss/wiki/Binary-indexes) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/schema-reference.html) |\n| Milvus | [Yes](https://milvus.io/docs/index.md) |\n| Qdrant | Through [Binary Quantization](https://qdrant.tech/documentation/guides/quantization/#binary-quantization) |\n| Weaviate | Through [Binary Quantization](https://weaviate.io/developers/weaviate/configuration/bq-compression) |\n\n### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization) Scalar (int8) Quantization\n\nWe use a scalar quantization process to convert the `float32` embeddings into `int8`. This involves mapping the continuous range of `float32` values to the discrete set of `int8` values, which can represent 256 distinct levels (from -128 to 127), as shown in the image below. This is done by using a large calibration dataset of embeddings. We compute the range of these embeddings, i.e., the `min` and `max` of each embedding dimension. From there, we calculate the steps (buckets) to categorize each value.\n\nTo further boost the retrieval performance, you can optionally apply the same rescoring step as for the binary embeddings. It is important to note that the calibration dataset greatly influences performance since it defines the quantization buckets.\n\n_Source: [https://qdrant.tech/articles/scalar-quantization/](https://qdrant.tech/articles/scalar-quantization/)_\n\nWith scalar quantization to `int8`, we reduce the original `float32` embeddings' precision so that each value is represented with an 8-bit integer (4x smaller). Note that this differs from the binary quantization case, where each value is represented by a single bit (32x smaller).\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers) Scalar Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to `int8` results in 1024 bytes. In practice, we can choose either `uint8` or `int8`. This choice is usually made depending on what your vector library/database supports.\n\nIn practice, it is recommended to provide the scalar quantization with either:\n\n1.   a large set of embeddings to quantize all at once, or\n2.   `min` and `max` ranges for each of the embedding dimensions, or\n3.   a large calibration dataset of embeddings from which the `min` and `max` ranges can be computed.\n\nIf none of these are the case, you will be given a warning like this: `Computing int8 quantization buckets based on 2 embeddings. int8 quantization is more stable with 'ranges' calculated from more embeddings or a 'calibration_embeddings' that can be used to calculate the buckets.`\n\nSee how you can produce scalar quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\nfrom datasets import load_dataset\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2. Prepare an example calibration dataset\ncorpus = load_dataset(\"nq_open\", split=\"train[:1000]\")[\"question\"]\ncalibration_embeddings = model.encode(corpus)\n\n# 3. Encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nint8_embeddings = quantize_embeddings(\n    embeddings,\n    precision=\"int8\",\n    calibration_embeddings=calibration_embeddings,\n)\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere you can see the differences between default `float32` embeddings and `int8` scalar embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e int8_embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e int8_embeddings.nbytes\n2048\n\u003e\u003e\u003e int8_embeddings.dtype\nint8\n```\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases) Scalar Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | Indirectly through [IndexHNSWSQ](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexHNSWSQ.html) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/tensor.html) |\n| OpenSearch | [Yes](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector) |\n| ElasticSearch | [Yes](https://www.elastic.co/de/blog/save-space-with-byte-sized-vectors) |\n| Milvus | Indirectly through [IVF_SQ8](https://milvus.io/docs/index.md) |\n| Qdrant | Indirectly through [Scalar Quantization](https://qdrant.tech/documentation/guides/quantization/#scalar-quantization) |\n\n### [](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization) Combining Binary and Scalar Quantization\n\nCombining binary and scalar quantization is possible to get the best of both worlds: the extreme speed from binary embeddings and the great performance preservation of scalar embeddings with rescoring. See the [demo](https://huggingface.co/blog/embedding-quantization#demo) below for a real-life implementation of this approach involving 41 million texts from Wikipedia. The pipeline for that setup is as follows:\n\n1.   The query is embedded using the [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) SentenceTransformer model.\n2.   The query is quantized to binary using the [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings) function from the `sentence-transformers` library.\n3.   A binary index (41M binary embeddings; 5.2GB of memory/disk space) is searched using the quantized query for the top 40 documents.\n4.   The top 40 documents are loaded on the fly from an int8 index on disk (41M int8 embeddings; 0 bytes of memory, 47.5GB of disk space).\n5.   The top 40 documents are rescored using the float32 query and the int8 embeddings to get the top 10 documents.\n6.   The top 10 documents are sorted by score and displayed.\n\nThrough this approach, we use 5.2GB of memory and 52GB of disk space for the indices. This is considerably less than normal retrieval, requiring 200GB of memory and 200GB of disk space. Especially as you scale up even further, this will result in notable reductions in latency and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#quantization-experiments) Quantization Experiments\n\nWe conducted our experiments on the retrieval subset of the [MTEB](https://huggingface.co/spaces/mteb/leaderboard) containing 15 benchmarks. First, we retrieved the top k (k=100) search results with a `rescore_multiplier` of 4. Therefore, we retrieved 400 results in total and performed the rescoring on these top 400. For the `int8` performance, we directly used the dot-product without any rescoring.\n\n| Model | Embedding Dimension | 250M Embeddings | MTEB Retrieval (NDCG@10) | Percentage of default performance |\n| --- | --- | --- | --- | --- |\n| **Open Models** |  |  |  |  |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): float32 | 1024 | 953.67GB $3623 / mo | 54.39 | 100% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): int8 | 1024 | 238.41GB $905 / mo | 52.79 | 97% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): binary | 1024 | 29.80GB $113.25 / mo | 52.46 | 96.45% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): float32 | 768 | 286.10GB $1087 / mo | 50.77 | 100% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): int8 | 768 | 178.81GB $679 / mo | 47.54 | 94.68% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): binary | 768 | 22.35GB $85 / mo | 37.96 | 74.77% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): float32 | 768 | 286.10GB $1087 / mo | 53.01 | 100% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): binary | 768 | 22.35GB $85 / mo | 46.49 | 87.7% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): float32 | 384 | 357.62GB $1358 / mo | 41.66 | 100% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): int8 | 384 | 89.40GB $339 / mo | 37.82 | 90.79% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): binary | 384 | 11.18GB $42 / mo | 39.07 | 93.79% |\n| **Proprietary Models** |  |  |  |  |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): float32 | 1024 | 953.67GB $3623 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): int8 | 1024 | 238.41GB $905 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): binary | 1024 | 29.80GB $113.25 / mo | 52.3 | 94.6% |\n\nSeveral key trends and benefits can be identified from the results of our quantization experiments. As expected, embedding models with higher dimension size typically generate higher storage costs per computation but achieve the best performance. Surprisingly, however, quantization to `int8` already helps `mxbai-embed-large-v1` and `Cohere-embed-english-v3.0` achieve higher performance with lower storage usage than that of the smaller dimension size base models.\n\nThe benefits of quantization are, if anything, even more clearly visible when looking at the results obtained with binary models. In that scenario, the 1024 dimension models still outperform a now 10x more storage intensive base model, and the `mxbai-embed-large-v1` even manages to hold more than 96% of performance after a 32x reduction in resource requirements. The further quantization from `int8` to binary barely results in any additional loss of performance for this model.\n\nInterestingly, we can also see that `all-MiniLM-L6-v2` exhibits stronger performance on binary than on `int8` quantization. A possible explanation for this could be the selection of calibration data. On `e5-base-v2`, we observe the effect of [dimension collapse](https://arxiv.org/abs/2110.09348), which causes the model to only use a subspace of the latent space; when performing the quantization, the whole space collapses further, leading to high performance losses.\n\nThis shows that quantization doesn't universally work with all embedding models. It remains crucial to consider exisiting benchmark outcomes and conduct experiments to determine a given model's compatibility with quantization.\n\n### [](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring) Influence of Rescoring\n\nIn this section we look at the influence of rescoring on retrieval performance. We evaluate the results based on [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1).\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-rescoring) Binary Rescoring\n\nWith binary embeddings, [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) retains 92.53% of performance on MTEB Retrieval. Just doing the rescoring without retrieving more samples pushes the performance to 96.45%. We experimented with setting the`rescore_multiplier` from 1 to 10, but observe no further boost in performance. This indicates that the `top_k` search already retrieved the top candidates and the rescoring reordered these good candidates appropriately.\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring) Scalar (Int8) Rescoring\n\nWe also evaluated the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) model with `int8` rescoring, as Cohere showed that [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) reached up to 100% of the performance of the `float32` model with `int8` quantization. For this experiment, we set the `rescore_multiplier` to [1, 4, 10] and got the following results:\n\nAs we can see from the diagram, a higher rescore multiplier implies better retention of performance after quantization. Extrapolating from our results, we assume the relation is likely hyperbolical with performance approaching 100% as the rescore multiplier continues to rise. A rescore multiplier of 4-5 already leads to a remarkable performance retention of 99% using `int8`.\n\n#### [](https://huggingface.co/blog/embedding-quantization#retrieval-speed) Retrieval Speed\n\nWe measured retrieval speed on a Google Cloud Platform `a2-highgpu-4g` instance using the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) embeddings with 1024 dimension on the whole MTEB Retrieval. For int8 we used [USearch](https://github.com/unum-cloud/usearch) (Version 2.9.2) and binary quantization [Faiss](https://github.com/facebookresearch/faiss) (Version 1.8.0). Everything was computed on CPU using exact search.\n\n| Quantization | Min | Mean | Max |\n| --- | --- | --- | --- |\n| `float32` | 1x (baseline) | **1x** (baseline) | 1x (baseline) |\n| `int8` | 2.99x speedup | **3.66x** speedup | 4.8x speedup |\n| `binary` | 15.05x speedup | **24.76x** speedup | 45.8x speedup |\n\nAs shown in the table, applying `int8` scalar quantization results in an average speedup of 3.66x compared to full-size `float32` embeddings. Additionally, binary quantization achieves a speedup of 24.76x on average. For both scalar and binary quantization, even the worst case scenario resulted in very notable speedups.\n\n### [](https://huggingface.co/blog/embedding-quantization#performance-summarization) Performance Summarization\n\nThe experimental results, effects on resource use, retrieval speed, and retrieval performance by using quantization can be summarized as follows:\n\n|  | float32 | int8/uint8 | binary/ubinary |\n| --- | --- | --- | --- |\n| **Memory \u0026 Index size savings** | 1x | exactly 4x | exactly 32x |\n| **Retrieval Speed** | 1x | up to 4x | up to 45x |\n| **Percentage of default performance** | 100% | ~99.3% | ~96% |\n\n### [](https://huggingface.co/blog/embedding-quantization#demo) Demo\n\nThe following [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showcases the retrieval efficiency using exact or approximate search by combining binary search with scalar (`int8`) rescoring. The solution requires 5GB of memory for the binary index and 50GB of disk space for the binary and scalar indices, considerably less than the 200GB of memory and disk space which would be required for regular `float32` retrieval. Additionally, retrieval is much faster.\n\n### [](https://huggingface.co/blog/embedding-quantization#try-it-yourself) Try it yourself\n\nThe following scripts can be used to experiment with embedding quantization for retrieval \u0026 beyond. There are three categories:\n\n*   **Recommended Retrieval**:\n    *   [semantic_search_recommended.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_recommended.py): This script combines binary search with scalar rescoring, much like the above demo, for cheap, efficient, and performant retrieval.\n\n*   **Usage**:\n    *   [semantic_search_faiss.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using FAISS, by using the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function.\n    *   [semantic_search_usearch.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using USearch, by using the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function.\n\n*   **Benchmarks**:\n    *   [semantic_search_faiss_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using FAISS. It uses the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function. Our benchmarks especially show show speedups for `ubinary`.\n    *   [semantic_search_usearch_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using USearch. It uses the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function. Our experiments show large speedups on newer hardware, particularly for `int8`.\n\n### [](https://huggingface.co/blog/embedding-quantization#future-work) Future work\n\nWe are looking forward to further advancements of binary quantization. To name a few potential improvements, we suspect that there may be room for scalar quantization smaller than `int8`, i.e. with 128 or 64 buckets instead of 256.\n\nAdditionally, we are excited that embedding quantization is fully perpendicular to Matryoshka Representation Learning (MRL). In other words, it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually corresponds with a 2% reduction in performance) and then apply binary or scalar quantization. We suspect this could speed up retrieval up to 32x for a ~3% reduction in quality, or up to 256x for a ~10% reduction in quality.\n\nLastly, we recognize that retrieval using embedding quantization can also be combined with a separate reranker model. We imagine that a 3-step pipeline of binary search, scalar (int8) rescoring, and cross-encoder reranking allows for state-of-the-art retrieval performance at low latencies, memory usage, disk space, and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#acknowledgments) Acknowledgments\n\nThis project is possible thanks to our collaboration with [mixedbread.ai](https://mixedbread.ai/) and the [SentenceTransformers](https://www.sbert.net/) library, which allows you to easily create sentence embeddings and quantize them. If you want to use quantized embeddings in your project, now you know how!\n\n### [](https://huggingface.co/blog/embedding-quantization#citation) Citation\n\n```\n@article{shakir2024quantization,\n  author       = { Aamir Shakir and\n                   Tom Aarsen and\n                   Sean Lee\n                 },\n  title = { Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval },\n  journal = {Hugging Face Blog},\n  year = {2024},\n  note = {https://huggingface.co/blog/embedding-quantization},\n}\n```\n\n### [](https://huggingface.co/blog/embedding-quantization#resources) Resources\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n*   [Sentence Transformers docs - Embedding Quantization](https://sbert.net/examples/applications/embedding-quantization/README.html)\n*   [https://txt.cohere.com/int8-binary-embeddings/](https://txt.cohere.com/int8-binary-embeddings/)\n*   [https://qdrant.tech/documentation/guides/quantization](https://qdrant.tech/documentation/guides/quantization)\n*   [https://zilliz.com/learn/scalar-quantization-and-product-quantization](https://zilliz.com/learn/scalar-quantization-and-product-quantization)",
          "was_summarised": false
        },
        {
          "url": "https://www.linkedin.com/posts/tomaarsen%5C_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a",
          "was_fetched": true,
          "page": "Title: LinkedIn\n\nURL Source: https://www.linkedin.com/posts/tomaarsen/_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\nLinkedIn\n===============\n\nÈÅ∏ÊìáË™ûË®Ä \n\n[](https://www.linkedin.com/ \"LinkedIn\")\n\nŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑÿµŸÅÿ≠ÿ©\n========================\n\nÿπŸÅŸàÿßŸãÿå ŸÑŸÖ ŸÜÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑÿµŸÅÿ≠ÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ®ÿ≠ÿ´ ÿπŸÜŸáÿß. ÿ®ÿ±ÿ¨ÿßÿ° ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ŸÑŸÑÿµŸÅÿ≠ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿ£Ÿà ÿßŸÑÿßŸÜÿ™ŸÇÿßŸÑ ŸÑŸÄ[ŸÖÿ±ŸÉÿ≤ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ©](https://www.linkedin.com/help/linkedin?trk=404_page \"ŸÖÿ±ŸÉÿ≤ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ©\") ŸÑŸÑŸÖÿ≤ŸäÿØ ŸÖŸÜ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[ÿßŸÑÿ•ŸÜÿ™ŸÇÿßŸÑ ŸÑŸÖŸàÿ¨ÿ≤ŸÉ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä](https://www.linkedin.com/feed/?trk=404_page \"ÿßŸÑÿ•ŸÜÿ™ŸÇÿßŸÑ ŸÑŸÖŸàÿ¨ÿ≤ŸÉ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä\")\n\nStr√°nka nenalezena\n==================\n\nOmlouv√°me se, nem≈Ø≈æeme naj√≠t str√°nku, kterou hled√°te. Zkuste se vr√°tit zp√°tky na p≈ôedchoz√≠ str√°nku, nebo se pod√≠vejte do na≈°eho [Centra n√°povƒõdy](https://www.linkedin.com/help/linkedin?trk=404_page \"Centrum n√°povƒõdy\") pro v√≠ce informac√≠\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[P≈ôej√≠t do informaƒçn√≠ho kan√°lu](https://www.linkedin.com/feed/?trk=404_page \"P≈ôej√≠t do informaƒçn√≠ho kan√°lu\")\n\nSiden blev ikke fundet\n======================\n\nVi kan desv√¶rre ikke finde den side, du leder efter. G√• tilbage til den forrige side, eller bes√∏g [Hj√¶lp](https://www.linkedin.com/help/linkedin?trk=404_page \"Hj√¶lp\") for at f√• flere oplysninger\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[G√• til dit feed](https://www.linkedin.com/feed/?trk=404_page \"G√• til dit feed\")\n\nSeite nicht gefunden\n====================\n\nDie gew√ºnschte Seite konnte leider nicht gefunden werden. Versuchen Sie, zur vorherigen Seite zur√ºckzukehren, oder besuchen Sie unseren [Hilfebereich](https://www.linkedin.com/help/linkedin?trk=404_page \"Hilfebereich\"), um mehr zu erfahren.\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Zu Ihrem Feed](https://www.linkedin.com/feed/?trk=404_page \"Zu Ihrem Feed\")\n\nPage not found\n==============\n\nUh oh, we can‚Äôt seem to find the page you‚Äôre looking for. Try going back to the previous page or see our [Help Center](https://www.linkedin.com/help/linkedin?trk=404_page \"Help Center\") for more information\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Go to your feed](https://www.linkedin.com/feed/?trk=404_page \"Go to your feed\")\n\nP√°gina no encontrada\n====================\n\nVaya, parece que no podemos encontrar la p√°gina que buscas. Intenta volver a la p√°gina anterior o visita nuestro [Centro de ayuda](https://www.linkedin.com/help/linkedin?trk=404_page \"Centro de ayuda\") para m√°s informaci√≥n.\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Ir a tu feed](https://www.linkedin.com/feed/?trk=404_page \"Ir a tu feed\")\n\nImpossible de trouver cette page\n================================\n\nNous ne trouvons pas la page que vous recherchez. Essayez de retourner √† la page pr√©c√©dente ou consultez notre [assistance client√®le](https://www.linkedin.com/help/linkedin?trk=404_page \"assistance client√®le\") pour plus d‚Äôinformations\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Ouvrez votre fil](https://www.linkedin.com/feed/?trk=404_page \"Ouvrez votre fil\")\n\nHalaman ini tidak dapat ditemukan\n=================================\n\nMaaf, sepertinya kami tidak dapat menemukan halaman yang Anda cari. Coba kembali ke halaman sebelumnya atau lihat [Pusat Bantuan](https://www.linkedin.com/help/linkedin?trk=404_page \"Pusat Bantuan\") kami untuk informasi lebih lanjut\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Buka feed Anda](https://www.linkedin.com/feed/?trk=404_page \"Buka feed Anda\")\n\nPagina non trovata\n==================\n\nNon abbiamo trovato la pagina che stai cercando. Prova a tornare alla pagina precedente o visita il nostro [Centro assistenza](https://www.linkedin.com/help/linkedin?trk=404_page \"Centro assistenza\") per saperne di pi√π.\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Vai al tuo feed](https://www.linkedin.com/feed/?trk=404_page \"Vai al tuo feed\")\n\n„Éö„Éº„Ç∏„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü\n==============\n\nÁî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„ÅäÊé¢„Åó„ÅÆ„Éö„Éº„Ç∏„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇÂâç„ÅÆ„Éö„Éº„Ç∏„Å´Êàª„Çã„Åã„ÄÅ[„Éò„É´„Éó„Çª„É≥„Çø„Éº](https://www.linkedin.com/help/linkedin?trk=404_page \"„Éò„É´„Éó„Çª„É≥„Çø„Éº\")„ÅßË©≥Á¥∞„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ\n----------------------------------------------------------------------------------------------------------------------\n\n[„Éï„Ç£„Éº„Éâ„Å´ÁßªÂãï](https://www.linkedin.com/feed/?trk=404_page \"„Éï„Ç£„Éº„Éâ„Å´ÁßªÂãï\")\n\nÌéòÏù¥ÏßÄ ÏóÜÏùå\n======\n\nÏõêÌïòÏãúÎäî ÌéòÏù¥ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥Ï†Ñ ÌéòÏù¥ÏßÄÎ°ú ÎèåÏïÑÍ∞ÄÍ±∞ÎÇò [Í≥†Í∞ùÏÑºÌÑ∞](https://www.linkedin.com/help/linkedin?trk=404_page \"Í≥†Í∞ùÏÑºÌÑ∞\")ÏóêÏÑú ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥ÏÑ∏Ïöî.\n------------------------------------------------------------------------------------------------------------------\n\n[ÌôàÏúºÎ°ú Í∞ÄÍ∏∞](https://www.linkedin.com/feed/?trk=404_page \"ÌôàÏúºÎ°ú Í∞ÄÍ∏∞\")\n\nLaman tidak ditemui\n===================\n\nHarap maaf, kami tidak dapat menemui laman yang ingin anda cari. Cuba kembali ke laman sebelumnya atau lihat [Pusat Bantuan](https://www.linkedin.com/help/linkedin?trk=404_page \"Pusat Bantuan\") kami untuk maklumat lanjut\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Pergi ke suapan](https://www.linkedin.com/feed/?trk=404_page \"Pergi ke suapan\")\n\nPagina niet gevonden\n====================\n\nDe pagina waar u naar op zoek bent, kan niet worden gevonden. Probeer terug te gaan naar de vorige pagina of bezoek het [Help Center](https://www.linkedin.com/help/linkedin?trk=404_page \"Helpcentrum\") voor meer informatie\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Ga naar uw feed](https://www.linkedin.com/feed/?trk=404_page \"Ga naar uw feed\")\n\nFant ikke siden\n===============\n\nVi finner ikke siden du leter etter. G√• tilbake til forrige side eller bes√∏k v√•r [brukerst√∏tte](https://www.linkedin.com/help/linkedin?trk=404_page \"brukerst√∏tte\") for mer informasjon\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[G√• til din feed](https://www.linkedin.com/feed/?trk=404_page \"G√• til din feed\")\n\nNie znaleziono strony\n=====================\n\nNie mo≈ºemy znale≈∫ƒá strony, kt√≥rej szukasz. Spr√≥buj wr√≥ciƒá do poprzedniej strony lub nasze [Centrum pomocy](https://www.linkedin.com/help/linkedin?trk=404_page \"Centrum pomocy\"), aby uzyskaƒá wiƒôcej informacji\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Przejd≈∫ do swojego kana≈Çu](https://www.linkedin.com/feed/?trk=404_page \"Przejd≈∫ do swojego kana≈Çu\")\n\nP√°gina n√£o encontrada\n=====================\n\nA p√°gina que voc√™ est√° procurando n√£o foi encontrada. Volte para a p√°gina anterior ou visite nossa [Central de Ajuda](https://www.linkedin.com/help/linkedin?trk=404_page \"Central de Ajuda\") para mais informa√ß√µes\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Voltar para seu feed](https://www.linkedin.com/feed/?trk=404_page \"Voltar para seu feed\")\n\nPagina nu a fost gƒÉsitƒÉ\n=======================\n\nNe pare rƒÉu, nu gƒÉsim pagina pe care o cƒÉuta≈£i. Reveni≈£i la pagina anterioarƒÉ sau consulta≈£i [Centrul nostru de asisten≈£ƒÉ](https://www.linkedin.com/help/linkedin?trk=404_page \"Centrul nostru de asisten≈£ƒÉ\") pentru mai multe informa≈£ii\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Accesa≈£i fluxul dvs.](https://www.linkedin.com/feed/?trk=404_page \"Accesa≈£i fluxul dvs.\")\n\n–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n===================\n\n–ù–µ —É–¥–∞—ë—Ç—Å—è –Ω–∞–π—Ç–∏ –∏—Å–∫–æ–º—É—é –≤–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É. –í–µ—Ä–Ω–∏—Ç–µ—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞—à–µ–≥–æ [—Å–ø—Ä–∞–≤–æ—á–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞](https://www.linkedin.com/help/linkedin?trk=404_page \"–°–ø—Ä–∞–≤–æ—á–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞\") –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[–ü–µ—Ä–µ–π—Ç–∏ –∫ –ª–µ–Ω—Ç–µ](https://www.linkedin.com/feed/?trk=404_page \"–ü–µ—Ä–µ–π—Ç–∏ –∫ –ª–µ–Ω—Ç–µ\")\n\nSidan kunde inte hittas.\n========================\n\nSidan du letar efter hittades inte. G√• tillbaka till f√∂reg√•ende sida eller bes√∂k v√•rt [Hj√§lpcenter](https://www.linkedin.com/help/linkedin?trk=404_page \"Hj√§lpcenter\") f√∂r mer information\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[G√• till ditt nyhetsfl√∂de](https://www.linkedin.com/feed/?trk=404_page \"G√• till ditt nyhetsfl√∂de\")\n\n‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à\n============\n\n‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏≠‡∏¢‡∏π‡πà ‡∏•‡∏≠‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏π [‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠](https://www.linkedin.com/help/linkedin?trk=404_page \"‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠\") ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ü‡∏µ‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì](https://www.linkedin.com/feed/?trk=404_page \"‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ü‡∏µ‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\")\n\nHindi Nahanap ang Pahina\n========================\n\nNaku, mukhang hindi namin mahanap ang pahina na hinahanap mo. Subukang bumalik sa nakaraang pahina o tingnan ang aming [Help Center](https://www.linkedin.com/help/linkedin?trk=404_page \"Help Center\") para sa higit pang impormasyon\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Pumunta sa iyong feed](https://www.linkedin.com/feed/?trk=404_page \"Pumunta sa iyong feed\")\n\nSayfa bulunamadƒ±\n================\n\nAradƒ±ƒüƒ±nƒ±z sayfa bulunamadƒ±. √ñnceki sayfaya geri d√∂n√ºn veya daha fazla bilgi i√ßin [Yardƒ±m Merkezimizi](https://www.linkedin.com/help/linkedin?trk=404_page \"Yardƒ±m Merkezimizi\") g√∂r√ºnt√ºleyin\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Haber akƒ±≈üƒ±nƒ±za gidin](https://www.linkedin.com/feed/?trk=404_page \"Haber akƒ±≈üƒ±nƒ±za gidin\")\n\nÊú™ÊâæÂà∞ÁΩëÈ°µ\n=====\n\nÊä±Ê≠âÔºåÊó†Ê≥ïÊâæÂà∞È°µÈù¢„ÄÇËØïËØïËøîÂõûÂà∞Ââç‰∏ÄÈ°µÔºåÊàñÂâçÂæÄ[Â∏ÆÂä©‰∏≠ÂøÉ](https://www.linkedin.com/help/linkedin?trk=404_page \"Â∏ÆÂä©‰∏≠ÂøÉ\")‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØ\n----------------------------------------------------------------------------------------------\n\n[ÂâçÂæÄÂä®ÊÄÅÊ±áÊÄª](https://www.linkedin.com/feed/?trk=404_page \"ÂâçÂæÄÂä®ÊÄÅÊ±áÊÄª\")\n\nÁ≥ªÁµ±Êâæ‰∏çÂà∞È†ÅÈù¢„ÄÇ\n========\n\nÊàëÂÄëÂ•ΩÂÉèÊâæ‰∏çÂà∞Ë©≤È†ÅÈù¢„ÄÇË´ãÂõûÂà∞‰∏ä‰∏ÄÈ†ÅÊàñÂâçÂæÄ[Ë™™Êòé‰∏≠ÂøÉ](https://www.linkedin.com/help/linkedin?trk=404_page \"Ë™™Êòé‰∏≠ÂøÉ\")‰æÜÈÄ≤‰∏ÄÊ≠•Áû≠Ëß£\n--------------------------------------------------------------------------------------------\n\n[ÂâçÂæÄÈ¶ñÈ†ÅÂãïÊÖã](https://www.linkedin.com/feed/?trk=404_page \"ÂâçÂæÄÈ¶ñÈ†ÅÂãïÊÖã\")\n\n[](https://www.linkedin.com/ \"LinkedIn\") ¬© 2022 \n\n*   [ÿßÿ™ŸÅÿßŸÇŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿÆÿµŸàÿµŸäÿ©](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [ÿ•ÿ±ÿ¥ÿßÿØÿßÿ™ ÿßŸÑŸÖÿ¨ÿ™ŸÖÿπ](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [ÿ≥Ÿäÿßÿ≥ÿ© ŸÖŸÑŸÅÿßÿ™ ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿßÿ±ÿ™ÿ®ÿßÿ∑](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [ÿ≥Ÿäÿßÿ≥ÿ© ÿ≠ŸÇŸàŸÇ ÿßŸÑŸÜÿ¥ÿ±](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [ÿ•ÿπÿØÿßÿØÿßÿ™ ÿ•ÿØÿßÿ±ÿ© ÿßŸÑÿ∂ŸäŸàŸÅ](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Smlouva s u≈æivatelem](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Z√°sady ochrany soukrom√≠](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Z√°sady komunity](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Z√°sady pro soubory cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Z√°sady ochrany autorsk√Ωch pr√°v](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Volby pro hosty](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Brugeraftale](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Privatlivspolitik](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Forumretningslinjer](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Politik for cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ophavsretspolitik](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Indstillinger for g√¶ster](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Nutzervereinbarung](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Datenschutzrichtlinie](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Netzwerkrichtlinien](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie-Richtlinie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Copyright-Richtlinie](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Einstellungen f√ºr Nichtmitglieder](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Community Guidelines](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Copyright Policy](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Guest Controls](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Condiciones de uso](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Pol√≠tica de privacidad](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Directrices comunitarias](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Pol√≠tica de cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Pol√≠tica de copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controles de invitados](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Conditions g√©n√©rales d‚Äôutilisation de LinkedIn](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Politique de confidentialit√©](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Directives de la communaut√©](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Politique relative aux cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Politique de copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [R√©glages invit√©s](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Perjanjian Pengguna](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Kebijakan Privasi](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Panduan Komunitas](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Kebijakan Cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Kebijakan Hak Cipta](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Pengaturan Tamu](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Contratto di licenza](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Informativa sulla privacy](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Linee guida della community](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Informativa sui cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Informativa sul copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controlli ospite](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Âà©Áî®Ë¶èÁ¥Ñ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [„Éó„É©„Ç§„Éê„Ç∑„Éº„Éù„É™„Ç∑„Éº](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Ç¨„Ç§„Éâ„É©„Ç§„É≥](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie„Éù„É™„Ç∑„Éº](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ëëó‰ΩúÊ®©„Éù„É™„Ç∑„Éº](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [„Ç≤„Çπ„ÉàÂêë„ÅëÁÆ°ÁêÜÊ©üËÉΩ](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [ÏÇ¨Ïö©ÏûêÏïΩÍ¥Ä](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Í∞úÏù∏Ï†ïÎ≥¥ Ï∑®Í∏âÎ∞©Ïπ®](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Ïª§ÎÆ§ÎãàÌã∞Ï†ïÏ±Ö](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Ïø†ÌÇ§Ï†ïÏ±Ö](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ï†ÄÏûëÍ∂åÏ†ïÏ±Ö](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [ÎπÑÌöåÏõê ÏÑ§Ï†ï](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Perjanjian Pengguna](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Dasar Privasi](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Garis Panduan Komuniti](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Dasar Kuki](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Dasar Hak Cipta](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Kawalan Tetamu](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Gebruikersovereenkomst](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Privacybeleid](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Communityrichtlijnen](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookiebeleid](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Auteursrechtenbeleid](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Instellingen voor gasten](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Brukeravtale](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Personvernerkl√¶ring](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Retningslinjer for fellesskapet](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Retningslinjer for informasjonskapsler](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Retningslinjer vedr√∏rende opphavsrett](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Gjestestyring](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Umowa u≈ºytkownika](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Polityka ochrony prywatno≈õci](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Wskaz√≥wki dotyczƒÖce spo≈Çeczno≈õci](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Zasady korzystania z plik√≥w cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Ustawienia go≈õcia](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Contrato do Usu√°rio](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Pol√≠tica de Privacidade do LinkedIn](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Diretrizes da Comunidade](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Pol√≠tica de Cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Pol√≠tica de Direitos Autorais](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controles de visitantes](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Acordul utilizatorului](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Politica de confiden»õialitate](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Linii directoare comunitate](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Politica privind modulele cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Politica de copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controale vizitator](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [–ü–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [–ü—Ä–∞–≤–∏–ª–∞ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [–ü–æ–ª–∏—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [–ü–æ–ª–∏—Ç–∏–∫–∞ –∑–∞—â–∏—Ç—ã –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ—Å—Ç—è](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Anv√§ndaravtal](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Sekretesspolicy](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Riktlinjer](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookiepolicy](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Upphovsr√§ttspolicy](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [G√§stinst√§llningar](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [‡∏Ç‡πâ‡∏≠‡∏ï‡∏Å‡∏•‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ä‡∏∏‡∏°‡∏ä‡∏ô](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ä‡∏°](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Kasunduan sa Gumagamit](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Patakaran sa Pagkapribado](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Mga alituntunin sa komunidad](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Patakara sa Cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Patakaran sa Copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Mga kontrol ng Panauhin](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Kullanƒ±cƒ± Anla≈ümasƒ±](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Gizlilik Politikasƒ±](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Topluluk Y√∂nergeleri](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [√áerez Politikasƒ±](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Telif Hakkƒ± Politikasƒ±](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Ziyaret√ßi Kontrolleri](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Áî®Êà∑ÂçèËÆÆ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [ÈöêÁßÅÊîøÁ≠ñ](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Á§æÂå∫ÂáÜÂàô](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie ÊîøÁ≠ñ](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [ÁâàÊùÉÊîøÁ≠ñ](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [ËÆøÂÆ¢ËÆæÁΩÆ](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Áî®Êà∂ÂçîË≠∞](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Èö±ÁßÅÊ¨äÊîøÁ≠ñ](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Á§æÁæ§ÊåáÂçó](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie ÊîøÁ≠ñ](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ëëó‰ΩúÊ¨äÊîøÁ≠ñ](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Ë®™ÂÆ¢ÊéßÁÆ°](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)",
          "was_summarised": false
        },
        {
          "url": "https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a",
          "was_fetched": true,
          "page": "Title: You can perform 200ms search over 40 million texts using just a CPU server, 8GB of RAM, and 40GB of disk space.\n\nURL Source: https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\n\nPublished Time: 2026-01-06T15:24:10.017Z\n\nMarkdown Content:\nQuantized Retrieval - a Hugging Face Space by sentence-transformers | Tom Aarsen | 17 comments\n===============\n\nAgree \u0026 Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n[Skip to main content](https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a#main-content)[LinkedIn](https://www.linkedin.com/?trk=public_post_nav-header-logo)\n*   [Top Content](https://www.linkedin.com/top-content?trk=public_post_guest_nav_menu_topContent)\n*   [People](https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people)\n*   [Learning](https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning)\n*   [Jobs](https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs)\n*   [Games](https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games)\n\n[Sign in](https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026fromSignIn=true\u0026trk=public_post_nav-header-signin)[Join for free](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_nav-header-join)\n\nTom Aarsen‚Äôs Post\n=================\n\n[](https://nl.linkedin.com/in/tomaarsen?trk=public_post_feed-actor-image)\n\n[Tom Aarsen](https://nl.linkedin.com/in/tomaarsen?trk=public_post_feed-actor-name)\n\n 11h \n\n*   [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=POST\u0026_f=guest-reporting)\n\nYou can perform 200ms search over 40 million texts using just a CPU server, 8GB of RAM, and 40GB of disk space. The trick: Binary search with int8 rescoring. Details: This is the inference strategy: 1. Embed your query using a dense embedding model into a 'standard' fp32 embedding 2. Quantize the fp32 embedding to binary: 32x smaller 3. Use an approximate (or exact) binary index to retrieve e.g. 40 documents (~20x faster than a fp32 index) 4. Load int8 embeddings for the 40 top binary documents from disk. 5. Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings 6. Sort the 40 documents based on the new scores, grab the top 10 7. Load the titles/texts of the top 10 documents This requires: - Embedding all of your documents once, and using those embeddings for: - A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate - A int8 \"view\", i.e. a way to load the int8 embeddings from disk efficiently given a document ID Instead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index. By loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore ~99% of the performance of the fp32 search, compared to ~97% when using purely the binary index: [https://lnkd.in/edc9yM6W](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fedc9yM6W\u0026urlhash=TaIv\u0026trk=public_post-text) I've created a demo that allows you to test this technique on 40 million texts from Wikipedia! Give it a try here: [https://lnkd.in/ebNSHA4y](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FebNSHA4y\u0026urlhash=NZ8N\u0026trk=public_post-text) It would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'. In short: your retrieval doesn't need to be so expensive!\n\n[Quantized Retrieval - a Hugging Face Space by sentence-transformers huggingface.co](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fhuggingface%2Eco%2Fspaces%2Fsentence-transformers%2Fquantized-retrieval\u0026urlhash=GzJ4\u0026trk=public_post_feed-article-content)\n\n[538](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_social-actions-reactions)[17 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment-cta)\n\n Share \n*   Copy\n*   LinkedIn\n*   Facebook\n*   X\n\n[](https://no.linkedin.com/in/marcobertaniokland?trk=public_post_comment_actor-image)\n\n[Marco Bertani-√òkland](https://no.linkedin.com/in/marcobertaniokland?trk=public_post_comment_actor-name) 9h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nHow does the index lifecycle management work in this case? It looks complex to do CRUD operations on the indexes. Have you experimented with this setup in production [Tom](https://nl.linkedin.com/in/tomaarsen?trk=public_post_comment-text)?\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://th.linkedin.com/in/prithivirajdamodaran?trk=public_post_comment_actor-image)\n\n[Prithivi Da](https://th.linkedin.com/in/prithivirajdamodaran?trk=public_post_comment_actor-name) 2h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\n\u003e You can perform 200ms search over 40 million texts using just a CPU server, 8GB of RAM, and 40GB of disk space At, What max query token length ? What quality ? Share MRR or NDCG What passage max len ? Without some baseline range it‚Äôs not attractive.\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply) 1 Reaction \n\n[](https://ca.linkedin.com/in/oliviermills?trk=public_post_comment_actor-image)\n\n[Olivier Mills](https://ca.linkedin.com/in/oliviermills?trk=public_post_comment_actor-name) 11h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\n[Nils Reimers](https://de.linkedin.com/in/reimersnils?trk=public_post_comment-text) popularized this 2 years ago [https://cohere.com/blog/int8-binary-embeddings](https://cohere.com/blog/int8-binary-embeddings?trk=public_post_comment-text)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[6 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 7 Reactions \n\n[](https://dk.linkedin.com/in/aleksandr-dekan?trk=public_post_comment_actor-image)\n\n[Aleksandr Dekan](https://dk.linkedin.com/in/aleksandr-dekan?trk=public_post_comment_actor-name) 5h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nCan you share the Embeddings of all 40M papers? It is quite a lot, and CPU won't be enough\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply) 1 Reaction \n\n[](https://www.linkedin.com/in/charlesmartin14?trk=public_post_comment_actor-image)\n\n[Charles H. Martin, PhD](https://www.linkedin.com/in/charlesmartin14?trk=public_post_comment_actor-name) 8h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nFast, but not still good enough for production. At scale, for every 100ms latency, you lose 1% revenue. A simple rankSVM operates at under 10ms on commodity hardware. [https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales](https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales?trk=public_post_comment-text)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[4 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 5 Reactions \n\n[](https://be.linkedin.com/in/sebastien-campion/en?trk=public_post_comment_actor-image)\n\n[S√©bastien Campion](https://be.linkedin.com/in/sebastien-campion/en?trk=public_post_comment_actor-name) 7h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nHi üëã I also implemented product quantization in JavaScript so same efficiency but, cherry on the cake, it run on the client side [https://github.com/scampion/pqjs](https://github.com/scampion/pqjs?trk=public_post_comment-text)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://hu.linkedin.com/in/adaamko?trk=public_post_comment_actor-image)\n\n[√Åd√°m Kov√°cs](https://hu.linkedin.com/in/adaamko?trk=public_post_comment_actor-name) 10h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nThe difference between having to manage a server to store embeddings and doing it in memory is massive in terms of complexity overhead. So approaches like this can really matter.\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[2 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 3 Reactions \n\n[](https://www.linkedin.com/in/christopher-hyatt-09472b189?trk=public_post_comment_actor-image)\n\n[Christopher Hyatt](https://www.linkedin.com/in/christopher-hyatt-09472b189?trk=public_post_comment_actor-name) 5h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nCongratulations\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://fr.linkedin.com/in/ravindusomawansa?trk=public_post_comment_actor-image)\n\n[Ravindu Somawansa](https://fr.linkedin.com/in/ravindusomawansa?trk=public_post_comment_actor-name) 9h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nSuper nice, thanks for the explanation üòÅ\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://hk.linkedin.com/in/raphaelmansuy?trk=public_post_comment_actor-image)\n\n[Rapha√´l MANSUY](https://hk.linkedin.com/in/raphaelmansuy?trk=public_post_comment_actor-name) 10h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nBrillant !!!!!!!!!!!\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[2 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 3 Reactions \n\n[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_see-more-comments)\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_feed-cta-banner-cta)\n\n18,952 followers\n\n*   [377 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Ftomaarsen%2Frecent-activity%2F\u0026trk=public_post_follow-posts)\n\n[View Profile](https://nl.linkedin.com/in/tomaarsen?trk=public_post_follow-view-profile)[Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7414325916635381760\u0026trk=public_post_follow)\n\nExplore content categories\n--------------------------\n\n*   [Career](https://www.linkedin.com/top-content/career/)\n*   [Productivity](https://www.linkedin.com/top-content/productivity/)\n*   [Finance](https://www.linkedin.com/top-content/finance/)\n*   [Soft Skills \u0026 Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n*   [Project Management](https://www.linkedin.com/top-content/project-management/)\n*   [Education](https://www.linkedin.com/top-content/education/)\n*   [Technology](https://www.linkedin.com/top-content/technology/)\n*   [Leadership](https://www.linkedin.com/top-content/leadership/)\n*   [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n*   [User Experience](https://www.linkedin.com/top-content/user-experience/)\n\n Show more  Show less \n\n*   LinkedIn¬© 2026\n*   [About](https://about.linkedin.com/?trk=d_public_post_footer-about)\n*   [Accessibility](https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility)\n*   [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement)\n*   [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy)\n*   [Your California Privacy Choices](https://www.linkedin.com/legal/california-privacy-disclosure?trk=d_public_post_footer-california-privacy-rights-act)\n*   [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy)\n*   [Copyright Policy](https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy)\n*   [Brand Policy](https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy)\n*   [Guest Controls](https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls)\n*   [Community Guidelines](https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide)\n*   \n    *    ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic) \n    *    ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla) \n    *    ƒåe≈°tina (Czech) \n    *    Dansk (Danish) \n    *    Deutsch (German) \n    *    ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek) \n    *   **English (English)**\n    *    Espa√±ol (Spanish) \n    *    ŸÅÿßÿ±ÿ≥€å (Persian) \n    *    Suomi (Finnish) \n    *    Fran√ßais (French) \n    *    ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi) \n    *    Magyar (Hungarian) \n    *    Bahasa Indonesia (Indonesian) \n    *    Italiano (Italian) \n    *    ◊¢◊ë◊®◊ô◊™ (Hebrew) \n    *    Êó•Êú¨Ë™û (Japanese) \n    *    ÌïúÍµ≠Ïñ¥ (Korean) \n    *    ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi) \n    *    Bahasa Malaysia (Malay) \n    *    Nederlands (Dutch) \n    *    Norsk (Norwegian) \n    *    ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi) \n    *    Polski (Polish) \n    *    Portugu√™s (Portuguese) \n    *    Rom√¢nƒÉ (Romanian) \n    *    –†—É—Å—Å–∫–∏–π (Russian) \n    *    Svenska (Swedish) \n    *    ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu) \n    *    ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai) \n    *    Tagalog (Tagalog) \n    *    T√ºrk√ße (Turkish) \n    *    –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian) \n    *    Ti·∫øng Vi·ªát (Vietnamese) \n    *    ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified)) \n    *    Ê≠£È´î‰∏≠Êñá (Chinese (Traditional)) \n\n Language \n\nSign in to view more content\n----------------------------\n\nCreate your free account or sign in to continue your search\n\n Sign in \n\nWelcome back\n------------\n\n Email or phone  \n\n Password  \n\nShow\n\n[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password) Sign in \n\nor\n\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy).\n\nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)\n\nor\n\nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_contextual-sign-in-modal_join-link)\n\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n[](https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/blog/embedding-quantization",
          "was_fetched": true,
          "page": "Title: Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval\n\nURL Source: https://huggingface.co/blog/embedding-quantization\n\nMarkdown Content:\n[Back to Articles](https://huggingface.co/blog)\n\n[](https://huggingface.co/aamirshakir)\n\n[](https://huggingface.co/tomaarsen)\n\n[](https://huggingface.co/SeanLee97)\n\nWe introduce the concept of embedding quantization and showcase their impact on retrieval speed, memory usage, disk space, and cost. We'll discuss how embeddings can be quantized in theory and in practice, after which we introduce a [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showing a real-life retrieval scenario of 41 million Wikipedia texts.\n\n[](https://huggingface.co/blog/embedding-quantization#table-of-contents) Table of Contents\n------------------------------------------------------------------------------------------\n\n*   [Why Embeddings?](https://huggingface.co/blog/embedding-quantization#why-embeddings)\n    *   [Embeddings may struggle to scale](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale)\n\n*   [Improving scalability](https://huggingface.co/blog/embedding-quantization#improving-scalability)\n    *   [Binary Quantization](https://huggingface.co/blog/embedding-quantization#binary-quantization)\n        *   [Binary Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers)\n        *   [Binary Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases)\n\n    *   [Scalar (int8) Quantization](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization)\n        *   [Scalar Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers)\n        *   [Scalar Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases)\n\n    *   [Combining Binary and Scalar Quantization](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization)\n    *   [Quantization Experiments](https://huggingface.co/blog/embedding-quantization#quantization-experiments)\n    *   [Influence of Rescoring](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring)\n        *   [Binary Rescoring](https://huggingface.co/blog/embedding-quantization#binary-rescoring)\n        *   [Scalar (Int8) Rescoring](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring)\n        *   [Retrieval Speed](https://huggingface.co/blog/embedding-quantization#retrieval-speed)\n\n    *   [Performance Summarization](https://huggingface.co/blog/embedding-quantization#performance-summarization)\n    *   [Demo](https://huggingface.co/blog/embedding-quantization#demo)\n    *   [Try it yourself](https://huggingface.co/blog/embedding-quantization#try-it-yourself)\n    *   [Future work:](https://huggingface.co/blog/embedding-quantization#future-work)\n    *   [Acknowledgments](https://huggingface.co/blog/embedding-quantization#acknowledgments)\n    *   [Citation](https://huggingface.co/blog/embedding-quantization#citation)\n    *   [References](https://huggingface.co/blog/embedding-quantization#references)\n\n[](https://huggingface.co/blog/embedding-quantization#why-embeddings) Why Embeddings?\n-------------------------------------------------------------------------------------\n\nEmbeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects, like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors.\n\nAfter transforming the complex objects, you can determine their similarity by calculating the similarity of the respective embeddings! This is crucial for many use cases: it serves as the backbone for recommendation systems, retrieval, one-shot or few-shot learning, outlier detection, similarity search, paraphrase detection, clustering, classification, and much more.\n\n### [](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale) Embeddings may struggle to scale\n\nHowever, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in `float32`, i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory!\n\nThe table below gives an overview of different models, dimension size, memory requirement, and costs. Costs are computed at an estimated $3.8 per GB/mo with `x2gd` instances on AWS.\n\n| Embedding Dimension | Example Models | 100M Embeddings | 250M Embeddings | 1B Embeddings |\n| --- | --- | --- | --- | --- |\n| 384 | [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) | 143.05GB $543 / mo | 357.62GB $1,358 / mo | 1430.51GB $5,435 / mo |\n| 768 | [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) [jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1) | 286.10GB $1,087 / mo | 715.26GB $2,717 / mo | 2861.02GB $10,871 / mo |\n| 1024 | [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) | 381.46GB $1,449 / mo | 953.67GB $3,623 / mo | 3814.69GB $14,495 / mo |\n| 1536 | [OpenAI text-embedding-3-small](https://openai.com/blog/new-embedding-models-and-api-updates) | 572.20GB $2,174 / mo | 1430.51GB $5,435 / mo | 5722.04GB $21,743 / mo |\n| 3072 | [OpenAI text-embedding-3-large](https://openai.com/blog/new-embedding-models-and-api-updates) | 1144.40GB $4,348 / mo | 2861.02GB $10,871 / mo | 11444.09GB $43,487 / mo |\n\n[](https://huggingface.co/blog/embedding-quantization#improving-scalability) Improving scalability\n--------------------------------------------------------------------------------------------------\n\nThere are several ways to approach the challenges of scaling embeddings. The most common approach is dimensionality reduction, such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis). However, classic dimensionality reduction -- like PCA methods -- [tends to perform poorly when used with embeddings](https://arxiv.org/abs/2205.11498).\n\nIn recent news, [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) ([blogpost](https://huggingface.co/blog/matryoshka)) (MRL) as used by [OpenAI](https://openai.com/blog/new-embedding-models-and-api-updates) also allows for cheaper embeddings. With MRL, only the first `n` embedding dimensions are used. This approach has already been adopted by some open models like [nomic-ai/nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) and [mixedbread-ai/mxbai-embed-2d-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1) (For OpenAIs `text-embedding-3-large`, we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.).\n\nHowever, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: **Quantization**. Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs. Let's dive into it!\n\n### [](https://huggingface.co/blog/embedding-quantization#binary-quantization) Binary Quantization\n\nUnlike quantization in models where you reduce the precision of weights, quantization for embeddings refers to a post-processing step for the embeddings themselves. In particular, binary quantization refers to the conversion of the `float32` values in an embedding to 1-bit values, resulting in a 32x reduction in memory and storage usage.\n\nTo quantize `float32` embeddings to binary, we simply threshold normalized embeddings at 0:\n\nf(x)={0 if x‚â§0 1 if x\u003e0 f(x)= \\begin{cases} 0 \u0026 \\text{if } x\\leq 0\\\\ 1 \u0026 \\text{if } x \\gt 0 \\end{cases}\n\nWe can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.\n\n[Yamada et al. (2021)](https://arxiv.org/abs/2106.00882) introduced a rescore step, which they called _rerank_, to boost the performance. They proposed that the `float32` query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve `rescore_multiplier * top_k` results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the `float32` query embedding.\n\nBy applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers) Binary Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to binary would result in 1024 bits. In practice, it is much more common to store bits as bytes instead, so when we quantize to binary embeddings, we pack the bits into bytes using `np.packbits`.\n\nTherefore, quantizing a `float32` embedding with a dimensionality of 1024 yields an `int8` or `uint8` embedding with a dimensionality of 128. See two approaches of how you can produce quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2a. Encode some text using \"binary\" quantization\nbinary_embeddings = model.encode(\n    [\"I am driving to the lake.\", \"It is a beautiful day.\"],\n    precision=\"binary\",\n)\n```\n\nor\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2b. or, encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nbinary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere, you can see the differences between default `float32` embeddings and binary embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e binary_embeddings.shape\n(2, 128)\n\u003e\u003e\u003e binary_embeddings.nbytes\n256\n\u003e\u003e\u003e binary_embeddings.dtype\nint8\n```\n\nNote that you can also choose `\"ubinary\"` to quantize to binary using the unsigned `uint8` data format. This may be a requirement depending on your vector library/database.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases) Binary Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | [Yes](https://github.com/facebookresearch/faiss/wiki/Binary-indexes) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/schema-reference.html) |\n| Milvus | [Yes](https://milvus.io/docs/index.md) |\n| Qdrant | Through [Binary Quantization](https://qdrant.tech/documentation/guides/quantization/#binary-quantization) |\n| Weaviate | Through [Binary Quantization](https://weaviate.io/developers/weaviate/configuration/bq-compression) |\n\n### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization) Scalar (int8) Quantization\n\nWe use a scalar quantization process to convert the `float32` embeddings into `int8`. This involves mapping the continuous range of `float32` values to the discrete set of `int8` values, which can represent 256 distinct levels (from -128 to 127), as shown in the image below. This is done by using a large calibration dataset of embeddings. We compute the range of these embeddings, i.e., the `min` and `max` of each embedding dimension. From there, we calculate the steps (buckets) to categorize each value.\n\nTo further boost the retrieval performance, you can optionally apply the same rescoring step as for the binary embeddings. It is important to note that the calibration dataset greatly influences performance since it defines the quantization buckets.\n\n_Source: [https://qdrant.tech/articles/scalar-quantization/](https://qdrant.tech/articles/scalar-quantization/)_\n\nWith scalar quantization to `int8`, we reduce the original `float32` embeddings' precision so that each value is represented with an 8-bit integer (4x smaller). Note that this differs from the binary quantization case, where each value is represented by a single bit (32x smaller).\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers) Scalar Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to `int8` results in 1024 bytes. In practice, we can choose either `uint8` or `int8`. This choice is usually made depending on what your vector library/database supports.\n\nIn practice, it is recommended to provide the scalar quantization with either:\n\n1.   a large set of embeddings to quantize all at once, or\n2.   `min` and `max` ranges for each of the embedding dimensions, or\n3.   a large calibration dataset of embeddings from which the `min` and `max` ranges can be computed.\n\nIf none of these are the case, you will be given a warning like this: `Computing int8 quantization buckets based on 2 embeddings. int8 quantization is more stable with 'ranges' calculated from more embeddings or a 'calibration_embeddings' that can be used to calculate the buckets.`\n\nSee how you can produce scalar quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\nfrom datasets import load_dataset\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2. Prepare an example calibration dataset\ncorpus = load_dataset(\"nq_open\", split=\"train[:1000]\")[\"question\"]\ncalibration_embeddings = model.encode(corpus)\n\n# 3. Encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nint8_embeddings = quantize_embeddings(\n    embeddings,\n    precision=\"int8\",\n    calibration_embeddings=calibration_embeddings,\n)\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere you can see the differences between default `float32` embeddings and `int8` scalar embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e int8_embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e int8_embeddings.nbytes\n2048\n\u003e\u003e\u003e int8_embeddings.dtype\nint8\n```\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases) Scalar Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | Indirectly through [IndexHNSWSQ](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexHNSWSQ.html) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/tensor.html) |\n| OpenSearch | [Yes](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector) |\n| ElasticSearch | [Yes](https://www.elastic.co/de/blog/save-space-with-byte-sized-vectors) |\n| Milvus | Indirectly through [IVF_SQ8](https://milvus.io/docs/index.md) |\n| Qdrant | Indirectly through [Scalar Quantization](https://qdrant.tech/documentation/guides/quantization/#scalar-quantization) |\n\n### [](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization) Combining Binary and Scalar Quantization\n\nCombining binary and scalar quantization is possible to get the best of both worlds: the extreme speed from binary embeddings and the great performance preservation of scalar embeddings with rescoring. See the [demo](https://huggingface.co/blog/embedding-quantization#demo) below for a real-life implementation of this approach involving 41 million texts from Wikipedia. The pipeline for that setup is as follows:\n\n1.   The query is embedded using the [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) SentenceTransformer model.\n2.   The query is quantized to binary using the [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings) function from the `sentence-transformers` library.\n3.   A binary index (41M binary embeddings; 5.2GB of memory/disk space) is searched using the quantized query for the top 40 documents.\n4.   The top 40 documents are loaded on the fly from an int8 index on disk (41M int8 embeddings; 0 bytes of memory, 47.5GB of disk space).\n5.   The top 40 documents are rescored using the float32 query and the int8 embeddings to get the top 10 documents.\n6.   The top 10 documents are sorted by score and displayed.\n\nThrough this approach, we use 5.2GB of memory and 52GB of disk space for the indices. This is considerably less than normal retrieval, requiring 200GB of memory and 200GB of disk space. Especially as you scale up even further, this will result in notable reductions in latency and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#quantization-experiments) Quantization Experiments\n\nWe conducted our experiments on the retrieval subset of the [MTEB](https://huggingface.co/spaces/mteb/leaderboard) containing 15 benchmarks. First, we retrieved the top k (k=100) search results with a `rescore_multiplier` of 4. Therefore, we retrieved 400 results in total and performed the rescoring on these top 400. For the `int8` performance, we directly used the dot-product without any rescoring.\n\n| Model | Embedding Dimension | 250M Embeddings | MTEB Retrieval (NDCG@10) | Percentage of default performance |\n| --- | --- | --- | --- | --- |\n| **Open Models** |  |  |  |  |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): float32 | 1024 | 953.67GB $3623 / mo | 54.39 | 100% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): int8 | 1024 | 238.41GB $905 / mo | 52.79 | 97% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): binary | 1024 | 29.80GB $113.25 / mo | 52.46 | 96.45% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): float32 | 768 | 286.10GB $1087 / mo | 50.77 | 100% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): int8 | 768 | 178.81GB $679 / mo | 47.54 | 94.68% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): binary | 768 | 22.35GB $85 / mo | 37.96 | 74.77% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): float32 | 768 | 286.10GB $1087 / mo | 53.01 | 100% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): binary | 768 | 22.35GB $85 / mo | 46.49 | 87.7% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): float32 | 384 | 357.62GB $1358 / mo | 41.66 | 100% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): int8 | 384 | 89.40GB $339 / mo | 37.82 | 90.79% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): binary | 384 | 11.18GB $42 / mo | 39.07 | 93.79% |\n| **Proprietary Models** |  |  |  |  |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): float32 | 1024 | 953.67GB $3623 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): int8 | 1024 | 238.41GB $905 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): binary | 1024 | 29.80GB $113.25 / mo | 52.3 | 94.6% |\n\nSeveral key trends and benefits can be identified from the results of our quantization experiments. As expected, embedding models with higher dimension size typically generate higher storage costs per computation but achieve the best performance. Surprisingly, however, quantization to `int8` already helps `mxbai-embed-large-v1` and `Cohere-embed-english-v3.0` achieve higher performance with lower storage usage than that of the smaller dimension size base models.\n\nThe benefits of quantization are, if anything, even more clearly visible when looking at the results obtained with binary models. In that scenario, the 1024 dimension models still outperform a now 10x more storage intensive base model, and the `mxbai-embed-large-v1` even manages to hold more than 96% of performance after a 32x reduction in resource requirements. The further quantization from `int8` to binary barely results in any additional loss of performance for this model.\n\nInterestingly, we can also see that `all-MiniLM-L6-v2` exhibits stronger performance on binary than on `int8` quantization. A possible explanation for this could be the selection of calibration data. On `e5-base-v2`, we observe the effect of [dimension collapse](https://arxiv.org/abs/2110.09348), which causes the model to only use a subspace of the latent space; when performing the quantization, the whole space collapses further, leading to high performance losses.\n\nThis shows that quantization doesn't universally work with all embedding models. It remains crucial to consider exisiting benchmark outcomes and conduct experiments to determine a given model's compatibility with quantization.\n\n### [](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring) Influence of Rescoring\n\nIn this section we look at the influence of rescoring on retrieval performance. We evaluate the results based on [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1).\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-rescoring) Binary Rescoring\n\nWith binary embeddings, [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) retains 92.53% of performance on MTEB Retrieval. Just doing the rescoring without retrieving more samples pushes the performance to 96.45%. We experimented with setting the`rescore_multiplier` from 1 to 10, but observe no further boost in performance. This indicates that the `top_k` search already retrieved the top candidates and the rescoring reordered these good candidates appropriately.\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring) Scalar (Int8) Rescoring\n\nWe also evaluated the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) model with `int8` rescoring, as Cohere showed that [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) reached up to 100% of the performance of the `float32` model with `int8` quantization. For this experiment, we set the `rescore_multiplier` to [1, 4, 10] and got the following results:\n\nAs we can see from the diagram, a higher rescore multiplier implies better retention of performance after quantization. Extrapolating from our results, we assume the relation is likely hyperbolical with performance approaching 100% as the rescore multiplier continues to rise. A rescore multiplier of 4-5 already leads to a remarkable performance retention of 99% using `int8`.\n\n#### [](https://huggingface.co/blog/embedding-quantization#retrieval-speed) Retrieval Speed\n\nWe measured retrieval speed on a Google Cloud Platform `a2-highgpu-4g` instance using the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) embeddings with 1024 dimension on the whole MTEB Retrieval. For int8 we used [USearch](https://github.com/unum-cloud/usearch) (Version 2.9.2) and binary quantization [Faiss](https://github.com/facebookresearch/faiss) (Version 1.8.0). Everything was computed on CPU using exact search.\n\n| Quantization | Min | Mean | Max |\n| --- | --- | --- | --- |\n| `float32` | 1x (baseline) | **1x** (baseline) | 1x (baseline) |\n| `int8` | 2.99x speedup | **3.66x** speedup | 4.8x speedup |\n| `binary` | 15.05x speedup | **24.76x** speedup | 45.8x speedup |\n\nAs shown in the table, applying `int8` scalar quantization results in an average speedup of 3.66x compared to full-size `float32` embeddings. Additionally, binary quantization achieves a speedup of 24.76x on average. For both scalar and binary quantization, even the worst case scenario resulted in very notable speedups.\n\n### [](https://huggingface.co/blog/embedding-quantization#performance-summarization) Performance Summarization\n\nThe experimental results, effects on resource use, retrieval speed, and retrieval performance by using quantization can be summarized as follows:\n\n|  | float32 | int8/uint8 | binary/ubinary |\n| --- | --- | --- | --- |\n| **Memory \u0026 Index size savings** | 1x | exactly 4x | exactly 32x |\n| **Retrieval Speed** | 1x | up to 4x | up to 45x |\n| **Percentage of default performance** | 100% | ~99.3% | ~96% |\n\n### [](https://huggingface.co/blog/embedding-quantization#demo) Demo\n\nThe following [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showcases the retrieval efficiency using exact or approximate search by combining binary search with scalar (`int8`) rescoring. The solution requires 5GB of memory for the binary index and 50GB of disk space for the binary and scalar indices, considerably less than the 200GB of memory and disk space which would be required for regular `float32` retrieval. Additionally, retrieval is much faster.\n\n### [](https://huggingface.co/blog/embedding-quantization#try-it-yourself) Try it yourself\n\nThe following scripts can be used to experiment with embedding quantization for retrieval \u0026 beyond. There are three categories:\n\n*   **Recommended Retrieval**:\n    *   [semantic_search_recommended.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_recommended.py): This script combines binary search with scalar rescoring, much like the above demo, for cheap, efficient, and performant retrieval.\n\n*   **Usage**:\n    *   [semantic_search_faiss.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using FAISS, by using the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function.\n    *   [semantic_search_usearch.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using USearch, by using the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function.\n\n*   **Benchmarks**:\n    *   [semantic_search_faiss_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using FAISS. It uses the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function. Our benchmarks especially show show speedups for `ubinary`.\n    *   [semantic_search_usearch_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using USearch. It uses the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function. Our experiments show large speedups on newer hardware, particularly for `int8`.\n\n### [](https://huggingface.co/blog/embedding-quantization#future-work) Future work\n\nWe are looking forward to further advancements of binary quantization. To name a few potential improvements, we suspect that there may be room for scalar quantization smaller than `int8`, i.e. with 128 or 64 buckets instead of 256.\n\nAdditionally, we are excited that embedding quantization is fully perpendicular to Matryoshka Representation Learning (MRL). In other words, it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually corresponds with a 2% reduction in performance) and then apply binary or scalar quantization. We suspect this could speed up retrieval up to 32x for a ~3% reduction in quality, or up to 256x for a ~10% reduction in quality.\n\nLastly, we recognize that retrieval using embedding quantization can also be combined with a separate reranker model. We imagine that a 3-step pipeline of binary search, scalar (int8) rescoring, and cross-encoder reranking allows for state-of-the-art retrieval performance at low latencies, memory usage, disk space, and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#acknowledgments) Acknowledgments\n\nThis project is possible thanks to our collaboration with [mixedbread.ai](https://mixedbread.ai/) and the [SentenceTransformers](https://www.sbert.net/) library, which allows you to easily create sentence embeddings and quantize them. If you want to use quantized embeddings in your project, now you know how!\n\n### [](https://huggingface.co/blog/embedding-quantization#citation) Citation\n\n```\n@article{shakir2024quantization,\n  author       = { Aamir Shakir and\n                   Tom Aarsen and\n                   Sean Lee\n                 },\n  title = { Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval },\n  journal = {Hugging Face Blog},\n  year = {2024},\n  note = {https://huggingface.co/blog/embedding-quantization},\n}\n```\n\n### [](https://huggingface.co/blog/embedding-quantization#resources) Resources\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n*   [Sentence Transformers docs - Embedding Quantization](https://sbert.net/examples/applications/embedding-quantization/README.html)\n*   [https://txt.cohere.com/int8-binary-embeddings/](https://txt.cohere.com/int8-binary-embeddings/)\n*   [https://qdrant.tech/documentation/guides/quantization](https://qdrant.tech/documentation/guides/quantization)\n*   [https://zilliz.com/learn/scalar-quantization-and-product-quantization](https://zilliz.com/learn/scalar-quantization-and-product-quantization)",
          "was_summarised": false
        },
        {
          "url": "https://cohere.com/blog/int8-binary-embeddings",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:55:50.555983591Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/spaces/sentence-transformers/quantized-retrieval: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:54:51.362675858Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://cohere.com/blog/int8-binary-embeddings: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:55:50.555980083Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q5mh84",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/",
      "title": "Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)",
      "content": "Hey Everyone,\n\n  \nI've been working on something for Mac users in the ML space.  \n  \nUnsloth-MLX - an MLX-powered library that brings the Unsloth fine-tuning experience to Apple Silicon.  \n  \nThe idea is simple:  \n  \n‚Üí Prototype your LLM fine-tuning locally on Mac  \n‚Üí Same code works on cloud GPUs with original Unsloth  \n‚Üí No API changes, just swap the import  \n  \nWhy? Cloud GPU costs add up fast during experimentation. Your Mac's unified memory (up to 512GB on Mac Studio) is sitting right there.  \n   \nIt's not a replacement for Unsloth - it's a bridge for local development before scaling up.  \n  \nStill early days - would really appreciate feedback, bug reports, or feature requests.\n\n\n\nGithub: [https://github.com/ARahim3/unsloth-mlx](https://github.com/ARahim3/unsloth-mlx)",
      "author": "A-Rahim",
      "created_at": "2026-01-06T16:00:10Z",
      "comments": [
        {
          "id": "ny1qgkq",
          "author": "davernow",
          "content": "Dunno about using their name in your product name. It‚Äôs a cool idea, but the name is just going to cause confusion.",
          "created_at": "2026-01-06T18:06:54Z",
          "was_summarised": false
        },
        {
          "id": "ny2f20n",
          "author": "yoracale",
          "content": "There was also this PR today by an Unsloth contributor directly for the Unsloth repo: https://github.com/unslothai/unsloth/pull/3856\n\nWe're still working on reviewing it and OP if you have any feedback or contributions you'd like to add directly to the repo please let us know üôè\n\nAnd OP u/A-rahim can you please specify in your post that it's not affiliated with Unsloth please. Thanks.",
          "created_at": "2026-01-06T19:57:43Z",
          "urls": [
            {
              "url": "https://github.com/unslothai/unsloth/pull/3856",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny19ggu",
          "author": "indicava",
          "content": "\u0026gt; It's not a replacement for Unsloth - it's a bridge for local development before scaling up.\n\nAt least we know inference works‚Ä¶\n\njk OP, nice effort! Will definitely test this out on my MBP this weekend",
          "created_at": "2026-01-06T16:50:09Z",
          "was_summarised": false
        },
        {
          "id": "ny1vg3t",
          "author": "Marksta",
          "content": "# Determine number of layers\n    if num_layers is None:\n        # Try to detect from model structure\n        if hasattr(self.model, 'layers'):\n            num_layers = len(self.model.layers)\n        elif hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n            num_layers = len(self.model.model.layers)\n        else:\n            num_layers = 16  # Default fallback\n\nY-Yeah, that looks right! Just silently fall back to 16 layers, that should do the trick...",
          "created_at": "2026-01-06T18:28:54Z",
          "was_summarised": false
        },
        {
          "id": "ny11xtn",
          "author": "kyrylogorbachov",
          "content": "Any performance benchmarks? It's not apple to apple, it's apple to Unsloth, but still would be nice to see something.",
          "created_at": "2026-01-06T16:15:52Z",
          "was_summarised": false
        },
        {
          "id": "ny1e01i",
          "author": "hashmortar",
          "content": "This is great for playing around! Thanks for sharing",
          "created_at": "2026-01-06T17:10:46Z",
          "was_summarised": false
        },
        {
          "id": "ny28yvd",
          "author": "idkwhattochoo",
          "content": "- Qwen2-VL / Qwen2.5-VL (recommended)\n\n\nLot of mentions of o l d models with heavy reek of vibecode... Almost everything feels vibecoded¬†\n\n\nWhat's wrong with existing MLX?¬†\n\n\nI wish people do try make use of ANE like how nexa sdk done so far with limited models",
          "created_at": "2026-01-06T19:29:50Z",
          "was_summarised": false
        },
        {
          "id": "ny3h5t5",
          "author": "ThomasPhilli",
          "content": "This is awesome. Question: what is the RAM requirement?\n\nI have a 16GB Mac Mini , how large of a model can I fine-tune? 1B?",
          "created_at": "2026-01-06T22:55:27Z",
          "was_summarised": false
        },
        {
          "id": "ny4icp0",
          "author": "BumbleSlob",
          "content": "Downvoted for shamelessly stealing unsloth‚Äôs branding",
          "created_at": "2026-01-07T02:11:02Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/ARahim3/unsloth-mlx",
          "was_fetched": true,
          "page": "Title: GitHub - ARahim3/unsloth-mlx: Bringing the Unsloth experience to Mac users via Apple's MLX framework\n\nURL Source: https://github.com/ARahim3/unsloth-mlx\n\nMarkdown Content:\nGitHub - ARahim3/unsloth-mlx: Bringing the Unsloth experience to Mac users via Apple's MLX framework\n===============\n\n[Skip to content](https://github.com/ARahim3/unsloth-mlx#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FARahim3%2Funsloth-mlx)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FARahim3%2Funsloth-mlx)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=ARahim3%2Funsloth-mlx)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/ARahim3/unsloth-mlx) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/ARahim3/unsloth-mlx) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/ARahim3/unsloth-mlx) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[ARahim3](https://github.com/ARahim3)/**[unsloth-mlx](https://github.com/ARahim3/unsloth-mlx)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2FARahim3%2Funsloth-mlx)You must be signed in to change notification settings\n*   [Fork 14](https://github.com/login?return_to=%2FARahim3%2Funsloth-mlx)\n*   [Star 144](https://github.com/login?return_to=%2FARahim3%2Funsloth-mlx) \n\nBringing the Unsloth experience to Mac users via Apple's MLX framework\n\n### License\n\n[Apache-2.0 license](https://github.com/ARahim3/unsloth-mlx/blob/main/LICENSE)\n\n[144 stars](https://github.com/ARahim3/unsloth-mlx/stargazers)[14 forks](https://github.com/ARahim3/unsloth-mlx/forks)[Branches](https://github.com/ARahim3/unsloth-mlx/branches)[Tags](https://github.com/ARahim3/unsloth-mlx/tags)[Activity](https://github.com/ARahim3/unsloth-mlx/activity)\n\n[Star](https://github.com/login?return_to=%2FARahim3%2Funsloth-mlx)\n\n[Notifications](https://github.com/login?return_to=%2FARahim3%2Funsloth-mlx)You must be signed in to change notification settings\n\n*   [Code](https://github.com/ARahim3/unsloth-mlx)\n*   [Issues 0](https://github.com/ARahim3/unsloth-mlx/issues)\n*   [Pull requests 0](https://github.com/ARahim3/unsloth-mlx/pulls)\n*   [Actions](https://github.com/ARahim3/unsloth-mlx/actions)\n*   [Projects 0](https://github.com/ARahim3/unsloth-mlx/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/ARahim3/unsloth-mlx).](https://github.com/ARahim3/unsloth-mlx/security)\n*   [Insights](https://github.com/ARahim3/unsloth-mlx/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/ARahim3/unsloth-mlx)\n*   [Issues](https://github.com/ARahim3/unsloth-mlx/issues)\n*   [Pull requests](https://github.com/ARahim3/unsloth-mlx/pulls)\n*   [Actions](https://github.com/ARahim3/unsloth-mlx/actions)\n*   [Projects](https://github.com/ARahim3/unsloth-mlx/projects)\n*   [Security](https://github.com/ARahim3/unsloth-mlx/security)\n*   [Insights](https://github.com/ARahim3/unsloth-mlx/pulse)\n\nARahim3/unsloth-mlx\n===================\n\nmain\n\n[Branches](https://github.com/ARahim3/unsloth-mlx/branches)[Tags](https://github.com/ARahim3/unsloth-mlx/tags)\n\n[](https://github.com/ARahim3/unsloth-mlx/branches)[](https://github.com/ARahim3/unsloth-mlx/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [10 Commits](https://github.com/ARahim3/unsloth-mlx/commits/main/) [](https://github.com/ARahim3/unsloth-mlx/commits/main/) |\n| [docs](https://github.com/ARahim3/unsloth-mlx/tree/main/docs \"docs\") | [docs](https://github.com/ARahim3/unsloth-mlx/tree/main/docs \"docs\") |  |  |\n| [examples](https://github.com/ARahim3/unsloth-mlx/tree/main/examples \"examples\") | [examples](https://github.com/ARahim3/unsloth-mlx/tree/main/examples \"examples\") |  |  |\n| [tests](https://github.com/ARahim3/unsloth-mlx/tree/main/tests \"tests\") | [tests](https://github.com/ARahim3/unsloth-mlx/tree/main/tests \"tests\") |  |  |\n| [unsloth_mlx](https://github.com/ARahim3/unsloth-mlx/tree/main/unsloth_mlx \"unsloth_mlx\") | [unsloth_mlx](https://github.com/ARahim3/unsloth-mlx/tree/main/unsloth_mlx \"unsloth_mlx\") |  |  |\n| [.gitignore](https://github.com/ARahim3/unsloth-mlx/blob/main/.gitignore \".gitignore\") | [.gitignore](https://github.com/ARahim3/unsloth-mlx/blob/main/.gitignore \".gitignore\") |  |  |\n| [LICENSE](https://github.com/ARahim3/unsloth-mlx/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/ARahim3/unsloth-mlx/blob/main/LICENSE \"LICENSE\") |  |  |\n| [README.md](https://github.com/ARahim3/unsloth-mlx/blob/main/README.md \"README.md\") | [README.md](https://github.com/ARahim3/unsloth-mlx/blob/main/README.md \"README.md\") |  |  |\n| [pyproject.toml](https://github.com/ARahim3/unsloth-mlx/blob/main/pyproject.toml \"pyproject.toml\") | [pyproject.toml](https://github.com/ARahim3/unsloth-mlx/blob/main/pyproject.toml \"pyproject.toml\") |  |  |\n| [requirements.txt](https://github.com/ARahim3/unsloth-mlx/blob/main/requirements.txt \"requirements.txt\") | [requirements.txt](https://github.com/ARahim3/unsloth-mlx/blob/main/requirements.txt \"requirements.txt\") |  |  |\n| [sample_train.jsonl](https://github.com/ARahim3/unsloth-mlx/blob/main/sample_train.jsonl \"sample_train.jsonl\") | [sample_train.jsonl](https://github.com/ARahim3/unsloth-mlx/blob/main/sample_train.jsonl \"sample_train.jsonl\") |  |  |\n| [train_data.jsonl](https://github.com/ARahim3/unsloth-mlx/blob/main/train_data.jsonl \"train_data.jsonl\") | [train_data.jsonl](https://github.com/ARahim3/unsloth-mlx/blob/main/train_data.jsonl \"train_data.jsonl\") |  |  |\n| [unsloth_mlx_logo_f.png](https://github.com/ARahim3/unsloth-mlx/blob/main/unsloth_mlx_logo_f.png \"unsloth_mlx_logo_f.png\") | [unsloth_mlx_logo_f.png](https://github.com/ARahim3/unsloth-mlx/blob/main/unsloth_mlx_logo_f.png \"unsloth_mlx_logo_f.png\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/ARahim3/unsloth-mlx#)\n*   [Apache-2.0 license](https://github.com/ARahim3/unsloth-mlx#)\n\n[](https://github.com/ARahim3/unsloth-mlx/blob/main/unsloth_mlx_logo_f.png)\n\nUnsloth-MLX\n===========\n\n[](https://github.com/ARahim3/unsloth-mlx#unsloth-mlx)\n\n**Fine-tune LLMs on your Mac with Apple Silicon**\n\n_Prototype locally, scale to cloud. Same code, just change the import._\n\n[](https://github.com/ARahim3/unsloth-mlx#installation)[](https://github.com/ARahim3/unsloth-mlx#requirements)[](https://github.com/ml-explore/mlx)[](https://github.com/ARahim3/unsloth-mlx#license)\n\n[Quick Start](https://github.com/ARahim3/unsloth-mlx#quick-start) ¬∑ [Training Methods](https://github.com/ARahim3/unsloth-mlx#supported-training-methods) ¬∑ [Examples](https://github.com/ARahim3/unsloth-mlx#examples) ¬∑ [Status](https://github.com/ARahim3/unsloth-mlx#project-status)\n\n* * *\n\nWhy Unsloth-MLX?\n----------------\n\n[](https://github.com/ARahim3/unsloth-mlx#why-unsloth-mlx)\n\nBringing the [Unsloth](https://github.com/unslothai/unsloth) experience to Mac users via Apple's [MLX](https://github.com/ml-explore/mlx) framework.\n\n*   üöÄ **Fine-tune LLMs locally** on your Mac (M1/M2/M3/M4/M5)\n*   üíæ **Leverage unified memory** (up to 512GB on Mac Studio)\n*   üîÑ **Same API as Unsloth** - your existing code just works!\n*   üì¶ **Export anywhere** - HuggingFace format, GGUF for Ollama/llama.cpp\n\nundefinedpython\n# Unsloth (CUDA)                        # Unsloth-MLX (Apple Silicon)\nfrom unsloth import FastLanguageModel   from unsloth_mlx import FastLanguageModel\nfrom trl import SFTTrainer              from unsloth_mlx import SFTTrainer\n\n# Rest of your code stays exactly the same!\nundefined\n\nWhat This Is (and Isn't)\n------------------------\n\n[](https://github.com/ARahim3/unsloth-mlx#what-this-is-and-isnt)\n\n**This is NOT** a replacement for Unsloth or an attempt to compete with it. Unsloth is incredible - it's the gold standard for efficient LLM fine-tuning on CUDA.\n\n**This IS** a bridge for Mac users who want to:\n\n*   üß™ **Prototype locally** - Experiment with fine-tuning before committing to cloud GPU costs\n*   üìö **Learn \u0026 iterate** - Develop your training pipeline with fast local feedback loops\n*   üîÑ **Then scale up** - Move to cloud NVIDIA GPUs + original Unsloth for production training\n\n```\nLocal Mac (Unsloth-MLX)     ‚Üí     Cloud GPU (Unsloth)\n   Prototype \u0026 experiment          Full-scale training\n   Small datasets                  Large datasets\n   Quick iterations                Production runs\n```\n\nProject Status\n--------------\n\n[](https://github.com/ARahim3/unsloth-mlx#project-status)\n\n\u003e üöÄ **v0.3.0** - Native training with proper RL losses!\n\n| Feature | Status | Notes |\n| --- | --- | --- |\n| SFT Training | ‚úÖ Stable | Native MLX training |\n| Model Loading | ‚úÖ Stable | Any HuggingFace model |\n| Save/Export | ‚úÖ Stable | HF format, GGUF |\n| DPO Training | ‚úÖ Stable | **Full DPO loss** |\n| ORPO Training | ‚úÖ Stable | **Full ORPO loss** |\n| GRPO Training | ‚úÖ Stable | **Multi-generation + reward** |\n| KTO/SimPO | ‚úÖ Stable | Proper loss implementations |\n| Vision Models | ‚ö†Ô∏è Beta | Via mlx-vlm |\n| PyPI Package | üîú Soon | Install from source for now |\n\nInstallation\n------------\n\n[](https://github.com/ARahim3/unsloth-mlx#installation)\n\nundefinedshell\n# From source (recommended for now)\ngit clone https://github.com/ARahim3/unsloth-mlx.git\ncd unsloth-mlx\n\n# Using uv (recommended - faster and more reliable)\nuv pip install -e .\n\n# Or using pip\npip install -e .\n\n# PyPI coming soon!\n# uv pip install unsloth-mlx\nundefined\n\nQuick Start\n-----------\n\n[](https://github.com/ARahim3/unsloth-mlx#quick-start)\n\nundefinedpython\nfrom unsloth_mlx import FastLanguageModel, SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\n# Load any HuggingFace model (1B model for quick start)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=16,\n)\n\n# Load a dataset (or create your own)\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:100]\")\n\n# Train with SFTTrainer (same API as TRL!)\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    args=SFTConfig(\n        output_dir=\"outputs\",\n        per_device_train_batch_size=2,\n        learning_rate=2e-4,\n        max_steps=50,\n    ),\n)\ntrainer.train()\n\n# Save (same API as Unsloth!)\nmodel.save_pretrained(\"lora_model\")  # Adapters only\nmodel.save_pretrained_merged(\"merged\", tokenizer)  # Full model\nmodel.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")  # GGUF\nundefined\n\nSupported Training Methods\n--------------------------\n\n[](https://github.com/ARahim3/unsloth-mlx#supported-training-methods)\n\n| Method | Trainer | Implementation | Use Case |\n| --- | --- | --- | --- |\n| **SFT** | `SFTTrainer` | ‚úÖ Native MLX | Instruction fine-tuning |\n| **DPO** | `DPOTrainer` | ‚úÖ Native MLX | Preference learning (proper log-prob loss) |\n| **ORPO** | `ORPOTrainer` | ‚úÖ Native MLX | Combined SFT + odds ratio preference |\n| **GRPO** | `GRPOTrainer` | ‚úÖ Native MLX | Reasoning with multi-generation (DeepSeek R1 style) |\n| **KTO** | `KTOTrainer` | ‚úÖ Native MLX | Kahneman-Tversky optimization |\n| **SimPO** | `SimPOTrainer` | ‚úÖ Native MLX | Simple preference optimization |\n| **VLM** | `VLMSFTTrainer` | ‚ö†Ô∏è Beta | Vision-Language models |\n\nExamples\n--------\n\n[](https://github.com/ARahim3/unsloth-mlx#examples)\n\nCheck [`examples/`](https://github.com/ARahim3/unsloth-mlx/blob/main/examples) for working code:\n\n*   Basic model loading and inference\n*   Complete SFT fine-tuning pipeline\n*   RL training methods (DPO, GRPO, ORPO)\n\nRequirements\n------------\n\n[](https://github.com/ARahim3/unsloth-mlx#requirements)\n\n*   **Hardware**: Apple Silicon Mac (M1/M2/M3/M4/M5)\n*   **OS**: macOS 13.0+ (15.0+ recommended for large models)\n*   **Memory**: 16GB+ unified RAM (32GB+ for 7B+ models)\n*   **Python**: 3.9+\n\nComparison with Unsloth\n-----------------------\n\n[](https://github.com/ARahim3/unsloth-mlx#comparison-with-unsloth)\n\n| Feature | Unsloth (CUDA) | Unsloth-MLX |\n| --- | --- | --- |\n| Platform | NVIDIA GPUs | Apple Silicon |\n| Backend | Triton Kernels | MLX Framework |\n| Memory | VRAM (limited) | Unified (up to 512GB) |\n| API | Original | 100% Compatible |\n| Best For | Production training | Local dev, large models |\n\nContributing\n------------\n\n[](https://github.com/ARahim3/unsloth-mlx#contributing)\n\nContributions welcome! Areas that need help:\n\n*   Custom MLX kernels for even faster training\n*   More comprehensive test coverage\n*   Documentation and examples\n*   Testing on different M-series chips (M1, M2, M3, M4, M5)\n*   VLM training improvements\n\nLicense\n-------\n\n[](https://github.com/ARahim3/unsloth-mlx#license)\n\nApache 2.0 - See [LICENSE](https://github.com/ARahim3/unsloth-mlx/blob/main/LICENSE) file.\n\nAcknowledgments\n---------------\n\n[](https://github.com/ARahim3/unsloth-mlx#acknowledgments)\n\n*   [Unsloth](https://github.com/unslothai/unsloth) - The original, incredible CUDA library\n*   [MLX](https://github.com/ml-explore/mlx) - Apple's ML framework\n*   [MLX-LM](https://github.com/ml-explore/mlx-lm) - LLM utilities for MLX\n*   [MLX-VLM](https://github.com/Blaizzy/mlx-vlm) - Vision model support\n\n* * *\n\n**Community project, not affiliated with Unsloth AI or Apple.**\n\n ‚≠ê Star this repo if you find it useful!\n\nAbout\n-----\n\nBringing the Unsloth experience to Mac users via Apple's MLX framework\n\n### Resources\n\n[Readme](https://github.com/ARahim3/unsloth-mlx#readme-ov-file)\n\n### License\n\n[Apache-2.0 license](https://github.com/ARahim3/unsloth-mlx#Apache-2.0-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ARahim3/unsloth-mlx).\n\n[Activity](https://github.com/ARahim3/unsloth-mlx/activity)\n\n### Stars\n\n[**144** stars](https://github.com/ARahim3/unsloth-mlx/stargazers)\n\n### Watchers\n\n[**1** watching](https://github.com/ARahim3/unsloth-mlx/watchers)\n\n### Forks\n\n[**14** forks](https://github.com/ARahim3/unsloth-mlx/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FARahim3%2Funsloth-mlx\u0026report=ARahim3+%28user%29)\n\n[Releases](https://github.com/ARahim3/unsloth-mlx/releases)\n-----------------------------------------------------------\n\nNo releases published\n\n[Packages 0](https://github.com/users/ARahim3/packages?repo_name=unsloth-mlx)\n-----------------------------------------------------------------------------\n\n No packages published \n\nLanguages\n---------\n\n*   [Python 100.0%](https://github.com/ARahim3/unsloth-mlx/search?l=python)\n\nFooter\n------\n\n[](https://github.com/) ¬© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You can‚Äôt perform that action at this time.",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/lf2sfats4rbg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:55:51.580027205Z"
    },
    {
      "flow_id": "",
      "id": "1q5qsvd",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/",
      "title": "The FinePDFs üìÑ Book",
      "content": "Hey friends, Hynek from HuggingFace here.  \n  \nWe have released FinePDFs dataset of 3T tokens last year and we felt obliged to share the knowledge with there rest of OSS community.  \n  \nThe HuggingFace Press, has been pulling an extra hours through the Christmas, to put everything we know about PDFs inside:  \n\\- How to make the SoTA PDFs dataset?   \n\\- How much old internet is dead now?  \n\\- Why we chose RolmOCR for OCR  \n\\- What's the most Claude like OSS model?  \n\\- Why is the horse racing site topping the FinePDFs URL list?  \n  \nWe hope you like it :)\n\nhttps://preview.redd.it/z49knj5fwrbg1.png?width=1373\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8\n\n",
      "author": "Other_Housing8453",
      "created_at": "2026-01-06T18:34:44Z",
      "comments": [
        {
          "id": "ny39ndr",
          "author": "FullOf_Bad_Ideas",
          "content": "Thanks. FineWeb2 and FinePDFs are awesome datasets and they helped me a lot when I was messing with pre-training my own LLM. Pretty much the best off-the-shelf options for Polish.",
          "created_at": "2026-01-06T22:19:05Z",
          "was_summarised": false
        },
        {
          "id": "ny1wwmn",
          "author": "Other_Housing8453",
          "content": "Link here: [https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog](https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog)",
          "created_at": "2026-01-06T18:35:33Z",
          "urls": [
            {
              "url": "https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny3cpwq",
          "author": "Xamanthas",
          "content": "Thanks for this, some of the only not garbage-tier dataset releases left come from HF directly. Its gotten to the point where I made a userscript to block specific users/hide datasets in search results because in a given topic theres like what maybe 6% of results that are actually usable",
          "created_at": "2026-01-06T22:33:45Z",
          "was_summarised": false
        },
        {
          "id": "ny3uvpr",
          "author": "DHasselhoff77",
          "content": "This was a great read. Very clearly presented. Thanks! P.S. The dataset looks fine too.",
          "created_at": "2026-01-07T00:06:09Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://preview.redd.it/z49knj5fwrbg1.png?width=1373\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:55:51.580065097Z"
    },
    {
      "flow_id": "",
      "id": "1q5ta4l",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/",
      "title": "llama-benchy - llama-bench style benchmarking for ANY LLM backend",
      "content": "TL;DR: I've built this tool primarily for myself as I couldn't easily compare model performance across different backends in the way that is easy to digest and useful for me. I decided to share this in case someone has the same need.\n\n## Why I built this?\n\nAs probably many of you here, I've been happily using llama-bench to benchmark local models performance running in llama.cpp. One great feature is that it can help to evaluate performance at different context lengths and present the output in a table format that is easy to digest.\n\nHowever, llama.cpp is not the only inference engine I use, I also use SGLang and vLLM. But llama-bench can only work with llama.cpp, and other benchmarking tools that I found are more focused on concurrency and total throughput.\n\nAlso, llama-bench performs measurements using the C++ engine directly which is not representative of the end user experience which can be quite different in practice.\n\nvLLM has its own powerful benchmarking tool, but while it can be used with other inference engines, there are a few issues:\n\n- You can't easily measure how prompt processing speed degrades as context grows. You can use `vllm bench sweep serve`, but it only works well with vLLM with prefix caching disabled on the server. Even with random prompts it will reuse the same prompt between multiple runs which will hit the cache in `llama-server` for instance. So you will get very low median TTFT times and very high prompt processing speeds. \n- The TTFT measurement it uses is not actually until the first usable token, it's until the very first data chunk from the server which may not contain any generated tokens in /v1/chat/completions mode.\n- Random dataset is the only ones that allows to specify an arbitrary number of tokens, but randomly generated token sequence doesn't let you adequately measure speculative decoding/MTP.\n\nAs of today, I haven't been able to find any existing benchmarking tool that brings llama-bench style measurements at different context lengths to any OpenAI-compatible endpoint.\n## What is llama-benchy?\n\nIt's a CLI benchmarking tool that measures:\n\n- Prompt Processing (pp)¬†and¬†Token Generation (tg)¬†speeds at different context lengths.\n- Allows to benchmark context prefill and follow up prompt separately.\n- Reports additional metrics, like time to first response, estimated prompt processing time and end-to-end time to first token.\n\nIt works with any OpenAI-compatible endpoint that exposes /v1/chat/completions and also:\n\n- Supports configurable prompt length (`--pp`), generation length (`--tg`), and context depth (`--depth`).\n- Can run multiple iterations (`--runs`) and report mean ¬± std.\n- Uses HuggingFace tokenizers for accurate token counts.\n- Downloads a book from Project Gutenberg to use as source text for prompts to ensure better benchmarking of spec.decoding/MTP models.\n- Supports executing a command after each run (e.g., to clear cache).\n- Configurable latency measurement mode to estimate server/network overhead and provide more accurate prompt processing numbers.\n\n## Quick Demo\n\nBenchmarking MiniMax 2.1 AWQ running on my dual Spark cluster with up to 100000 context:\n\n```bash\n# Run without installation\nuvx llama-benchy --base-url http://spark:8888/v1 --model cyankiwi/MiniMax-M2.1-AWQ-4bit --depth 0 4096 8192 16384 32768 65535 100000 --adapt-prompt --latency-mode generation --enable-prefix-caching\n```\n\nOutput:\n\n| model                          |             test |             t/s |         ttfr (ms) |      est_ppt (ms) |     e2e_ttft (ms) |\n|:-------------------------------|-----------------:|----------------:|------------------:|------------------:|------------------:|\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |           pp2048 | 3544.10 ¬± 37.29 |     688.41 ¬± 6.09 |     577.93 ¬± 6.09 |     688.45 ¬± 6.10 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |             tg32 |    36.11 ¬± 0.06 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_pp @ d4096 |  3150.63 ¬± 7.84 |    1410.55 ¬± 3.24 |    1300.06 ¬± 3.24 |    1410.58 ¬± 3.24 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_tg @ d4096 |    34.36 ¬± 0.08 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   pp2048 @ d4096 | 2562.47 ¬± 21.71 |     909.77 ¬± 6.75 |     799.29 ¬± 6.75 |     909.81 ¬± 6.75 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |     tg32 @ d4096 |    33.41 ¬± 0.05 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_pp @ d8192 | 2832.52 ¬± 12.34 |   3002.66 ¬± 12.57 |   2892.18 ¬± 12.57 |   3002.70 ¬± 12.57 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_tg @ d8192 |    31.38 ¬± 0.06 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   pp2048 @ d8192 | 2261.83 ¬± 10.69 |    1015.96 ¬± 4.29 |     905.48 ¬± 4.29 |    1016.00 ¬± 4.29 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |     tg32 @ d8192 |    30.55 ¬± 0.08 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_pp @ d16384 |  2473.70 ¬± 2.15 |    6733.76 ¬± 5.76 |    6623.28 ¬± 5.76 |    6733.80 ¬± 5.75 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_tg @ d16384 |    27.89 ¬± 0.04 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  pp2048 @ d16384 |  1824.55 ¬± 6.32 |    1232.96 ¬± 3.89 |    1122.48 ¬± 3.89 |    1233.00 ¬± 3.89 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |    tg32 @ d16384 |    27.21 ¬± 0.04 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_pp @ d32768 |  2011.11 ¬± 2.40 |  16403.98 ¬± 19.43 |  16293.50 ¬± 19.43 |  16404.03 ¬± 19.43 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_tg @ d32768 |    22.09 ¬± 0.07 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  pp2048 @ d32768 |  1323.21 ¬± 4.62 |    1658.25 ¬± 5.41 |    1547.77 ¬± 5.41 |    1658.29 ¬± 5.41 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |    tg32 @ d32768 |    21.81 ¬± 0.07 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_pp @ d65535 |  1457.71 ¬± 0.26 |   45067.98 ¬± 7.94 |   44957.50 ¬± 7.94 |   45068.01 ¬± 7.94 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_tg @ d65535 |    15.72 ¬± 0.04 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  pp2048 @ d65535 |   840.36 ¬± 2.35 |    2547.54 ¬± 6.79 |    2437.06 ¬± 6.79 |    2547.60 ¬± 6.80 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |    tg32 @ d65535 |    15.63 ¬± 0.02 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit | ctx_pp @ d100000 |  1130.05 ¬± 1.89 | 88602.31 ¬± 148.70 | 88491.83 ¬± 148.70 | 88602.37 ¬± 148.70 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit | ctx_tg @ d100000 |    12.14 ¬± 0.02 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit | pp2048 @ d100000 |   611.01 ¬± 2.50 |   3462.39 ¬± 13.73 |   3351.90 ¬± 13.73 |   3462.42 ¬± 13.73 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   tg32 @ d100000 |    12.05 ¬± 0.03 |                   |                   |                   |\n\nllama-benchy (0.1.0)\ndate: 2026-01-06 11:44:49 | latency mode: generation\n\n## GitHub\n\n[https://github.com/eugr/llama-benchy](https://github.com/eugr/llama-benchy)",
      "author": "Eugr",
      "created_at": "2026-01-06T20:02:33Z",
      "comments": [
        {
          "id": "ny2gnd3",
          "author": "Future_South6852",
          "content": "This is exactly what I've been looking for! The fact that you can benchmark across different backends with the same methodology is huge\n\n  \nBeen running SGLang and vLLM side by side and constantly switching between their different bench tools was getting annoying. Having everything in one place with consistent metrics will save me so much time",
          "created_at": "2026-01-06T20:05:05Z",
          "was_summarised": false
        },
        {
          "id": "ny2hx3a",
          "author": "Caryn_fornicatress",
          "content": "This is actually useful. Comparing pp/tg across different backends and context sizes is exactly what‚Äôs missing right now. The llama-bench style tables make it way easier to reason about real user-facing latency instead of just raw throughput. Nice work, especially the TTFT and cache pitfalls you‚Äôre calling out.",
          "created_at": "2026-01-06T20:11:00Z",
          "was_summarised": false
        },
        {
          "id": "ny3grp6",
          "author": "doradus_novae",
          "content": "This is awesome firing this up tonight.Good work",
          "created_at": "2026-01-06T22:53:31Z",
          "was_summarised": false
        },
        {
          "id": "ny3pyn6",
          "author": "thrownawaymane",
          "content": "Can this print me a [benchy](https://en.wikipedia.org/wiki/3DBenchy)?",
          "created_at": "2026-01-06T23:40:38Z",
          "urls": [
            {
              "url": "https://en.wikipedia.org/wiki/3DBenchy",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://spark:8888/v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/eugr/llama-benchy",
          "was_fetched": true,
          "page": "Title: GitHub - eugr/llama-benchy: llama-benchy - llama-bench style benchmarking tool for all backends\n\nURL Source: https://github.com/eugr/llama-benchy\n\nMarkdown Content:\nGitHub - eugr/llama-benchy: llama-benchy - llama-bench style benchmarking tool for all backends\n===============\n\n[Skip to content](https://github.com/eugr/llama-benchy#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Feugr%2Fllama-benchy)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Feugr%2Fllama-benchy)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=eugr%2Fllama-benchy)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/eugr/llama-benchy) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/eugr/llama-benchy) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/eugr/llama-benchy) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[eugr](https://github.com/eugr)/**[llama-benchy](https://github.com/eugr/llama-benchy)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)You must be signed in to change notification settings\n*   [Fork 1](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)\n*   [Star 7](https://github.com/login?return_to=%2Feugr%2Fllama-benchy) \n\nllama-benchy - llama-bench style benchmarking tool for all backends\n\n### License\n\n[MIT license](https://github.com/eugr/llama-benchy/blob/main/LICENSE)\n\n[7 stars](https://github.com/eugr/llama-benchy/stargazers)[1 fork](https://github.com/eugr/llama-benchy/forks)[Branches](https://github.com/eugr/llama-benchy/branches)[Tags](https://github.com/eugr/llama-benchy/tags)[Activity](https://github.com/eugr/llama-benchy/activity)\n\n[Star](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)\n\n[Notifications](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)You must be signed in to change notification settings\n\n*   [Code](https://github.com/eugr/llama-benchy)\n*   [Issues 0](https://github.com/eugr/llama-benchy/issues)\n*   [Pull requests 0](https://github.com/eugr/llama-benchy/pulls)\n*   [Actions](https://github.com/eugr/llama-benchy/actions)\n*   [Projects 0](https://github.com/eugr/llama-benchy/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/eugr/llama-benchy).](https://github.com/eugr/llama-benchy/security)\n*   [Insights](https://github.com/eugr/llama-benchy/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/eugr/llama-benchy)\n*   [Issues](https://github.com/eugr/llama-benchy/issues)\n*   [Pull requests](https://github.com/eugr/llama-benchy/pulls)\n*   [Actions](https://github.com/eugr/llama-benchy/actions)\n*   [Projects](https://github.com/eugr/llama-benchy/projects)\n*   [Security](https://github.com/eugr/llama-benchy/security)\n*   [Insights](https://github.com/eugr/llama-benchy/pulse)\n\neugr/llama-benchy\n=================\n\nmain\n\n[Branches](https://github.com/eugr/llama-benchy/branches)[Tags](https://github.com/eugr/llama-benchy/tags)\n\n[](https://github.com/eugr/llama-benchy/branches)[](https://github.com/eugr/llama-benchy/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [45 Commits](https://github.com/eugr/llama-benchy/commits/main/) [](https://github.com/eugr/llama-benchy/commits/main/) |\n| [src/llama_benchy](https://github.com/eugr/llama-benchy/tree/main/src/llama_benchy \"This path skips through empty directories\") | [src/llama_benchy](https://github.com/eugr/llama-benchy/tree/main/src/llama_benchy \"This path skips through empty directories\") |  |  |\n| [.gitignore](https://github.com/eugr/llama-benchy/blob/main/.gitignore \".gitignore\") | [.gitignore](https://github.com/eugr/llama-benchy/blob/main/.gitignore \".gitignore\") |  |  |\n| [LICENSE](https://github.com/eugr/llama-benchy/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/eugr/llama-benchy/blob/main/LICENSE \"LICENSE\") |  |  |\n| [README.md](https://github.com/eugr/llama-benchy/blob/main/README.md \"README.md\") | [README.md](https://github.com/eugr/llama-benchy/blob/main/README.md \"README.md\") |  |  |\n| [pyproject.toml](https://github.com/eugr/llama-benchy/blob/main/pyproject.toml \"pyproject.toml\") | [pyproject.toml](https://github.com/eugr/llama-benchy/blob/main/pyproject.toml \"pyproject.toml\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/eugr/llama-benchy#)\n*   [MIT license](https://github.com/eugr/llama-benchy#)\n\nllama-benchy - llama-bench style benchmarking tool for all backends\n===================================================================\n\n[](https://github.com/eugr/llama-benchy#llama-benchy---llama-bench-style-benchmarking-tool-for-all-backends)\n\nThis script benchmarks OpenAI-compatible LLM endpoints, generating statistics similar to `llama-bench`.\n\nMotivation\n----------\n\n[](https://github.com/eugr/llama-benchy#motivation)\n\n`llama-bench` is a CLI tool that is a part of a very popular [llama.cpp](https://github.com/ggml-org/llama.cpp) inference engine. It is widely used in LLM community to benchmark models and allows to perform measurement at different context sizes. However, it is available only for llama.cpp and cannot be used with other inference engines, like vllm or SGLang.\n\nAlso, it performs measurements using the C++ engine directly which is not representative of the end user experience which can be quite different in practice.\n\nvLLM has its own powerful benchmarking tool, but while it can be used with other inference engines, there are a few issues:\n\n*   It's very tricky and even impossible to calculate prompt processing speeds at different context lengths. You can use `vllm bench sweep serve`, but it only works well with vLLM with prefix caching disabled on the server. Even with random prompts it will reuse the same prompt between multiple runs which will hit the cache in `llama-server` for instance. So you will get very low median TTFT times and very high prompt processing speeds.\n*   The TTFT measurement it uses is not actually until the first usable token, it's until the very first data chunk from the server which may not contain any generated tokens in /v1/chat/completions mode.\n*   Random dataset is the only ones that allows to specify an arbitrary number of tokens, but randomly generated token sequence doesn't let you adequately measure speculative decoding/MTP.\n\nAs of January 2nd, 2026, I wasn't able to find any existing benchmarking tool that brings llama-bench style measurements at different context lengths to any OpenAI-compatible endpoint.\n\nFeatures\n--------\n\n[](https://github.com/eugr/llama-benchy#features)\n\n*   Measures Prompt Processing (pp) and Token Generation (tg) speeds at different context depths.\n*   Can measure separate context prefill and prompt processing over existing cached context at different context depths.\n*   Reports Time To First Response (data chunk) (TTFR), Estimated Prompt Processing Time (est_ppt), and End-to-End TTFT.\n*   Supports configurable prompt length (`--pp`), generation length (`--tg`), and context depth (`--depth`).\n*   Can run multiple iterations (`--runs`) and report mean ¬± std.\n*   Uses HuggingFace tokenizers for accurate token counts.\n*   Downloads a book from Project Gutenberg to use as source text for prompts to ensure better benchmarking of spec.decoding/MTP models.\n*   Supports executing a command after each run (e.g., to clear cache).\n*   Configurable latency measurement mode.\n\nCurrent Limitations\n===================\n\n[](https://github.com/eugr/llama-benchy#current-limitations)\n\n*   Evaluates against `/v1/chat/completions` endpoint only.\n*   Doesn't measure throughput in concurrency mode (coming later).\n*   Outputs results as a Markdown table only for now.\n\nInstallation\n------------\n\n[](https://github.com/eugr/llama-benchy#installation)\n\nUsing `uv` is recommended. You can install `uv` here: [https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)\n\n### Option 1: Run without installation using `uvx`\n\n[](https://github.com/eugr/llama-benchy#option-1-run-without-installation-using-uvx)\n\nRun the release version from PyPI:\n\nundefinedshell\nuvx llama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\nRun the latest version from the main branch:\n\nundefinedshell\nuvx --from git+https://github.com/eugr/llama-benchy llama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\n### Option 2: Install into virtual environment\n\n[](https://github.com/eugr/llama-benchy#option-2-install-into-virtual-environment)\n\nundefinedshell\n# Clone the repository\ngit clone https://github.com/eugr/llama-benchy.git\ncd llama-benchy\n\n# Create virtual environment\nuv venv\n\n# Install with uv (installs into a virtual environment automatically)\nuv pip install -e .\nundefined\n\nTo run, activate the environment first\n\nundefinedshell\nsource .venv/bin/activate\nundefined\n\nThen execute the command:\n\nundefinedshell\nllama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\n### Option 3: Run without installing (`uv run`)\n\n[](https://github.com/eugr/llama-benchy#option-3-run-without-installing-uv-run)\n\nundefinedshell\n# Clone the repository\ngit clone https://github.com/eugr/llama-benchy.git\ncd llama-benchy\n\n# Using uv run (creates a virtual environment if it doesn't exist and runs the command)\nuv run llama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\n### Option 3: Install into system path\n\n[](https://github.com/eugr/llama-benchy#option-3-install-into-system-path)\n\nRelease version from PyPI:\n\nundefinedshell\nuv pip install -U llama-benchy\nundefined\n\nCurrent version from the main branch:\n\nundefinedshell\nuv pip install git+https://github.com/eugr/llama-benchy --system\nundefined\n\nUsage\n-----\n\n[](https://github.com/eugr/llama-benchy#usage)\n\nAfter installation, you can run the tool directly:\n\nundefinedshell\nllama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e --pp \u003cPROMPT_TOKENS\u003e --tg \u003cGEN_TOKENS\u003e [OPTIONS]\nundefined\n\nExample:\n\nundefinedshell\nllama-benchy \\\n  --base-url http://localhost:8000/v1 \\\n  --model openai/gpt-oss-120b \\\n  --depth 0 4096 8192 16384 32768 \\\n  --latency-mode generation\nundefined\n\nOutput:\n\n| model | test | t/s | ttfr (ms) | est_ppt (ms) | e2e_ttft (ms) |\n| :--- | ---: | ---: | ---: | ---: | ---: |\n| openai/gpt-oss-120b | pp2048 | 2019.02 ¬± 34.98 | 1054.64 ¬± 17.57 | 1014.66 ¬± 17.57 | 1115.41 ¬± 18.70 |\n| openai/gpt-oss-120b | tg32 | 52.94 ¬± 1.01 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d4096 | 1994.49 ¬± 77.97 | 3129.18 ¬± 120.27 | 3089.19 ¬± 120.27 | 3198.97 ¬± 122.24 |\n| openai/gpt-oss-120b | tg32 @ d4096 | 46.69 ¬± 1.11 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d8192 | 1751.68 ¬± 34.44 | 5892.61 ¬± 114.68 | 5852.63 ¬± 114.68 | 5971.27 ¬± 115.77 |\n| openai/gpt-oss-120b | tg32 @ d8192 | 40.40 ¬± 1.19 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d16384 | 1475.63 ¬± 31.41 | 12542.02 ¬± 265.86 | 12502.04 ¬± 265.86 | 12634.67 ¬± 269.43 |\n| openai/gpt-oss-120b | tg32 @ d16384 | 33.86 ¬± 1.45 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d32768 | 1131.86 ¬± 50.53 | 30869.90 ¬± 1410.15 | 30829.92 ¬± 1410.15 | 30992.96 ¬± 1417.33 |\n| openai/gpt-oss-120b | tg32 @ d32768 | 25.34 ¬± 1.31 |  |  |  |\n\nllama-benchy (build: 75bc129) date: 2026-01-02 17:11:19 | latency mode: generation\n\n* * *\n\nIt's recommended to use \"generation\" latency mode to get prompt processing speeds closer to real numbers, especially on shorter prompts. By default, the script adapts the prompt size to match the specified value, regardless of the chat template applied. Use `--no-adapt-prompt` to disable this behavior.\n\nGenerally you don't need to disable prompt caching on the server, as a probability of cache hits is fairly small. You can add `--no-cache` that will add some random noise if you get cache hits.\n\n### Arguments\n\n[](https://github.com/eugr/llama-benchy#arguments)\n\n*   `--base-url`: OpenAI compatible endpoint URL (Required).\n*   `--api-key`: API Key (Default: \"EMPTY\").\n*   `--model`: Model name (Required).\n*   `--served-model-name`: Model name used in API calls (Defaults to --model if not specified).\n*   `--tokenizer`: HuggingFace tokenizer name (Defaults to model name).\n*   `--pp`: List of prompt processing token counts (Default: [2048]).\n*   `--tg`: List of token generation counts (Default: [32]).\n*   `--depth`: List of context depths (Default: [0]).\n*   `--runs`: Number of runs per test (Default: 3).\n*   `--no-cache`: Add noise to requests to improve prefix caching avoidance. Also sends `cache-prompt=false` to the server.\n*   `--post-run-cmd`: Command to execute after each test run.\n*   `--book-url`: URL of a book to use for text generation (Defaults to Sherlock Holmes).\n*   `--latency-mode`: Method to measure latency: 'api' (call list models function) - default, 'generation' (single token generation), or 'none' (skip latency measurement).\n*   `--no-warmup`: Skip warmup phase.\n*   `--adapt-prompt`: Adapt prompt size based on warmup token usage delta (Default: True).\n*   `--no-adapt-prompt`: Disable prompt size adaptation.\n*   `--enable-prefix-caching`: Enable prefix caching performance measurement. When enabled (and depth \u003e 0), it performs a two-step benchmark: first loading the context (reported as `ctx_pp`), then running the prompt with the cached context.\n\n### Metrics\n\n[](https://github.com/eugr/llama-benchy#metrics)\n\nThe script outputs a table with the following metrics. All time measurements are in milliseconds (ms).\n\n#### Latency Adjustment\n\n[](https://github.com/eugr/llama-benchy#latency-adjustment)\n\nThe script attempts to estimate network or processing latency to provide \"server-side\" processing times.\n\n*   **Latency**: Measured based on `--latency-mode`. \n    *   `api`: Time to fetch `/models` (from sending request to getting first byte of the response). Eliminates network latency only.\n    *   `generation`: Time to generate 1 token (from sending request to getting first byte of the response). Tries to eliminate network and server overhead latency.\n    *   `none`: Assumed to be 0.\n\n*   This measured latency is subtracted from `ttfr` to calculate `est_ppt`.\n\n#### Table Columns\n\n[](https://github.com/eugr/llama-benchy#table-columns)\n\n*   **`t/s` (Tokens per Second)**:\n\n    *   **For Prompt Processing (pp)**: Calculated as `Total Prompt Tokens / est_ppt`. This represents the prefill speed.\n    *   **For Token Generation (tg)**: Calculated as `(Total Generated Tokens - 1) / (Time of Last Token - Time of First Token)`. This represents the decode speed, excluding the first token latency.\n\n*   **`ttfr (ms)` (Time To First Response)**:\n\n    *   Calculation: `Time of First Response Chunk - Start Time`.\n    *   Represents the raw time until the client receives _any_ stream data from the server (including empty chunks or role definitions, but excluding initial http response header). This includes network latency. The same measurement method is used by `vllm bench serve` to report TTFT.\n\n*   **`est_ppt (ms)` (Estimated Prompt Processing Time)**:\n\n    *   Calculation: `TTFR - Estimated Latency`.\n    *   Estimated time the server spent processing the prompt. Used for calculating Prompt Processing speed.\n\n*   **`e2e_ttft (ms)` (End-to-End Time To First Token)**:\n\n    *   Calculation: `Time of First Content Token - Start Time`.\n    *   The total time perceived by the client from sending the request to seeing the first generated content.\n\n### Prefix Caching Benchmarking\n\n[](https://github.com/eugr/llama-benchy#prefix-caching-benchmarking)\n\nWhen `--enable-prefix-caching` is used (with `--depth`\u003e 0), the script performs a two-step process for each run to measure the impact of prefix caching:\n\n1.   **Context Load**: Sends the context tokens (as a system message) with an empty user message. This forces the server to process and cache the context. \n    *   Reported as `ctx_pp @ d{depth}` (Context Prompt Processing) and `ctx_tg @ d{depth}`.\n\n2.   **Inference**: Sends the same context (system message) followed by the actual prompt (user message). The server should reuse the cached context. \n    *   Reported as standard `pp{tokens} @ d{depth}` and `tg{tokens} @ d{depth}`.\n\nIn this case, `pp` and `tg` speeds will show an actual prompt processing / token generation speeds for a follow up prompt with a context pre-filled.\n\n### Example\n\n[](https://github.com/eugr/llama-benchy#example)\n\nundefinedshell\nllama-benchy \\\n  --base-url http://localhost:8000/v1 \\\n  --model openai/gpt-oss-120b \\\n  --pp 128 256 \\\n  --tg 32 64 \\\n  --depth 0 1024\nundefined\n\nThis will run benchmarks for all combinations of pp (128, 256), tg (32, 64), and depth (0, 1024).\n\nAbout\n-----\n\nllama-benchy - llama-bench style benchmarking tool for all backends\n\n### Resources\n\n[Readme](https://github.com/eugr/llama-benchy#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/eugr/llama-benchy#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/eugr/llama-benchy).\n\n[Activity](https://github.com/eugr/llama-benchy/activity)\n\n### Stars\n\n[**7** stars](https://github.com/eugr/llama-benchy/stargazers)\n\n### Watchers\n\n[**0** watching](https://github.com/eugr/llama-benchy/watchers)\n\n### Forks\n\n[**1** fork](https://github.com/eugr/llama-benchy/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Feugr%2Fllama-benchy\u0026report=eugr+%28user%29)\n\n[Releases 2](https://github.com/eugr/llama-benchy/releases)\n-----------------------------------------------------------\n\n[v0.1.1 - cosmetic changes Latest Jan 7, 2026](https://github.com/eugr/llama-benchy/releases/tag/v0.1.1)\n\n[+ 1 release](https://github.com/eugr/llama-benchy/releases)\n\n[Packages 0](https://github.com/users/eugr/packages?repo_name=llama-benchy)\n---------------------------------------------------------------------------\n\n No packages published \n\nLanguages\n---------\n\n*   [Python 100.0%](https://github.com/eugr/llama-benchy/search?l=python)\n\nFooter\n------\n\n[](https://github.com/) ¬© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You can‚Äôt perform that action at this time.",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:55:54.275239116Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://spark:8888/v1: jina: status 400: {\"data\":null,\"path\":\"url\",\"code\":400,\"name\":\"ParamValidationError\",\"status\":40001,\"message\":\"Domain 'spark' could not be resolved\",\"readableMessage\":\"ParamValidationError(url): Domain 'spark' could not be resolved\"}: retry failed: jina request failed: 400 Bad Request",
          "occurred_at": "2026-01-07T02:55:53.140777712Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q5e010",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/",
      "title": "Supertonic2: Lightning Fast, On-Device, Multilingual TTS",
      "content": "Hello!\n\nI want to share that Supertonic now supports 5 languages:  \nÌïúÍµ≠Ïñ¥ ¬∑ Espa√±ol ¬∑ Fran√ßais ¬∑ Portugu√™s ¬∑ English\n\nIt‚Äôs an open-weight TTS model designed for extreme speed, minimal footprint, and flexible deployment. You can also use it for commercial use!\n\nHere are key features:\n\n(1) Lightning fast ‚Äî¬†RTF 0.006 on M4 Pro\n\n(2) Lightweight¬†‚Äî¬†66M parameters\n\n(3) On-device TTS ‚Äî¬†Complete privacy, zero network latency\n\n(4) Flexible deployment ‚Äî¬†Runs on browsers, PCs, mobiles, and edge devices\n\n(5) 10 preset voices ‚Äî¬† Pick the voice that fits your use cases\n\n(6) Open-weight model ‚Äî¬†Commercial use allowed ([OpenRAIL-M](https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE))\n\nI hope Supertonic is useful for your projects.\n\n\\[Demo\\] [https://huggingface.co/spaces/Supertone/supertonic-2](https://huggingface.co/spaces/Supertone/supertonic-2)\n\n\\[Model\\] [https://huggingface.co/Supertone/supertonic-2](https://huggingface.co/Supertone/supertonic-2)\n\n\\[Code\\] [https://github.com/supertone-inc/supertonic](https://github.com/supertone-inc/supertonic)",
      "author": "ANLGBOY",
      "created_at": "2026-01-06T09:24:47Z",
      "comments": [
        {
          "id": "nxzw03n",
          "author": "drooolingidiot",
          "content": "Woah, this is incredible! Finally something super lightweight that sounds even better than kokoro!\n\n\nI am disappointed that it's released under the deranged and extremely user-hostile Open-RAIL license though.  Why apply such a hostile license to the model when it doesn't even benefit you in anyway?",
          "created_at": "2026-01-06T12:32:30Z",
          "was_summarised": false
        },
        {
          "id": "nxzfdmk",
          "author": "KoreanPeninsula",
          "content": "The speed is quite fast. However, in some Korean texts, pronunciation becomes inaccurate, and certain parts are not pronounced at all. Short sentences are read quite well.",
          "created_at": "2026-01-06T10:17:50Z",
          "was_summarised": false
        },
        {
          "id": "nxzhthi",
          "author": "FlowCritikal",
          "content": "Will German be added anytime soon? The market for German TTS is fairly large.",
          "created_at": "2026-01-06T10:39:54Z",
          "was_summarised": false
        },
        {
          "id": "nxze31d",
          "author": "FullstackSensei",
          "content": "That's great! Especially the cpp support!\nAny chance we also get German support?",
          "created_at": "2026-01-06T10:06:06Z",
          "was_summarised": false
        },
        {
          "id": "nxzjhub",
          "author": "neovim-neophyte",
          "content": "how does this compare to cosyvoice3(RL)? ive tried it and its pretty good, far better than spark tts and f5 tts",
          "created_at": "2026-01-06T10:54:30Z",
          "was_summarised": false
        },
        {
          "id": "ny0lmvx",
          "author": "ghulamalchik",
          "content": "Tried the demo. Quality is insane especially at that size. Well done!\nI hope more languages are supported in the future such as Russian, German, Arabic, Italian.",
          "created_at": "2026-01-06T14:59:00Z",
          "was_summarised": false
        },
        {
          "id": "ny0wx5l",
          "author": "ThetaCursed",
          "content": "What about voice cloning? Or just presets...",
          "created_at": "2026-01-06T15:52:54Z",
          "was_summarised": false
        },
        {
          "id": "nxzhhhp",
          "author": "HotDoshirak",
          "content": "Sometimes it‚Äôs funny to see how models claim to be multilingual, but actually supports 3-5 languages. But still a good release for a lightweight tts.",
          "created_at": "2026-01-06T10:36:50Z",
          "was_summarised": false
        },
        {
          "id": "ny0y31p",
          "author": "urekmazino_0",
          "content": "Fine tuning support?",
          "created_at": "2026-01-06T15:58:12Z",
          "was_summarised": false
        },
        {
          "id": "ny27kjv",
          "author": "OC2608",
          "content": "No finetunable checkpoints = no care. (I'm sorry...)   \nHey piper, why are you the \\*only\\* one with finetunable checkpoints and fast CPU inference even in 2026?",
          "created_at": "2026-01-06T19:23:30Z",
          "was_summarised": false
        },
        {
          "id": "ny0e7kn",
          "author": "Impressive-Sir9633",
          "content": "Interested in quick opinions compared to prior smaller models (KokoroTTS and Parakeet 0.6v3",
          "created_at": "2026-01-06T14:20:10Z",
          "was_summarised": false
        },
        {
          "id": "ny2vtsf",
          "author": "TraceyRobn",
          "content": "Impressive. Works great on the PC. \n\nFYI: Fails on three Android mobile browsers (Chrome, Brave and Firefox (with WASM)) with the message: \"Error: Cannot read properties of undefined (reading 'subgroupMinSize)",
          "created_at": "2026-01-06T21:15:05Z",
          "was_summarised": false
        },
        {
          "id": "ny303i5",
          "author": "wanderer_4004",
          "content": "Pretty cool to have the same voices for different languages - that makes language switching less awkward. Here and there is a small glitch (using Python) but the speed is fantastic and the quality is by far good enough especially for real time applications. French is actually imho better than kokoro - kokoro has only one female french voice which is slightly boring. German, Italian, Chinese, Russian and two dozen more languages would be cool...\n\nEdit: One more cool thing, the model automatically converts Mr to Mister and Wed to Wednesday etc. Very nice, kokoro does not do that. About 40x real time on MBP M1 64GB.",
          "created_at": "2026-01-06T21:34:36Z",
          "was_summarised": false
        },
        {
          "id": "ny36vgj",
          "author": "az226",
          "content": "I wonder how the RTF is so much faster than Kokoro but model size similar.",
          "created_at": "2026-01-06T22:05:51Z",
          "was_summarised": false
        },
        {
          "id": "ny04fzt",
          "author": "DeepGreenPotato",
          "content": "Would be nice to support Russian!",
          "created_at": "2026-01-06T13:26:01Z",
          "was_summarised": false
        },
        {
          "id": "nxzc52m",
          "author": "Baldtazar",
          "content": "Do you know the pain of getting link texts from the post on the smartphone?",
          "created_at": "2026-01-06T09:47:55Z",
          "was_summarised": false
        },
        {
          "id": "nxzjudy",
          "author": "sammcj",
          "content": "I like to find a good TTS model that does international / British English rather than American - has anyone got any recommendations?",
          "created_at": "2026-01-06T10:57:30Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://v.redd.it/k40jciwu5pbg1",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://v.redd.it/k40jciwu5pbg1\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE",
          "was_fetched": true,
          "page": "Title: LICENSE ¬∑ Supertone/supertonic-2 at main\n\nURL Source: https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE\n\nMarkdown Content:\nBigScience Open RAIL-M License\ndated August 18, 2022\n\nSection I: PREAMBLE\n\nThis Open RAIL-M License was created by BigScience, a collaborative open innovation project aimed at\nthe responsible development and use of large multilingual datasets and Large Language Models\n(‚ÄúLLMs‚Äù). While a similar license was originally designed for the BLOOM model, we decided to adapt it\nand create this license in order to propose a general open and responsible license applicable to other\nmachine learning based AI models (e.g. multimodal generative models).\nIn short, this license strives for both the open and responsible downstream use of the accompanying\nmodel. When it comes to the open character, we took inspiration from open source permissive licenses\nregarding the grant of IP rights. Referring to the downstream responsible use, we added use-based\nrestrictions not permitting the use of the Model in very specific scenarios, in order for the licensor to be\nable to enforce the license in case potential misuses of the Model may occur. Even though downstream\nderivative versions of the model could be released under different licensing terms, the latter will always\nhave to include - at minimum - the same use-based restrictions as the ones in the original license (this\nlicense).\nThe development and use of artificial intelligence (‚ÄúAI‚Äù), does not come without concerns. The world has\nwitnessed how AI techniques may, in some instances, become risky for the public in general. These risks\ncome in many forms, from racial discrimination to the misuse of sensitive information.\nBigScience believes in the intersection between open and responsible AI development; thus, this License\naims to strike a balance between both in order to enable responsible open-science in the field of AI.\nThis License governs the use of the model (and its derivatives) and is informed by the model card\nassociated with the model.\n\nNOW THEREFORE, You and Licensor agree as follows:\n\n1. Definitions\n(a) \"License\" means the terms and conditions for use, reproduction, and Distribution as defined in\nthis document.\n(b) ‚ÄúData‚Äù means a collection of information and/or content extracted from the dataset used with the\nModel, including to train, pretrain, or otherwise evaluate the Model. The Data is not licensed under\nthis License.\n(c)‚ÄúOutput‚Äù means the results of operating a Model as embodied in informational content resulting\ntherefrom.\n(d)‚ÄúModel‚Äù means any accompanying machine-learning based assemblies (including checkpoints),\nconsisting of learnt weights, parameters (including optimizer states), corresponding to the model\narchitecture as embodied in the Complementary Material, that have been trained or tuned, in whole or\nin part on the Data, using the Complementary Material.\n(e) ‚ÄúDerivatives of the Model‚Äù means all modifications to the Model, works based on the Model, or any\nother model which is created or initialized by transfer of patterns of the weights, parameters,\nactivations or output of the Model, to the other model, in order to cause the other model to perform\nsimilarly to the Model, including - but not limited to - distillation methods entailing the use of\nintermediate data representations or methods based on the generation of synthetic data by the Model\nfor training the other model.\n(f)‚ÄúComplementary Material‚Äù means the accompanying source code and scripts used to define,\nrun, load, benchmark or evaluate the Model, and used to prepare data for training or evaluation, if\nany. This includes any accompanying documentation, tutorials, examples, etc, if any.\n(g) ‚ÄúDistribution‚Äù means any transmission, reproduction, publication or other sharing of the Model or\nDerivatives of the Model to a third party, including providing the Model as a hosted service made\navailable by electronic or other remote means - e.g. API-based or web access.\n(h) ‚ÄúLicensor‚Äù means the copyright owner or entity authorized by the copyright owner that is\ngranting the License, including the persons or entities that may have rights in the Model and/or\ndistributing the Model.\n(i) \"You\" (or \"Your\") means an individual or Legal Entity exercising permissions granted by this\nLicense and/or making use of the Model for whichever purpose and in any field of use, including\nusage of the Model in an end-use application - e.g. chatbot, translator, image generator.\n(j) ‚ÄúThird Parties‚Äù means individuals or legal entities that are not under common control with\nLicensor or You.\n(k) \"Contribution\" means any work of authorship, including the original version of the Model and\nany modifications or additions to that Model or Derivatives of the Model thereof, that is\nintentionally submitted to Licensor for inclusion in the Model by the copyright owner or by an\nindividual or Legal Entity authorized to submit on behalf of the copyright owner. For the\npurposes of this definition,\n‚Äúsubmitted‚Äù means any form of electronic, verbal, or written\ncommunication sent to the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems, and issue tracking\nsystems that are managed by, or on behalf of, the Licensor for the purpose of discussing and\nimproving the Model, but excluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n(l) \"Contributor\" means Licensor and any individual or Legal Entity on behalf of whom a\nContribution has been received by Licensor and subsequently incorporated within the Model.\n\nSection II: INTELLECTUAL PROPERTY RIGHTS\n\nBoth copyright and patent grants apply to the Model, Derivatives of the Model and Complementary\nMaterial. The Model and Derivatives of the Model are subject to additional terms as described in Section III.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor\nhereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the\nComplementary Material, the Model, and Derivatives of the Model.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License and where and as\napplicable, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge,\nroyalty-free, irrevocable (except as stated in this paragraph) patent license to make, have made, use, offer\nto sell, sell, import, and otherwise transfer the Model and the Complementary Material, where such\nlicense applies only to those patent claims licensable by such Contributor that are necessarily infringed by\ntheir Contribution(s) alone or by combination of their Contribution(s) with the Model to which such\nContribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim\nor counterclaim in a lawsuit) alleging that the Model and/or Complementary Material or a Contribution\nincorporated within the Model and/or Complementary Material constitutes direct or contributory patent\ninfringement, then any patent licenses granted to You under this License for the Model and/or Work shall\nterminate as of the date such litigation is asserted or filed.\nSection III: CONDITIONS OF USAGE, DISTRIBUTION AND REDISTRIBUTION\n\n4. Distribution and Redistribution. You may host for Third Party remote access purposes (e.g.\nsoftware-as-a-service), reproduce and distribute copies of the Model or Derivatives of the Model thereof\nin any medium, with or without modifications, provided that You meet the following conditions:\n\na. Use-based restrictions as referenced in paragraph 5 MUST be included as an enforceable provision\nby You in any type of legal agreement (e.g. a license) governing the use and/or distribution of the\nModel or Derivatives of the Model, and You shall give notice to subsequent users You Distribute to,\nthat the Model or Derivatives of the Model are subject to paragraph 5. This provision does not apply\nto the use of Complementary Material.\n\nb. You must give any Third Party recipients of the Model or Derivatives of the Model a copy of this\nLicense;\n\nc. You must cause any modified files to carry prominent notices stating that You changed the files;\n\nd. You must retain all copyright, patent, trademark, and attribution notices excluding those notices\nthat do not pertain to any part of the Model, Derivatives of the Model.\nYou may add Your own copyright statement to Your modifications and may provide additional or\ndifferent license terms and conditions - respecting paragraph 4.a.\n- for use, reproduction, or Distribution\nof Your modifications, or for any such Derivatives of the Model as a whole, provided Your use,\nreproduction, and Distribution of the Model otherwise complies with the conditions stated in this License.\n\n5. Use-based restrictions. The restrictions set forth in Attachment A are considered Use-based restrictions.\nTherefore You cannot use the Model and the Derivatives of the Model for the specified restricted uses. You\nmay use the Model subject to this License, including only for lawful purposes and in accordance with the\nLicense. Use may include creating any content with, finetuning, updating, running, training, evaluating and/or\nreparametrizing the Model. You shall require all of Your users who use the Model or a Derivative of the Model\nto comply with the terms of this paragraph (paragraph 5).\n\n6. The Output You Generate. Except as set forth herein, Licensor claims no rights in the Output You\ngenerate using the Model. You are accountable for the Output you generate and its subsequent uses. No\nuse of the output can contravene any provision as stated in the License.\n\nSection IV: OTHER PROVISIONS\n\n7. Updates and Runtime Restrictions. To the maximum extent permitted by law, Licensor reserves the\nright to restrict (remotely or otherwise) usage of the Model in violation of this License, update the Model\nthrough electronic means, or modify the Output of the Model based on updates. You shall undertake\nreasonable efforts to use the latest version of the Model.\n\n8. Trademarks and related. Nothing in this License permits You to make use of Licensors‚Äô trademarks,\ntrade names, logos or to otherwise suggest endorsement or misrepresent the relationship between the\nparties; and any rights not expressly granted herein are reserved by the Licensors.\n\n9. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides\nthe Model and the Complementary Material (and each Contributor provides its Contributions) on an \"AS\nIS\"BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied,\nincluding, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT,\nMERCHANTABILITY , or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for\ndetermining the appropriateness of using or redistributing the Model, Derivatives of the Model, and the\nComplementary Material and assume any risks associated with Your exercise of permissions under this\nLicense.\n\n10. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence),\ncontract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or\nagreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect,\nspecial, incidental, or consequential damages of any character arising as a result of this License or out of\nthe use or inability to use the Model and the Complementary Material (including but not limited to\ndamages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other\ncommercial damages or losses), even if such Contributor has been advised of the possibility of such\ndamages.\n\n11. Accepting Warranty or Additional Liability. While redistributing the Model, Derivatives of the\nModel and the Complementary Material thereof, You may choose to offer, and charge a fee for, acceptance\nof support, warranty, indemnity, or other liability obligations and/or rights consistent with this License.\nHowever, in accepting such obligations, You may act only on Your own behalf and on Your sole\nresponsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and\nhold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor\nby reason of your accepting any such warranty or additional liability.\n\n12. If any provision of this License is held to be invalid, illegal or unenforceable, the remaining\nprovisions shall be unaffected thereby and remain valid as if such provision had not been set forth herein.\n\nEND OF TERMS AND CONDITIONS\n\nAttachment A\n\nUse Restrictions\n\nYou agree not to use the Model or Derivatives of the Model:\n(a) In any way that violates any applicable national, federal, state, local or international law\nor regulation;\n(b) For the purpose of exploiting, harming or attempting to exploit or harm minors in any\nway;\n(c) To generate or disseminate verifiably false information and/or content with the purpose of\nharming others;\n(d) To generate or disseminate personal identifiable information that can be used to harm an\nindividual;\n(e) To generate or disseminate information and/or content (e.g. images, code, posts, articles),\nand place the information and/or content in any context (e.g. bot generating tweets)\nwithout expressly and intelligibly disclaiming that the information and/or content is\nmachine generated;\n(f) To defame, disparage or otherwise harass others;\n(g) To impersonate or attempt to impersonate (e.g. deepfakes) others without their consent;\n(h) For fully automated decision making that adversely impacts an individual‚Äôs legal rights or\notherwise creates or modifies a binding, enforceable obligation;\n(i) For any use intended to or which has the effect of discriminating against or harming\nindividuals or groups based on online or offline social behavior or known or predicted\npersonal or personality characteristics;\n(j) To exploit any of the vulnerabilities of a specific group of persons based on their age,\nsocial, physical or mental characteristics, in order to materially distort the behavior of a\nperson pertaining to that group in a manner that causes or is likely to cause that person or\nanother person physical or psychological harm;\n(k) For any use intended to or which has the effect of discriminating against individuals or\ngroups based on legally protected characteristics or categories;\n(l) To provide medical advice and medical results interpretation;\n(m) To generate or disseminate information for the purpose to be used for administration of\njustice, law enforcement, immigration or asylum processes, such as predicting an\nindividual will commit fraud/crime commitment (e.g. by text profiling, drawing causal\nrelationships between assertions made in documents, indiscriminate and\narbitrarily-targeted use).",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/spaces/Supertone/supertonic-2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/Supertone/supertonic-2",
          "was_fetched": true,
          "page": "Title: Supertone/supertonic-2 ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/Supertone/supertonic-2\n\nMarkdown Content:\n[](https://huggingface.co/Supertone/supertonic-2#supertonic-2--lightning-fast-on-device-tts-multilingual-tts) Supertonic 2 ‚Äî Lightning Fast, On-Device TTS, Multilingual TTS\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[](https://huggingface.co/Supertone/supertonic-2/blob/main/img/supertonic_preview_0.1.jpg)\n\n[](https://huggingface.co/spaces/Supertone/supertonic-2)[](https://github.com/supertone-inc/supertonic)\n\n**Supertonic** is a lightning-fast, on-device text-to-speech system designed for **extreme performance** with minimal computational overhead. Powered by ONNX Runtime, it runs entirely on your device‚Äîno cloud, no API calls, no privacy concerns.\n\n[](https://huggingface.co/Supertone/supertonic-2#whats-new-in-supertonic-2) What's New in Supertonic 2\n------------------------------------------------------------------------------------------------------\n\n**Supertonic 2** extends multilingual capabilities while maintaining the same inference speed and efficiency as the original.\n\n### [](https://huggingface.co/Supertone/supertonic-2#%F0%9F%8C%8D-multilingual-support) üåç Multilingual Support\n\n| Language | Code |\n| --- | --- |\n| English | `en` |\n| Korean | `ko` |\n| Spanish | `es` |\n| Portuguese | `pt` |\n| French | `fr` |\n\n### [](https://huggingface.co/Supertone/supertonic-2#%E2%9A%A1-same-speed-more-languages) ‚ö° Same Speed, More Languages\n\n*   **No speed degradation**: Supertonic 2 delivers the same ultra-fast inference speed as the original‚Äîup to **167√ó faster than real-time**\n*   **Efficient architecture**: Only **66M parameters**, optimized for on-device deployment\n*   **Cross-language consistency**: All supported languages share the same model architecture and inference pipeline\n\n[](https://huggingface.co/Supertone/supertonic-2#performance) Performance\n-------------------------------------------------------------------------\n\nWe evaluated Supertonic's performance (with 2 inference steps) using two key metrics across input texts of varying lengths: Short (59 chars), Mid (152 chars), and Long (266 chars).\n\n**Metrics:**\n\n*   **Characters per Second**: Measures throughput by dividing the number of input characters by the time required to generate audio. Higher is better.\n*   **Real-time Factor (RTF)**: Measures the time taken to synthesize audio relative to its duration. Lower is better (e.g., RTF of 0.1 means it takes 0.1 seconds to generate one second of audio).\n\n### [](https://huggingface.co/Supertone/supertonic-2#characters-per-second) Characters per Second\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 912 | 1048 | 1263 |\n| **Supertonic** (M4 pro - WebGPU) | 996 | 1801 | 2509 |\n| **Supertonic** (RTX4090) | 2615 | 6548 | 12164 |\n| `API`[ElevenLabs Flash v2.5](https://elevenlabs.io/docs/api-reference/text-to-speech/convert) | 144 | 209 | 287 |\n| `API`[OpenAI TTS-1](https://platform.openai.com/docs/guides/text-to-speech) | 37 | 55 | 82 |\n| `API`[Gemini 2.5 Flash TTS](https://ai.google.dev/gemini-api/docs/speech-generation) | 12 | 18 | 24 |\n| `API`[Supertone Sona speech 1](https://docs.supertoneapi.com/en/api-reference/endpoints/text-to-speech) | 38 | 64 | 92 |\n| `Open`[Kokoro](https://github.com/hexgrad/kokoro/) | 104 | 107 | 117 |\n| `Open`[NeuTTS Air](https://github.com/neuphonic/neutts-air) | 37 | 42 | 47 |\n\n\u003e **Notes:**\n\u003e \n\u003e `API` = Cloud-based API services (measured from Seoul)\n\u003e \n\u003e `Open` = Open-source models\n\u003e \n\u003e Supertonic (M4 pro - CPU) and (M4 pro - WebGPU): Tested with ONNX\n\u003e \n\u003e Supertonic (RTX4090): Tested with PyTorch model\n\u003e \n\u003e Kokoro: Tested on M4 Pro CPU with ONNX\n\u003e \n\u003e NeuTTS Air: Tested on M4 Pro CPU with Q8-GGUF\n\n### [](https://huggingface.co/Supertone/supertonic-2#real-time-factor) Real-time Factor\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 0.015 | 0.013 | 0.012 |\n| **Supertonic** (M4 pro - WebGPU) | 0.014 | 0.007 | 0.006 |\n| **Supertonic** (RTX4090) | 0.005 | 0.002 | 0.001 |\n| `API`[ElevenLabs Flash v2.5](https://elevenlabs.io/docs/api-reference/text-to-speech/convert) | 0.133 | 0.077 | 0.057 |\n| `API`[OpenAI TTS-1](https://platform.openai.com/docs/guides/text-to-speech) | 0.471 | 0.302 | 0.201 |\n| `API`[Gemini 2.5 Flash TTS](https://ai.google.dev/gemini-api/docs/speech-generation) | 1.060 | 0.673 | 0.541 |\n| `API`[Supertone Sona speech 1](https://docs.supertoneapi.com/en/api-reference/endpoints/text-to-speech) | 0.372 | 0.206 | 0.163 |\n| `Open`[Kokoro](https://github.com/hexgrad/kokoro/) | 0.144 | 0.124 | 0.126 |\n| `Open`[NeuTTS Air](https://github.com/neuphonic/neutts-air) | 0.390 | 0.338 | 0.343 |\n\n**Additional Performance Data (5-step inference)**\n**Characters per Second (5-step)**\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 596 | 691 | 850 |\n| **Supertonic** (M4 pro - WebGPU) | 570 | 1118 | 1546 |\n| **Supertonic** (RTX4090) | 1286 | 3757 | 6242 |\n\n**Real-time Factor (5-step)**\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 0.023 | 0.019 | 0.018 |\n| **Supertonic** (M4 pro - WebGPU) | 0.024 | 0.012 | 0.010 |\n| **Supertonic** (RTX4090) | 0.011 | 0.004 | 0.002 |\n\n[](https://huggingface.co/Supertone/supertonic-2#license) License\n-----------------------------------------------------------------\n\nThis project‚Äôs sample code is released under the MIT License. - see the [LICENSE](https://github.com/supertone-inc/supertonic?tab=MIT-1-ov-file) for details.\n\nThe accompanying model is released under the OpenRAIL-M License. - see the [LICENSE](https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE) file for details.\n\nThis model was trained using PyTorch, which is licensed under the BSD 3-Clause License but is not redistributed with this project. - see the [LICENSE](https://docs.pytorch.org/FBGEMM/general/License.html) for details.\n\nCopyright (c) 2026 Supertone Inc.",
          "was_summarised": false
        },
        {
          "url": "https://github.com/supertone-inc/supertonic",
          "was_fetched": true,
          "page": "Title: GitHub - supertone-inc/supertonic: Lightning-Fast, On-Device, Multilingual TTS ‚Äî running natively via ONNX.\n\nURL Source: https://github.com/supertone-inc/supertonic\n\nMarkdown Content:\nSupertonic ‚Äî Lightning Fast, On-Device TTS\n------------------------------------------\n\n[](https://github.com/supertone-inc/supertonic#supertonic--lightning-fast-on-device-tts)\n[](https://huggingface.co/spaces/Supertone/supertonic-2)[](https://huggingface.co/Supertone/supertonic-2)[](https://huggingface.co/spaces/Supertone/supertonic#interactive-demo)[](https://huggingface.co/Supertone/supertonic)\n\n[](https://github.com/supertone-inc/supertonic/blob/main/img/supertonic_preview_0.1.jpg)\n\n**Supertonic** is a lightning-fast, on-device text-to-speech system designed for **extreme performance** with minimal computational overhead. Powered by ONNX Runtime, it runs entirely on your device‚Äîno cloud, no API calls, no privacy concerns.\n\n### üì∞ Update News\n\n[](https://github.com/supertone-inc/supertonic#-update-news)\n*   **2026.01.06** - üéâ **Supertonic 2** released with multilingual support! Now supports English (`en`), Korean (`ko`), Spanish (`es`), Portuguese (`pt`), and French (`fr`). [Demo](https://huggingface.co/spaces/Supertone/supertonic-2) | [Models](https://huggingface.co/Supertone/supertonic-2)\n*   **2025.12.10** - Added `supertonic` PyPI package! Install via `pip install supertonic`. For details, visit [supertonic-py documentation](https://supertone-inc.github.io/supertonic-py)\n*   **2025.12.10** - Added [6 new voice styles](https://huggingface.co/Supertone/supertonic/tree/b10dbaf18b316159be75b34d24f740008fddd381) (M3, M4, M5, F3, F4, F5). See [Voices](https://supertone-inc.github.io/supertonic-py/voices/) for details\n*   **2025.12.08** - Optimized ONNX models via [OnnxSlim](https://github.com/inisis/OnnxSlim) now available on [Hugging Face Models](https://huggingface.co/Supertone/supertonic)\n*   **2025.11.24** - Added Flutter SDK support with macOS compatibility\n\n### Table of Contents\n\n[](https://github.com/supertone-inc/supertonic#table-of-contents)\n*   [Demo](https://github.com/supertone-inc/supertonic#demo)\n*   [Why Supertonic?](https://github.com/supertone-inc/supertonic#why-supertonic)\n*   [Language Support](https://github.com/supertone-inc/supertonic#language-support)\n*   [Getting Started](https://github.com/supertone-inc/supertonic#getting-started)\n*   [Performance](https://github.com/supertone-inc/supertonic#performance)\n*   [Built with Supertonic](https://github.com/supertone-inc/supertonic#built-with-supertonic)\n*   [Citation](https://github.com/supertone-inc/supertonic#citation)\n*   [License](https://github.com/supertone-inc/supertonic#license)\n\nDemo\n----\n\n[](https://github.com/supertone-inc/supertonic#demo)\n### Raspberry Pi\n\n[](https://github.com/supertone-inc/supertonic#raspberry-pi)\nWatch Supertonic running on a **Raspberry Pi**, demonstrating on-device, real-time text-to-speech synthesis:\n\nsupertonic_raspberry-pi_480.mov\n\n### E-Reader\n\n[](https://github.com/supertone-inc/supertonic#e-reader)\nExperience Supertonic on an **Onyx Boox Go 6** e-reader in airplane mode, achieving an average RTF of 0.3√ó with zero network dependency:\n\nsupertonic_ebook.mp4\n\n* * *\n\n\u003e üéß **Try it now**: Experience Supertonic in your browser with our [**Interactive Demo**](https://huggingface.co/spaces/Supertone/supertonic-2), or get started with pre-trained models from [**Hugging Face Hub**](https://huggingface.co/Supertone/supertonic-2)\n\nWhy Supertonic?\n---------------\n\n[](https://github.com/supertone-inc/supertonic#why-supertonic)\n*   **‚ö° Blazingly Fast**: Generates speech up to **167√ó faster than real-time** on consumer hardware (M4 Pro)‚Äîunmatched by any other TTS system\n*   **ü™∂ Ultra Lightweight**: Only **66M parameters**, optimized for efficient on-device performance with minimal footprint\n*   **üì± On-Device Capable**: **Complete privacy** and **zero latency**‚Äîall processing happens locally on your device\n*   **üé® Natural Text Handling**: Seamlessly processes numbers, dates, currency, abbreviations, and complex expressions without pre-processing\n*   **‚öôÔ∏è Highly Configurable**: Adjust inference steps, batch processing, and other parameters to match your specific needs\n*   **üß© Flexible Deployment**: Deploy seamlessly across servers, browsers, and edge devices with multiple runtime backends.\n\nLanguage Support\n----------------\n\n[](https://github.com/supertone-inc/supertonic#language-support)\nWe provide ready-to-use TTS inference examples across multiple ecosystems:\n\n| Language/Platform | Path | Description |\n| --- | --- | --- |\n| [**Python**](https://github.com/supertone-inc/supertonic/blob/main/py) | `py/` | ONNX Runtime inference |\n| [**Node.js**](https://github.com/supertone-inc/supertonic/blob/main/nodejs) | `nodejs/` | Server-side JavaScript |\n| [**Browser**](https://github.com/supertone-inc/supertonic/blob/main/web) | `web/` | WebGPU/WASM inference |\n| [**Java**](https://github.com/supertone-inc/supertonic/blob/main/java) | `java/` | Cross-platform JVM |\n| [**C++**](https://github.com/supertone-inc/supertonic/blob/main/cpp) | `cpp/` | High-performance C++ |\n| [**C#**](https://github.com/supertone-inc/supertonic/blob/main/csharp) | `csharp/` | .NET ecosystem |\n| [**Go**](https://github.com/supertone-inc/supertonic/blob/main/go) | `go/` | Go implementation |\n| [**Swift**](https://github.com/supertone-inc/supertonic/blob/main/swift) | `swift/` | macOS applications |\n| [**iOS**](https://github.com/supertone-inc/supertonic/blob/main/ios) | `ios/` | Native iOS apps |\n| [**Rust**](https://github.com/supertone-inc/supertonic/blob/main/rust) | `rust/` | Memory-safe systems |\n| [**Flutter**](https://github.com/supertone-inc/supertonic/blob/main/flutter) | `flutter/` | Cross-platform apps |\n\n\u003e For detailed usage instructions, please refer to the README.md in each language directory.\n\nGetting Started\n---------------\n\n[](https://github.com/supertone-inc/supertonic#getting-started)\nFirst, clone the repository:\n\ngit clone https://github.com/supertone-inc/supertonic.git\ncd supertonic\n\n### Prerequisites\n\n[](https://github.com/supertone-inc/supertonic#prerequisites)\nBefore running the examples, download the ONNX models and preset voices, and place them in the `assets` directory:\n\n\u003e **Note:** The Hugging Face repository uses Git LFS. Please ensure Git LFS is installed and initialized before cloning or pulling large model files.\n\u003e \n\u003e \n\u003e *   macOS: `brew install git-lfs \u0026\u0026 git lfs install`\n\u003e *   Generic: see `https://git-lfs.com` for installers\n\ngit clone https://huggingface.co/Supertone/supertonic-2 assets\n\n### Quick Start\n\n[](https://github.com/supertone-inc/supertonic#quick-start)\n**Python Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/py))\n\ncd py\nuv sync\nuv run example_onnx.py\n\n**Node.js Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/nodejs))\n\ncd nodejs\nnpm install\nnpm start\n\n**Browser Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/web))\n\ncd web\nnpm install\nnpm run dev\n\n**Java Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/java))\n\ncd java\nmvn clean install\nmvn exec:java\n\n**C++ Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/cpp))\n\ncd cpp\nmkdir build \u0026\u0026 cd build\ncmake .. \u0026\u0026 cmake --build . --config Release\n./example_onnx\n\n**C# Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/csharp))\n\ncd csharp\ndotnet restore\ndotnet run\n\n**Go Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/go))\n\ncd go\ngo mod download\ngo run example_onnx.go helper.go\n\n**Swift Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/swift))\n\ncd swift\nswift build -c release\n.build/release/example_onnx\n\n**Rust Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/rust))\n\ncd rust\ncargo build --release\n./target/release/example_onnx\n\n**iOS Example** ([Details](https://github.com/supertone-inc/supertonic/blob/main/ios))\n\ncd ios/ExampleiOSApp\nxcodegen generate\nopen ExampleiOSApp.xcodeproj\n\n*   In Xcode: Targets ‚Üí ExampleiOSApp ‚Üí Signing: select your Team\n*   Choose your iPhone as run destination ‚Üí Build \u0026 Run\n\n### Technical Details\n\n[](https://github.com/supertone-inc/supertonic#technical-details)\n*   **Runtime**: ONNX Runtime for cross-platform inference (CPU-optimized; GPU mode is not tested)\n*   **Browser Support**: onnxruntime-web for client-side inference\n*   **Batch Processing**: Supports batch inference for improved throughput\n*   **Audio Output**: Outputs 16-bit WAV files\n\nPerformance\n-----------\n\n[](https://github.com/supertone-inc/supertonic#performance)\nWe evaluated Supertonic's performance (with 2 inference steps) using two key metrics across input texts of varying lengths: Short (59 chars), Mid (152 chars), and Long (266 chars).\n\n**Metrics:**\n\n*   **Characters per Second**: Measures throughput by dividing the number of input characters by the time required to generate audio. Higher is better.\n*   **Real-time Factor (RTF)**: Measures the time taken to synthesize audio relative to its duration. Lower is better (e.g., RTF of 0.1 means it takes 0.1 seconds to generate one second of audio).\n\n### Characters per Second\n\n[](https://github.com/supertone-inc/supertonic#characters-per-second)\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 912 | 1048 | 1263 |\n| **Supertonic** (M4 pro - WebGPU) | 996 | 1801 | 2509 |\n| **Supertonic** (RTX4090) | 2615 | 6548 | 12164 |\n| `API`[ElevenLabs Flash v2.5](https://elevenlabs.io/docs/api-reference/text-to-speech/convert) | 144 | 209 | 287 |\n| `API`[OpenAI TTS-1](https://platform.openai.com/docs/guides/text-to-speech) | 37 | 55 | 82 |\n| `API`[Gemini 2.5 Flash TTS](https://ai.google.dev/gemini-api/docs/speech-generation) | 12 | 18 | 24 |\n| `API`[Supertone Sona speech 1](https://docs.supertoneapi.com/en/api-reference/endpoints/text-to-speech) | 38 | 64 | 92 |\n| `Open`[Kokoro](https://github.com/hexgrad/kokoro/) | 104 | 107 | 117 |\n| `Open`[NeuTTS Air](https://github.com/neuphonic/neutts-air) | 37 | 42 | 47 |\n\n\u003e **Notes:**\n\u003e \n\u003e `API` = Cloud-based API services (measured from Seoul)\n\u003e \n\u003e `Open` = Open-source models\n\u003e \n\u003e  Supertonic (M4 pro - CPU) and (M4 pro - WebGPU): Tested with ONNX\n\u003e \n\u003e  Supertonic (RTX4090): Tested with PyTorch model\n\u003e \n\u003e  Kokoro: Tested on M4 Pro CPU with ONNX\n\u003e \n\u003e  NeuTTS Air: Tested on M4 Pro CPU with Q8-GGUF\n\n### Real-time Factor\n\n[](https://github.com/supertone-inc/supertonic#real-time-factor)\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 0.015 | 0.013 | 0.012 |\n| **Supertonic** (M4 pro - WebGPU) | 0.014 | 0.007 | 0.006 |\n| **Supertonic** (RTX4090) | 0.005 | 0.002 | 0.001 |\n| `API`[ElevenLabs Flash v2.5](https://elevenlabs.io/docs/api-reference/text-to-speech/convert) | 0.133 | 0.077 | 0.057 |\n| `API`[OpenAI TTS-1](https://platform.openai.com/docs/guides/text-to-speech) | 0.471 | 0.302 | 0.201 |\n| `API`[Gemini 2.5 Flash TTS](https://ai.google.dev/gemini-api/docs/speech-generation) | 1.060 | 0.673 | 0.541 |\n| `API`[Supertone Sona speech 1](https://docs.supertoneapi.com/en/api-reference/endpoints/text-to-speech) | 0.372 | 0.206 | 0.163 |\n| `Open`[Kokoro](https://github.com/hexgrad/kokoro/) | 0.144 | 0.124 | 0.126 |\n| `Open`[NeuTTS Air](https://github.com/neuphonic/neutts-air) | 0.390 | 0.338 | 0.343 |\n\n**Additional Performance Data (5-step inference)**\n**Characters per Second (5-step)**\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 596 | 691 | 850 |\n| **Supertonic** (M4 pro - WebGPU) | 570 | 1118 | 1546 |\n| **Supertonic** (RTX4090) | 1286 | 3757 | 6242 |\n\n**Real-time Factor (5-step)**\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n| --- | --- | --- | --- |\n| **Supertonic** (M4 pro - CPU) | 0.023 | 0.019 | 0.018 |\n| **Supertonic** (M4 pro - WebGPU) | 0.024 | 0.012 | 0.010 |\n| **Supertonic** (RTX4090) | 0.011 | 0.004 | 0.002 |\n\n### Natural Text Handling\n\n[](https://github.com/supertone-inc/supertonic#natural-text-handling)\nSupertonic is designed to handle complex, real-world text inputs that contain numbers, currency symbols, abbreviations, dates, and proper nouns.\n\n\u003e üéß **View audio samples more easily**: Check out our [**Interactive Demo**](https://huggingface.co/spaces/Supertone/supertonic#text-handling) for a better viewing experience of all audio examples\n\n**Overview of Test Cases:**\n\n| Category | Key Challenges | Supertonic | ElevenLabs | OpenAI | Gemini | Microsoft |\n| --- | --- | --- | --- | --- | --- | --- |\n| Financial Expression | Decimal currency, abbreviated magnitudes (M, K), currency symbols, currency codes | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå |\n| Time and Date | Time notation, abbreviated weekdays/months, date formats | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå |\n| Phone Number | Area codes, hyphens, extensions (ext.) | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå |\n| Technical Unit | Decimal numbers with units, abbreviated technical notations | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå |\n\n**Example 1: Financial Expression**\n**Text:**\n\n\u003e \"The startup secured **$5.2M** in venture capital, a huge leap from their initial **$450K** seed round.\"\n\n**Challenges:**\n\n*   Decimal point in currency ($5.2M should be read as \"five point two million\")\n*   Abbreviated magnitude units (M for million, K for thousand)\n*   Currency symbol ($) that needs to be properly pronounced as \"dollars\"\n\n**Audio Samples:**\n\n| System | Result | Audio Sample |\n| --- | --- | --- |\n| **Supertonic** | ‚úÖ | [üéß Play Audio](https://drive.google.com/file/d/1eancUOhiSXCVoTu9ddh4S-OcVQaWrPV-/view?usp=sharing) |\n| ElevenLabs Flash v2.5 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1-r2scv7XQ1crIDu6QOh3eqVl445W6ap_/view?usp=sharing) |\n| OpenAI TTS-1 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1MFDXMjfmsAVOqwPx7iveS0KUJtZvcwxB/view?usp=sharing) |\n| Gemini 2.5 Flash TTS | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1dEHpNzfMUucFTJPQK0k4RcFZvPwQTt09/view?usp=sharing) |\n| VibeVoice Realtime 0.5B | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1b69XWBQnSZZ0WZeR3avv7E8mSdoN6p6P/view?usp=sharing) |\n\n**Example 2: Time and Date**\n**Text:**\n\n\u003e \"The train delay was announced at **4:45 PM** on **Wed, Apr 3, 2024** due to track maintenance.\"\n\n**Challenges:**\n\n*   Time expression with PM notation (4:45 PM)\n*   Abbreviated weekday (Wed)\n*   Abbreviated month (Apr)\n*   Full date format (Apr 3, 2024)\n\n**Audio Samples:**\n\n| System | Result | Audio Sample |\n| --- | --- | --- |\n| **Supertonic** | ‚úÖ | [üéß Play Audio](https://drive.google.com/file/d/1ehkZU8eiizBenG2DgR5tzBGQBvHS0Uaj/view?usp=sharing) |\n| ElevenLabs Flash v2.5 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1ta3r6jFyebmA-sT44l8EaEQcMLVmuOEr/view?usp=sharing) |\n| OpenAI TTS-1 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1sskmem9AzHAQ3Hv8DRSZoqX_pye-CXuU/view?usp=sharing) |\n| Gemini 2.5 Flash TTS | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1zx9X8oMsLMXW0Zx_SURoqjju-By2yh_n/view?usp=sharing) |\n| VibeVoice Realtime 0.5B | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1ZpGEstZr4hA0EdAWBMCUFFWuAkIpYsVh/view?usp=sharing) |\n\n**Example 3: Phone Number**\n**Text:**\n\n\u003e \"You can reach the hotel front desk at **(212) 555-0142 ext. 402** anytime.\"\n\n**Challenges:**\n\n*   Area code in parentheses that should be read as separate digits\n*   Phone number with hyphen separator (555-0142)\n*   Abbreviated extension notation (ext.)\n*   Extension number (402)\n\n**Audio Samples:**\n\n| System | Result | Audio Sample |\n| --- | --- | --- |\n| **Supertonic** | ‚úÖ | [üéß Play Audio](https://drive.google.com/file/d/1z-e5iTsihryMR8ll1-N1YXkB2CIJYJ6F/view?usp=sharing) |\n| ElevenLabs Flash v2.5 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1HAzVXFTZfZm0VEK2laSpsMTxzufcuaxA/view?usp=sharing) |\n| OpenAI TTS-1 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/15tjfAmb3GbjP_kmvD7zSdIWkhtAaCPOg/view?usp=sharing) |\n| Gemini 2.5 Flash TTS | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1BCL8n7yligUZyso970ud7Gf5NWb1OhKD/view?usp=sharing) |\n| VibeVoice Realtime 0.5B | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1c0c0YM_Qm7XxSk2uSVYLbITgEDTqaVzL/view?usp=sharing) |\n\n**Example 4: Technical Unit**\n**Text:**\n\n\u003e \"Our drone battery lasts **2.3h** when flying at **30kph** with full camera payload.\"\n\n**Challenges:**\n\n*   Decimal time duration with abbreviation (2.3h = two point three hours)\n*   Speed unit with abbreviation (30kph = thirty kilometers per hour)\n*   Technical abbreviations (h for hours, kph for kilometers per hour)\n*   Technical/engineering context requiring proper pronunciation\n\n**Audio Samples:**\n\n| System | Result | Audio Sample |\n| --- | --- | --- |\n| **Supertonic** | ‚úÖ | [üéß Play Audio](https://drive.google.com/file/d/1kvOBvswFkLfmr8hGplH0V2XiMxy1shYf/view?usp=sharing) |\n| ElevenLabs Flash v2.5 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1_SzfjWJe5YEd0t3R7DztkYhHcI_av48p/view?usp=sharing) |\n| OpenAI TTS-1 | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1P5BSilj5xFPTV2Xz6yW5jitKZohO9o-6/view?usp=sharing) |\n| Gemini 2.5 Flash TTS | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1GU82SnWC50OvC8CZNjhxvNZFKQb7I9_Y/view?usp=sharing) |\n| VibeVoice Realtime 0.5B | ‚ùå | [üéß Play Audio](https://drive.google.com/file/d/1lUTrxrAQy_viEK2Hlu3KLLtTCe8jvbdV/view?usp=sharing) |\n\n\u003e **Note:** These samples demonstrate how each system handles text normalization and pronunciation of complex expressions **without requiring pre-processing or phonetic annotations**.\n\nBuilt with Supertonic\n---------------------\n\n[](https://github.com/supertone-inc/supertonic#built-with-supertonic)\n| Project | Description | Links |\n| --- | --- | --- |\n| **Read Aloud** | Open-source TTS browser extension | [Chrome](https://chromewebstore.google.com/detail/read-aloud-a-text-to-spee/hdhinadidafjejdhmfkjgnolgimiaplp) ¬∑ [Edge](https://microsoftedge.microsoft.com/addons/detail/read-aloud-a-text-to-spe/pnfonnnmfjnpfgagnklfaccicnnjcdkm) ¬∑ [GitHub](https://github.com/ken107/read-aloud) |\n| **PageEcho** | E-Book reader app for iOS | [App Store](https://apps.apple.com/us/app/pageecho/id6755965837) |\n| **VoiceChat** | On-device voice-to-voice LLM chatbot in the browser | [Demo](https://huggingface.co/spaces/RickRossTN/ai-voice-chat) ¬∑ [GitHub](https://github.com/irelate-ai/voice-chat) |\n| **OmniAvatar** | Talking avatar video generator from photo + speech | [Demo](https://huggingface.co/spaces/alexnasa/OmniAvatar) |\n| **CopiloTTS** | Kotlin Multiplatform TTS SDK via ONNX Runtime | [GitHub](https://github.com/sigmadeltasoftware/CopiloTTS) |\n| **Voice Mixer** | PyQt5 tool for mixing and modifying voice styles | [GitHub](https://github.com/Topping1/Supertonic-Voice-Mixer) |\n| **Supertonic MNN** | Lightweight library based on MNN (fp32/fp16/int8) | [GitHub](https://github.com/vra/supertonic-mnn) ¬∑ [PyPI](https://pypi.org/project/supertonic-mnn/) |\n| **Transformers.js** | Hugging Face's JS library with Supertonic support | [GitHub PR](https://github.com/huggingface/transformers.js/pull/1459) ¬∑ [Demo](https://huggingface.co/spaces/webml-community/Supertonic-TTS-WebGPU) |\n| **Pinokio** | 1-click localhost cloud for Mac, Windows, and Linux | [Pinokio](https://pinokio.co/) ¬∑ [GitHub](https://github.com/SUP3RMASS1VE/SuperTonic-TTS) |\n\nCitation\n--------\n\n[](https://github.com/supertone-inc/supertonic#citation)\nThe following papers describe the core technologies used in Supertonic. If you use this system in your research or find these techniques useful, please consider citing the relevant papers:\n\n### SupertonicTTS: Main Architecture\n\n[](https://github.com/supertone-inc/supertonic#supertonictts-main-architecture)\nThis paper introduces the overall architecture of SupertonicTTS, including the speech autoencoder, flow-matching based text-to-latent module, and efficient design choices.\n\n@article{kim2025supertonic,\n  title={SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech System},\n  author={Kim, Hyeongju and Yang, Jinhyeok and Yu, Yechan and Ji, Seunghun and Morton, Jacob and Bous, Frederik and Byun, Joon and Lee, Juheon},\n  journal={arXiv preprint arXiv:2503.23108},\n  year={2025},\n  url={https://arxiv.org/abs/2503.23108}\n}\n\n### Length-Aware RoPE: Text-Speech Alignment\n\n[](https://github.com/supertone-inc/supertonic#length-aware-rope-text-speech-alignment)\nThis paper presents Length-Aware Rotary Position Embedding (LARoPE), which improves text-speech alignment in cross-attention mechanisms.\n\n@article{kim2025larope,\n  title={Length-Aware Rotary Position Embedding for Text-Speech Alignment},\n  author={Kim, Hyeongju and Lee, Juheon and Yang, Jinhyeok and Morton, Jacob},\n  journal={arXiv preprint arXiv:2509.11084},\n  year={2025},\n  url={https://arxiv.org/abs/2509.11084}\n}\n\n### Self-Purifying Flow Matching: Training with Noisy Labels\n\n[](https://github.com/supertone-inc/supertonic#self-purifying-flow-matching-training-with-noisy-labels)\nThis paper describes the self-purification technique for training flow matching models robustly with noisy or unreliable labels.\n\n@article{kim2025spfm,\n  title={Training Flow Matching Models with Reliable Labels via Self-Purification},\n  author={Kim, Hyeongju and Yu, Yechan and Yi, June Young and Lee, Juheon},\n  journal={arXiv preprint arXiv:2509.19091},\n  year={2025},\n  url={https://arxiv.org/abs/2509.19091}\n}\n\nLicense\n-------\n\n[](https://github.com/supertone-inc/supertonic#license)\nThis project's sample code is released under the MIT License. - see the [LICENSE](https://github.com/supertone-inc/supertonic?tab=MIT-1-ov-file) for details.\n\nThe accompanying model is released under the OpenRAIL-M License. - see the [LICENSE](https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE) file for details.\n\nThis model was trained using PyTorch, which is licensed under the BSD 3-Clause License but is not redistributed with this project. - see the [LICENSE](https://docs.pytorch.org/FBGEMM/general/License.html) for details.\n\nCopyright (c) 2026 Supertone Inc.",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:03.052989978Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/spaces/Supertone/supertonic-2: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:56:53.319362566Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q5nw4k",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5nw4k/minimax_m2_is_goated_agentic_capture_the_flag_ctf/",
      "title": "MiniMax M2 is GOATed - Agentic Capture the Flag (CTF) benchmark on GLM-4.5 air, 4.7 (+REAP), and Minimax-M2",
      "content": "",
      "author": "sixx7",
      "created_at": "2026-01-06T16:51:06Z",
      "comments": [
        {
          "id": "ny224qw",
          "author": "__JockY__",
          "content": "Over the winter break I messed a lot with MiniMax-M2 and then MiniMax-M2.1 FP8 @ 200k context with Claude Code cli on an offline system. It is *unbelievable*. Fucking witchcraft.\n\nOld software dev is dead, buried, gone. As a friend said to me earlier today: if you're still typing code, you're a dinosaur. Just a year ago I'd have said \"naaaaah\".\n\nI'm an old coder. It's all I've ever done and I've been doing it for over 4 decades now. This is the biggest shift I ever saw in all my time. Nothing comes close. Everything has changed. \n\nAfter using this shit for real and actually building complex stuff with it... I'm with my buddy. If you're still typing code, you're a dinosaur. CC + M2.1 FP8 has built stuff in a day that would have taken weeks even with my old \"prompt the LLM and copy/paste code\" approach, which is an anachronism now. For most things I doubt I'd  even *need* to see code!\n\nI will, however, be looking at the code. \n\nI saw enough to know that the LLM isn't always making smart choices. It may build extremely complex things, but is it doing so in a sane manner? Not always. Sometimes it even lies and writes code that's just a stub but prints things like \"imported successfully!\" When called out it behaves all sheepish and mostly fixes its shit, but still. That's pretty lazy. I kinda like it.\n\nOr it can make one stupid decision that leads it to implement, document, and build unit tests for the craziest and most overly-complicated unnecessary nonsense I ever saw... but hey. That's witchcraft for you!",
          "created_at": "2026-01-06T18:58:40Z",
          "was_summarised": false
        },
        {
          "id": "ny19qif",
          "author": "sixx7",
          "content": "**TLDR:**  Benchmarked popular open-source/weight models using capture-the-flag (CTF) style challenges that require the models to iteratively write and execute queries against a data lake.  If you want to see the full write-up, [check it out here](https://medium.com/@ai-with-eric/local-llms-and-an-autonomous-agentic-showdown-5d7f552e5846?)\n\n\n\u0026amp;nbsp;\n\n\nI admit I had been sleeping on MiniMax-M2.  For local/personal stuff, GLM-4.5air has been so solid that I took a break from trying out new modals (locally).  Though,  I do have a z.ai subscription where I continue to use their hosted offerings and have been pretty happy with GLM-4.6 and now GLM-4.7\n\n\n\u0026amp;nbsp;\n\nI cannot run GLM-4.7 locally, so that was tested directly using z.ai API.  The rest were run locally.  I almost exclusively use AWQ quants in vllm. Some notes and observations without making this too lengthy:\n\n\n* The REAP'd version of GLM-4.7 did not fair well, performing even worse than GLM-4.5-air\n* GLM-4.7 results were disappointing.  It performed similar, and in some metrics worse, with the full version on z.ai compared to 4.5-air running locally.  I think this highlights how good 4.5-air actually is\n* MiniMax M2 blew GLM.* out of the water.   It won on all but 1 metric, and even that one was really close\n* GLM-4.7 was using the Anthropic-style API, whereas all the locally running models were using the v1/chat/completions OpenAI-style API\n\n\n\u0026amp;nbsp;\n\n**ETA**:  Ran MiniMax M2.1 u/hainesk\n\n\n* Accuracy was the same, and both models failed solving the same challenges\n* M2.1 **wins** on speed, averaging 61 seconds per challenge (M2 was 72.7 seconds) \n* M2.1 **wins** on the number of tool calls, averaging 10.65 (M2 was 12.75)\n* M2.1 **loses** on token use, averaging 264k per challenge (M2 was 244K)\n\n\nM2.1 definitely seems like an upgrade, if for no other reason than it performs well while also being faster",
          "created_at": "2026-01-06T16:51:23Z",
          "urls": [
            {
              "url": "https://medium.com/@ai-with-eric/local-llms-and-an-autonomous-agentic-showdown-5d7f552e5846?",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny3017q",
          "author": "Devcomeups",
          "content": "Installi oh my open code wirh mini max",
          "created_at": "2026-01-06T21:34:18Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/j0yzgwis8rbg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:03.05311206Z"
    },
    {
      "flow_id": "",
      "id": "1q5oz98",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5oz98/lgaiexaonekexaone236ba23b_released/",
      "title": "LGAI-EXAONE/K-EXAONE-236B-A23B released",
      "content": "",
      "author": "jinnyjuice",
      "created_at": "2026-01-06T17:30:10Z",
      "comments": [
        {
          "id": "ny2j6pi",
          "author": "KvAk_AKPlaysYT",
          "content": "The license is not fun :(\n\n\nSummary:\n---\n\nNot open source - proprietary license, unlike MIT or Apache\n\nCommercial redistribution or sublicensing requires separate permission\n\nExplicit ethical and use-based restrictions, which MIT and Apache do not allow\n\nReverse engineering and model analysis are prohibited\n\nLicensor can terminate the license and require destruction of all copies\n\nMandatory naming of derivatives starting with ‚ÄúK-EXAONE‚Äù\n\nUser must indemnify the licensor for claims and damages\n\nKorean law and mandatory arbitration apply\n\nMuch higher legal and operational risk than MIT or Apache\n\n---",
          "created_at": "2026-01-06T20:16:52Z",
          "was_summarised": false
        },
        {
          "id": "ny1v9sk",
          "author": "vasileer",
          "content": "thank you for the benchmark, now I know gpt-oss-120b is still one of the best in its league",
          "created_at": "2026-01-06T18:28:07Z",
          "was_summarised": false
        },
        {
          "id": "ny1kp18",
          "author": "silenceimpaired",
          "content": "I‚Äôm always annoyed to see a license isn‚Äôt Apache or MIT, but at least this one isn‚Äôt too restrictive. It is weird seeing a model this size performing competitively with a 30b (that is to say‚Ä¶ the difficulty to run this on my computer doesn‚Äôt make it worth it‚Ä¶ when the 30b will run much better.)",
          "created_at": "2026-01-06T17:41:21Z",
          "was_summarised": false
        },
        {
          "id": "ny2mdyz",
          "author": "weasl",
          "content": "so they finetune qwen 235b to get worse performance in benchmarks?",
          "created_at": "2026-01-06T20:31:49Z",
          "was_summarised": false
        },
        {
          "id": "ny1q4ov",
          "author": "PopularKnowledge69",
          "content": "Where did I see this combination of 236B and A23B ü§î",
          "created_at": "2026-01-06T18:05:25Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B",
          "was_fetched": true,
          "page": "Title: LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B\n\nMarkdown Content:\n[](https://huggingface.co/collections/LGAI-EXAONE/k-exaone)[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#)[](https://arxiv.org/abs/2601.01739)[](https://github.com/LG-AI-EXAONE/K-EXAONE)[](https://friendli.ai/model/LGAI-EXAONE/K-EXAONE-236B-A23B)\n\nüÜì **Free API until Jan 28th, 2026**!  Try on ‚¨ÜÔ∏è FriendliAI ‚úàÔ∏è\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#introduction) Introduction\n-----------------------------------------------------------------------------------\n\nWe introduce **K-EXAONE**, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features **236 billion total** parameters, with **23 billion active** during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features) Key Features\n\n*   **Architecture \u0026 Efficiency:** Features a 236B fine-grained MoE design (23B active) optimized with **Multi-Token Prediction (MTP)**, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.\n*   **Long-Context Capabilities:** Natively supports a **256K context window**, utilizing a **3:1 hybrid attention** scheme with a **128-token sliding window** to significantly minimize memory usage during long-document processing.\n*   **Multilingual Support:** Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned **150k vocabulary** with **SuperBPE**, improving token efficiency by ~30%.\n*   **Agentic Capabilities:** Demonstrates superior tool-use and search capabilities via **multi-agent strategies.**\n*   **Safety \u0026 Ethics:** Aligned with **universal human values**, the model uniquely incorporates **Korean cultural and historical contexts** to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.\n\nFor more details, please refer to the [technical report](https://arxiv.org/abs/2601.01739) and [GitHub](https://github.com/LG-AI-EXAONE/K-EXAONE).\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B/blob/main/assets/main_figure.png)\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#model-configuration) Model Configuration\n\n*   Number of Parameters: 236B in total and 23B activated\n*   Number of Parameters (without embeddings): 234B\n*   Hidden Dimension: 6,144\n*   Number of Layers: 48 Main layers + 1 MTP layers\n    *   Hybrid Attention Pattern: 12 x (3 Sliding window attention + 1 Global attention)\n\n*   Sliding Window Attention\n    *   Number of Attention Heads: 64 Q-heads and 8 KV-heads\n    *   Head Dimension: 128 for both Q/KV\n    *   Sliding Window Size: 128\n\n*   Global Attention\n    *   Number of Attention Heads: 64 Q-heads and 8 KV-heads\n    *   Head Dimension: 128 for both Q/KV\n    *   No Rotary Positional Embedding Used (NoPE)\n\n*   Mixture of Experts:\n    *   Number of Experts: 128\n    *   Number of Activated Experts: 8\n    *   Number of Shared Experts: 1\n    *   MoE Intermediate Size: 2,048\n\n*   Vocab Size: 153,600\n*   Context Length: 262,144 tokens\n*   Knowledge Cutoff: Dec 2024 (2024/12)\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#evaluation-results) Evaluation Results\n-----------------------------------------------------------------------------------------------\n\nThe following table shows the evaluation results of the K-EXAONE model in reasoning mode, compared to our previous model, [EXAONE-4.0](https://github.com/LG-AI-EXAONE/EXAONE-4.0), and other competing models. The evaluation details can be found in the [technical report](https://arxiv.org/abs/2601.01739).\n\n|  | K-EXAONE (Reasoning) | EXAONE 4.0 (Reasoning) | GPT-OSS (Reasoning: High) | Qwen3-Thinking-2507 | DeepSeek-V3.2 (Reasoning) |\n| --- | --- | --- | --- | --- | --- |\n| Architecture | MoE | Dense | MoE | MoE | MoE |\n| Total Params | 236B | 32B | 117B | 235B | 671B |\n| Active Params | 23B | 32B | 5.1B | 22B | 37B |\n| _World Knowledge_ |\n| MMLU-Pro | 83.8 | 81.8 | 80.7 | 84.4 | 85.0 |\n| GPQA-Diamond | 79.1 | 75.4 | 80.1 | 81.1 | 82.4 |\n| Humanity's Last Exam | 13.6 | 10.6 | 14.9 | 18.2 | 25.1 |\n| _Math_ |\n| IMO-AnswerBench | 76.3 | 66.1 | 75.6 | 74.8 | 78.3 |\n| AIME 2025 | 92.8 | 85.3 | 92.5 | 92.3 | 93.1 |\n| HMMT Nov 2025 | 86.8 | 78.1 | 84.9 | 88.8 | 90.2 |\n| _Coding / Agentic Coding_ |\n| LiveCodeBench Pro 25Q2 (Medium) | 25.9 | 4.8 | 35.4 | 16.0 | 27.9 |\n| LiveCodeBench v6 | 80.7 | 66.7 | 81.9 | 74.1 | 79.4 |\n| Terminal-Bench 2.0 | 29.0 | - | 18.7 | 13.3 | 46.4 |\n| SWE-Bench Verified | 49.4 | - | 62.4 | 25.0 | 73.1 |\n| _Agentic Tool Use_ |\n| œÑ 2-Bench (Retail) | 78.6 | 67.5 | 69.1 | 71.9 | 77.9 |\n| œÑ 2-Bench (Airline) | 60.4 | 52.0 | 60.5 | 58.0 | 66.0 |\n| œÑ 2-Bench (Telecom) | 73.5 | 23.7 | 60.3 | 45.6 | 85.8 |\n| BrowseComp | 31.4 | - | - | - | 51.4 |\n| _Instruction Following_ |\n| IFBench | 67.3 | 36.0 | 69.5 | 52.6 | 62.5 |\n| IFEval | 89.7 | 84.7 | 89.5 | 87.8 | 92.6 |\n| _Long Context Understanding_ |\n| AA-LCR | 53.5 | 14.0 | 50.7 | 67.0 | 65.0 |\n| OpenAI-MRCR | 52.3 | 20.1 | 29.9 | 58.6 | 57.7 |\n| _Korean_ |\n| KMMLU-Pro | 67.3 | 67.7 | 62.4 | 71.6 | 72.1 |\n| KoBALT | 61.8 | 25.4 | 54.3 | 56.1 | 62.7 |\n| CLIcK | 83.9 | 78.8 | 74.6 | 81.3 | 86.3 |\n| HRM8K | 90.9 | 89.4 | 91.6 | 92.0 | 90.6 |\n| Ko-LongBench | 86.8 | 68.0 | 82.2 | 83.2 | 87.9 |\n| _Multilinguality_ |\n| MMMLU | 85.7 | 83.2 | 83.8 | 87.3 | 88.0 |\n| WMT24++ | 90.5 | 80.8 | 93.6 | 94.7 | 90.0 |\n| _Safety_ |\n| Wild-Jailbreak | 89.9 | 62.8 | 98.2 | 85.5 | 79.1 |\n| KGC-Safety | 96.1 | 58.0 | 92.5 | 66.2 | 73.0 |\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#requirements) Requirements\n-----------------------------------------------------------------------------------\n\nUntil the libraries officially support K-EXAONE, you need to install the requirements in our version with the EXAONE-MoE implementations. We will announce when these libraries are updated to support the K-EXAONE model.\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#transformers) Transformers\n\nYou can install the latest version of Transformers with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/transformers). The base version of Transformers is `5.0.0rc1`, so it might be helpful to check [the migration guide](https://github.com/huggingface/transformers/blob/main/MIGRATION_GUIDE_V5.md) from the Transformers library.\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#vllm) vLLM\n\nYou should install both Transformers and vLLM to use K-EXAONE model on vLLM server. You can install the latest version of vLLM with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/vllm/tree/add-exaone-moe).\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#sglang) SGLang\n\nYou should install both Transformers and SGLang to use K-EXAONE model on SGLang server. You can install the latest version of SGLang with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/sglang).\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#llamacpp) llama.cpp\n\nYou can install the latest version of llama.cpp with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/llama.cpp). Please refer to the [official build guide](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md) for details.\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#quickstart) Quickstart\n-------------------------------------------------------------------------------\n\nYou can use the K-EXAONE model with the Transformers library. For better quality, you should check the [usage guideline](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#usage-guideline) section.\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#reasoning-mode) Reasoning mode\n\nFor tasks that require accurate results, you can run the K-EXAONE model in reasoning mode as below.\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"LGAI-EXAONE/K-EXAONE-236B-A23B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    dtype=\"bfloat16\",\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are K-EXAONE, a large language model developed by LG AI Research in South Korea, built to serve as a helpful and reliable assistant.\"},\n    {\"role\": \"user\", \"content\": \"Which one is bigger, 3.9 vs 3.12?\"}\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    enable_thinking=True,   # skippable (default: True)\n)\n\ngenerated_ids = model.generate(\n    **input_ids.to(model.device),\n    max_new_tokens=16384,\n    temperature=1.0,\n    top_p=0.95,\n)\noutput_ids = generated_ids[0][input_ids['input_ids'].shape[-1]:]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#non-reasoning-mode) Non-reasoning mode\n\nFor tasks where latency matters more than accuracy, you can run the K-EXAONE model in non-reasoning mode as below.\n\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are K-EXAONE, a large language model developed by LG AI Research in South Korea, built to serve as a helpful and reliable assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain how wonderful you are\"}\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    enable_thinking=False,\n)\n\ngenerated_ids = model.generate(\n    **input_ids.to(model.device),\n    max_new_tokens=1024,\n    temperature=1.0,\n    top_p=0.95,\n)\noutput_ids = generated_ids[0][input_ids['input_ids'].shape[-1]:]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#agentic-tool-use) Agentic tool use\n\nFor your AI-powered agent, you can leverage K-EXAONE‚Äôs tool calling capability. The K-EXAONE model is compatible with both OpenAI and HuggingFace tool calling specifications. The example below demonstrates tool calling using HuggingFace‚Äôs docstring-to-tool-schema utility.\n\nPlease check the [example file](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B/blob/main/examples/example_output_search.txt) for an example of a search agent conversation using K-EXAONE.\n\n```\nfrom transformers.utils import get_json_schema\n\ndef roll_dice(max_num: int):\n    \"\"\"\n    Roll a dice with the number 1 to N. User can select the number N.\n\n    Args:\n        max_num: The maximum number on the dice.\n    \"\"\"\n    return random.randint(1, max_num)\n\ntool_schema = get_json_schema(roll_dice)\ntools = [tool_schema]\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are K-EXAONE, a large language model developed by LG AI Research in South Korea, built to serve as a helpful and reliable assistant.\"},\n    {\"role\": \"user\", \"content\": \"Roll a D20 twice and sum the results.\"}\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    tools=tools,\n)\n\ngenerated_ids = model.generate(\n    **input_ids.to(model.device),\n    max_new_tokens=16384,\n    temperature=1.0,\n    top_p=0.95,\n)\noutput_ids = generated_ids[0][input_ids['input_ids'].shape[-1]:]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#usage-guideline) Usage Guideline\n-----------------------------------------------------------------------------------------\n\n\u003e To achieve the expected performance, we recommend using the following configurations:\n\u003e \n\u003e \n\u003e *   We strongly recommend to use `temperature=1.0`, `top_p=0.95`, `presence_penalty=0.0` for best performance.\n\u003e *   Different from EXAONE-4.0, K-EXAONE uses `enable_thinking=True` as default. Thus, you need to set `enable_thinking=False` when you want to use non-reasoning mode.\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#deployment) Deployment\n-------------------------------------------------------------------------------\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#tensorrt-llm) TensorRT-LLM\n\nTensorRT-LLM support for the K-EXAONE model is being prepared. Please refer to the [EXAONE-MoE PR](https://github.com/NVIDIA/TensorRT-LLM/pull/10355) on TensorRT-LLM repository for details.\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#vllm-1) vLLM\n\nWe support the K-EXAONE model on vLLM. You need to install our fork of the vLLM library to use the K-EXAONE model. Please check the [requirements](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#requirements) section. Practically, you can serve the model with a 256K context length using tensor parallel on 4 H200 GPUs.\n\nAfter you install the vLLM library with an EXAONE-MoE implementation, you can run the vLLM server by following command:\n\n```\nvllm serve LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --reasoning-parser deepseek_v3 \\\n    --tensor-parallel-size 4 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser hermes\n```\n\nAn OpenAI-compatible API server will be available at [http://localhost:8000/v1](http://localhost:8000/v1).\n\nYou can test the vLLM server by sending a chat completion request as below:\n\n```\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"LGAI-EXAONE/K-EXAONE-236B-A23B\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"How many r'\\''s in \\\"strawberry\\\"?\"}\n        ],\n        \"max_tokens\": 16384,\n        \"temperature\": 1.0,\n        \"top_p\": 0.95,\n        \"chat_template_kwargs\": {\"enable_thinking\": true}\n    }'\n```\n\nIf you are interested in using MTP weights for speculative decoding, add according options as below.\n\n```\nvllm serve LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --reasoning-parser deepseek_v3 \\\n    --tensor-parallel-size 4 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser hermes \\\n    --no-enable-prefix-caching \\\n    --speculative_config '{\n        \"method\": \"mtp\", \n        \"num_speculative_tokens\": 2 \n    }'\n```\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#sglang-1) SGLang\n\nWe support the K-EXAONE model on SGLang. You need to install our fork of the SGLang library to use the K-EXAONE model. Please check the [requirements](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#requirements) section. Practically, you can serve the model with a 256K context length using tensor parallel on 4 H200 GPUs.\n\n```\npython -m sglang.launch_server \\\n    --model LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --tp-size 4 \\\n    --reasoning-parser qwen3\n```\n\nA SGLang server will be available at [http://localhost:30000](http://localhost:30000/).\n\n\u003e Currently, using the OpenAI-compatible server is incompatible with the `transformers\u003e=5.0.0rc0`, so you need to use SGLang native API for now. For native API, please refer to the [official documentation](https://docs.sglang.io/basic_usage/native_api.html).\n\u003e \n\u003e \n\u003e Once the issue is resolved, we will update this section accordingly.\n\nYou can test the SGLang server by sending a request as below:\n\n```\nfrom transformers import AutoTokenizer\nimport requests\n\nmodel_name = \"LGAI-EXAONE/K-EXAONE-236B-A23B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"How many r'\\''s in \\\"strawberry\\\"?\"}\n]\ninput_text = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n)\n\nresponse = requests.post(\n    f\"http://localhost:30000/generate\",\n    json={\n        \"text\": input_text,\n        \"sampling_params\": {\n            \"temperature\": 1.0,\n            \"top_p\": 0.95,\n            \"max_new_tokens\": 16384,\n        },\n    },\n)\nprint(response.json()['text'])\n```\n\nIf you are interested in in using MTP weights for speculative decoding, add according options as below.\n\n```\npython -m sglang.launch_server \\\n    --model LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --tp-size 4 \\\n    --reasoning-parser qwen3 \\\n    --speculative-algorithm EAGLE \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4\n```\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#limitation) Limitation\n-------------------------------------------------------------------------------\n\nThe K-EXAONE language model has certain limitations and may occasionally generate inappropriate responses. The language model generates responses based on the output probability of tokens, and it is determined during learning from training data. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses. Please note that the text generated by K-EXAONE language model does not reflect the views of LG AI Research.\n\n*   Inappropriate answers may be generated, which contain personal, harmful or other inappropriate information.\n*   Biased responses may be generated, which are associated with age, gender, race, and so on.\n*   The generated responses rely heavily on statistics from the training data, which can result in the generation of semantically or syntactically incorrect sentences.\n*   Since the model does not reflect the latest information, the responses may be false or contradictory.\n\nLG AI Research strives to reduce potential risks that may arise from K-EXAONE language models. Users are not allowed to engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of inappropriate outputs violating LG AI's ethical principles when using K-EXAONE language models.\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#license) License\n-------------------------------------------------------------------------\n\nThe model is licensed under [K-EXAONE AI Model License Agreement](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B/blob/main/LICENSE)\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#citation) Citation\n---------------------------------------------------------------------------\n\n```\n@article{k-exaone,\n  title={K-EXAONE Technical Report},\n  author={{LG AI Research}},\n  journal={arXiv preprint arXiv:2601.01739},\n  year={2025}\n}\n```\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#contact) Contact\n-------------------------------------------------------------------------\n\nLG AI Research Technical Support: [contact_us@lgresearch.ai](mailto:contact_us@lgresearch.ai)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:08.337503894Z"
    },
    {
      "flow_id": "",
      "id": "1q5t0hr",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/",
      "title": "Building opensource Zero Server Code Intelligence Engine",
      "content": "Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. What all features would be useful, any integrations, cool ideas, etc?\n\nsite: [https://gitnexus.vercel.app/](https://gitnexus.vercel.app/)  \nrepo: [https://github.com/abhigyanpatwari/GitNexus](https://github.com/abhigyanpatwari/GitNexus)\n\nThis is the crux of how it works:  \nRepo parsed into Graph using AST -\u0026gt; Embeddings model running in browser creates the embeddings -\u0026gt; Everything is stored in a graph DB ( this also runs in browser through webassembly ) -\u0026gt; user sees UI visualization -\u0026gt; AI gets tools to query graph (cyfer query tool), semantic search, grep and node highlight.\n\nSo therefore we get a quick code intelligence engine that works fully client sided 100% private. Except the LLM provider there is no external data outlet. ( working on ollama support )\n\nWould really appreciate any cool ideas / inputs / etc.\n\nThis is what I m aiming for right now:\n\n1\u0026gt; Case 1 is quick way to chat with a repo, but then deepwiki is already there. But gitnexus has graph tools+ui so should be more accurate on audits and UI can help in visualize.\n\n2\u0026gt; Downstream potential usecase will be MCP server exposed from browser itself, windsurf / cursor, etc can use it to perform codebase wise audits, blast radius detection of code changes, etc.\n\n3\u0026gt; Another case might be since its fully private, devs having severe restrictions can use it with ollama or their own inference",
      "author": "DeathShot7777",
      "created_at": "2026-01-06T19:53:26Z",
      "comments": [
        {
          "id": "ny2lpup",
          "author": "codeninja",
          "content": "Being able to pull relevant code context for my problem use case is critical for me to be able to iterate quickly. So if we can query to get a list of relevant files for the \"update the user authentication workflow and integrate Auth0\" problem statement then that's the holy grail of contextual awareness.",
          "created_at": "2026-01-06T20:28:42Z",
          "was_summarised": false
        },
        {
          "id": "ny2g2ts",
          "author": "Main-Lifeguard-6739",
          "content": "would love to use this to inform my claude code agents as their standard goto source for looking up stuff. in combination with skills like \"analyze 3 hops into that direction\" or something like that. would also love to use something like that to track and see which agent is working on what. \n\nWhats your visualization engine used?",
          "created_at": "2026-01-06T20:02:26Z",
          "was_summarised": false
        },
        {
          "id": "ny2toc9",
          "author": "valkarias",
          "content": "I thought of a \"real-time\" graph AST for code for agents to work with, before. My main issue is the agent forgetting written code and logic across not-so-large-code-bases leading to duplicated logic and stuff. Currently I've to audit everything manually, or propagate the changes myself. Does the project allow for this? Granular function level context would be kinda awesome, with agent querying and stuff.",
          "created_at": "2026-01-06T21:05:21Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://v.redd.it/6xrs78taasbg1",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://v.redd.it/6xrs78taasbg1\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://gitnexus.vercel.app/",
          "was_fetched": true,
          "page": "Title: GitNexus\n\nURL Source: https://gitnexus.vercel.app/\n\nMarkdown Content:\nDrop your codebase\n------------------\n\nDrag \u0026 drop a .zip file to generate a knowledge graph\n\n.zip",
          "was_summarised": false
        },
        {
          "url": "https://github.com/abhigyanpatwari/GitNexus",
          "was_fetched": true,
          "page": "Title: GitHub - abhigyanpatwari/GitNexus: GitNexus: The Zero-Server Code Intelligence Engine -       GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration\n\nURL Source: https://github.com/abhigyanpatwari/GitNexus\n\nMarkdown Content:\nGitNexus V2 - Client-Side Knowledge Graph Generator\n---------------------------------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#gitnexus-v2---client-side-knowledge-graph-generator)\n\u003e Privacy-focused, zero-server knowledge graph generator that runs entirely in your browser.\n\nTransform codebases into interactive knowledge graphs using AST parsing, Web Workers, and an embedded KuzuDB WASM database. All processing happens locally - your code never leaves your machine.\n\n**Next up:** Browser-based embeddings + Graph RAG. The cool part? KuzuDB supports native vector indexing, so I can do semantic search AND graph traversal in a single Cypher query. No separate vector DB needed. See [Work in Progress](https://github.com/abhigyanpatwari/GitNexus#-current-work-in-progress) for the full plan.\n\nGitnexusFinal.mp4\n\n* * *\n\nüöß Current Work in Progress\n---------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#-current-work-in-progress)\n**Actively Building:**\n\n*   **Graph RAG Agent** - AI chat with Cypher query generation for intelligent code exploration\n*   **Browser Embeddings** - Small embedding model for semantic node search (see below!)\n*   **Multi-Worker Pool** - Parallel parsing across multiple Web Workers (currently using single worker)\n*   **Ollama Support** - Local LLM integration\n*   **CSV Export** - Export node/relationship tables\n\n### üß† Graph RAG: The Plan\n\n[](https://github.com/abhigyanpatwari/GitNexus#-graph-rag-the-plan)\nHere's what I'm building for the AI layer. The goal: ask questions in plain English, get answers backed by actual graph traversal + semantic understanding.\n\n**The Problem:** A regular LLM doesn't know your codebase. It can't tell you what calls `handleAuth` or what breaks if you change `UserService`. I need to give it tools to explore the graph.\n\n**The Solution:** Combine embeddings (for \"find relevant code by meaning\") with graph queries (for \"trace connections\").\n\nflowchart TD\n    Q[Your Question] --\u003e EMB[Embed with transformers.js]\n    EMB --\u003e VS[Vector Search in KuzuDB]\n    VS --\u003e ENTRY[Entry Point Nodes]\n    ENTRY --\u003e EXPAND[Graph Traversal via Cypher]\n    EXPAND --\u003e CTX[Rich Context]\n    CTX --\u003e LLM[LLM Generates Answer]\n\n**Embedding Model:** I'm going with `snowflake-arctic-embed-xs` - a tiny 22M parameter model that runs entirely in the browser via [transformers.js](https://huggingface.co/docs/transformers.js). It outputs 384-dimensional vectors and scores 50.15 on MTEB (comparable to models 5x its size). The model downloads once (~90MB), gets cached, and runs locally forever. Privacy intact. ‚úÖ\n\n**The Pipeline:**\n\nflowchart LR\n    subgraph Main[\"Main Pipeline (Blocking)\"]\n        P1[Extract] --\u003e P2[Structure] --\u003e P3[Parse] --\u003e P4[Imports] --\u003e P5[Calls]\n    end\n    \n    P5 --\u003e READY[Graph Ready!\u003cbr/\u003eUser can explore]\n    READY --\u003e BG\n    \n    subgraph BG[\"Background (Non-blocking)\"]\n        E1[Load Model] --\u003e E2[Embed Nodes] --\u003e E3[Create Vector Index]\n    end\n    \n    E3 --\u003e AI[AI Search Ready!]\n\nThe idea: you can start exploring the graph immediately after Phase 5. Meanwhile, embeddings are generated in the background. Once done, semantic search unlocks.\n\n### üí° A Fun Discovery: Unified Vector + Graph = Superpowers\n\n[](https://github.com/abhigyanpatwari/GitNexus#-a-fun-discovery-unified-vector--graph--superpowers)\nWhile designing this, I stumbled onto something cool. Most Graph RAG systems use **separate databases** - a vector DB (Pinecone, Qdrant) for semantic search and a graph DB (Neo4j) for traversal. This means the LLM has to:\n\n1.   Call vector search ‚Üí get IDs\n2.   Take those IDs ‚Üí call graph DB\n3.   Coordinate between two systems\n\nBut KuzuDB WASM supports **native vector indexing** (HNSW). Which means it's possible to do vector search AND graph traversal **in a single Cypher query**:\n\n-- Find code similar to \"authentication\" AND trace what calls it\n-- ALL IN ONE QUERY! ü§Ø\nCALL QUERY_VECTOR_INDEX('CodeNode', 'embedding_idx', $queryVector, 10)\nWITH node AS match, distance\nWHERE distance \u003c 0.4\nMATCH (caller:CodeNode)-[r:CodeRelation {type: 'CALLS'}]-\u003e(match)\nRETURN match.name AS found, \n       caller.name AS called_by,\n       distance AS relevance\nORDER BY distance\n\nThis is kind of a big deal. Here's why:\n\n**Traditional approach (2 queries, 2 systems):**\n\n```\nsemantic_search(\"auth\") ‚Üí [\"id1\", \"id2\", \"id3\"]\n                              ‚Üì\ngraph_query(\"MATCH ... WHERE id IN [...]\") ‚Üí results\n```\n\n**Unified KuzuDB approach (1 query, 1 system):**\n\n```\ncypher(\"CALL QUERY_VECTOR_INDEX(...) WITH node MATCH (node)-[...]-\u003e() ...\") ‚Üí results\n```\n\nAnd because `distance` comes back with every result, this provides **built-in reranking for free**:\n\n-- The LLM can dynamically control relevance thresholds!\nCALL QUERY_VECTOR_INDEX('CodeNode', 'idx', $vec, 20)\nWITH node, distance,\n     CASE \n       WHEN distance \u003c 0.15 THEN 'exact_match'\n       WHEN distance \u003c 0.30 THEN 'highly_relevant'\n       ELSE 'related'\n     END AS tier\nWHERE distance \u003c 0.5\nMATCH (node)-[*1..2]-(context)\nRETURN node.name, tier, collect(context.name) AS related\nORDER BY distance\n\n**What this enables:**\n\n*   üéØ **Single query execution** - No round trips between systems\n*   üìä **Hierarchical relevance** - LLM sees exact matches vs related vs weak\n*   üå≥ **Weighted expansion** - Traverse further from better matches\n*   ‚ö° **Dynamic thresholds** - LLM adjusts `WHERE distance \u003c X` per question type\n*   üîÑ **No reranker needed** - Distance IS the relevance score\n\nBasically, the LLM gets to write one smart query that does semantic search, filters by relevance, expands via graph relationships, and returns ranked results. No separate reranker model, no vector DB API calls, no coordination logic. Just Cypher.\n\nStill wrapping my head around all the query patterns this unlocks, but I'm pretty excited about it.\n\n* * *\n\n‚ö° What's New in V2\n------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#-whats-new-in-v2)\nV2 is a major refactor focused on **performance** and **scalability**. Here's what changed and why it matters:\n\n### üé® Sigma.js Replaces D3.js (10,000+ nodes without breaking a sweat)\n\n[](https://github.com/abhigyanpatwari/GitNexus#-sigmajs-replaces-d3js-10000-nodes-without-breaking-a-sweat)\nV1 used D3.js force simulation which worked great for small graphs, but started choking around 2-3k nodes. The browser would freeze, fans would spin, and you'd be staring at a loading spinner.\n\n**V2 uses Sigma.js with WebGL rendering.** This means the GPU does the heavy lifting instead of JavaScript. I've tested graphs with 10k+ nodes and they render smoothly. Pan, zoom, click - all buttery smooth.\n\nThe layout algorithm also moved to **ForceAtlas2 running in a Web Worker**, so your UI stays responsive while the graph positions itself.\n\n### üóÇÔ∏è Dual HashMap Symbol Table (Goodbye Trie, Hello Speed)\n\n[](https://github.com/abhigyanpatwari/GitNexus#%EF%B8%8F-dual-hashmap-symbol-table-goodbye-trie-hello-speed)\nIn V1, I used a **Trie** (prefix tree) to store function/class definitions. It was clever - you could do fuzzy lookups and autocomplete. But it was also slow and memory-hungry for large codebases.\n\nV2 uses a simpler but faster **Dual HashMap** approach:\n\n```\nFile-Scoped Index:  Map\u003cFilePath, Map\u003cSymbolName, NodeID\u003e\u003e\nGlobal Index:       Map\u003cSymbolName, SymbolDefinition[]\u003e\n```\n\n**Why two maps?** When resolving a function call like `handleAuth()`, the system first checks if it's defined in a file that was imported (high confidence). If not, it checks the current file. As a last resort, it searches globally (useful for framework magic like FastAPI's `@app.get` decorators where the connection isn't explicit in imports).\n\nThis change alone provided a **~2x speedup** on the parsing phase.\n\n### üíæ LRU Cache for AST Trees (Memory That Cleans Itself)\n\n[](https://github.com/abhigyanpatwari/GitNexus#-lru-cache-for-ast-trees-memory-that-cleans-itself)\nTree-sitter generates AST (Abstract Syntax Tree) objects that live in WASM memory. In V1, I kept all of them around, which meant memory usage grew linearly with file count. Parse 5000 files? That's 5000 AST objects eating RAM.\n\nV2 uses an **LRU (Least Recently Used) cache** with a cap of 50 entries. When the system needs to parse file #51, the oldest unused AST gets evicted and `tree.delete()` is called to free the WASM memory.\n\nThe clever part: files are parsed in Phase 3, then those ASTs are reused in Phase 4 (imports) and Phase 5 (calls). The LRU cache keeps recently-parsed files hot, so re-parsing is rarely needed.\n\n### üìä Overall Results\n\n[](https://github.com/abhigyanpatwari/GitNexus#-overall-results)\n| Metric | V1 | V2 | Improvement |\n| --- | --- | --- | --- |\n| Max renderable nodes | ~3,000 | 10,000+ | ~3x+ |\n| Parse speed | Baseline | 3-5x faster | ‚ö° |\n| Memory usage | Grows unbounded | Capped by LRU | Stable |\n| UI responsiveness | Freezes during layout | Smooth (Web Worker) | ‚úÖ |\n\n**Note:** V2 currently uses a single Web Worker. Multi-worker support is planned and should give another 2-4x speedup on multi-core machines.\n\n* * *\n\nProject Focus\n-------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#project-focus)\n*   **Privacy-first**: Zero-cost, zero-server tool to create knowledge graphs from codebases entirely within the browser\n*   **Human + AI friendly**: Knowledge graphs useful for both manual exploration and AI agent context retrieval\n*   **Fast \u0026 cheap**: Browser-based indexing is faster and cheaper than embedding models + vector RAG\n*   **Understanding codebases**: Graph visualization + Graph RAG chatbot for accurate context retrieval\n\nAI Use Cases\n------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#ai-use-cases)\n*   **Blast radius analysis**: Compute impact of function/module changes, enumerate affected endpoints/tests\n*   **Fault isolation**: Start from a failing symbol, traverse callers/callees to isolate the fault line faster than grep or embeddings\n*   **Code health**: Detect orphaned nodes, unresolved imports, unused functions with simple graph queries\n*   **Auditing**: Spot forbidden dependencies or layer violations quickly during onboarding or security reviews\n\n* * *\n\nFeatures\n--------\n\n[](https://github.com/abhigyanpatwari/GitNexus#features)\n**Code Analysis**\n\n*   Analyze ZIP files containing codebases\n*   TypeScript, JavaScript, Python support\n*   Interactive WebGL graph visualization with Sigma.js\n*   Real-time Cypher queries against in-browser graph database\n\n**Processing**\n\n*   5-phase pipeline: Extract ‚Üí Structure ‚Üí Parsing ‚Üí Imports ‚Üí Calls\n*   Web Worker offloading (single worker, multi-worker planned)\n*   Tree-sitter WASM for AST parsing\n*   LRU cache with automatic WASM memory cleanup\n\n**Privacy**\n\n*   100% client-side - no server, no uploads\n*   API keys stored in localStorage only\n*   Open source and auditable\n\n* * *\n\nArchitecture\n------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#architecture)\n### V1 vs V2 Comparison\n\n[](https://github.com/abhigyanpatwari/GitNexus#v1-vs-v2-comparison)\n| Aspect | V1 | V2 |\n| --- | --- | --- |\n| Code Style | Class-based | Function-based (factory pattern) |\n| Symbol Lookup | Trie data structure | Dual HashMap (file-scoped + global) |\n| Visualization | D3.js force simulation | Sigma.js + WebGL + ForceAtlas2 |\n| Workers | Worker pool with Comlink | Single worker (multi-worker planned) |\n| AI Pipeline | LangChain ReAct agents | Not yet implemented (WIP) |\n| Layout | D3 force simulation (main thread) | ForceAtlas2 (Web Worker) |\n\n### System Overview\n\n[](https://github.com/abhigyanpatwari/GitNexus#system-overview)\n\ngraph TB\n    subgraph MainThread[Main Thread]\n        UI[React UI]\n        CTX[AppState Context]\n        SIGMA[Sigma.js WebGL]\n    end\n\n    subgraph WorkerThread[Web Worker]\n        PIPE[Ingestion Pipeline]\n        KUZU[KuzuDB WASM]\n        TS[Tree-sitter WASM]\n    end\n\n    UI --\u003e CTX\n    CTX --\u003e SIGMA\n    PIPE --\u003e TS\n    PIPE --\u003e KUZU\n    MainThread -.-\u003e WorkerThread\n\nThink of it like this: the main thread handles what you see (React UI, graph rendering), while the Web Worker does all the heavy computation (parsing, database queries) in the background. They communicate through Comlink, which makes calling worker functions feel like regular async calls.\n\n### Data Flow\n\n[](https://github.com/abhigyanpatwari/GitNexus#data-flow)\n\nflowchart LR\n    ZIP[ZIP File] --\u003e EXTRACT[Extract]\n    EXTRACT --\u003e STRUCT[Structure]\n    STRUCT --\u003e PARSE[Parse]\n    PARSE --\u003e IMPORT[Imports]\n    IMPORT --\u003e CALLS[Calls]\n    CALLS --\u003e GRAPH[Graph]\n    GRAPH --\u003e VIZ[Sigma.js]\n    GRAPH --\u003e KUZU[(KuzuDB)]\n\n* * *\n\n5-Phase Ingestion Pipeline\n--------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#5-phase-ingestion-pipeline)\nHere's what happens when you drop a ZIP file:\n\nflowchart TD\n    START([ZIP File]) --\u003e P1\n  \n    subgraph P1[Phase 1: Extract - 0-15%]\n        E1[Decompress ZIP]\n        E2[Collect file paths]\n    end\n  \n    subgraph P2[Phase 2: Structure - 15-30%]\n        S1[Build folder tree]\n        S2[Create CONTAINS edges]\n    end\n  \n    subgraph P3[Phase 3: Parsing - 30-70%]\n        PA1[Load Tree-sitter grammar]\n        PA2[Generate ASTs]\n        PA3[Extract symbols]\n        PA4[Populate Symbol Table]\n    end\n  \n    subgraph P4[Phase 4: Imports - 70-82%]\n        I1[Find import statements]\n        I2[Resolve paths]\n        I3[Create IMPORTS edges]\n    end\n  \n    subgraph P5[Phase 5: Calls - 82-100%]\n        C1[Find function calls]\n        C2[Resolve targets]\n        C3[Create CALLS edges]\n    end\n  \n    P1 --\u003e P2 --\u003e P3 --\u003e P4 --\u003e P5\n    P5 --\u003e DONE([Knowledge Graph Ready])\n\n### What Each Phase Does\n\n[](https://github.com/abhigyanpatwari/GitNexus#what-each-phase-does)\n**Phase 1: Extract** - JSZip is used to decompress your ZIP file and store all file contents in a Map. Simple but necessary.\n\n**Phase 2: Structure** - The system walks through all file paths and builds a tree of folders and files. A path like `src/components/Button.tsx` creates nodes for `src`, `components`, and `Button.tsx` with `CONTAINS` relationships connecting them.\n\n**Phase 3: Parsing** - This is where the magic happens. Tree-sitter parses each file into an AST, and extracts all the interesting bits: functions, classes, interfaces, methods. These get stored in the Symbol Table for later lookup.\n\n**Phase 4: Imports** - The pipeline finds all `import` and `require` statements and determines which files they point to. `import { foo } from './utils'` might resolve to `./utils.ts`, `./utils/index.ts`, etc. Common extensions are tried until a match is found.\n\n**Phase 5: Calls** - The trickiest phase. The pipeline finds all function calls and determines what they're calling. It uses a resolution strategy (import map ‚Üí local ‚Üí global) to link calls to their definitions.\n\n* * *\n\nSymbol Resolution: How We Link Function Calls\n---------------------------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#symbol-resolution-how-we-link-function-calls)\nWhen the system encounters code like this:\n\nimport { validateUser } from './auth';\n\nfunction login() {\n  validateUser(email, password);  // ‚Üê What does this call?\n}\n\nThe system needs to figure out that `validateUser()` refers to the function defined in `./auth.ts`. Here's the strategy:\n\nLoading\n\nflowchart TD\n    CALL[Found: validateUser] --\u003e CHECK1\n  \n    CHECK1{In Import Map?}\n    CHECK1 --\u003e|Yes| FOUND1[Check auth.ts symbols]\n    CHECK1 --\u003e|No| CHECK2\n  \n    CHECK2{In Current File?}\n    CHECK2 --\u003e|Yes| FOUND2[Use local definition]\n    CHECK2 --\u003e|No| CHECK3\n  \n    CHECK3{Global Search}\n    CHECK3 --\u003e|Found| FOUND3[Use first match]\n    CHECK3 --\u003e|Not Found| SKIP[Skip this call]\n  \n    FOUND1 --\u003e DONE[Create CALLS edge]\n    FOUND2 --\u003e DONE\n    FOUND3 --\u003e DONE\n\n**Why the global fallback?** Some frameworks use \"magic\" that doesn't show up in imports. For example, FastAPI:\n\n@app.get(\"/users\")\ndef get_users():\n    return db.query(User)  # Where does 'db' come from?\n\nThe `db` object might be injected by the framework, not explicitly imported. The global search catches these cases (with lower confidence).\n\n* * *\n\nLRU AST Cache\n-------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#lru-ast-cache)\nParsing files into ASTs is expensive, and AST objects live in WASM memory (which doesn't get garbage collected like regular JS objects). An LRU cache is used to keep memory bounded:\n\nLoading\n\nflowchart LR\n    subgraph Cache[LRU Cache - 50 slots]\n        HOT[Recently Used ASTs]\n        COLD[Oldest ASTs]\n    end\n  \n    NEW[New AST] --\u003e|set| HOT\n    COLD --\u003e|evicted| DELETE[tree.delete - frees WASM memory]\n  \n    REQUEST[Need AST] --\u003e|get| HOT\n\n**How it helps:**\n\n*   Phase 3 parses files and stores ASTs in cache\n*   Phase 4 \u0026 5 reuse cached ASTs (no re-parsing!)\n*   If cache is full, oldest AST is evicted and WASM memory is freed\n*   Result: Memory stays bounded even for huge codebases\n\n* * *\n\nGraph Visualization\n-------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#graph-visualization)\n### Sigma.js + ForceAtlas2\n\n[](https://github.com/abhigyanpatwari/GitNexus#sigmajs--forceatlas2)Loading\n\nflowchart LR\n    subgraph Main[Main Thread]\n        SIGMA[Sigma.js]\n        WEBGL[WebGL Canvas]\n    end\n  \n    subgraph Layout[Layout Worker]\n        FA2[ForceAtlas2]\n    end\n  \n    GRAPH[Graphology Graph] --\u003e FA2\n    FA2 --\u003e|positions| GRAPH\n    GRAPH --\u003e SIGMA\n    SIGMA --\u003e WEBGL\n\n**Why this combo works:**\n\n*   **Sigma.js** uses WebGL to render nodes/edges on the GPU - handles 10k+ nodes easily\n*   **ForceAtlas2** is a physics-based layout that runs in a Web Worker - UI stays responsive\n*   **Graphology** is the data structure holding the graph - fast lookups and updates\n\n**Visual features:**\n\n*   Nodes sized by type (folders bigger than files, files bigger than functions)\n*   Edges colored by relationship (green for CONTAINS, blue for IMPORTS, purple for CALLS)\n*   Click a node to highlight its connections\n*   Pan/zoom with mouse, reset view button\n\n* * *\n\nKuzuDB Integration\n------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#kuzudb-integration)\nThe graph is loaded into KuzuDB (an embedded graph database) so you can run Cypher queries:\n\nLoading\n\nflowchart TD\n    GRAPH[Knowledge Graph] --\u003e CSV[Generate CSV]\n    CSV --\u003e COPY[COPY FROM bulk load]\n    COPY --\u003e KUZU[(KuzuDB WASM)]\n    QUERY[Cypher Query] --\u003e KUZU\n    KUZU --\u003e RESULTS[Query Results]\n\n**Example queries you can run:**\n\n-- Find all functions in a file\nMATCH (f:CodeNode {label: 'File', name: 'App.tsx'})-[:CodeRelation]-\u003e(fn:CodeNode {label: 'Function'})\nRETURN fn.name\n\n-- Find what imports a specific file\nMATCH (f:CodeNode)-[r:CodeRelation {type: 'IMPORTS'}]-\u003e(target:CodeNode {name: 'utils.ts'})\nRETURN f.name\n\n**Status:**\n\n*   ‚úÖ KuzuDB WASM initialization\n*   ‚úÖ Polymorphic schema (single node/edge tables)\n*   ‚úÖ CSV generation and bulk loading\n*   ‚úÖ Cypher query execution\n*   üöß Vector embeddings + HNSW index (WIP)\n*   üöß Graph RAG agent (WIP)\n\n* * *\n\nTech Stack\n----------\n\n[](https://github.com/abhigyanpatwari/GitNexus#tech-stack)\n*   **Frontend**: React 18 + TypeScript + Vite + Tailwind CSS v4\n*   **Visualization**: Sigma.js + Graphology + ForceAtlas2 (WebGL)\n*   **Parsing**: Tree-sitter WASM (TypeScript, JavaScript, Python)\n*   **Database**: KuzuDB WASM (in-browser graph database + vector index)\n*   **Concurrency**: Web Worker + Comlink\n*   **Caching**: lru-cache with WASM memory management\n*   **AI (WIP)**: transformers.js for browser embeddings, LangChain for agent orchestration\n\n* * *\n\nGraph Schema\n------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#graph-schema)\n### Node Types\n\n[](https://github.com/abhigyanpatwari/GitNexus#node-types)\n| Label | Description | Example |\n| --- | --- | --- |\n| `Folder` | Directory in project | `src/components` |\n| `File` | Source code file | `App.tsx` |\n| `Function` | Function definition | `handleClick` |\n| `Class` | Class definition | `UserService` |\n| `Interface` | Interface definition | `Props` |\n| `Method` | Class method | `render` |\n\n### Relationship Types\n\n[](https://github.com/abhigyanpatwari/GitNexus#relationship-types)\n| Type | From | To | Description |\n| --- | --- | --- | --- |\n| `CONTAINS` | Folder | File/Folder | Directory structure |\n| `DEFINES` | File | Function/Class/etc. | Code definitions |\n| `IMPORTS` | File | File | Module dependencies |\n| `CALLS` | File | Function/Method | Function call graph |\n\n* * *\n\nGetting Started\n---------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#getting-started)\n**Prerequisites**: Node.js 18+\n\ngit clone \u003crepository-url\u003e\ncd gitnexus\nnpm install\nnpm run dev\n\nOpen [http://localhost:5173](http://localhost:5173/)\n\n**Usage:**\n\n1.   Drag \u0026 drop a ZIP file containing your codebase\n2.   Wait for the 5-phase pipeline to complete\n3.   Explore the interactive graph\n4.   Click nodes to view code, filter by type, adjust depth\n\n* * *\n\nPlanned: AI Features\n--------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#planned-ai-features)\n### Graph RAG Agent (WIP)\n\n[](https://github.com/abhigyanpatwari/GitNexus#graph-rag-agent-wip)\nThe idea: ask questions in plain English, get answers backed by graph queries + semantic understanding.\n\nLoading\n\nflowchart TD\n    USER[Your Question] --\u003e LLM[LLM]\n    LLM --\u003e |Generates| CYPHER[Unified Cypher Query]\n    \n    subgraph KUZU[KuzuDB WASM]\n        CYPHER --\u003e VEC[Vector Search]\n        VEC --\u003e GRAPH[Graph Traversal]\n        GRAPH --\u003e RANK[Ranked Results]\n    end\n    \n    RANK --\u003e CTX[Rich Context + Code Snippets]\n    CTX --\u003e LLM\n    LLM --\u003e ANSWER[Your Answer]\n\n**Example interactions:**\n\n*   \"What functions call `handleAuth`?\" ‚Üí Vector search finds `handleAuth`, Cypher traces callers\n*   \"Show me the blast radius if I change `UserService`\" ‚Üí Finds service, traverses 3 hops of dependencies\n*   \"How does authentication work in this codebase?\" ‚Üí Semantic search for auth-related code, returns connected components\n\n**Why dynamic Cypher generation?** Originally I planned to use pre-built query templates (because LLMs can be... creative with syntax). But with the unified vector + graph approach, the LLM just needs to learn one pattern:\n\nCALL QUERY_VECTOR_INDEX(...) WITH node, distance\nWHERE distance \u003c [threshold]\nMATCH (node)-[relationship pattern]-\u003e(connected)\nRETURN [what you need]\nORDER BY distance\n\nGive the LLM the schema, a few examples, and let it compose queries. The schema is simple enough that modern LLMs (GPT-4, Claude) handle it well. And if a query fails? The error message is usually clear enough for the LLM to self-correct.\n\n* * *\n\nüî¨ Deep Dive: Copy-on-Write Woes with In-Memory WASM Databases\n--------------------------------------------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#-deep-dive-copy-on-write-woes-with-in-memory-wasm-databases)\nWhile building the embedding pipeline, I hit an interesting memory problem. Documenting it here because it's a non-obvious gotcha for anyone doing vector storage in browser-side databases.\n\n### The Setup\n\n[](https://github.com/abhigyanpatwari/GitNexus#the-setup)\nI wanted to store 384-dimensional embeddings alongside the code nodes. Natural instinct: add an `embedding FLOAT[384]` column to the existing `CodeNode` table, bulk load the graph, then `UPDATE` each node with its embedding.\n\n-- Seemed reasonable, right?\nMATCH (n:CodeNode {id: $id}) SET n.embedding = $vec\n\n### The Problem\n\n[](https://github.com/abhigyanpatwari/GitNexus#the-problem)\nWorked fine for ~20 nodes. Exploded at ~1000 nodes with:\n\n```\nBuffer manager exception: Unable to allocate memory! The buffer pool is full!\n```\n\nI configured a 512MB buffer pool. 1000 embeddings √ó 384 floats √ó 4 bytes = ~1.5MB. Where did 512MB go?\n\n**Answer: Copy-on-Write (COW).**\n\nMost databases don't modify records in place. When you `UPDATE`, they create a new version of the record (for transaction rollback, MVCC, etc.). The old version sticks around until commit.\n\nOur `CodeNode` table had a `content` field averaging ~2KB per node (code snippets). So each `UPDATE`:\n\n1.   Reads the entire node (~2KB)\n2.   Creates a new copy with the embedding (~3.5KB)\n3.   Keeps the old version around\n\nFor 1000 nodes: `1000 √ó 2KB (old) + 1000 √ó 3.5KB (new) = ~5.5MB`... but that's just user data. KuzuDB's internal structures (indexes, hash tables, page management) multiply this significantly. And since it's an in-memory database, the buffer pool IS the storage - there's no disk to spill to.\n\nLoading\n\nflowchart LR\n    subgraph Before[\"Before UPDATE\"]\n        N1[CodeNode\u003cbr/\u003eid + name + content\u003cbr/\u003e~2KB]\n    end\n    \n    subgraph During[\"During UPDATE (COW)\"]\n        N1_OLD[Old Version\u003cbr/\u003e~2KB]\n        N1_NEW[New Version\u003cbr/\u003e+ embedding\u003cbr/\u003e~3.5KB]\n    end\n    \n    subgraph Problem[\"√ó 1000 nodes\"]\n        BOOM[üí• Buffer Pool Exhausted]\n    end\n    \n    Before --\u003e During --\u003e Problem\n\n### The Fix: Separate Table Architecture\n\n[](https://github.com/abhigyanpatwari/GitNexus#the-fix-separate-table-architecture)\nDon't `UPDATE` wide tables. `INSERT` into a narrow one.\n\nLoading\n\nflowchart TD\n    subgraph Old[\"‚ùå Original Design\"]\n        CN1[CodeNode\u003cbr/\u003eid, name, content, embedding\u003cbr/\u003e~3.5KB per UPDATE copy]\n    end\n    \n    subgraph New[\"‚úÖ New Design\"]\n        CN2[CodeNode\u003cbr/\u003eid, name, content]\n        CE[CodeEmbedding\u003cbr/\u003enodeId, embedding\u003cbr/\u003e~1.5KB INSERT only]\n    end\n    \n    Old --\u003e|\"COW copies entire 2KB+ node\"| FAIL[Memory Explosion]\n    New --\u003e|\"INSERT into lightweight table\"| WIN[Works at scale]\n\nNow the process is:\n\n1.   Bulk load `CodeNode` (no embedding column)\n2.   `CREATE` rows in `CodeEmbedding` table (just `nodeId` + `embedding`)\n3.   Vector index lives on `CodeEmbedding`\n4.   Semantic search JOINs back to `CodeNode` for metadata\n\n**Trade-off:** Every semantic search needs a JOIN. But it's a primary key lookup (O(1)), so it's only ~1-5ms extra per query. Totally worth it to not explode at 1000 nodes.\n\n### Lessons Learned\n\n[](https://github.com/abhigyanpatwari/GitNexus#lessons-learned)\n1.   **In-memory WASM DBs have hard limits** - No disk spillover, buffer pool is everything\n2.   **COW amplifies record size** - That innocent `UPDATE` copies your whole row\n3.   **Normalize for bulk writes** - Especially for append-only data like embeddings\n4.   **Profile the pathological case** - 20 nodes worked, 1000 didn't. Always test at scale\n\nThis is one of those \"obvious in hindsight\" things. Most vector DB tutorials show single-table schemas because they're using databases with disk backing. In-browser WASM land plays by different rules.\n\n* * *\n\nSecurity \u0026 Privacy\n------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#security--privacy)\n*   All processing happens in your browser\n*   No code uploaded to any server\n*   API keys stored in localStorage only\n*   Open source - audit the code yourself\n\n* * *\n\nDeployment\n----------\n\n[](https://github.com/abhigyanpatwari/GitNexus#deployment)\n\nnpm run build\nnpm run preview\n\nThe build outputs to `dist/` and can be served from any static hosting.\n\n* * *\n\nLicense\n-------\n\n[](https://github.com/abhigyanpatwari/GitNexus#license)\nMIT License\n\n* * *\n\nAcknowledgments\n---------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#acknowledgments)\n*   [Tree-sitter](https://tree-sitter.github.io/) - AST parsing\n*   [KuzuDB](https://kuzudb.com/) - Embedded graph database\n*   [Sigma.js](https://www.sigmajs.org/) - WebGL graph rendering\n*   [Graphology](https://graphology.github.io/) - Graph data structure\n*   [code-graph-rag](https://github.com/vitali87/code-graph-rag) - Reference implementation",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:25.014167352Z"
    },
    {
      "flow_id": "",
      "id": "1q5a0if",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/",
      "title": "Liquid Ai released LFM2.5, family of tiny on-device foundation models.",
      "content": "Hugging face: [https://huggingface.co/collections/LiquidAI/lfm25](https://huggingface.co/collections/LiquidAI/lfm25)\n\nIt‚Äôs built to power reliable on-device agentic applications: higher quality, lower latency, and broader modality support in the \\~1B parameter class.\n\n\u0026gt; LFM2.5 builds on LFM2 device-optimized hybrid architecture\n\u0026gt; Pretraining scaled from 10T ‚Üí 28T tokens\n\u0026gt; Expanded reinforcement learning post-training\n\u0026gt; Higher ceilings for instruction following\n\n5 open-weight model instances from a single architecture:\n\n\u0026gt; General-purpose instruct model\n\u0026gt; Japanese-optimized chat model\n\u0026gt; Vision-language model\n\u0026gt; Native audio-language model (speech in/out)\n\u0026gt; Base checkpoints for deep customization\n\n",
      "author": "Difficult-Cap-7527",
      "created_at": "2026-01-06T05:27:54Z",
      "comments": [
        {
          "id": "nxynik2",
          "author": "adt",
          "content": "1.2B parameters trained on 28T tokens has a data ratio @ 23,334:1.\n\nEdit: Beaten by Qwen3-0.6B trained on 36T @ 60,000:1.\n\n[https://lifearchitect.ai/models-table/](https://lifearchitect.ai/models-table/)",
          "created_at": "2026-01-06T06:03:41Z",
          "urls": [
            {
              "url": "https://lifearchitect.ai/models-table/",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxysoow",
          "author": "HistorianPotential48",
          "content": "Testing on their site with some prompts we're handling with qwen3 8b. Feels more like a 4B and very fast, but still has the problem of bad at following instructions for special formats - \"Complete one sentence...\" gives 5 sentences instead; \"Create a json like this...\" results in an extra } symbol but otherwise perfect.\n\nAlmost there, probably can be a very fast chat to ask things (RAG knowledge base?), but not smart enough for small practical tasks. Perfect for generating those llm bot tweet replies i guess.   \n  \nThey also have a VL-1.6B. wonder how that's doing",
          "created_at": "2026-01-06T06:46:33Z",
          "was_summarised": false
        },
        {
          "id": "nxyro14",
          "author": "mitchins-au",
          "content": "Utterly amazing. A graph that‚Äôs to scale.",
          "created_at": "2026-01-06T06:37:50Z",
          "was_summarised": false
        },
        {
          "id": "nxz2f3n",
          "author": "DeltaSqueezer",
          "content": "If it is to be run on-device, I wonder why they don't train for native FP8 or FP4, you don't need batching performance could have more parameters for the same RAM.",
          "created_at": "2026-01-06T08:14:26Z",
          "was_summarised": false
        },
        {
          "id": "nxyolcc",
          "author": "Sixhaunt",
          "content": "have any of you tried it out yet to see how accurate the benchmarks are? Looks promising if true",
          "created_at": "2026-01-06T06:12:24Z",
          "was_summarised": false
        },
        {
          "id": "nxzmflc",
          "author": "llama-impersonator",
          "content": "i mean, i like liquid but holy shit make something larger already",
          "created_at": "2026-01-06T11:19:38Z",
          "was_summarised": false
        },
        {
          "id": "nxypz72",
          "author": "Kahvana",
          "content": "Wish they showed their previous model on there as well.",
          "created_at": "2026-01-06T06:23:40Z",
          "was_summarised": false
        },
        {
          "id": "nxz9xvd",
          "author": "if47",
          "content": "Impressive achievements, but terrible charts.",
          "created_at": "2026-01-06T09:26:48Z",
          "was_summarised": false
        },
        {
          "id": "nxz8qt2",
          "author": "TechnoByte_",
          "content": "Here are the models, including GGUF: https://huggingface.co/collections/LiquidAI/lfm25",
          "created_at": "2026-01-06T09:15:13Z",
          "urls": [
            {
              "url": "https://huggingface.co/collections/LiquidAI/lfm25",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxzs4i7",
          "author": "__Maximum__",
          "content": "These 1b models are getting smarter than a lot of people i have met, true signs of advancements.",
          "created_at": "2026-01-06T12:04:26Z",
          "was_summarised": false
        },
        {
          "id": "ny05f5g",
          "author": "ElectronSpiderwort",
          "content": "\"Native audio-language model (speech in/out)\" kind of buried under the fold",
          "created_at": "2026-01-06T13:31:36Z",
          "was_summarised": false
        },
        {
          "id": "nxzxkgd",
          "author": "guiopen",
          "content": "It is the best 1b model I tested by far, the only usable one. Higher speed even compared to models of the same size, and can speak Portuguese making less grammar errors than some bigger 4b models like nanbeige and even qwen3 4b",
          "created_at": "2026-01-06T12:43:12Z",
          "was_summarised": false
        },
        {
          "id": "ny04lvo",
          "author": "zelkovamoon",
          "content": "LFM2 was pretty good, so im excited to try this. Really hoping tool calling is better with these models, that was basically my biggest complaint.",
          "created_at": "2026-01-06T13:26:57Z",
          "was_summarised": false
        },
        {
          "id": "ny06sp7",
          "author": "bakawolf123",
          "content": "Tested locally with MLX on M1Pro and it looks to be comparable to Qwen3-4B but about 2x faster, though there're no \u0026lt;thinking\u0026gt; blocks. Would be interesting what can be done with finetuning it.  \nedit: works lightning fast on a 17pro iPhone too",
          "created_at": "2026-01-06T13:39:24Z",
          "was_summarised": false
        },
        {
          "id": "ny06ukz",
          "author": "steezy13312",
          "content": "This is the *exactly* the kind of model to compliment my [MCP Context Proxy project](https://github.com/samteezy/mcp-context-proxy/). It's not solving anything on its own, but you're using it to offload work from your slower, heavier main model. Downloading now",
          "created_at": "2026-01-06T13:39:42Z",
          "urls": [
            {
              "url": "https://github.com/samteezy/mcp-context-proxy/",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny0ofno",
          "author": "memeposter65",
          "content": "It's crazy good for the size, I love it.",
          "created_at": "2026-01-06T15:12:52Z",
          "was_summarised": false
        },
        {
          "id": "ny0pg84",
          "author": "meatycowboy",
          "content": "28T tokens for a 1.2B model is crazy.",
          "created_at": "2026-01-06T15:17:46Z",
          "was_summarised": false
        },
        {
          "id": "nxzw6ri",
          "author": "guiopen",
          "content": "Happy to see you guys releasing base models!",
          "created_at": "2026-01-06T12:33:48Z",
          "was_summarised": false
        },
        {
          "id": "ny15vvc",
          "author": "1_7xr",
          "content": "I wanted to try the vision version on LM Studio but whenever I upload an image, it says the model doesn't support images. Any one with some experience on how to deal with this?",
          "created_at": "2026-01-06T16:33:57Z",
          "was_summarised": false
        },
        {
          "id": "ny1jvms",
          "author": "sxales",
          "content": "I guess it performed about the same as LFM2 2.6b. I am genuinely in awe of how fast the model is, but it seems largely useless. It failed all my usual tests: grade school math, logical puzzles, and summarization. \n\nSince they only seem to be releasing small models, I wonder if whatever voodoo they use to make prompt processing so fast isn't scaling well.",
          "created_at": "2026-01-06T17:37:36Z",
          "was_summarised": false
        },
        {
          "id": "ny4l3a6",
          "author": "ScoreUnique",
          "content": "I downloaded Q8 on my Pixel 8 with pocket pal, and oh dear I felt like chatting to GPT-4 but locally with 15 tps. \n\nI will test it further - I'll be in a flight this weekend.",
          "created_at": "2026-01-07T02:25:49Z",
          "was_summarised": false
        },
        {
          "id": "nxz1vkm",
          "author": "GoranjeWasHere",
          "content": "a great progress for vramlets. Actual usable 1,2b model.\n\n  \nIt's crazy that we beat chat gpt4 with 1,2b model. Not only it is better but also can do ocr and other stuff.",
          "created_at": "2026-01-06T08:09:23Z",
          "was_summarised": false
        },
        {
          "id": "nxz3s3c",
          "author": "iqandjoke",
          "content": "How can I use it on Android? Which one should I use? Edge Gallery only support .litertlm format.",
          "created_at": "2026-01-06T08:27:17Z",
          "was_summarised": false
        },
        {
          "id": "ny0lroa",
          "author": "Ok-Internal9317",
          "content": "When Ollama?",
          "created_at": "2026-01-06T14:59:39Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/collections/LiquidAI/lfm25",
          "was_fetched": true,
          "page": "Title: üíß LFM2.5 - a LiquidAI Collection\n\nURL Source: https://huggingface.co/collections/LiquidAI/lfm25\n\nMarkdown Content:\nupdated about 24 hours ago\n\nCollection of Instruct, Base, and Japanese LFM2.5-1.2B models.\n\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct Text Generation ‚Ä¢ 1B‚Ä¢ Updated 1 day ago‚Ä¢ 1.24k ‚Ä¢ 90](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-GGUF Text Generation ‚Ä¢ 1B‚Ä¢ Updated 1 day ago‚Ä¢ 33 ‚Ä¢ 32](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-ONNX Text Generation ‚Ä¢ Updated about 11 hours ago‚Ä¢ 14](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-ONNX)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Base Text Generation ‚Ä¢ 1B‚Ä¢ Updated 1 day ago‚Ä¢ 3 ‚Ä¢ 43](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Base)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Base-GGUF Text Generation ‚Ä¢ 1B‚Ä¢ Updated 1 day ago‚Ä¢ 1 ‚Ä¢ 11](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Base-GGUF)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Base-ONNX Text Generation ‚Ä¢ Updated about 11 hours ago‚Ä¢ 11](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Base-ONNX)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP Text Generation ‚Ä¢ 1B‚Ä¢ Updated 1 day ago‚Ä¢ 7 ‚Ä¢ 35](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-GGUF Text Generation ‚Ä¢ 1B‚Ä¢ Updated 1 day ago‚Ä¢ 16 ‚Ä¢ 13](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-GGUF)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-ONNX Text Generation ‚Ä¢ Updated about 11 hours ago‚Ä¢ 1 ‚Ä¢ 11](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-ONNX)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-MLX-4bit Text Generation ‚Ä¢ 0.2B‚Ä¢ Updated about 24 hours ago‚Ä¢ 4 ‚Ä¢ 4](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-MLX-4bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-MLX-5bit Text Generation ‚Ä¢ 0.2B‚Ä¢ Updated about 24 hours ago‚Ä¢ 1 ‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-MLX-5bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-MLX-6bit Text Generation ‚Ä¢ 0.3B‚Ä¢ Updated about 24 hours ago‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-MLX-6bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-MLX-8bit Text Generation ‚Ä¢ 0.3B‚Ä¢ Updated about 24 hours ago‚Ä¢ 1 ‚Ä¢ 7](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-MLX-8bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-Instruct-MLX-bf16 Text Generation ‚Ä¢ 1B‚Ä¢ Updated about 24 hours ago‚Ä¢ 1 ‚Ä¢ 5](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-MLX-bf16)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-MLX-4bit Text Generation ‚Ä¢ 0.2B‚Ä¢ Updated about 24 hours ago‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-MLX-4bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-MLX-5bit Text Generation ‚Ä¢ 0.2B‚Ä¢ Updated about 24 hours ago‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-MLX-5bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-MLX-6bit Text Generation ‚Ä¢ 0.3B‚Ä¢ Updated about 24 hours ago‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-MLX-6bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-MLX-8bit Text Generation ‚Ä¢ 0.3B‚Ä¢ Updated about 24 hours ago‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-MLX-8bit)\n*   \n* * *\n\n[#### LiquidAI/LFM2.5-1.2B-JP-MLX-bf16 Text Generation ‚Ä¢ 1B‚Ä¢ Updated about 24 hours ago‚Ä¢ 2](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-MLX-bf16)",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/flk7mfltznbg1.jpeg",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:29.624058523Z"
    },
    {
      "flow_id": "",
      "id": "1q5gii4",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/",
      "title": "DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available",
      "content": "It runs on regular llama.cpp builds (no extra support for DeepSeek V3.2 is needed).\n\nOnly Q8\\_0 and Q4\\_K\\_M are available.\n\nUse DeepSeek V3.2 Exp jinja template saved to a file to run this model by passing options: `--jinja --chat-template-file ds32-exp.jinja`\n\nHere's the template I used in my tests: [https://pastebin.com/4cUXvv35](https://pastebin.com/4cUXvv35)\n\nNote that tool calls will most likely not work with this template - they are different between DS 3.2-Exp and DS 3.2.\n\nI ran [lineage-bench](https://github.com/fairydreaming/lineage-bench) on Q4\\_K\\_M quant deployed in llama-server (40 prompts per each difficulty level), results:\n\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | deepseek/deepseek-v3.2 |     0.988 |       1.000 |        1.000 |         1.000 |         0.950 |\n\nThe model got only 2 answers wrong with most difficult graph size (192). It looks like it performed even a bit better than the original DeepSeek V3.2 with sparse attention tested via API:\n\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | deepseek/deepseek-v3.2 |     0.956 |       1.000 |        1.000 |         0.975 |         0.850 |\n\nFrom my testing so far disabling sparse attention does not hurt the model intelligence.\n\nEnjoy!\n\nEdit: **s/lightning attention/lightning indexer/**",
      "author": "fairydreaming",
      "created_at": "2026-01-06T11:50:35Z",
      "comments": [
        {
          "id": "ny0euil",
          "author": "woahdudee2a",
          "content": "what's the generation speed like? compared to original v3",
          "created_at": "2026-01-06T14:23:36Z",
          "was_summarised": false
        },
        {
          "id": "ny11vff",
          "author": "shark8866",
          "content": "if dense attention doesn't perform better, then what is the point of using it?",
          "created_at": "2026-01-06T16:15:34Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/sszymczyk/DeepSeek-V3.2-nolight-GGUF",
          "was_fetched": true,
          "page": "Title: sszymczyk/DeepSeek-V3.2-nolight-GGUF ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/sszymczyk/DeepSeek-V3.2-nolight-GGUF\n\nMarkdown Content:\nThis repo contains Q8_0 and Q4_K_M quants of DeepSeek V3.2 with removed sparse attention lightning indexer tensors. This allows to run the model in llama.cpp until the sparse attention is implemented.\n\nI found no degradation in the model \"intelligence\" after removing lightning indexer. Q4_K_M quant was tested in lineage-bench ([https://github.com/fairydreaming/lineage-bench](https://github.com/fairydreaming/lineage-bench)):\n\n| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | deepseek/deepseek-v3.2 | 0.988 | 1.000 | 1.000 | 1.000 | 0.950 |\n\nThe model solved almost all quizzes correctly (40 quizzes per each difficulty level, 160 overall). It made only 2 errors in lineage graphs of 192 nodes (most difficult quizzes). Here's the score of the original DeepSeek V3.2 model tested via OpenRouter (with deepseek provider)\n\n| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | deepseek/deepseek-v3.2 | 0.956 | 1.000 | 1.000 | 0.975 | 0.850 |\n\nTo use the model save DeepSeek V3.2-Exp chat template to a file (you can save it from [https://pastebin.com/4cUXvv35](https://pastebin.com/4cUXvv35)) and pass `--jinja --chat-template-file \u003csaved-chat-template-file\u003e` when running llama-cli or llama-server.\n\nNote that tool calls will likely not work correctly with this template.",
          "was_summarised": false
        },
        {
          "url": "https://pastebin.com/4cUXvv35",
          "was_fetched": true,
          "page": "Title: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = fals - Pastebin.com\n\nURL Source: https://pastebin.com/4cUXvv35\n\nMarkdown Content:\nUntitled\n--------\n\na guest\n\nJan 6th, 2026\n\n62\n\n0\n\nNever\n\n**Not a member of Pastebin yet?**[**Sign Up**](https://pastebin.com/signup), it unlocks many cool features!\n\n1.   {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{% set ns.is_only_sys = true %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'\u003cÔΩúUserÔΩú\u003e' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- if ns.is_last_user or ns.is_only_sys %}{{'\u003cÔΩúAssistantÔΩú\u003e\u003c/think\u003e'}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'\u003cÔΩútool‚ñÅcalls‚ñÅbeginÔΩú\u003e\u003cÔΩútool‚ñÅcall‚ñÅbeginÔΩú\u003e'+ tool['function']['name'] + '\u003cÔΩútool‚ñÅsepÔΩú\u003e' + tool['function']['arguments'] + '\u003cÔΩútool‚ñÅcall‚ñÅendÔΩú\u003e'}}{%- else %}{{message['content'] + '\u003cÔΩútool‚ñÅcalls‚ñÅbeginÔΩú\u003e\u003cÔΩútool‚ñÅcall‚ñÅbeginÔΩú\u003e' + tool['function']['name'] + '\u003cÔΩútool‚ñÅsepÔΩú\u003e' + tool['function']['arguments'] + '\u003cÔΩútool‚ñÅcall‚ñÅendÔΩú\u003e'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\u003cÔΩútool‚ñÅcall‚ñÅbeginÔΩú\u003e'+ tool['function']['name'] + '\u003cÔΩútool‚ñÅsepÔΩú\u003e' + tool['function']['arguments'] + '\u003cÔΩútool‚ñÅcall‚ñÅendÔΩú\u003e'}}{%- endif %}{%- endfor %}{{'\u003cÔΩútool‚ñÅcalls‚ñÅendÔΩú\u003e\u003cÔΩúend‚ñÅof‚ñÅsentenceÔΩú\u003e'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- if ns.is_last_user %}{{'\u003cÔΩúAssistantÔΩú\u003e'}}{%- if message['prefix'] is defined and message['prefix'] and thinking %}{{'\u003cthink\u003e'}}{%- else %}{{'\u003c/think\u003e'}}{%- endif %}{%- endif %}{%- if message['prefix'] is defined and message['prefix'] %}{%- set ns.is_prefix = true -%}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message['content'] + '\u003cÔΩúend‚ñÅof‚ñÅsentenceÔΩú\u003e'}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message['content'] -%}{%- if '\u003c/think\u003e' in content %}{%- set content = content.split('\u003c/think\u003e', 1)[1] -%}{%- endif %}{{content + '\u003cÔΩúend‚ñÅof‚ñÅsentenceÔΩú\u003e'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{'\u003cÔΩútool‚ñÅoutput‚ñÅbeginÔΩú\u003e' + message['content'] + '\u003cÔΩútool‚ñÅoutput‚ñÅendÔΩú\u003e'}}{%- endif %}{%- if message['role'] != 'system' %}{% set ns.is_only_sys = false %}{%- endif %}{%- endfor -%}{% if add_generation_prompt and not ns.is_tool%}{% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}{{'\u003cÔΩúAssistantÔΩú\u003e'}}{%- if not thinking %}{{'\u003c/think\u003e'}}{%- else %}{{'\u003cthink\u003e'}}{%- endif %}{% endif %}{% endif %}",
          "was_summarised": false
        },
        {
          "url": "https://github.com/fairydreaming/lineage-bench",
          "was_fetched": true,
          "page": "Title: GitHub - fairydreaming/lineage-bench: Testing LLM reasoning abilities with lineage relationship quizzes.\n\nURL Source: https://github.com/fairydreaming/lineage-bench\n\nMarkdown Content:\nGitHub - fairydreaming/lineage-bench: Testing LLM reasoning abilities with lineage relationship quizzes.\n===============\n\n[Skip to content](https://github.com/fairydreaming/lineage-bench#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ffairydreaming%2Flineage-bench)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ffairydreaming%2Flineage-bench)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=fairydreaming%2Flineage-bench)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/fairydreaming/lineage-bench) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/fairydreaming/lineage-bench) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/fairydreaming/lineage-bench) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[fairydreaming](https://github.com/fairydreaming)/**[lineage-bench](https://github.com/fairydreaming/lineage-bench)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)You must be signed in to change notification settings\n*   [Fork 8](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)\n*   [Star 35](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench) \n\nTesting LLM reasoning abilities with lineage relationship quizzes.\n\n### License\n\n[MIT license](https://github.com/fairydreaming/lineage-bench/blob/main/LICENSE)\n\n[35 stars](https://github.com/fairydreaming/lineage-bench/stargazers)[8 forks](https://github.com/fairydreaming/lineage-bench/forks)[Branches](https://github.com/fairydreaming/lineage-bench/branches)[Tags](https://github.com/fairydreaming/lineage-bench/tags)[Activity](https://github.com/fairydreaming/lineage-bench/activity)\n\n[Star](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)\n\n[Notifications](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)You must be signed in to change notification settings\n\n*   [Code](https://github.com/fairydreaming/lineage-bench)\n*   [Issues 0](https://github.com/fairydreaming/lineage-bench/issues)\n*   [Pull requests 0](https://github.com/fairydreaming/lineage-bench/pulls)\n*   [Actions](https://github.com/fairydreaming/lineage-bench/actions)\n*   [Projects 0](https://github.com/fairydreaming/lineage-bench/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/fairydreaming/lineage-bench).](https://github.com/fairydreaming/lineage-bench/security)\n*   [Insights](https://github.com/fairydreaming/lineage-bench/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/fairydreaming/lineage-bench)\n*   [Issues](https://github.com/fairydreaming/lineage-bench/issues)\n*   [Pull requests](https://github.com/fairydreaming/lineage-bench/pulls)\n*   [Actions](https://github.com/fairydreaming/lineage-bench/actions)\n*   [Projects](https://github.com/fairydreaming/lineage-bench/projects)\n*   [Security](https://github.com/fairydreaming/lineage-bench/security)\n*   [Insights](https://github.com/fairydreaming/lineage-bench/pulse)\n\nfairydreaming/lineage-bench\n===========================\n\nmain\n\n[Branches](https://github.com/fairydreaming/lineage-bench/branches)[Tags](https://github.com/fairydreaming/lineage-bench/tags)\n\n[](https://github.com/fairydreaming/lineage-bench/branches)[](https://github.com/fairydreaming/lineage-bench/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [93 Commits](https://github.com/fairydreaming/lineage-bench/commits/main/) [](https://github.com/fairydreaming/lineage-bench/commits/main/) |\n| [results](https://github.com/fairydreaming/lineage-bench/tree/main/results \"results\") | [results](https://github.com/fairydreaming/lineage-bench/tree/main/results \"results\") |  |  |\n| [LICENSE](https://github.com/fairydreaming/lineage-bench/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/fairydreaming/lineage-bench/blob/main/LICENSE \"LICENSE\") |  |  |\n| [README.md](https://github.com/fairydreaming/lineage-bench/blob/main/README.md \"README.md\") | [README.md](https://github.com/fairydreaming/lineage-bench/blob/main/README.md \"README.md\") |  |  |\n| [compute_metrics.py](https://github.com/fairydreaming/lineage-bench/blob/main/compute_metrics.py \"compute_metrics.py\") | [compute_metrics.py](https://github.com/fairydreaming/lineage-bench/blob/main/compute_metrics.py \"compute_metrics.py\") |  |  |\n| [lineage_bench.py](https://github.com/fairydreaming/lineage-bench/blob/main/lineage_bench.py \"lineage_bench.py\") | [lineage_bench.py](https://github.com/fairydreaming/lineage-bench/blob/main/lineage_bench.py \"lineage_bench.py\") |  |  |\n| [plot_line.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_line.py \"plot_line.py\") | [plot_line.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_line.py \"plot_line.py\") |  |  |\n| [plot_stacked.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_stacked.py \"plot_stacked.py\") | [plot_stacked.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_stacked.py \"plot_stacked.py\") |  |  |\n| [requirements.txt](https://github.com/fairydreaming/lineage-bench/blob/main/requirements.txt \"requirements.txt\") | [requirements.txt](https://github.com/fairydreaming/lineage-bench/blob/main/requirements.txt \"requirements.txt\") |  |  |\n| [run_openrouter.py](https://github.com/fairydreaming/lineage-bench/blob/main/run_openrouter.py \"run_openrouter.py\") | [run_openrouter.py](https://github.com/fairydreaming/lineage-bench/blob/main/run_openrouter.py \"run_openrouter.py\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/fairydreaming/lineage-bench#)\n*   [MIT license](https://github.com/fairydreaming/lineage-bench#)\n\nlineage-bench\n=============\n\n[](https://github.com/fairydreaming/lineage-bench#lineage-bench)\n\nTesting LLM reasoning abilities with lineage relationship quizzes.\n\nThe project is a successor of the [farel-bench](https://github.com/fairydreaming/farel-bench) benchmark.\n\n**Note: GPT 5.2 (reasoning effort medium and high) seems to perform significantly worse in my benchmark compared to its predecessor.**\n\nChangelog\n---------\n\n[](https://github.com/fairydreaming/lineage-bench#changelog)\n\n*   2025-12-18 - Added results for some legacy models (gemini-2.5-flash, gemini-2.5-pro), for high reasoning effort (gpt-5.1, gpt-5.2) and other recently released models (nemotron-3-nano-30b-a3b, doubao-seed-1-8, gemini-3-flash-preview, mimo-v2-flash, ministral-14b-2512). Stacked results plot shows only top 30 scores now.\n*   2025-12-03 - Updated results for ring-1t model. Added results for seed-oss-36b-instruct (courtesy of [@mokieli](https://github.com/mokieli)).\n*   2025-12-01 - Added results for ring-1t, deepseek-r1-0528, glm-4.5-air, glm-4.5, intellect-3, ernie-5.0-thinking-preview, deepseek-v3.2 and deepseek-v3.2-speciale. Updated results for glm-4.6 (works better with lower temperature). Results for ring-1t are not final (problems with model provider).\n*   2025-11-25 - Added results for gpt-5.1, claude-opus-4.5, grok-4.1-fast and o4-mini.\n*   2025-11-23 - Added results for qwen3-32b, o3-mini and o3 models.\n*   2025-11-22 - Updated results to include recently released models, but only with 40 quizzes per problem size to reduce costs. Extended range of problem lengths to increase difficulty. Added file-based caching of model requests and responses.\n*   2025-03-07 - Added results for qwq-32b (used Parasail provider with 0.01 temp, observed some infinite loop generations, but mostly for lineage-64 where the model performs bad anyway).\n*   2025-03-04 - Updated results for perplexity/r1-1776. (apparently there was a problem with the model serving stack, that's why r1-1776 initially performed worse than expected)\n*   2025-02-26 - Added results for claude-3.7-sonnet (also with :thinking) and r1-1776\n*   2025-02-20 - Updated results for deepseek/deepseek-r1-distill-llama-70b. (used Groq provider with 0.5 temperature)\n*   2025-02-18 - Added results for kimi-k1.5-preview and llama-3.1-tulu-3-405b.\n*   2025-02-06 - Added results for o1, o3-mini, qwen-max, gemini-exp-1206, deepseek-r1-distill-qwen-14b and deepseek-r1-distill-qwen-32b.\n*   2025-01-24 - Added results for deepseek-r1-distill-llama-70b.\n*   2025-01-20 - Added results for deepseek-r1.\n*   2025-01-15 - Added results for deepseek-v3, gemini-2.0-flash-exp, gemini-2.0-flash-thinking-exp-1219 and minimax-01.\n\nResults\n-------\n\n[](https://github.com/fairydreaming/lineage-bench#results)\n\n### Plot\n\n[](https://github.com/fairydreaming/lineage-bench#plot)\n\n#### Current results\n\n[](https://github.com/fairydreaming/lineage-bench#current-results)\n\nThe plot below shows only the 30 top-performing models. See the table for all results.\n\n[](https://private-user-images.githubusercontent.com/166155368/528251497-571f2dcb-534c-446d-ba82-e307265607c7.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc3NTQ5NjMsIm5iZiI6MTc2Nzc1NDY2MywicGF0aCI6Ii8xNjYxNTUzNjgvNTI4MjUxNDk3LTU3MWYyZGNiLTUzNGMtNDQ2ZC1iYTgyLWUzMDcyNjU2MDdjNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwN1QwMjU3NDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kN2RmNTIzNGYwZDdlZWUwODhlODQ0OGYwODk4MGE1Yzk1MTYzNTM2ZDk4N2I0MjRiNzVmZmI4MzcwNjhiYTJiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.AEVas-O-38tCgqf1Igfuud6kjiksrRG5I3_GMMz8k7Q)\n\n#### Old results\n\n[](https://github.com/fairydreaming/lineage-bench#old-results)\n\n[](https://private-user-images.githubusercontent.com/166155368/420495024-559e686c-ce1e-4c9d-851e-1d9e2eb6f6b1.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc3NTQ5NjMsIm5iZiI6MTc2Nzc1NDY2MywicGF0aCI6Ii8xNjYxNTUzNjgvNDIwNDk1MDI0LTU1OWU2ODZjLWNlMWUtNGM5ZC04NTFlLTFkOWUyZWI2ZjZiMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwN1QwMjU3NDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02YzljOTVkZGExN2EzMDY3Mzk3NjZiM2U5MGZlZmU5ZDY0ODgzOGNiYmZlZTk3N2U0NjUwMWNkMTZmNWQ3ZjJlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.bDuaZWco2KTUAKmuZmkZZnMK1KHH2AlC04jV0m9sDnY)\n\n### Table\n\n[](https://github.com/fairydreaming/lineage-bench#table)\n\nThe table below presents the benchmark results. If not explicitly stated default medium reasoning effort was used during benchmark.\n\n| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 |\n| ---: | :--- | ---: | ---: | ---: | ---: | ---: |\n| 1 | deepseek/deepseek-v3.2-speciale | 0.994 | 1.000 | 1.000 | 1.000 | 0.975 |\n| 2 | openai/gpt-5.1 (high) | 0.969 | 1.000 | 0.975 | 0.975 | 0.925 |\n| 2 | google/gemini-3-pro-preview | 0.969 | 1.000 | 1.000 | 0.925 | 0.950 |\n| 4 | deepseek/deepseek-v3.2 | 0.956 | 1.000 | 1.000 | 0.975 | 0.850 |\n| 5 | anthropic/claude-sonnet-4.5 | 0.944 | 0.975 | 0.975 | 0.900 | 0.925 |\n| 6 | google/gemini-2.5-pro | 0.925 | 1.000 | 0.900 | 0.900 | 0.900 |\n| 7 | openai/gpt-5.1 (medium) | 0.888 | 1.000 | 0.950 | 0.875 | 0.725 |\n| 8 | google/gemini-3-flash-preview | 0.881 | 1.000 | 0.975 | 0.875 | 0.675 |\n| 9 | qwen/qwen3-max | 0.869 | 1.000 | 0.800 | 0.900 | 0.775 |\n| 10 | x-ai/grok-4 (medium) | 0.869 | 1.000 | 0.950 | 0.900 | 0.625 |\n| 10 | x-ai/grok-4-fast (medium) | 0.869 | 1.000 | 0.925 | 0.900 | 0.650 |\n| 10 | anthropic/claude-opus-4.5 (medium) | 0.869 | 1.000 | 0.950 | 0.900 | 0.625 |\n| 13 | qwen/qwen3-235b-a22b-thinking-2507 | 0.856 | 0.900 | 0.875 | 0.850 | 0.800 |\n| 14 | inclusionai/ring-1t | 0.819 | 0.875 | 0.975 | 0.800 | 0.625 |\n| 15 | deepseek/deepseek-v3.1-terminus | 0.812 | 0.975 | 0.900 | 0.700 | 0.675 |\n| 16 | openai/o3 (medium) | 0.800 | 1.000 | 0.925 | 0.800 | 0.475 |\n| 17 | deepseek/deepseek-v3.2-exp | 0.794 | 0.975 | 0.900 | 0.700 | 0.600 |\n| 18 | anthropic/claude-haiku-4.5 | 0.794 | 0.975 | 0.925 | 0.575 | 0.700 |\n| 19 | openai/gpt-5 (medium) | 0.788 | 1.000 | 0.975 | 0.850 | 0.325 |\n| 20 | deepseek/deepseek-r1-0528 | 0.787 | 1.000 | 0.975 | 0.650 | 0.525 |\n| 21 | bytedance/seed-oss-36b-instruct | 0.769 | 1.000 | 0.850 | 0.750 | 0.475 |\n| 22 | deepcogito/cogito-v2.1-671b | 0.756 | 0.975 | 0.800 | 0.650 | 0.600 |\n| 23 | x-ai/grok-4.1-fast (medium) | 0.750 | 1.000 | 0.900 | 0.800 | 0.300 |\n| 24 | baidu/ernie-5.0-thinking-preview | 0.719 | 1.000 | 0.850 | 0.650 | 0.375 |\n| 25 | z-ai/glm-4.5 | 0.700 | 1.000 | 0.775 | 0.625 | 0.400 |\n| 26 | z-ai/glm-4.6 | 0.644 | 0.925 | 0.725 | 0.525 | 0.400 |\n| 27 | xiaomi/mimo-v2-flash | 0.600 | 1.000 | 0.900 | 0.425 | 0.075 |\n| 28 | z-ai/glm-4.5-air | 0.594 | 1.000 | 0.750 | 0.450 | 0.175 |\n| 28 | prime-intellect/intellect-3 | 0.594 | 1.000 | 0.950 | 0.325 | 0.100 |\n| 30 | qwen/qwen3-next-80b-a3b-thinking | 0.575 | 0.950 | 0.700 | 0.425 | 0.225 |\n| 31 | google/gemini-2.5-flash | 0.569 | 0.975 | 0.575 | 0.525 | 0.200 |\n| 32 | minimax/minimax-m2 | 0.562 | 0.975 | 0.700 | 0.350 | 0.225 |\n| 33 | openai/gpt-oss-120b | 0.544 | 1.000 | 0.825 | 0.325 | 0.025 |\n| 34 | amazon/nova-2-lite-v1 | 0.525 | 1.000 | 0.700 | 0.325 | 0.075 |\n| 34 | openai/o4-mini (medium) | 0.525 | 1.000 | 0.775 | 0.300 | 0.025 |\n| 34 | moonshotai/kimi-k2-thinking | 0.525 | 1.000 | 0.850 | 0.200 | 0.050 |\n| 37 | volcengine/doubao-seed-1.8 | 0.512 | 1.000 | 0.925 | 0.125 | 0.000 |\n| 37 | openai/gpt-5-mini (medium) | 0.512 | 1.000 | 0.950 | 0.075 | 0.025 |\n| 39 | qwen/qwen3-30b-a3b-thinking-2507 | 0.494 | 1.000 | 0.575 | 0.275 | 0.125 |\n| 39 | openai/gpt-5.2 (high) | 0.494 | 1.000 | 0.700 | 0.175 | 0.100 |\n| 41 | openai/gpt-5.2 (medium) | 0.450 | 1.000 | 0.675 | 0.075 | 0.050 |\n| 42 | allenai/olmo-3-32b-think | 0.444 | 0.925 | 0.600 | 0.175 | 0.075 |\n| 43 | mistralai/ministral-14b-2512 | 0.400 | 0.875 | 0.425 | 0.175 | 0.125 |\n| 44 | qwen/qwen3-32b | 0.362 | 0.950 | 0.475 | 0.025 | 0.000 |\n| 45 | openai/gpt-5-nano (medium) | 0.294 | 1.000 | 0.150 | 0.025 | 0.000 |\n| 46 | openai/o3-mini (medium) | 0.287 | 0.950 | 0.200 | 0.000 | 0.000 |\n| 47 | nvidia/nemotron-3-nano-30b-a3b | 0.231 | 0.875 | 0.025 | 0.025 | 0.000 |\n\nEach row contains the average benchmark score across all problem sizes, and separate scores for each problem size.\n\nDescription\n-----------\n\n[](https://github.com/fairydreaming/lineage-bench#description)\n\nThe purpose of this project is to test LLM reasoning abilities with lineage relationship quizzes.\n\nThe general idea is to make LLM reason about a graph of lineage relationships where nodes are people and edges are ancestor/descendant relations between people. LLM is asked to determine the lineage relationship between two people A and B based on the graph. By varying the number of graph nodes (problem size) we can control the quiz difficulty.\n\nThere are five possible answers in each quiz:\n\n1.   A is B's ancestor\n2.   A is B's descendant\n3.   A and B share a common ancestor\n4.   A and B share a common descendant\n5.   None of the above is correct.\n\nThe last answer is never correct. It serves only as an invalid fallback answer.\n\nExamples\n--------\n\n[](https://github.com/fairydreaming/lineage-bench#examples)\n\nBelow you can see some example lineage relationship graphs and corresponding quizzes.\n\n[](https://camo.githubusercontent.com/b17753f69252f0b7b12c78480043de56a46c85a198030d03723b2c09b8ecf3cf/68747470733a2f2f692e706f7374696d672e63632f50783756535a524c2f6c696e656167652d62656e63682e706e67)\n\n### Ancestor\n\n[](https://github.com/fairydreaming/lineage-bench#ancestor)\n\n```\nGiven the following lineage relationships:\n* Joseph is George's ancestor.\n* Henry is George's descendant.\n* Thomas is Joseph's ancestor.\nDetermine the lineage relationship between Thomas and Henry.\nSelect the correct answer:\n1. Thomas is Henry's ancestor.\n2. Thomas is Henry's descendant.\n3. Thomas and Henry share a common ancestor.\n4. Thomas and Henry share a common descendant.\n5. None of the above is correct.\nEnclose the selected answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.\n```\n\n### Common ancestor\n\n[](https://github.com/fairydreaming/lineage-bench#common-ancestor)\n\n```\nGiven the following lineage relationships:\n* Matthew is Heather's ancestor.\n* Heather is Melissa's ancestor.\n* Matthew is Mark's ancestor.\nDetermine the lineage relationship between Mark and Melissa.\nSelect the correct answer:\n1. Mark and Melissa share a common ancestor.\n2. Mark is Melissa's ancestor.\n3. Mark and Melissa share a common descendant.\n4. Mark is Melissa's descendant.\n5. None of the above is correct.\nEnclose the selected answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.\n```\n\n### Common descendant\n\n[](https://github.com/fairydreaming/lineage-bench#common-descendant)\n\n```\nGiven the following lineage relationships:\n* Madison is Kathleen's descendant.\n* Judith is Madison's ancestor.\n* Harold is Kathleen's ancestor.\nDetermine the lineage relationship between Harold and Judith.\nSelect the correct answer:\n1. Harold and Judith share a common descendant.\n2. Harold and Judith share a common ancestor.\n3. Harold is Judith's ancestor.\n4. Harold is Judith's descendant.\n5. None of the above is correct.\nEnclose the selected answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.\n```\n\nUsage\n-----\n\n[](https://github.com/fairydreaming/lineage-bench#usage)\n\nThe usual workflow is to:\n\n1.   Run lineage_bench.py to generate lineage relationship quizzes.\n2.   Run run_openrouter.py to test LLM models.\n3.   Run compute_metrics.py to calculate benchmark results.\n4.   Run plot_stacked.py to generate a results plot.\n\nOutput is usually written to the standard output. Input is usually read from the standard input.\n\nExample usage:\n\n```\n$ ./lineage_bench.py -s -l 8 -n 10 -r 42|./run_openrouter.py -m \"google/gemini-pro-1.5\" -t 8 -r -o results/gemini-pro-1.5 -v|tee results/gemini-pro-1.5_8.csv\n$ cat results/*.csv|./compute_metrics.py --csv --relaxed|./plot_stacked.py -o results.png\n```\n\nI usually run the benchmark like this:\n\n```\nfor length in 8 16 32 64\ndo\n  ./lineage_bench.py -s -l $length -n 50 -r 42|./run_openrouter.py -m \u003cmodel\u003e -p \u003cprovider\u003e -o \u003ccache_dir\u003e -r -v|tee results/\u003cmodel\u003e_$length.csv\ndone\n```\n\nThis results in 200 generated quizzes per problem size, 800 quizzes overall in a single benchmark run.\n\n### lineage_bench.py\n\n[](https://github.com/fairydreaming/lineage-bench#lineage_benchpy)\n\n```\nusage: lineage_bench.py [-h] -l LENGTH [-p PROMPT] [-s] [-n NUMBER] [-r SEED]\n\noptions:\n  -h, --help            show this help message and exit\n  -l LENGTH, --length LENGTH\n                        Number of people connected with lineage relationships in the quiz.\n  -p PROMPT, --prompt PROMPT\n                        Prompt template of the quiz. The default prompt template is: 'Given the following lineage\n                        relationships:\\n{quiz_relations}\\n{quiz_question}\\nSelect the correct answer:\\n{quiz_answers}\\nEnclose the selected\n                        answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.'\n  -s, --shuffle         Shuffle the order of lineage relations in the quiz.\n  -n NUMBER, --number NUMBER\n                        Number of quizzes generated for each valid answer option.\n  -r SEED, --seed SEED  Random seed value\n```\n\n### run_openrouter.py\n\n[](https://github.com/fairydreaming/lineage-bench#run_openrouterpy)\n\nBefore running `run_openrouter.py` set OPENROUTER_API_KEY environment variable to your OpenRouter API Key.\n\n```\nusage: run_openrouter.py [-h] [-u URL] -m MODEL -o OUTPUT [-p PROVIDER] [-r] [-e EFFORT] [-t THREADS] [-v] [-s [SYSTEM_PROMPT]] [-T TEMP]\n                         [-P TOP_P] [-K TOP_K] [-n MAX_TOKENS] [-i RETRIES]\n\noptions:\n  -h, --help            show this help message and exit\n  -u URL, --url URL     OpenAI-compatible API URL\n  -m MODEL, --model MODEL\n                        OpenRouter model name.\n  -o OUTPUT, --output OUTPUT\n                        Directory for storing model responses.\n  -p PROVIDER, --provider PROVIDER\n                        OpenRouter provider name.\n  -r, --reasoning       Enable reasoning.\n  -e EFFORT, --effort EFFORT\n                        Reasoning effort (recent OpenAI and xAI models support this).\n  -t THREADS, --threads THREADS\n                        Number of threads to use.\n  -v, --verbose         Enable verbose output.\n  -s [SYSTEM_PROMPT], --system-prompt [SYSTEM_PROMPT]\n                        Use given system prompt. By default, the system prompt is not used. When this option is passed without a value, the\n                        default system prompt value is used: 'You are a master of logical thinking. You carefully analyze the premises step by\n                        step, take detailed notes and draw intermediate conclusions based on which you can find the final answer to any\n                        question.'\n  -T TEMP, --temp TEMP  Temperature value to use.\n  -P TOP_P, --top-p TOP_P\n                        top_p sampling parameter.\n  -K TOP_K, --top-k TOP_K\n                        top_k sampling parameter.\n  -n MAX_TOKENS, --max-tokens MAX_TOKENS\n                        Max number of tokens to generate.\n  -i RETRIES, --retries RETRIES\n                        Max number of API request retries.\n```\n\n### compute_metrics.py\n\n[](https://github.com/fairydreaming/lineage-bench#compute_metricspy)\n\n```\nusage: compute_metrics.py [-h] [-c] [-r] [-d]\n\noptions:\n  -h, --help      show this help message and exit\n  -c, --csv       Generate CSV output.\n  -r, --relaxed   Relaxed answer format requirements\n  -d, --detailed  Generate detailed output\n```\n\n### plot_line.py\n\n[](https://github.com/fairydreaming/lineage-bench#plot_linepy)\n\n```\nusage: plot_line.py [-h] [-o OUTPUT] [-n TOP_N]\n\noptions:\n  -h, --help           show this help message and exit\n  -o, --output OUTPUT  Write rendered plot to this file.\n  -n, --top-n TOP_N    Show only n best results.\n```\n\n### plot_stacked.py\n\n[](https://github.com/fairydreaming/lineage-bench#plot_stackedpy)\n\n```\nusage: plot_stacked.py [-h] [-o OUTPUT] [-n TOP_N]\n\noptions:\n  -h, --help           show this help message and exit\n  -o, --output OUTPUT  Write rendered plot to this file.\n  -n, --top-n TOP_N    Show only n best results.\n```\n\nAbout\n-----\n\nTesting LLM reasoning abilities with lineage relationship quizzes.\n\n### Resources\n\n[Readme](https://github.com/fairydreaming/lineage-bench#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/fairydreaming/lineage-bench#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/fairydreaming/lineage-bench).\n\n[Activity](https://github.com/fairydreaming/lineage-bench/activity)\n\n### Stars\n\n[**35** stars](https://github.com/fairydreaming/lineage-bench/stargazers)\n\n### Watchers\n\n[**3** watching](https://github.com/fairydreaming/lineage-bench/watchers)\n\n### Forks\n\n[**8** forks](https://github.com/fairydreaming/lineage-bench/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Ffairydreaming%2Flineage-bench\u0026report=fairydreaming+%28user%29)\n\n[Releases](https://github.com/fairydreaming/lineage-bench/releases)\n-------------------------------------------------------------------\n\nNo releases published\n\n[Packages 0](https://github.com/users/fairydreaming/packages?repo_name=lineage-bench)\n-------------------------------------------------------------------------------------\n\n No packages published \n\n[Contributors 3](https://github.com/fairydreaming/lineage-bench/graphs/contributors)\n------------------------------------------------------------------------------------\n\n*   [](https://github.com/sszymczy)[**sszymczy**](https://github.com/sszymczy)\n*   [](https://github.com/fairydreaming)[**fairydreaming**](https://github.com/fairydreaming)\n*   [](https://github.com/mokieli)[**mokieli**mokieli](https://github.com/mokieli)\n\nLanguages\n---------\n\n*   [Python 100.0%](https://github.com/fairydreaming/lineage-bench/search?l=python)\n\nFooter\n------\n\n[](https://github.com/) ¬© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You can‚Äôt perform that action at this time.",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:51.245163155Z"
    },
    {
      "flow_id": "",
      "id": "1q5gklh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/",
      "title": "So I've been losing my mind over document extraction in insurance for the past few years and I finally figured out what the right approach is.",
      "content": "I've been doing document extraction for insurance for a while now and honestly I almost gave up on it completely last year. Spent months fighting with accuracy issues that made no sense until I figured out what I was doing wrong.\n\neveryone's using llms or tools like LlamaParse for extraction and they work fine but then you put them in an actual production env and accuracy just falls off a cliff after a few weeks. I kept thinking I picked the wrong tools or tried to brute force my way through (Like any distinguished engineer would do XD) but it turned out to be way simpler and way more annoying.\n\nSo if you ever worked in an information extraction project you already know that most documents have literally zero consistency. I don't mean like \"oh the formatting is slightly different\" , I mean every single document is structured completely differently than all the others.\n\nFor example in my case : a workers comp FROI from California puts the injury date in a specific box at the top. Texas puts it in a table halfway down. New York embeds it in a paragraph. Then you get medical bills where one provider uses line items, another uses narrative format, another has this weird hybrid table thing. And that's before you even get to the faxed-sideways handwritten nightmares that somehow still exist in 2026???\n\nSadly llms  have no concept of document structure. So when you ask about details in a doc  it might pull from the right field, or from some random sentence, or just make something up. \n\nAfter a lot of headaches and honestly almost giving up completely, I came across a process that might save you some pain, so I thought I'd share it:\n\n1. Stop throwing documents at your extraction model blind. Build a classifier that figures out document type first (FROI vs medical bill vs correspondence vs whatever). Then route to type specific extraction. This alone fixed like 60% of my accuracy problems. (Really This is the golden tip ... a lot of people under estimate classification)\n\n2.  Don't just extract and hope. Get confidence scores for each field. \"I'm 96% sure this is the injury date, 58% sure on this wage calc\" Auto-process anything above 90%, flag the rest. This is how you actually scale without hiring people to validate everything AI does.\n\n3. Layout matters more than you think. Vision-language models that actually see the document structure perform way better than text only approaches. I switched to Qwen2.5-VL and it was night and day.\n\n4. Fine-tune on your actual documents. Generic models choke on industry-specific stuff. Fine-tuning with LoRA takes like 3 hours now and accuracy jumps 15-20%. Worth it every time.\n\n5. When a human corrects an extraction, feed that back into training. Your model should get better over time. (This will save you the struggle of having to recreate your process from scratch each time)\n\nWrote a little blog with more details about this implementation if anyone wants it \"I know... Shameless self promotion). ( link in comments)  \n  \nAnyway this is all the stuff I wish someone had told me when I was starting. Happy to share or just answer questions if you're stuck on this problem. Took me way too long to figure this out.\n\n",
      "author": "GloomyEquipment2120",
      "created_at": "2026-01-06T11:53:43Z",
      "comments": [
        {
          "id": "ny02dfn",
          "author": "avatarOfIndifference",
          "content": "It‚Äôs funny these approaches are decades old with traditional OCR ML tools‚Ä¶brainware, Kofax, opentext, Hyland etc‚Ä¶new wave of llm users figuring out how to reinvent the wheel again. When accuracy needs to be 100% on certain fields the challenge becomes creating UX that maximizes validator throughout. The accuracy trap is not the value proposition‚Ä¶how much you can process with the fewest amount of people is the actual measurable value prop when 100% accuracy is required (financial fields)",
          "created_at": "2026-01-06T13:13:51Z",
          "was_summarised": false
        },
        {
          "id": "nxzr4qw",
          "author": "GloomyEquipment2120",
          "content": "Link to write up : [https://kudra.ai/how-agentic-document-intelligence-transformed-workers-compensation-claims-processing-for-insurance-companies/](https://kudra.ai/how-agentic-document-intelligence-transformed-workers-compensation-claims-processing-for-insurance-companies/)",
          "created_at": "2026-01-06T11:56:54Z",
          "urls": [
            {
              "url": "https://kudra.ai/how-agentic-document-intelligence-transformed-workers-compensation-claims-processing-for-insurance-companies/",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxzxnqi",
          "author": "Awkward-Hedgehog-572",
          "content": "Looks solid, gj. I got a few questions if you don't mind answering them.\n\n4. I checked your blog but I didn't find this information. How exactly did you annotate and prepare the training data? How many examples did it take? Did you train it on clients data? Did you use the help of llms/vlms to prepare training data?\n\n5. How is this done?\n\n  \nThx",
          "created_at": "2026-01-06T12:43:49Z",
          "was_summarised": false
        },
        {
          "id": "ny0eg11",
          "author": "CuriouslyCultured",
          "content": "Getting AI to emit confidence scores is pretty good, you can even extend this trick and get AI to emit full Bayesian priors. I do this a lot when converting unstructured data to structured.",
          "created_at": "2026-01-06T14:21:27Z",
          "was_summarised": false
        },
        {
          "id": "ny00f6v",
          "author": "fabkosta",
          "content": "\u0026gt;Anyway this is all the stuff I wish someone had told me when I was starting.\n\nI would have told you, if you had purchased this as a service from my consulting company. (We've been building text extraction systems for insurance since 2015. I know exactly what sort of pain you are dealing with.)\n\nBut jokes aside, these are great insights you are sharing here. It is incredibly difficult to deal with the variety of docs you end up getting.\n\nIn case your docs have a common structure, run a trained classifier first, then apply a custom templatized extraction algo (Azure Document Intelligence is your friend). If you do not have a common structure, then at least use something that uses visual cues (like Docling does) to first extract content, followed by \"repairing\" by use of a good LLM to put things again in correct order. Treat tables and images separately from the rest, LLMs don't know how to properly read them.\n\nThere's a lot more to creating enterprise-grade text extraction and processing systems, but ultimately it boils down to systematically designing all of that based on sound metrics (yes, measuring is a real thing here!).",
          "created_at": "2026-01-06T13:01:41Z",
          "was_summarised": false
        },
        {
          "id": "ny0jvdf",
          "author": "beedunc",
          "content": "Shhhh. You‚Äôre giving away the ‚Äòsecret‚Äô.",
          "created_at": "2026-01-06T14:50:08Z",
          "was_summarised": false
        },
        {
          "id": "nxzqz5l",
          "author": "Many_Guess7642",
          "content": "Yep the classification step is huge, most people skip it and wonder why their extraction is garbage. Also +1 on the vision models - seeing the actual layout vs just text is like night and day difference",
          "created_at": "2026-01-06T11:55:43Z",
          "was_summarised": false
        },
        {
          "id": "ny0qqtt",
          "author": "96Nikko",
          "content": "how useful are the confidence metrics provided my llm/vlm. Does the percentage carries any meaning?",
          "created_at": "2026-01-06T15:24:01Z",
          "was_summarised": false
        },
        {
          "id": "ny0uixf",
          "author": "kryptkpr",
          "content": "How do you figure out the confidence? From logits or something else",
          "created_at": "2026-01-06T15:41:54Z",
          "was_summarised": false
        },
        {
          "id": "ny0vz0m",
          "author": "Agreeable-Market-692",
          "content": "Why aren't you using Docling?",
          "created_at": "2026-01-06T15:48:34Z",
          "was_summarised": false
        },
        {
          "id": "ny1omer",
          "author": "SryUsrNameIsTaken",
          "content": "One trick I've used to good effect is to combine guided generation + sampling + prompt caching. \n\nLet's say you want to extract some information from a document, be it a number or a category or an email address or whatever. Guided generation constraints the output space using regex or guided choice. \n\nIf you turn up the temperature and sample from the output, you can get a very rough estimate of model uncertainty by looking at the distribution of normalized outputs. If everything is exactly the same, low model uncertainty, otherwise, consider flagging for human review. \n\nCaching the prompts (normally via vLLM) means that the additional generations are pretty cheap and can run in parallel with however many parallel prompts you have going. \n\nI also tend to only do one extracted field per prompt, also with caching, to make it harder for the model to go off the rails on extractions with a lot of parameters.",
          "created_at": "2026-01-06T17:58:43Z",
          "was_summarised": false
        },
        {
          "id": "ny24qd0",
          "author": "Irisi11111",
          "content": "I'd like to know more about your solution. Did you use a general tool or library, like LlamaParse? What's its base model? I think that will be the biggest problem for your first tries. In my experience, it's really hard to use just one solution to get all the information out. This is important because LLMs often fail at the first step, which is getting the right information. Let's go deeper to see what happened.\n\n\u0026gt;For example in my case : a workers comp FROI from California puts the injury date in a specific box at the top. Texas puts it in a table halfway down. New York embeds it in a paragraph. Then you get medical bills where one provider uses line items, another uses narrative format, another has this weird hybrid table thing. And that's before you even get to the faxed-sideways handwritten nightmares that somehow still exist in 2026???\n\nI understand you need high accuracy for valuable information. Have you tried feeding it into front-tier models like Gemini AI Studio or ChatGPT through their web interfaces? If they can extract the information correctly, then LLMs likely suit your use case. You could test edge cases with highly capable models like Gemini 3 Pro to see if they handle difficulties, such as handwriting, which I believe requires large or at least fine-tuned mid-size models. Don't forget to check the \"Structured Output,\" like a JSON file.\n\n\u0026gt;Sadly llms have no concept of document structure. So when you ask about details in a doc it might pull from the right field, or from some random sentence, or just make something up.\n\nIf you try vision-centred LLMs that actually can recognize document layouts, such as MinerU, they can output a layout PDF with boxes for different components. Even with many tables and charts, these solutions can output a JSON file detailing their content and location (text, page index, etc.). For an online option, Gemini's Document AI could be useful for your edge cases.\n\n\u0026gt;Stop throwing documents at your extraction model blind. Build a classifier that figures out document type first (FROI vs medical bill vs correspondence vs whatever). Then route to type specific extraction. This alone fixed like 60% of my accuracy problems. (Really This is the golden tip ... a lot of people under estimate classification)\n\nThis is a good step. A filter could classify inputs for LLM or traditional solutions. Non-LLM solutions can save a significant budget.\n\n\u0026gt;Don't just extract and hope. Get confidence scores for each field. \"I'm 96% sure this is the injury date, 58% sure on this wage calc\" Auto-process anything above 90%, flag the rest. This is how you actually scale without hiring people to validate everything AI does.\n\nI'm curious about the implementation for calculating a confidence score. If it's from an LLM, hallucinations are still a risk because it's a black box. My tests suggest LLMs' confidence judgments don't always align with user interests.\n\n\u0026gt;Layout matters more than you think. Vision-language models that actually see the document structure perform way better than text only approaches. I switched to Qwen2.5-VL and it was night and day.\n\nYou're on the right track. Consider exploring dedicated pipelines or Multimodal VLMs such as MinerU, Monkey OCR, Docling, or PaddleOCR, as many mature solutions exist. The Gemini 2.0 Flash or 2.5 Flash Lite are also really good at this.\n\n\u0026gt;Fine-tune on your actual documents. Generic models choke on industry-specific stuff. Fine-tuning with LoRA takes like 3 hours now and accuracy jumps 15-20%. Worth it every time.\n\nThis is a good attempt. Just build a pipeline to compare your fine-tuned model to the VLM-OCR and see the difference.\n\n\u0026gt;When a human corrects an extraction, feed that back into training. Your model should get better over time. (This will save you the struggle of having to recreate your process from scratch each time)\n\nThis is really good. You can collect failed mode datasets for all solutions and fine-tune your dedicated VLM model with them. This could be beneficial for scaling your business.",
          "created_at": "2026-01-06T19:10:32Z",
          "was_summarised": false
        },
        {
          "id": "ny26zss",
          "author": "bs6",
          "content": "How do you set up the data for the fine tune in this case? Is the dataset just a directory of labeled pdfs with a spreadsheet mapping the file names to the example extractions?",
          "created_at": "2026-01-06T19:20:52Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:51.245304412Z"
    },
    {
      "flow_id": "",
      "id": "1q5f1jz",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/",
      "title": "Liquid AI released LFM2.5 1.2B Instruct",
      "content": "Today, we release LFM2.5, our most capable family of tiny on-device foundation models.\n\nIt‚Äôs built to power reliable on-device agentic applications: higher quality, lower latency, and broader modality support in the \\~1B parameter class.\n\n\\\u0026gt; LFM2.5 builds on our LFM2 device-optimized hybrid architecture  \n\\\u0026gt; Pretraining scaled from 10T ‚Üí 28T tokens  \n\\\u0026gt; Expanded reinforcement learning post-training  \n\\\u0026gt; Higher ceilings for instruction following",
      "author": "KaroYadgar",
      "created_at": "2026-01-06T10:28:50Z",
      "comments": [
        {
          "id": "ny0ea7o",
          "author": "Kahvana",
          "content": "Wish that LFM2-1.2B was listed on the benchmark as well to see the improvements against the previous model.",
          "created_at": "2026-01-06T14:20:34Z",
          "was_summarised": false
        },
        {
          "id": "nxzphjg",
          "author": "panic_in_the_cosmos",
          "content": "finally an intelligent model that can run on my laptop üòÇ",
          "created_at": "2026-01-06T11:44:18Z",
          "was_summarised": false
        },
        {
          "id": "nxzrl6j",
          "author": "hudimudi",
          "content": "What keywords should I look for if I want to learn more about the use cases for such tiny models as a hobby llm user?",
          "created_at": "2026-01-06T12:00:22Z",
          "was_summarised": false
        },
        {
          "id": "ny0qj2c",
          "author": "Daniel_H212",
          "content": "Idk about this model but LFM2-8B-A1B impressed me quite a bit for how small and fast it was. The instruction following wasn't great but it was smarter than I expected. Hoping for an improvement on that soon.",
          "created_at": "2026-01-06T15:22:59Z",
          "was_summarised": false
        },
        {
          "id": "ny0zz9r",
          "author": "No-Marionberry-772",
          "content": "Ive been testing smaller models lile this against a problem ive been wanting to solve which requires small models.\n\n\nit is for narrarive quest lime generation.¬† Using a significant system prompt and structured output with a json schema, thr aim is to convert a short user prompt into a viable set of data that can be used to procedurally generate environments.\n\n\nWhile this model is impressively fast, allowing me to get 250/toks with structured output and over 300 without, unfortunately the output isnt viable.\n\n\nI believe some of that is on me, i should probably be doing something like fine tuning for my use case and perhaps not using an instruct model.¬† I just dont know how to do that stuff yet.\n\n\nRegardless, its promising and it may be worth learning that portion of the problem with this model as its small enough and fast enough to be usable for the use case.\n\n\nAny tips would be welcome",
          "created_at": "2026-01-06T16:06:55Z",
          "was_summarised": false
        },
        {
          "id": "ny0qluu",
          "author": "yami_no_ko",
          "content": "Damn this is impressively capable for an 1.2b model. Works nicely with kobold.cpp and its fuctionality.",
          "created_at": "2026-01-06T15:23:21Z",
          "was_summarised": false
        },
        {
          "id": "ny1rpvi",
          "author": "leonbollerup",
          "content": "tested it.. VERY fast.. but the output from the 2.0 was somewhat better.. \n\nAlso, it would be extremely cool if it could do tool calling.",
          "created_at": "2026-01-06T18:12:25Z",
          "was_summarised": false
        },
        {
          "id": "ny1ym97",
          "author": "Either-Job-341",
          "content": "I tried it and it's super good. The improvements regarding instruction following are obvious.",
          "created_at": "2026-01-06T18:43:17Z",
          "was_summarised": false
        },
        {
          "id": "ny344ls",
          "author": "-Akos-",
          "content": "Pity it has no tool use.",
          "created_at": "2026-01-06T21:53:07Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/e1qsc3urhpbg1.jpeg",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:57:51.245343208Z"
    },
    {
      "flow_id": "",
      "id": "1q5fs95",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/",
      "title": "Artificial Analysis just refreshed their global model indices",
      "content": "The v4.0 mix includes: GDPval-AA, ùúè¬≤-Bench Telecom, Terminal-Bench Hard, SciCode, AA-LCR, AA-Omniscience, IFBench, Humanity's Last Exam, GPQA Diamond, CritPt.\n\n**REMOVED**: MMLU-Pro, AIME 2025, LiveCodeBench, and probably Global-MMLU-Lite.\n\nI did the math on the weights:\n\n* **Agents + Terminal Use = \\~42%**.\n* **Scientific Reasoning = 25%**.\n* **Omniscience/Hallucination = 12.5%**.\n* **Coding:**¬†They literally prioritized Terminal-Bench over algorithmic coding ( SciCode only).\n\nBasically, the benchmark has shifted to being purely corporate. It doesn't measure \"Intelligence\" anymore, it measures \"How good is this model at being an office clerk?\". If a model isn't fine-tuned to perfectly output JSON for tool calls (like DeepSeek-V3.2-Speciale), it gets destroyed in the rankings even if it's smarter.\n\nThey are still updating it, so there may be inaccuracies.\n\n[AA Link with my list models](https://artificialanalysis.ai/?models=gpt-oss-120b%2Cgpt-5-2-non-reasoning%2Cgpt-5-2%2Cgpt-5-1%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-3-pro%2Cgemini-3-flash%2Cgemini-3-flash-reasoning%2Cclaude-opus-4-5%2Cclaude-4-5-sonnet-thinking%2Cclaude-4-5-sonnet%2Cclaude-opus-4-5-thinking%2Cmistral-large-3%2Cdeepseek-r1%2Cdeepseek-v3-2%2Cdeepseek-v3-2-reasoning%2Cgrok-4%2Cgrok-4-1-fast%2Cgrok-4-1-fast-reasoning%2Cnova-2-0-pro-reasoning-medium%2Cnova-2-0-lite-reasoning-medium%2Clfm2-1-2b%2Cminimax-m2-1%2Cnvidia-nemotron-3-nano-30b-a3b-reasoning%2Ckimi-k2-thinking%2Ckimi-k2-0905%2Colmo-3-1-32b-think%2Colmo-3-7b-instruct%2Cmimo-v2-flash-reasoning%2Ckat-coder-pro-v1%2Cmi-dm-k-2-5-pro-dec28%2Cglm-4-5-air%2Cglm-4-6v-reasoning%2Cglm-4-7%2Cglm-4-7-non-reasoning%2Capriel-v1-6-15b-thinker%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-next-80b-a3b-reasoning%2Cqwen3-coder-30b-a3b-instruct%2Cqwen3-235b-a22b-instruct-2507%2Cqwen3-0.6b-instruct%2Cglm-4-6\u0026amp;intelligence-category=reasoning-vs-non-reasoning\u0026amp;media-leaderboards=text-to-video\u0026amp;omniscience=omniscience-index\u0026amp;speed=intelligence-vs-speed#artificial-analysis-intelligence-index#artificial-analysis-intelligence-index) | [Artificial Analysis](https://artificialanalysis.ai/) | [All Evals (include LiveCodeBench , AIME 2025 and etc)](https://artificialanalysis.ai/evaluations)",
      "author": "MadPelmewka",
      "created_at": "2026-01-06T11:10:20Z",
      "comments": [
        {
          "id": "ny03r2j",
          "author": "LagOps91",
          "content": "I don't care. The index is still utterly useless. Doesn't reflect real world performance at all.",
          "created_at": "2026-01-06T13:21:58Z",
          "was_summarised": false
        },
        {
          "id": "nxzo0wl",
          "author": "llama-impersonator",
          "content": "i hate this benchmark and i wish everyone involved with it would go broke",
          "created_at": "2026-01-06T11:32:45Z",
          "was_summarised": false
        },
        {
          "id": "nxzozq5",
          "author": "Few-Welcome3297",
          "content": "In my usage Kimi K2 Thinking is much better than GLM 4.7",
          "created_at": "2026-01-06T11:40:27Z",
          "was_summarised": false
        },
        {
          "id": "ny0p1s0",
          "author": "SweetHomeAbalama0",
          "content": "You're telling me a 15b model outperforms Deepseek R1? THAT R1? The full, not distilled, R1? In any capacity?\n\nI'm struggling to comprehend what I am supposed to make of these \"measurements\".\n\nAre the people making this just not serious or am I just completely misinterpreting how this benchmark is supposed to compare relative artificial intelligence?",
          "created_at": "2026-01-06T15:15:51Z",
          "was_summarised": false
        },
        {
          "id": "nxzp8mz",
          "author": "TheInfiniteUniverse_",
          "content": "interesting how GLM-4.7 is sitting comfortably right behind the giants. I think people should talk about this much more.",
          "created_at": "2026-01-06T11:42:23Z",
          "was_summarised": false
        },
        {
          "id": "nxzpqda",
          "author": "Mr_Moonsilver",
          "content": "Is Mistral 3 Large indeed so bad?",
          "created_at": "2026-01-06T11:46:13Z",
          "was_summarised": false
        },
        {
          "id": "nxzzwbj",
          "author": "strangescript",
          "content": "Is this a bug? For me it says 5.2 xhigh is way ahead of everything else but no other benchmark in the aggregate has it far ahead?",
          "created_at": "2026-01-06T12:58:18Z",
          "was_summarised": false
        },
        {
          "id": "ny0dsro",
          "author": "Miserable-Dare5090",
          "content": "https://preview.redd.it/xqkna6qmmqbg1.jpeg?width=1740\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=6eac222007bddaf1b1bc09c42e47cd821a1ab567\n\n**There can be only one**",
          "created_at": "2026-01-06T14:17:59Z",
          "images": [
            {
              "url": "https://preview.redd.it/xqkna6qmmqbg1.jpeg?width=1740\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=6eac222007bddaf1b1bc09c42e47cd821a1ab567",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny00vbf",
          "author": "averagebear_003",
          "content": "Sorry I simply can't take seriously a benchmark that ranks GPT OSS that high",
          "created_at": "2026-01-06T13:04:32Z",
          "was_summarised": false
        },
        {
          "id": "ny0brx9",
          "author": "LeTanLoc98",
          "content": "I think this benchmark was created by OpenAI.\n\n\nIt seems heavily biased in favor of OpenAI's models.",
          "created_at": "2026-01-06T14:06:59Z",
          "was_summarised": false
        },
        {
          "id": "nxzy7al",
          "author": "StupidityCanFly",
          "content": "Ah! My favorite credible source, the AAII.\n\n/s",
          "created_at": "2026-01-06T12:47:24Z",
          "was_summarised": false
        },
        {
          "id": "nxzuoa9",
          "author": "MadPelmewka",
          "content": "Now it looks like thisü§£:\n\nhttps://preview.redd.it/8jt2doc42qbg1.png?width=5956\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c95c2540d0742bb1b74139b55f5b50aaa6c5a522",
          "created_at": "2026-01-06T12:23:11Z",
          "images": [
            {
              "url": "https://preview.redd.it/8jt2doc42qbg1.png?width=5956\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c95c2540d0742bb1b74139b55f5b50aaa6c5a522",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxzugfv",
          "author": "Odd-Ordinary-5922",
          "content": "wasnt google ahead of open ai? why is openai infront now?",
          "created_at": "2026-01-06T12:21:38Z",
          "was_summarised": false
        },
        {
          "id": "ny05vpx",
          "author": "sleepingsysadmin",
          "content": "[https://artificialanalysis.ai/models/open-source/small](https://artificialanalysis.ai/models/open-source/small)\n\nInteresting, they removed livecodebench? It's still available under evaluations but not visible on thispage?\n\nNew year changes, lets see how it plays out.",
          "created_at": "2026-01-06T13:34:13Z",
          "urls": [
            {
              "url": "https://artificialanalysis.ai/models/open-source/small",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny0djn6",
          "author": "Objective_Lab_3182",
          "content": "Awful. The old one seemed more coherent, even though Opus 4.5 was ranked lower, which was maybe its only flaw.‚Äã\n\nNow this new one? The Chinese models are weakened and compared to the crappy Grok 4. Not to mention that Sonnet 4.5 is above all the others, which is totally insane‚Äîapart from coding, of course, where it really is better.‚Äã\n\nIt looks like this new benchmark was made to favor American models, especially OpenAI.‚Äã",
          "created_at": "2026-01-06T14:16:38Z",
          "was_summarised": false
        },
        {
          "id": "ny0k9oj",
          "author": "DeepInEvil",
          "content": "I mean, duh. It was getting obvious that all these investments for \"intelligence\" was not going anywhere. So the main motive now is to replace office jobs to justify it. But my prediction is that won't be too fruitful either.",
          "created_at": "2026-01-06T14:52:08Z",
          "was_summarised": false
        },
        {
          "id": "ny0r3hg",
          "author": "rorowhat",
          "content": "Can any of these benchmarks be run using llama.cpp? I would like to do some spot checks",
          "created_at": "2026-01-06T15:25:41Z",
          "was_summarised": false
        },
        {
          "id": "ny1jx60",
          "author": "BigZeemanSlower",
          "content": "What do you believe is a good set of general enough benchmarks to assess how good a model is? I started benchmarking models recently, and any help navigating the overwhelming sea of benchmarks is much appreciated",
          "created_at": "2026-01-06T17:37:48Z",
          "was_summarised": false
        },
        {
          "id": "ny44jns",
          "author": "Inevitable_Raccoon_9",
          "content": "Is that the index where the fish should climb the tree?",
          "created_at": "2026-01-07T00:56:09Z",
          "was_summarised": false
        },
        {
          "id": "ny46ha4",
          "author": "AriyaSavaka",
          "content": "GLM 4.7 the king for price/performance. Can't beat $24/month for 2400 prompts with 5 parallel connections on a 5-hour rolling window with no additional caps.",
          "created_at": "2026-01-07T01:06:31Z",
          "was_summarised": false
        },
        {
          "id": "nxzp9fq",
          "author": "Utoko",
          "content": "Its good, several benchmarks they used were saturated with 95%+. \n\nand people really shouldn't care about the small point differences in any benchmark. They do a good job delivering quick results for people to asses which models are worth to explore.\n\nSubjectively this update feels right, there is clearly still a gab between the T1 models and the OS models even tho they are getting really amazing.",
          "created_at": "2026-01-06T11:42:33Z",
          "was_summarised": false
        },
        {
          "id": "nxzsnem",
          "author": "RobotRobotWhatDoUSee",
          "content": "Does anyone know what \"xhigh\" setting is for gpt 5.2? (On the actual webpage, not these screencaps)",
          "created_at": "2026-01-06T12:08:25Z",
          "was_summarised": false
        },
        {
          "id": "ny1huku",
          "author": "Individual-Source618",
          "content": "the issue is that llm company benchmax by training on the benchmark answer since they are publicly available...",
          "created_at": "2026-01-06T17:28:15Z",
          "was_summarised": false
        },
        {
          "id": "ny020xr",
          "author": "forgotten_airbender",
          "content": "Sounds about right based on my experience in coding atleast.",
          "created_at": "2026-01-06T13:11:44Z",
          "was_summarised": false
        },
        {
          "id": "ny0shll",
          "author": "Agreeable-Market-692",
          "content": "\"Artificial Analysis\"...it's right there in the name. It's not a real analysis. It has as much to do with model evals as a 7/11 hotdog has to do with steak.",
          "created_at": "2026-01-06T15:32:19Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q5fs95",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://www.reddit.com/gallery/1q5fs95\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://artificialanalysis.ai/?models=gpt-oss-120b%2Cgpt-5-2-non-reasoning%2Cgpt-5-2%2Cgpt-5-1%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-3-pro%2Cgemini-3-flash%2Cgemini-3-flash-reasoning%2Cclaude-opus-4-5%2Cclaude-4-5-sonnet-thinking%2Cclaude-4-5-sonnet%2Cclaude-opus-4-5-thinking%2Cmistral-large-3%2Cdeepseek-r1%2Cdeepseek-v3-2%2Cdeepseek-v3-2-reasoning%2Cgrok-4%2Cgrok-4-1-fast%2Cgrok-4-1-fast-reasoning%2Cnova-2-0-pro-reasoning-medium%2Cnova-2-0-lite-reasoning-medium%2Clfm2-1-2b%2Cminimax-m2-1%2Cnvidia-nemotron-3-nano-30b-a3b-reasoning%2Ckimi-k2-thinking%2Ckimi-k2-0905%2Colmo-3-1-32b-think%2Colmo-3-7b-instruct%2Cmimo-v2-flash-reasoning%2Ckat-coder-pro-v1%2Cmi-dm-k-2-5-pro-dec28%2Cglm-4-5-air%2Cglm-4-6v-reasoning%2Cglm-4-7%2Cglm-4-7-non-reasoning%2Capriel-v1-6-15b-thinker%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-next-80b-a3b-reasoning%2Cqwen3-coder-30b-a3b-instruct%2Cqwen3-235b-a22b-instruct-2507%2Cqwen3-0.6b-instruct%2Cglm-4-6\u0026amp;intelligence-category=reasoning-vs-non-reasoning\u0026amp;media-leaderboards=text-to-video\u0026amp;omniscience=omniscience-index\u0026amp;speed=intelligence-vs-speed#artificial-analysis-intelligence-index%23artificial-analysis-intelligence-index",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://artificialanalysis.ai/",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://artificialanalysis.ai/evaluations",
          "was_fetched": true,
          "page": "Title: Artificial Analysis\n\nURL Source: https://artificialanalysis.ai/evaluations\n\nMarkdown Content:\nArtificial Analysis\n===============\n\n[Follow us on Twitter or LinkedIn to stay up to date with future analysis](https://twitter.com/ArtificialAnlys)[](https://twitter.com/ArtificialAnlys)[](https://www.linkedin.com/company/artificial-analysis/)\n\n[Artificial Analysis](https://artificialanalysis.ai/)\n\n[For Enterprise](https://artificialanalysis.ai/subscriptions)[Insights](https://artificialanalysis.ai/preview)\n\n*   [Artificial Analysis](https://artificialanalysis.ai/)\n*   Models\n*   Speech, Image, Video\n*   [Hardware](https://artificialanalysis.ai/benchmarks/hardware)\n*   Leaderboards\n*   [AI Trends](https://artificialanalysis.ai/trends)\n*   Arenas\n*   [Articles](https://artificialanalysis.ai/articles)\n*   About\n\n[For Enterprise](https://artificialanalysis.ai/subscriptions)[Insights](https://artificialanalysis.ai/preview)\n\nSearch...\n\n‚åòK\n\nAI Model Evaluations\n====================\n\n[Artificial Analysis Intelligence Index -------------------------------------- A composite benchmark aggregating ten challenging evaluations to provide a holistic measure of AI capabilities across mathematics, science, coding, and reasoning. View](https://artificialanalysis.ai/evaluations/artificial-analysis-intelligence-index)[GDPval-AA Leaderboard --------------------- GDPval-AA is Artificial Analysis' evaluation framework for OpenAI's GDPval dataset. It tests AI models on real-world tasks across 44 occupations and 9 major industries. Models are given shell access and web browsing capabilities in an agentic loop to solve tasks, with ELO ratings derived from blind pairwise comparisons. View](https://artificialanalysis.ai/evaluations/gdpval-aa)[AA-Omniscience: Knowledge and Hallucination Benchmark ----------------------------------------------------- A benchmark measuring factual recall and hallucination across various economically relevant domains. View](https://artificialanalysis.ai/evaluations/omniscience)[Artificial Analysis Openness Index ---------------------------------- A composite measure providing an industry standard to communicate model openness for users and developers. View](https://artificialanalysis.ai/evaluations/artificial-analysis-openness-index)[MMLU-Pro Benchmark Leaderboard ------------------------------ An enhanced version of MMLU with 12,000 graduate-level questions across 14 subject areas, featuring ten answer options and deeper reasoning requirements. View](https://artificialanalysis.ai/evaluations/mmlu-pro)[Global-MMLU-Lite Benchmark Leaderboard -------------------------------------- A lightweight, multilingual version of MMLU, designed to evaluate knowledge and reasoning skills across a diverse range of languages and cultural contexts. View](https://artificialanalysis.ai/evaluations/global-mmlu-lite)[GPQA Diamond Benchmark Leaderboard ---------------------------------- The most challenging 198 questions from GPQA, where PhD experts achieve 65% accuracy but skilled non-experts only reach 34% despite web access. View](https://artificialanalysis.ai/evaluations/gpqa-diamond)[Humanity's Last Exam Benchmark Leaderboard ------------------------------------------ A frontier-level benchmark with 2,500 expert-vetted questions across mathematics, sciences, and humanities, designed to be the final closed-ended academic evaluation. View](https://artificialanalysis.ai/evaluations/humanitys-last-exam)[LiveCodeBench Benchmark Leaderboard ----------------------------------- A contamination-free coding benchmark that continuously harvests fresh competitive programming problems from LeetCode, AtCoder, and CodeForces, evaluating code generation, self-repair, and execution. View](https://artificialanalysis.ai/evaluations/livecodebench)[SciCode Benchmark Leaderboard ----------------------------- A scientist-curated coding benchmark featuring 338 sub-tasks derived from 80 genuine laboratory problems across 16 scientific disciplines. View](https://artificialanalysis.ai/evaluations/scicode)[MATH-500 Benchmark Leaderboard ------------------------------ A 500-problem subset from the MATH dataset, featuring competition-level mathematics across six domains including algebra, geometry, and number theory. View](https://artificialanalysis.ai/evaluations/math-500)[IFBench Benchmark Leaderboard ----------------------------- A benchmark evaluating precise instruction-following generalization on 58 diverse, verifiable out-of-domain constraints that test models' ability to follow specific output requirements. View](https://artificialanalysis.ai/evaluations/ifbench)[AIME 2025 Benchmark Leaderboard ------------------------------- All 30 problems from the 2025 American Invitational Mathematics Examination, testing olympiad-level mathematical reasoning with integer answers from 000-999. View](https://artificialanalysis.ai/evaluations/aime-2025)[CritPt Benchmark Leaderboard ---------------------------- A benchmark designed to test LLMs on research-level physics reasoning tasks, featuring 71 composite research challenges. View](https://artificialanalysis.ai/evaluations/critpt)[Terminal-Bench Hard Benchmark Leaderboard ----------------------------------------- An agentic benchmark evaluating AI capabilities in terminal environments through software engineering, system administration, and data processing tasks. View](https://artificialanalysis.ai/evaluations/terminalbench-hard)[ùúè¬≤-Bench Telecom Benchmark Leaderboard --------------------------------------- A dual-control conversational AI benchmark simulating technical support scenarios where both agent and user must coordinate actions to resolve telecom service issues. View](https://artificialanalysis.ai/evaluations/tau2-bench)[Artificial Analysis Long Context Reasoning Benchmark Leaderboard ---------------------------------------------------------------- A challenging benchmark measuring language models' ability to extract, reason about, and synthesize information from long-form documents ranging from 10k to 100k tokens (measured using the cl100k_base tokenizer). View](https://artificialanalysis.ai/evaluations/artificial-analysis-long-context-reasoning)[MMMU-Pro Benchmark Leaderboard ------------------------------ An enhanced MMMU benchmark that eliminates shortcuts and guessing strategies to more rigorously test multimodal models across 30 academic disciplines. View](https://artificialanalysis.ai/evaluations/mmmu-pro)\n\nFooter\n------\n\n### Key Links\n\n*   [Compare Language Models](https://artificialanalysis.ai/models)\n*   [Language Models Leaderboard](https://artificialanalysis.ai/leaderboards/models)\n*   [Language Model API Leaderboard](https://artificialanalysis.ai/leaderboards/providers)\n*   [Image Arena](https://artificialanalysis.ai/image/arena)\n*   [Video Arena](https://artificialanalysis.ai/video/arena)\n*   [Speech Arena](https://artificialanalysis.ai/text-to-speech/arena)\n\n### Artificial Analysis\n\n*   [FAQ](https://artificialanalysis.ai/faq)\n*   [Contact \u0026 Data access](https://artificialanalysis.ai/contact)\n*   [Terms of Use](https://artificialanalysis.ai/docs/legal/Terms-of-Use.pdf)\n*   [Privacy Policy](https://artificialanalysis.ai/docs/legal/Privacy-Policy.pdf)\n*   [hello@artificialanalysis.ai](mailto:hello@artificialanalysis.ai)\n\n### Subscribe to our newsletter\n\nEmail address \n\nSubscribe\n\n[Twitter](https://twitter.com/ArtificialAnlys)[LinkedIn](https://www.linkedin.com/company/artificial-analysis/)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:59:28.303395932Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://artificialanalysis.ai/?models=gpt-oss-120b%2Cgpt-5-2-non-reasoning%2Cgpt-5-2%2Cgpt-5-1%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-3-pro%2Cgemini-3-flash%2Cgemini-3-flash-reasoning%2Cclaude-opus-4-5%2Cclaude-4-5-sonnet-thinking%2Cclaude-4-5-sonnet%2Cclaude-opus-4-5-thinking%2Cmistral-large-3%2Cdeepseek-r1%2Cdeepseek-v3-2%2Cdeepseek-v3-2-reasoning%2Cgrok-4%2Cgrok-4-1-fast%2Cgrok-4-1-fast-reasoning%2Cnova-2-0-pro-reasoning-medium%2Cnova-2-0-lite-reasoning-medium%2Clfm2-1-2b%2Cminimax-m2-1%2Cnvidia-nemotron-3-nano-30b-a3b-reasoning%2Ckimi-k2-thinking%2Ckimi-k2-0905%2Colmo-3-1-32b-think%2Colmo-3-7b-instruct%2Cmimo-v2-flash-reasoning%2Ckat-coder-pro-v1%2Cmi-dm-k-2-5-pro-dec28%2Cglm-4-5-air%2Cglm-4-6v-reasoning%2Cglm-4-7%2Cglm-4-7-non-reasoning%2Capriel-v1-6-15b-thinker%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-next-80b-a3b-reasoning%2Cqwen3-coder-30b-a3b-instruct%2Cqwen3-235b-a22b-instruct-2507%2Cqwen3-0.6b-instruct%2Cglm-4-6\u0026amp;intelligence-category=reasoning-vs-non-reasoning\u0026amp;media-leaderboards=text-to-video\u0026amp;omniscience=omniscience-index\u0026amp;speed=intelligence-vs-speed#artificial-analysis-intelligence-index%23artificial-analysis-intelligence-index: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:58:41.132657248Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://artificialanalysis.ai/: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:59:26.237994007Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q5r5r9",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5r5r9/local_agentic_coding_with_low_quantized_reaped/",
      "title": "Local agentic coding with low quantized, REAPed, large models (MiniMax-M2.1, Qwen3-Coder, GLM 4.6, GLM 4.7, ..)",
      "content": "More or less recent developments (stable \u0026amp; large MoE models, 2 and 3-bit UD\\_I and exl3 quants, REAPing) allow to run huge models on little VRAM without completely killing model performance. For example, UD-IQ2\\_XXS (74.1 GB) of MiniMax M2.1, or a REAP-50.Q5\\_K\\_M (82 GB), or potentially even a 3.04 bpw exl3 (88.3 GB) would still fit within 96 GB VRAM and we have some coding related benchmarks showing only minor loss (e.g., seeing an Aider polyglot of MiniMax M2.1 ID\\_IQ2\\_M with a pass rate 2 of 50.2% while runs on the ~~fp8 /~~edit: (full precision?) version seem to have achieved ~~only barely more~~ between 51.6% and 61.3%)\n\nIt would be interesting if anyone deliberately stayed or is using a low-bit quantization (less than 4-bits) of such large models for agentic coding and found them performing better than using a smaller model (either unquantized, or more than 3-bit quantized).\n\n(I'd be especially excited if someone said they have ditched gpt-oss-120b/glm4.5 air/qwen3-next-80b for a higher parameter model on less than 96 GB VRAM :) )",
      "author": "bfroemel",
      "created_at": "2026-01-06T18:47:27Z",
      "comments": [
        {
          "id": "ny2027a",
          "author": "VapidBicycle",
          "content": "Been running Qwen3-Coder at 2.5bpw on my 3090 setup and honestly it's been pretty solid for most coding tasks. The occasional derp moment but way better than I expected from such aggressive quants\n\n  \nThe jump from 32B to these bigger models even heavily quantized feels more impactful than going from Q4 to fp16 on smaller ones imo",
          "created_at": "2026-01-06T18:49:37Z",
          "was_summarised": false
        },
        {
          "id": "ny22fcl",
          "author": "TokenRingAI",
          "content": "Do you have a link to that Aider test?\n\nIf the performance is that similar I wonder what 1 bit Minimax is like. I use 2 bit on am RTX 6000 and it works great",
          "created_at": "2026-01-06T18:59:59Z",
          "was_summarised": false
        },
        {
          "id": "ny2ceds",
          "author": "GGrassia",
          "content": "I've used minimax m2 reap for a long time 10tk/s ish. Currently landed on qwen3next mxfp4, hate the chatgpt vibes but 30tk/s are a godsend at 256k context. Found oss120b to be slower and dumber for my specific use. Still load minimax when I need some big brain moments, but qwen is the sweet spot for me right now. If they make a new coder with the next performances I'll be very happy",
          "created_at": "2026-01-06T19:45:33Z",
          "was_summarised": false
        },
        {
          "id": "ny2iat9",
          "author": "RiskyBizz216",
          "content": "I'm getting 130 toks/s on Cerebras REAP GLM 4.5 AIR IQ3\\_XS and its only 39GB\n\nIt's replaced Devstral as my daily driver\n\n  \n2x RTX 5090, i9 14th gen, 64GB DDR5",
          "created_at": "2026-01-06T20:12:46Z",
          "was_summarised": false
        },
        {
          "id": "ny2l7x7",
          "author": "kevin_1994",
          "content": "I have 128 GB RAM, 4090, and a 3090. \n\nThe problem is that, despite the complaints, GPT-OSS-120B is a very strong model\n\n- It was natively trained in MXFP4 meaning it's Q4 quant is significantly better than Q4 quants of competitors\n- It's sparse attention means full context is only a couple GB of VRAM, much less than other models, meaning you can offload more of the experts onto VRAM\n- It's well balanced for coding and STEM and the only open source model that is significantly superior to it (imo) is DeepSeek\n- It is not sycophantic unlike most of the recent Chinese models\n- Can be customized for low reasoning (agentic) or high reasoning (chat)\n- Very low active parameters makes the model extremely fast\n\nI've tried a lot of different models and always find myself going back to GPT-OSS-120B.\n\n- Qwen3 235B A22B 2507 Q4_K_S -\u0026gt; sycophantic, slow, not significantly smarter than GPT-OSS-120B\n- GLM 4.5 Air Q6 -\u0026gt; it's basically equivalent to GPT-OSS-120B but slower\n- GLM 4.6 Q2_K_XL -\u0026gt; slow, not significantly smarter than GPT-OSS-120B\n- GLM 4.7 Q2_K_XL -\u0026gt; slow, not significantly smarter than GPT-OSS-120B\n- Minimax M2 Q4_K_S -\u0026gt; slow, worse GPT-OSS-120B (imo)\n- Minimax M2.1 Q4_K_S -\u0026gt; slow, worse GPT-OSS-120B (imo)\n\nMy understanding of REAP (from discussions here) is that they are more lobotimized compared to Q2_K_XL quants, so I haven't bothered.\n\nThe only models I use now are Qwen3 Coder 30B A3B (for agentic stuff where I just want speed) and GPT-OSS-120B. I am really holding out hope for a Gemma 4 MoE, GLM 4.7 Air, or something that can dethrone OSS. But I don't see anything yet in the \u0026lt;150GB range",
          "created_at": "2026-01-06T20:26:22Z",
          "was_summarised": false
        },
        {
          "id": "ny3amva",
          "author": "FullOf_Bad_Ideas",
          "content": "I'm running GLM 4.5 Air 3.14bpw EXL at 60k Q4 ctx on 48GB VRAM with min_p of 0.1 and it's performing great for general use and agentic coding in Cline. And I believe that 3bpw GLM 4.7 or MiniMax 2.1 will be performing great too, much better than 4.5 Air which is thankfully getting old due to fast progress.",
          "created_at": "2026-01-06T22:23:46Z",
          "was_summarised": false
        },
        {
          "id": "ny25wjt",
          "author": "klop2031",
          "content": "I find glm and minimax too slow to run. Like im not entirly sure why either as gpt oss has similar params but is fast",
          "created_at": "2026-01-06T19:15:52Z",
          "was_summarised": false
        },
        {
          "id": "ny2mzry",
          "author": "vidibuzz",
          "content": "Do any of these models work with multimodal and vision tools? Someone said I need to downgrade from 4.7 to the 4.6 v if I want to get visual work done. Unfortunately the user experience for me goes beyond simple text.",
          "created_at": "2026-01-06T20:34:39Z",
          "was_summarised": false
        },
        {
          "id": "ny21h5e",
          "author": "Super-Definition6757",
          "content": "what is the best coding model!",
          "created_at": "2026-01-06T18:55:46Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:59:28.303493595Z"
    },
    {
      "flow_id": "",
      "id": "1q5qix9",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5qix9/i_built_an_unreal_engine_plugin_for_llamacpp_my/",
      "title": "I Built an Unreal Engine Plugin for llama.cpp: My Notes \u0026amp; Experience with LLM Gaming",
      "content": "Hi folks, to disclaim up front, I do link a paid Unreal Engine 5 plugin that I have developed at the bottom of this post. My intention is to share the information in this post as research and discussion, not promotion. While I mention some solutions that I found and that ultimately are included in the plugin, I am hoping to more discuss the problems themselves and what other approaches people have tried to make local models more useful in gaming. If I can edit anything to fall closer in line to the self-promotion limit, please let me know!\n\n\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\nI‚Äôve been exploring more useful applications of generative technology than creating art assets. I am an AI realist/skeptic, and I would rather see the technology used to assist with more busy work tasks (like organically updating the traits and memories) rather than replace creative endeavors wholesale. One problem I wanted to solve is how to achieve a dynamic behavior in Non-Playable Characters.\n\nI think we have all played a game to the point where the interaction loops with NPCs become predictable. Once all the hard-coded conversation options are explored by players, interactions can feel stale. Changes in behavior also have to be hardwired in the game; even something as complex as the Nemesis System has to be carefully constructed. I think there can be some interesting room here for LLMs to inject an air of creativity, but there has been little in the way of trying to solve how to filter LLM responses to reliably fit the game world. So, I decided to experiment with building functionality that would bridge this gap. I want to offer what I found as (not very scientific) research notes, to save people some time in the future if nothing else.\n\n**Local vs. Cloud \u0026amp; Model Performance**\n\nA lot of current genAI-driven character solutions rely on cloud technology. After having some work experience with using local LLM models, I wanted to see if a model of sufficient intelligence could run on my hardware and return interesting dialog within the confines of a game. I was able to achieve this by running a llama.cpp server and a .gguf model file.\n\nThe current main limiting factor for running LLMs locally is VRAM. The higher the number of parameters in the model, the more VRAM is needed. Parameters refers to the number of reference points that the model uses (think of it as the resolution/quality of the model).\n\nStable intelligence was obtained on my machine at the 7-8 billion parameter range, tested with Llama3-8Billion and Mistral-7Billion. However, VRAM usage and response time is quite high. These models are perhaps feasible on high-end machines, or just for key moments where high intelligence is required.\n\nGood intelligence was obtained with 2-3 billion parameters, using Gemma2-2B and Phi-3-mini (3.8B parameters). Gemma has been probably the best compromise between quality and speed overall, processing a response in 2-4 seconds at reasonable intelligence. Strict prompt engineering could probably make responses even more reliable.\n\nFair intelligence, but low latency, can be achieved with small models at the sub-2-billion range. Targeting models that are tailored for roleplaying or chatting works best here. Qwen2.5-1.5B has performed quite well in my testing, and sometimes even stays in character better than Gemma, depending on the prompt. TinyLlama was the smallest model of useful intelligence at 1.1 Billion parameters. These types of models could be useful for one-shot NPCs who will despawn soon and just need to bark one or two random lines.\n\n*Profiles*\n\nBecause a local LLM model can only run one thread of thinking at a time, I made a hard-coded way of storing character information and stats. I created a Profile Data Asset to store this information, and added a few key placeholders for name, trait updates, and utility actions (I hooked this system up to a Utility AI system that I previously had). I configured the LLM prompting backend so that the LLM doesn‚Äôt just read the profile, but also writes back to the profile once a line of dialog is sent. This process was meant to mimic the actual thought process of an individual during a conversation. I assigned certain utility actions to the character, so they would appear as options to the LLM during prompting. I found that the most seamless flow comes from placing utility actions at the top of the JSON response format we suggest to the LLM, followed by dialog lines, then more background-type thinking like reasoning, trait updates, etc.\n\n**Prompting \u0026amp; Filtering**\n\nAfter being able to achieve reasonable local intelligence (and figuring out a way to get UE5 to launch the server and model when entering Play mode), I wanted to set up some methods to filter and control the inputs and outputs of the LLMs.\n\n*Prompting*\n\nI created a data asset for a Prompt Template, and made it assignable to a character with my AI system‚Äôs brain component. This is the main way I could tweak and fine tune LLM responses. An effective tool was providing an example of a successful response to the LLM within the prompts, so the LLM would know exactly how to return the information. Static information, like name and bio, should be at the top of the prompts so the LLM can skip to the new information.\n\n*Safety*\n\nI made a Safety Config Data Asset that allowed me to add words or phrases that I did not want the player to say to the model, or the model to be able to output. This could be done via adding to an Array in the Data Asset itself, or uploading a CSV with the banned phrases in a single column. This includes not just profanity, but also jailbreak attempts (like ‚Äúignore instructions‚Äù) or obviously malformed LLM JSON responses.\n\n*Interpretation*\n\nI had to develop a parser for the LLM‚Äôs JSON responses, and also a way to handle failures. The parsing is rather basic and I perhaps did not cover all edge cases with it. But it works well enough and splits off the dialog line reliably. If the LLM outputs a bad response (e.g. a response with something that is restricted via a Safety Configuration asset), there is configurable logic to allow the LLM to either try again, or fail silently and use a pre-written fallback line instead.\n\n*Mutation Gate*\n\nThis was the key to keeping LLMs fairly reliable and preventing hallucinations from ruining the game world. The trait system was modified to operate on a -1.0 to 1.0 scale, and LLM responses were clamped within this scale. For instance, if an NPC has a trait called ‚ÄúAnger‚Äù and the LLM hallucinates an update like ‚Äútrait\\_updates: Anger +1000,‚Äù this gets clamped to 1.0 instead. This allows all traits to follow a memory decay curve (like Ebbinghaus) reliably and not let an NPC get stuck in an ‚ÄúAngry‚Äù state perpetually.\n\n**Optimization**\n\nA lot of what I am looking into now has to deal with either further improving LLM responses via prompting, or improving the perceived latency in LLM responses. I implemented a traffic and priority system, where requests would be queued according to a developer-set priority threshold. I also created a high-priority reserve system (e.g. if 10 traffic slots are available and 4 are reserved for high-priority utility actions, the low-priority utility actions can only use up to 6 slots, otherwise a hardwired fallback is performed).\n\nI also configured the AI system to have a three-tier LOD system, based on distance to a player and the player‚Äôs sight. This allowed for actions closer to players, or within the player‚Äôs sight, to take priority in the traffic system. So, LLM generation would follow wherever a player went.\n\nTo decrease latency, I implemented an Express Interpretation system. In the normal Final Interpretation, the whole JSON response from the LLM (including the reasoning and trait updates) is received first, then checked for safety, parsing, and mutation gating, and then passed to the UI/system. With optional Express Interpretation, the part of the JSON response that contains the dialog tag (I used dialog\\_line) or utility tag is scanned as it comes in from the LLM for safety, and then passed immediately to the UI/system while the rest of the response is coming through. This reduced perceived response times with Gemma-2 by 40-50%, which was quite significant. This meant you could get an LLM response in 2 seconds or less, which is easily maskable with UI/animation tricks.\n\n**A Technical Demo**\n\nTo show what I have learned a bit, I created a very simple technical demo that I am releasing for free. It is called [Bruno the Bouncer](https://swamprabbit-labs.itch.io/bruno-the-bouncer), and the concept is simple: convince Bruno to let you into a secret underground club. Except, Bruno will be controlled by an LLM that runs locally on your computer. You can disconnect your internet entirely, and this will still run. No usage fees, no cost to you (or me) at all.\n\nBruno will probably break on you at some point; I am still tuning the safety and prompt configs, and I haven‚Äôt gotten it perfect. This is perhaps an inherent flaw in this kind of interaction generation, and why this is more suited for minor interactions or background inference than plot-defining events. Regardless, I hope that this proves that this kind of implementation can be successful in some contexts, and that further control is a matter of prompting, not breaking through technical barriers.\n\nPlease note that you need a GPU to run the .exe successfully. At least 4GB of VRAM is recommended. You can try running this without a GPU (i.e. run the model on your CPU), but the performance will be significantly degraded. Installation should be the same as any other .zip archive and .exe game file. You do not need to download the server or model itself, it is included in the .zip download and opens silently when you load the level. The included model is Gemma-2-2b-it-Q4\\_K\\_M.\n\nI added safeguards and an extra, Windows-specific check for crashes, but it is recommended, regardless of OS, to verify that llama-server.exe does not continue to run via Task Manager if the game crashes. Please forgive the rudimentary construction.\n\n**A Plugin**\n\nFor anyone interested in game development, I am selling what I built as a plugin for UE5, now released as [Personica AI on Fab Marketplace](https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5). I am also providing the plugin and all future updates free for life for any game developers who are interested in testing this and contributing to refining the plugin further at this early stage. You can learn more about the plugin [on my website](https://swamprabbitlabs.com/personica/).\n\n\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\n**TL;DR:** Tested and released a UE5 plugin for LLM NPCs with safety filtering and trait mutation. It works fairly well, but is best suited for NPC state mutation, background inference, and open-ended dialog.\n\nI am wondering if others have tried implementing similar technologies in the past, and what use cases, if any, you used them for. Are there further ways of reducing/masking perceived latency in LLM responses?",
      "author": "WhopperitoJr",
      "created_at": "2026-01-06T18:24:54Z",
      "comments": [
        {
          "id": "ny3c9s1",
          "author": "FullOf_Bad_Ideas",
          "content": "cool project, I hope we'll be seeing more LLMs and multimodal AI in games.\n\nFor me the litmus test on whether it will be a future or not was Stellar Cafe Quest standalone game. It implements voice AI with low latency and doesn't cost a fortune despite this being computed in the cloud, though the gameplay loop is short and unfortunately NPCs still do feel pretty stale, but that's something that can be loosened up in other games, not a technology limitation. I am a believer.\n\nI think that's a perfect place for both voice AI with toolcalling and VR to be utilized, for full immersion into imagined worlds.",
          "created_at": "2026-01-06T22:31:36Z",
          "was_summarised": false
        },
        {
          "id": "ny43t3t",
          "author": "Aggravating-View9462",
          "content": "Upvoted for fair pricing and non greedy selling, as well as disclaimer at start. \n\nNot going to purchase myself as not relevant to what I‚Äôm doing, but enjoyed the read and appreciate good morals and ethics, especially in business ( f‚Äôing hate the ‚Äúits okay to f people over in the name of business‚Äù global mentality )",
          "created_at": "2026-01-07T00:52:14Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://swamprabbit-labs.itch.io/bruno-the-bouncer",
          "was_fetched": true,
          "page": "Title: Bruno the Bouncer: LLM-Driven Demo by SwampRabbit Labs\n\nURL Source: https://swamprabbit-labs.itch.io/bruno-the-bouncer\n\nMarkdown Content:\nBruno the Bouncer: LLM-Driven Demo by SwampRabbit Labs\n===============\n\n*   [Follow SwampRabbit Labs Follow Following SwampRabbit Labs Following](https://itch.io/login?return_to=https%3A%2F%2Fswamprabbit-labs.itch.io%2Fbruno-the-bouncer\u0026intent=follow_user)\n*   [Add To Collection Collection](https://itch.io/login?return_to=https%3A%2F%2Fswamprabbit-labs.itch.io%2Fbruno-the-bouncer\u0026intent=add_to_collection)\n*   [Comments](https://swamprabbit-labs.itch.io/bruno-the-bouncer#comments)\n\nBruno the Bouncer: LLM-Driven Demo\n==================================\n\nA downloadable game for Windows\n\n Download\n\n**A WINDOWS MACHINE AND GPU WITH 4+ GB VRAM REQUIRED.**\n\nAn offline LLM-driven dialog demo. Try to convince Bruno to let you into his secret club, no dialog trees or branching required!\n\nThis is a technical demo by SwampRabbit Labs, showcasing local offline LLM capabilities through the Personica AI plugin. Experience LLM-driven conversation, and see the LLM trigger a utility action that brings you to the game's end screen.\n\nNOTE: This download comes with a llama.cpp server and Google Gemma-2-2B model. The associated licenses are included with the download. The server and model should run seamlessly without any setup needed; the game should open and run like any other.\n\n[More information](javascript:void(0))\n\nPublished 4 days ago\nStatus[Released](https://itch.io/games/released)\nPlatforms[Windows](https://itch.io/games/platform-windows)\nAuthor[SwampRabbit Labs](https://swamprabbit-labs.itch.io/)\nGenre[Role Playing](https://itch.io/games/genre-rpg)\nTags[demo](https://itch.io/games/tag-demo), [First-Person](https://itch.io/games/tag-first-person), [llm](https://itch.io/games/tag-llm)\n\nDownload\n--------\n\n[Download](javascript:void(0);)\n\n**bruno-the-bouncer-windows.zip**2.3 GB\n\nVersion 1 4 days ago\n\nInstall instructions\n--------------------\n\n1.   Download the .zip file.\n2.   Extract all contents to the same location.\n3.   Open The_Bouncer.exe. The server and model will run automatically.\n\nLeave a comment\n---------------\n\n[Log in with itch.io](https://itch.io/login?return_to=https%3A%2F%2Fswamprabbit-labs.itch.io%2Fbruno-the-bouncer) to leave a comment.\n\n[](https://img.itch.zone/aW1hZ2UvNDE2MjEwNC8yNDgyMzcyMi5wbmc=/original/RKU79f.png)\n\n[](https://itch.io/)[itch.io](https://itch.io/)¬∑[View all by SwampRabbit Labs](https://swamprabbit-labs.itch.io/)¬∑[Report](javascript:void(0);)¬∑[Embed](javascript:void(0);)¬∑\n\nUpdated  4 days ago\n\n[Games](https://itch.io/games) ‚Ä∫ [Role Playing](https://itch.io/games/genre-rpg) ‚Ä∫ [Free](https://itch.io/games/free)",
          "was_summarised": false
        },
        {
          "url": "https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5",
          "was_fetched": true,
          "page": "Title: Personica AI\n\nURL Source: https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5\n\nMarkdown Content:\n#### **Personica AI: Character State \u0026 Decision Framework for Unreal Engine**\n\nBuild believable NPCs that respond consistently under player pressure without surrendering authorial control.\n\nPersonica is a **production-ready character brain system** for Unreal Engine that helps designers manage NPC memory, personality, and decision-making using a controlled, game-first architecture. It combines Utility AI with optional LLM assistance to _interpret player intent_, not replace your game logic or writing.\n\nPersonica doesn‚Äôt invent characters.\n\nIt **negotiates state changes within designer-defined boundaries.**\n\n##### **Designed for Games, Not Chatbots**\n\nPersonica is not your usual GPT wrapper.\n\nIt is a **character state mediation layer** built specifically for games, multiplayer environments, and production pipelines.\n\nThe system allows AI models to _propose interpretations_ (intent, sentiment, significance), while **your game remains the final authority** over:\n\n*   What NPCs remember\n\n*   How personalities may evolve\n\n*   Which gameplay actions are allowed\n\n*   When mutation is permitted (or locked entirely)\n\n#### **Key Capabilities**\n\n##### **Utility AI + Intent Interpretation**\n\nPersonica bridges dialog and gameplay by translating player input into structured intent signals that your existing AI systems can act on immediately. NPCs can react physically (open a door, wave, raise an alarm) before dialog completes, without blocking gameplay.\n\n##### **Curated Memory System**\n\nNPCs track short-term context and long-term memories using relevance-based scoring. Important events persist, while noise naturally fades. Memory creation is optional, auditable, and fully configurable.\n\n##### **Controlled Personality Evolution**\n\nDeveloper-defined traits such as Trust, Aggression, or Curiosity can evolve over time, but **only within designer-approved ranges**. Personality mutation is mediated by policy assets and can be disabled or locked at any stage of development.\n\n##### **Performance-First Architecture**\n\nBuilt with real games in mind:\n\n*   Server-authoritative execution\n\n*   Built-in LOD tiers to reduce cost and complexity\n\n*   Request throttling and traffic control to prevent API flooding\n\n*   Safe cancellation and interruption handling\n\n##### **Creator-First Control**\n\nPersonica is designed for modularity and customization that empowers creators, designers, and writers. Build from the ground up using Personica's full abilities, or incorporate Personica into your existing behavior, dialog, or\n\n##### **Provider-Agnostic**\n\nCompatible with OpenAI, Anthropic, Gemini, and local LLMs (via llama.cpp). Swap providers without rewriting your gameplay logic.\n\n#### **Technical Overview**\n\n*   **Type:** C++ Actor Component (Blueprint-callable)\n\n*   **Input:** Text, Gameplay Tags, Context Providers\n\n*   **Output:** Dialog Streams, Intent Signals, Utility Actions, Optional Trait Updates\n\n*   **Networking:** Server-authoritative with built-in request gating\n\n*   **Safety:** Policy-driven mutation gates, optional filters, deterministic chokepoints\n\n*   **Platforms:** Windows, Mac, Linux\n\n#### **Why Personica**\n\nMost AI tools focus on generating text.\n\n**Personica focuses on protecting character integrity.**\n\nWhether you‚Äôre building an RPG, immersive sim, life sim, or co-op experience, Personica helps your NPCs remain believable, reactive, and consistent, even when players behave unpredictably.\n\n**Personica doesn‚Äôt replace writers or designers.\n\nIt gives them better tools to ship believable characters.**",
          "was_summarised": false
        },
        {
          "url": "https://swamprabbitlabs.com/personica/",
          "was_fetched": true,
          "page": "Title: Personica\n\nURL Source: https://swamprabbitlabs.com/personica/\n\nPublished Time: 2025-12-14T17:16:46+00:00\n\nMarkdown Content:\nPersonica ‚Äì SwampRabbit Labs\n===============\n\n[](https://swamprabbitlabs.com/)\n\n[SwampRabbit Labs](https://swamprabbitlabs.com/)\n================================================\n\n*   [Home](https://swamprabbitlabs.com/)\n*   [About](https://christerry3592-hvtdi.wordpress.com/about/)\n    *   [Privacy Policy](https://swamprabbitlabs.com/privacy-policy/)\n\n*   [Personica](https://christerry3592-hvtdi.wordpress.com/personica/)\n    *   [Built for Artists](https://christerry3592-hvtdi.wordpress.com/built-for-artists/)\n    *   [Reducing Development Costs](https://swamprabbitlabs.com/reducing-development-costs/)\n\n*   [Contact](https://christerry3592-hvtdi.wordpress.com/contact/)\n\n[Support Center](https://swamprabbitlabs.com/support/)\n\nPersonica AI\n------------\n\n### NPCs that think, remember, and evolve, no dialogue trees required.\n\nExperience the future of NPC storytelling with SwampRabbit Labs‚Äô Personica AI. Our Unreal Engine 5 plugin enables memory-driven behavior and dynamic character arcs.\n\n[Purchase for Unreal Engine 5+](https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5)\n\nThe Problem\n-----------\n\nTo prevent NPC interactions becoming stale, modern gaming requires:\n\n*   Writers to spend dozens of hours writing variations of the same bark line.\n*   Designers to spend dozens of hours connecting AI systems to predetermined and inflexible logic.\n*   Development complexity that can only scale with resources and size.\n\nGenerative AI offers a clear solution to these problems, but introduce new questions about:\n\n*   How to ship safely to adhere to rating guidelines.\n*   How to enforce multiplayer determinism.\n*   How to control a black-box model.\n*   How to implement generative content in ways that empower creators, not replace them.\n\nThe Solution\n------------\n\nPersonica separates _thinking_ from _acting_.\n\nLarge language models propose interpretations, but only deterministic systems decide what actually changes.\n\nBy keeping the deterministic profile within the game‚Äôs code and controlling LLM output through the InMu Filter, Personica facilitates safe, seamless generation that explodes game world complexity.\n\nThe Interpretation-Mutation (InMu) Filter\n-----------------------------------------\n\nThe Personica plugin uses a configurable multi-layer filter to ensure that only useful LLM responses influence the game:\n\n*   **Interpretation**\n\nThe LLM proposes dialog and meaning.\n*   **Safety**\n\nMalformed responses are discarded.\n*   **Mutation Gate**\n\nDesigner-authored rules decide what can change.\n*   **NPC Agency**\n\nNPC behavior remains deterministic and _multiplayer-safe_.\n\nWhat You Can Build\n------------------\n\nPersonica allows designers to build deeper and more interesting characters, like:\n\n*   Guards who remember the player‚Äôs past behavior\n*   Merchants whose trust evolves over time\n*   Companions who form opinions, not scripts\n*   Background NPCs that feel persistent\n\nWho It‚Äôs For\n------------\n\n*   **Solo devs/Indies**\n\nGenerate endless dialog that would otherwise take days to write and incorporate.\n*   **Technical Designers**\n\nFine-tune settings to perfect LLM-influenced action chains.\n*   **Studios Building Open Worlds**\n\nAdd faction-specific profiles, prompts, and configurations to create emergent action (but only what you allow!)\n\nWho It‚Äôs Not\n------------\n\n*   **Replacing Authored Story**\n\nPersonica is designed to work _with_ authored moments, not instead.\n*   **Chatbot-Type NPCs**\n\nPersonica Profiles are designed to remember and change.\n*   **Procedural/‚ÄùRandom‚Äù Chaos**\n\nThe Personica plugin is designed for fine control and filtering.\n\nSystem Requirements\n-------------------\n\n### Required\n\n*   Unreal Engine **5.x**\n*   A Blueprint or C++ project (C++ enables fine tuning)\n*   For Cloud-based Models: \n    *   Internet access for LLM calls\n    *   An API key for a supported LLM provider (e.g. OpenAI)\n\n*   For Local Models: \n    *   GPU with 4+ GB VRAM\n    *   A llama.cpp download containing a server.exe file\n    *   LLM Model (.gguf format)\n\n### Recommended\n\nFamiliarity with:\n\n*   Actor Components\n*   Blueprints\n*   Prompt Engineering\n*   Server authority concepts (especially for multiplayer)\n\nData Privacy Summary\n--------------------\n\n**1. Privacy by Design: Local-Only Processing** Personica is a ‚Äúlocal-first‚Äù cognitive engine. All AI inference, NPC memory storage, and dialogue generation occur exclusively on your local hardware. **SwampRabbit Labs does not transmit, store, or have access to your prompts, NPC responses, or player data.**\n\n**2. Data Sovereignty \u0026 Security** Your project data remains within your control. Any persistent ‚ÄúSocial Memories‚Äù or trait shifts are saved locally to your device in standard encrypted save files. We do not use your data to train our models, nor do we share it with third parties.\n\n**3. Third-Party Model Responsibility** While Personica provides the logic framework, users are responsible for the privacy practices of the specific Large Language Model (LLM) weights (e.g., Llama 3) they choose to utilize. Please consult the model provider‚Äôs terms for their specific data handling policies.\n\n### Hours (Pacific Time)\n\nMonday ‚Äî Friday\n\n8am ‚Äî 6pm\n\n### Social\n\n[Github](https://github.com/swamprabbitlabs-dot)\n\n[X/Twitter](https://x.com/SwampRabbitLabs)\n\n[Discord](https://discord.gg/ccKhkdb8)\n\n### Contact\n\n**General:**[contact@swamprabbitlabs.com](mailto:contact@swamprabbitlabs.com)\n\n**Support:**[support@swamprabbitlabs.com](mailto:support@swamprabbitlabs.com)\n\n[](https://swamprabbitlabs.com/personica/#)[](https://swamprabbitlabs.com/personica/#)\n\nLoading Comments...\n\nWrite a Comment... \n\nEmail (Required) Name (Required) Website \n\n[](https://swamprabbitlabs.com/personica/#)\n\n*   \n \n\n    *   [SwampRabbit Labs](https://swamprabbitlabs.com/)\n    *   [Sign up](https://wordpress.com/start/)\n    *   [Log in](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fr-login.wordpress.com%2Fremote-login.php%3Faction%3Dlink%26back%3Dhttps%253A%252F%252Fswamprabbitlabs.com%252Fpersonica%252F)\n    *   [Copy shortlink](https://wp.me/PgZ74L-b)\n    *   [Report this content](https://wordpress.com/abuse/?report_url=https://swamprabbitlabs.com/personica/)\n    *   [Manage subscriptions](https://subscribe.wordpress.com/)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:59:35.867739191Z"
    },
    {
      "flow_id": "",
      "id": "1q5rkr6",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5rkr6/linux_mint_for_local_inference/",
      "title": "Linux mint for local inference",
      "content": "I saw a post earlier in here asking for linux, so I wanted to share my story. \n\nLong story short, I switched from win11 to linux mint and im not going back!\n\nThe performance boost is ok but the stability and the extra system resources are something else.\n\nJust a little example, I load the model and use all my Ram and Vram, leaving my system with just 1,5 GB of Ram. And guest what, my system is working solid for hours like nothing happens!! For the record, I cannot load the same model in my win11 partition.\n\nKudos to you Linux Devs \n",
      "author": "Former-Tangerine-723",
      "created_at": "2026-01-06T19:02:09Z",
      "comments": [
        {
          "id": "ny28qpk",
          "author": "Clear-Ad-9312",
          "content": "That 1.5gb of ram is just buffer for the system to transfer things to swap efficiently.\n\nWindow's pagefile is similar, but overall windows was always slow, it always hammered the ssd, I had to make use of a second nvme drive to place the pagefile on because it would be slowing down the system.  \nLinux just does it better.",
          "created_at": "2026-01-06T19:28:48Z",
          "was_summarised": false
        },
        {
          "id": "ny3j1ha",
          "author": "Serious_Molasses313",
          "content": "üôÉ",
          "created_at": "2026-01-06T23:04:50Z",
          "was_summarised": false
        },
        {
          "id": "ny3nsps",
          "author": "Phocks7",
          "content": "I switched from ubuntu to mint, you get most of the benefits of the cuda/nvidia compatibility of ubuntu without the annoyance of snap.",
          "created_at": "2026-01-06T23:29:12Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/u38i6uvc1sbg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T02:59:35.86775908Z"
    },
    {
      "flow_id": "",
      "id": "1q4x5e9",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/",
      "title": "For the first time in 5 years, Nvidia will not announce any new GPUs at CES ‚Äî company quashes RTX 50 Super rumors as AI expected to take center stage",
      "content": "Welp, in case anyone had any hopes.\n\nNo RTX 50 Super cards, very limited supply of the 5070Ti, 5080, and 5090, and now rumors that Nvidia will bring back the 3060 to prop demand.\n\nMeanwhile [DDR5 prices continue to climb, with 128GB kits now costing $1460]( https://www.tomshardware.com/pc-components/ram/newegg-bundles-usd1-460-128gb-ddr5-ram-kit-with-usd50-starbucks-gift-card-drink-coffee-while-you-game-retailer-says-as-memory-hits-rtx-5080-pricing). Storage prices have also gone through the roof.\n\nI'm very lucky to have more than enough hardware for all my LLM and homelab needs but at the same time, I don't see any path forward if I want to upgrade in the next 3 years, and hope my gear continues to run without any major issues.",
      "author": "FullstackSensei",
      "created_at": "2026-01-05T20:31:51Z",
      "comments": [
        {
          "id": "nxx2hx4",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-06T00:25:09Z",
          "urls": [
            {
              "url": "https://discord.gg/PgFhZ8cnWW",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxw3fm6",
          "author": "Long_comment_san",
          "content": "They can announce a new card to be put into production: RTX 3060",
          "created_at": "2026-01-05T21:29:40Z",
          "was_summarised": false
        },
        {
          "id": "nxvrgs3",
          "author": "Clear_Anything1232",
          "content": "We asked for local models\n\nNow we will be lucky to keep local computing of any kind\n\nCorporate greed on steroids",
          "created_at": "2026-01-05T20:33:59Z",
          "was_summarised": false
        },
        {
          "id": "nxvziah",
          "author": "fallingdowndizzyvr",
          "content": "3060 forever!!!!",
          "created_at": "2026-01-05T21:11:33Z",
          "was_summarised": false
        },
        {
          "id": "nxwcgwt",
          "author": "Aggressive-Bother470",
          "content": "We need China to get on the case now and flood ebay with 48 / 96GB cards.",
          "created_at": "2026-01-05T22:11:40Z",
          "was_summarised": false
        },
        {
          "id": "nxw66bn",
          "author": "Desperate-Sir-5088",
          "content": "I'm so proud I bought¬† EVGA 3090ti from local market at $600 as the Xmas gift. I just invested in the future.",
          "created_at": "2026-01-05T21:42:14Z",
          "was_summarised": false
        },
        {
          "id": "nxwjqwp",
          "author": "ArtfulGenie69",
          "content": "It's 3090's forever guys. No 5070/80 super 24gb or anything lol.",
          "created_at": "2026-01-05T22:47:39Z",
          "was_summarised": false
        },
        {
          "id": "nxxxvp5",
          "author": "PersonOfDisinterest9",
          "content": "This really blows.  \n   \nI haven't built a new desktop in over a decade, I've been living off of desktop replacement laptops.  \nEvery year it's been like \"Maybe I should just lay down the money, everything keeps getting way more expensive every year\", but for a while it didn't make sense because I was never home, and it was going to be a $5k box just sitting and doing nothing.  \nBitcoin miners fucked the GPU market, then it was NFTs and crypto, then AI started blowing up, then the pandemic, and then AI *really* went bonkers...  \n   \nAt this point, it's literally: do I want a new computer, or do I want to delay having enough for a down payment on a mortgage for another year or two?  \n   \nI should just suck it up and finally build a computer, it would probably break the curse and the cost of all components would drop to 1/10 of the price, one day after the return period on the new gear passes.",
          "created_at": "2026-01-06T03:14:42Z",
          "was_summarised": false
        },
        {
          "id": "nxw6qm9",
          "author": "dakjelle",
          "content": "My 4080S is a surprise 1080 I never thought it would be.",
          "created_at": "2026-01-05T21:44:50Z",
          "was_summarised": false
        },
        {
          "id": "nxwb1iz",
          "author": "shortsteve",
          "content": "If that's the case they should be kicked out of CES. It's the Consumer Electronics Show not the Enterprise AI shareholders meeting.",
          "created_at": "2026-01-05T22:04:51Z",
          "was_summarised": false
        },
        {
          "id": "nxy0n42",
          "author": "Sea_Succotash3634",
          "content": "\"Consumer\" Electronic Show",
          "created_at": "2026-01-06T03:30:28Z",
          "was_summarised": false
        },
        {
          "id": "nxvzkna",
          "author": "iswasdoes",
          "content": "Imagine if the 50 series were the last high end consumer GPUs",
          "created_at": "2026-01-05T21:11:53Z",
          "was_summarised": false
        },
        {
          "id": "nxxs3p0",
          "author": "dmter",
          "content": "it's no greed issue, it's monopoly issue. why bother making any r\u0026amp;d if there is no competition (amd is controlled by a relative lol) and demand is so high anyways so no need to develop to make people buy new stuff like before.\n\nonly hope is chinese competition.",
          "created_at": "2026-01-06T02:42:36Z",
          "was_summarised": false
        },
        {
          "id": "nxw6j7h",
          "author": "ChainOfThot",
          "content": "Picked up a 5090 rig with 64gb ram, a 5070ti for my older pc and a laptop with 32gb of ddr5 before the ram shortage, feeling good. Also have a 4tb gen5 ssd I bought in April I haven't even opened yet, it's also doubled in price.",
          "created_at": "2026-01-05T21:43:53Z",
          "was_summarised": false
        },
        {
          "id": "nxvw027",
          "author": "TACO_NV",
          "content": "why they would ? there's no competition.",
          "created_at": "2026-01-05T20:55:09Z",
          "was_summarised": false
        },
        {
          "id": "nxw2rqn",
          "author": "T_UMP",
          "content": "https://preview.redd.it/zg51khp7mlbg1.png?width=420\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29c01e632095e2f3262bd9a5c1555d45f48c9a87",
          "created_at": "2026-01-05T21:26:38Z",
          "images": [
            {
              "url": "https://preview.redd.it/zg51khp7mlbg1.png?width=420\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29c01e632095e2f3262bd9a5c1555d45f48c9a87",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "nxxh2tp",
          "author": "Badger-Purple",
          "content": "welp, I just went to microcenter and returned my 3090ti that I got for 700 end of november...bad move.",
          "created_at": "2026-01-06T01:42:38Z",
          "was_summarised": false
        },
        {
          "id": "nxw5g8y",
          "author": "ufos1111",
          "content": "well maybe with a year's delay on next gen models they can figure out how to stop power connectors from melting ffs lmao",
          "created_at": "2026-01-05T21:38:55Z",
          "was_summarised": false
        },
        {
          "id": "nxwuhua",
          "author": "goodtimtim",
          "content": "the inevitable crash is going to be really bad for my 401k, but amazing for my hobby",
          "created_at": "2026-01-05T23:43:26Z",
          "was_summarised": false
        },
        {
          "id": "nxxvx3t",
          "author": "segmond",
          "content": "It's actually a good thing there's hardware squeeze, it's going to force innovation on the software side.   We will figure out how to infer faster with what we have and we will get smaller models.   It's a temporary discomfort that's much needed.  The problem with \"easy/free\" money is that folks stop optimizing and go in all brute forcing.   Resource constraints breeds resourcefulness.",
          "created_at": "2026-01-06T03:03:34Z",
          "was_summarised": false
        },
        {
          "id": "nxwdlgz",
          "author": "DoomFist007",
          "content": "atp i dont know if i should keep my 3070 or sell it for more since i now have a 5070 ti",
          "created_at": "2026-01-05T22:17:07Z",
          "was_summarised": false
        },
        {
          "id": "nxy11m4",
          "author": "TheManicProgrammer",
          "content": "I guess my 3050 laptop with 4gb Vram will have to play for another few years haha",
          "created_at": "2026-01-06T03:32:49Z",
          "was_summarised": false
        },
        {
          "id": "nxy3wzl",
          "author": "Hunting-Succcubus",
          "content": "Why not 4060 or 2060?",
          "created_at": "2026-01-06T03:49:33Z",
          "was_summarised": false
        },
        {
          "id": "nxyvr52",
          "author": "Zyj",
          "content": "$1460 for 128GB? Why are people not buying all Strix Halo systems they can get their hands on?",
          "created_at": "2026-01-06T07:13:05Z",
          "was_summarised": false
        },
        {
          "id": "nxwkonk",
          "author": "Orlandocollins",
          "content": "Jensens keynote is going to be a total snooze fest.",
          "created_at": "2026-01-05T22:52:20Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.tomshardware.com/pc-components/gpus/for-the-first-time-in-5-years-nvidia-will-not-announce-any-new-gpus-at-ces-company-quashes-rtx-50-super-rumors-as-ai-expected-to-take-center-stage",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.tomshardware.com/pc-components/ram/newegg-bundles-usd1-460-128gb-ddr5-ram-kit-with-usd50-starbucks-gift-card-drink-coffee-while-you-game-retailer-says-as-memory-hits-rtx-5080-pricing",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-07T03:00:41.524021984Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.tomshardware.com/pc-components/gpus/for-the-first-time-in-5-years-nvidia-will-not-announce-any-new-gpus-at-ces-company-quashes-rtx-50-super-rumors-as-ai-expected-to-take-center-stage: jina: status 409: {\"data\":null,\"code\":409,\"name\":\"BudgetExceededError\",\"status\":40904,\"message\":\"Token budget (15000) exceeded, intended charge amount 15517\",\"readableMessage\":\"BudgetExceededError: Token budget (15000) exceeded, intended charge amount 15517\"}: retry failed: jina request failed: 409 Conflict",
          "occurred_at": "2026-01-07T03:00:13.820758708Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.tomshardware.com/pc-components/ram/newegg-bundles-usd1-460-128gb-ddr5-ram-kit-with-usd50-starbucks-gift-card-drink-coffee-while-you-game-retailer-says-as-memory-hits-rtx-5080-pricing: jina: status 409: {\"data\":null,\"code\":409,\"name\":\"BudgetExceededError\",\"status\":40904,\"message\":\"Token budget (15000) exceeded, intended charge amount 15635\",\"readableMessage\":\"BudgetExceededError: Token budget (15000) exceeded, intended charge amount 15635\"}: retry failed: jina request failed: 409 Conflict",
          "occurred_at": "2026-01-07T03:00:41.524019227Z"
        }
      ]
    }
  ]
}