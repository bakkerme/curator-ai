{
  "blocks": [
    {
      "flow_id": "",
      "id": "1mpk2va",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/",
      "title": "Announcing LocalLlama discord server \u0026amp; bot!",
      "content": "INVITE: https://discord.gg/rC922KfEwj\n\nThere used to be one old discord server for the subreddit but it was deleted by the previous mod.\n\nWhy?\nThe subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).\n\nWe have a discord bot to test out open source models.\n\nBetter contest and events organization.\n\nBest for quick questions or showcasing your rig!",
      "author": "HOLUPREDICTIONS",
      "created_at": "2025-08-13T23:21:05Z",
      "comments": [
        {
          "id": "n8of8m1",
          "author": "HOLUPREDICTIONS",
          "content": "Alt invite: [https://discord.gg/4R7xS5hMdN](https://discord.gg/4R7xS5hMdN)",
          "created_at": "2025-08-14T16:38:28Z",
          "was_summarised": false
        },
        {
          "id": "n8qp4xq",
          "author": "BusRevolutionary9893",
          "content": "Discord, where valuable information goes to die and never be seen again.",
          "created_at": "2025-08-14T23:29:32Z",
          "was_summarised": false
        },
        {
          "id": "n8ms41x",
          "author": "TheMrCake",
          "content": "Please No.\n\nDiscord is a dystopian, gated version of what forums used to be.\nNo I don't want to give Discord all my data. This is peak enshittification.\n\nThis is yet another community where all information will be gated behind discords walls.\nWe as a society used to have real forums that were kept alive by donations, not VC money.",
          "created_at": "2025-08-14T11:22:55Z",
          "was_summarised": false
        },
        {
          "id": "n8z7nu8",
          "author": "shadow-studio",
          "content": "i'd prefer the \"more technical discussion\" to happen right here. discord isn't designed for keeping a knowlegde base that everyone can access freely. i'm ok with the idea of having a bot to test models, but otherwise i consider discord very inadequate and annoying for most things, just like whatsapp groups.\n\n\nalso, platform ensh!ttification is a thing.",
          "created_at": "2025-08-16T08:40:08Z",
          "was_summarised": false
        },
        {
          "id": "nc5tw7v",
          "author": "Appropriate_Cry8694",
          "content": "Discord is a cancer especially for open source projects, a lot of useful info simply dies with the server in the end.",
          "created_at": "2025-09-03T09:01:11Z",
          "was_summarised": false
        },
        {
          "id": "n8lsrr6",
          "author": "bephire",
          "content": "Would it be feasible to release regular archives of all messages/discussions in the server? We wouldn't want to lose valuable discussions and information that would otherwise become inaccessible if anything were to happen to the server.",
          "created_at": "2025-08-14T06:01:12Z",
          "was_summarised": false
        },
        {
          "id": "n8lzmct",
          "author": "Cosack",
          "content": "I do get what this is for, but still... you gotta appreciate the irony lol",
          "created_at": "2025-08-14T07:03:46Z",
          "was_summarised": false
        },
        {
          "id": "n9hjv2e",
          "author": "thedatawhiz",
          "content": "Not a good idea...",
          "created_at": "2025-08-19T06:43:49Z",
          "was_summarised": false
        },
        {
          "id": "n8mp0l9",
          "author": "a_beautiful_rhind",
          "content": "Discord asks for phone number and now face verification in some places.",
          "created_at": "2025-08-14T11:00:08Z",
          "was_summarised": false
        },
        {
          "id": "nc0z0i4",
          "author": "raysar",
          "content": "Be carefull discort is cool for friend but not for sharing public data. It's impossible to find information by searching on discord. Even bot can't do that XD",
          "created_at": "2025-09-02T15:12:51Z",
          "was_summarised": false
        },
        {
          "id": "nhaiysx",
          "author": "crantob",
          "content": "Discord sucks.  \n\nNext?",
          "created_at": "2025-10-02T01:36:16Z",
          "was_summarised": false
        },
        {
          "id": "na350q5",
          "author": "TheRealCookieLord",
          "content": "Cannot wait to invite the bot to multiple servers and make the heaviest thinking model contemplate life's meaning.",
          "created_at": "2025-08-22T15:11:34Z",
          "was_summarised": false
        },
        {
          "id": "n8krv0e",
          "author": "mrjackspade",
          "content": "Hopefully this server fairs better. The last server fucking sucked because the general chat was the landing room, 99%+ of the users only joined for tech support, and no one enforced the help channel rules because they didn't want to be \"mean\"\n\nSo at any given point in time the general chat was just being flooded by the same easy-to-google tech support questions, squashing any real conversation.",
          "created_at": "2025-08-14T01:41:28Z",
          "was_summarised": false
        },
        {
          "id": "n8o2667",
          "author": "Unigma",
          "content": "Could we add a few channels about Machine Learning and AI in general? For people that are training their own models and or creating projects with LLMs in it? I am working on some RL based fine-tuning, and wanted to know if there's a more technical oriented channel to discuss this.",
          "created_at": "2025-08-14T15:35:06Z",
          "was_summarised": false
        },
        {
          "id": "n8k52hw",
          "author": "Accomplished_Ad9530",
          "content": "Nice! Does the bot have retrieval capabilities? It might be useful for novices to have access to a chatbot with access to documentation for common inference engines and front ends, and perhaps localllama discussions. Sort of an interactive FAQ, which would help out with a lot of repeat posts here.",
          "created_at": "2025-08-13T23:30:05Z",
          "was_summarised": false
        },
        {
          "id": "n8kr3sa",
          "author": "TheRealSerdra",
          "content": "You may want to pin this for a few days, just to get some more activity",
          "created_at": "2025-08-14T01:36:54Z",
          "was_summarised": false
        },
        {
          "id": "n8mrbin",
          "author": "DeepWisdomGuy",
          "content": "Is this really running on a cerebras cluster?",
          "created_at": "2025-08-14T11:17:09Z",
          "was_summarised": false
        },
        {
          "id": "n8oc0xy",
          "author": "ilintar",
          "content": "Says the invite is invalid/expired.",
          "created_at": "2025-08-14T16:22:57Z",
          "was_summarised": false
        },
        {
          "id": "neh0mdc",
          "author": "boomboominkimspants",
          "content": "Isnâ€™t that why they call it vibe coding tho?.. like you still checking that itâ€™s meeting your vision, but you know enough and are engaged enough that u notices itâ€™s messing up",
          "created_at": "2025-09-16T04:41:45Z",
          "was_summarised": false
        },
        {
          "id": "nj6lixo",
          "author": "the100rabh",
          "content": "Invite seems to be expired",
          "created_at": "2025-10-12T23:01:24Z",
          "was_summarised": false
        },
        {
          "id": "nofhibc",
          "author": "ceramic-road",
          "content": "Thanks for relaunching the official Discord!   \n  \nThe community surpassing half a million members, having a dedicated space for deeper technical discussion makes sense.  \n  \nFor those of us running llama.cpp or Ollama on modest hardware, will there be channels dedicated to optimization tips or hardware discussion? Already joined",
          "created_at": "2025-11-12T09:27:58Z",
          "was_summarised": false
        },
        {
          "id": "ntjjg1y",
          "author": "roculus",
          "content": "can you make the \"your post is getting popular\" discord ad spam a private message to the poster so we don't have to see it every time? Thanks!",
          "created_at": "2025-12-11T22:12:32Z",
          "was_summarised": false
        },
        {
          "id": "n9h8tm1",
          "author": "GodSpeedMode",
          "content": "This sounds awesome! It's great to see the LocalLLaMA community expanding and finding ways to foster more technical discussions. With 500k users, I totally get the need for a space that caters to those of us who want to dive deeper into the nitty-gritty. The bot for testing out open-source models is a fantastic additionâ€”I can't wait to try it out! I'm also really looking forward to the contests and events; it's nice to have a central hub for that. Just hopped on the Discordâ€”letâ€™s see what everyone is working on!",
          "created_at": "2025-08-19T05:03:22Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1mpk2va",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://www.reddit.com/gallery/1mpk2va\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:33:53.4568046Z"
    },
    {
      "flow_id": "",
      "id": "1q7qcux",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "title": "The NO FAKES Act has a \"Fingerprinting\" Trap that kills Open Source. We need to lobby for a Safe Harbor.",
      "content": "Hey everyone,\nâ€‹Iâ€™ve been reading the text of the \"NO FAKES Act\" currently in Congress, and itâ€™s worse than I thought.\nâ€‹The Tldr: It creates a \"digital replica right\" for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who \"makes available\" a tool that is primarily used for replicas.  \nâ€‹The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation).\nâ€‹There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.\n\nWhat I did:\nI contacted my reps email to flag this as an \"innovation killer.\" If you run a repo or care about open weights, you might want to do the same. We need them to add a \"Safe Harbor\" for tool devs.\n\nS.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress https://share.google/u6dpy7ZQDvZWUrlfc",
      "author": "PostEasy7183",
      "created_at": "2026-01-08T22:33:33Z",
      "comments": [
        {
          "id": "nyhm9rk",
          "author": "Revolutionalredstone",
          "content": "Making your own devs liable is how you turn your country into a third world nation.\n\nPeople who make it easy to USE the tools are the only ones who should be liable.\n\nThere are plenty of countries which won't play these silly blame games and their devs will keep releasing all their stuff either way.\n\nDevs are the inventors of ideas and making them liable for how others missuse them just cuts you off from new ideas completely, what we need todo is make operators / sites / places the normal people go to use the less desirable filters and tech liable (Instagram etc)\n\nAlso dev software licenses say you can't misuse their tech etc, so it's a joke to pretend they are in the wrong if users abuse their license.\n\nThat's a bit like holding petrol companies liable for people who stupidly try to throw bottles of gasoline onto fires:\n\nhttps://www.youtube.com/watch?v=3l50QZiPwnY\n\nEverything can be abused / used in a destructive way / used other than - intended usage.\n\nPowerful open source technologies always win and if your country is not compatible with openness then it's gonna get left behind (think north korea starving and surviving on cracked old builds of windows xp)",
          "created_at": "2026-01-08T22:44:14Z",
          "was_summarised": false
        },
        {
          "id": "nyhobge",
          "author": "jferments",
          "content": "This has been the point of the astro-turf \"anti-AI\" movement all along. I firmly believe that big tech corporations like Google, Microsoft, and OpenAI are behind the bots spreading \"anti-AI\" propaganda that supports laws that will essentially centralize control of AI and make open-source AI illegal.",
          "created_at": "2026-01-08T22:54:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhtae5",
          "author": "fortpatches",
          "content": "I understand where you are coming from, however, you may be misreading the text.\n\nSpecifically, you seem to have overlooked the phrase \"of a specifically identified individual\". E.g., (c)(1)(B)(i) states \"is primarily designed to produce 1 or more digital replicas of a specifically identified individual or individuals without \\[authorization\\].\"  The following subsections (ii) and (iii) have similar \"specifically identified individual\" language.\n\nThis would be more like making an AI designed to make you sound like Arnold Schwarzenegger as opposed to making an AI designed to make you sound like whatever audio sample you provide to it. Or Text-to-speech that makes \"AI Arnold\" say whatever you type.\n\nMoreover, is your AI \"primarily\" designed to produce \"AI Arnold\" audio?\n\nFurther, to actual knowledge is required: (c)(3)(B) states \"with respect to an activity carried out under paragraph (2) by an individual ..., the individual ... must have actual knowledge, ... that the applicable material isâ€” (i) a digital replica that was not authorized by the applicable right holder; or (ii) a product or service described in paragraph (2)(B).\"\n\nIn other words, the liability only attaches if the dev has \"actual knowledge\" that their service \"is \\*primarily\\* designed to produce a digital replica of a \\*specifically identified individual\\*.\"",
          "created_at": "2026-01-08T23:18:46Z",
          "was_summarised": false
        },
        {
          "id": "nyhkxhr",
          "author": "Aromatic-Low-4578",
          "content": "Don't most software licenses already try to protect the developer from liability due to users?  Will be interested to see how it plays out.",
          "created_at": "2026-01-08T22:37:58Z",
          "was_summarised": false
        },
        {
          "id": "nyhzwpj",
          "author": "AutomaticDriver5882",
          "content": "I would just have different developers, make different pieces of the code and then you would snap in the modules so no one person is responsible but maybe thatâ€™s an oversimplification",
          "created_at": "2026-01-08T23:52:54Z",
          "was_summarised": false
        },
        {
          "id": "nyif4r5",
          "author": "davedcne",
          "content": "Honest question, do you think your rep even understood what you were trying to explain to them? I think most of our politicians are so out of touch with technology that its like trying to teach a cave man calculus.",
          "created_at": "2026-01-09T01:11:48Z",
          "was_summarised": false
        },
        {
          "id": "nyile2z",
          "author": "ortegaalfredo",
          "content": "If you think about it, It's way more disturbing than you think:\n\nThey don't want to criminalize porn, they want to criminalize FAKE porn, why? because they need to be in control of the porn generation, so men and particularly young men can be controlled with it.",
          "created_at": "2026-01-09T01:45:04Z",
          "was_summarised": false
        },
        {
          "id": "nyhq9v2",
          "author": "Technical_Ad_440",
          "content": "i doubt it will pass sounds like a complete dud. the top ai will fight this cause it would kill all their tools in that case. personally for me i would rather have tons of fakes and misinfo why? cause in the future those that do due diligence will be the people you want to keep around those that dont fall for the missinfo and use tools to check origins and such they are the people you want. either as fans, work colleagues etc. \n\nwe need more and more fakes and we need people to start checking stuff not banning the tools. the future people are gonna be so hardened against all the missinfo and such that it wont make much difference. it actually baffles me why people dont want to flood fakes and such to just go oh yeh that incident it was AI and move on. its like a shield against most the normal attacks people do and actually weakens the attack vectors people have. kinda like how if people know where someone is people cant dox them cause people already know. trying it just makes the person go well people already know.\n\nyou cant trust most things anymore anyways so if i see things \"exposing\" stuff i just assume its probably AI until otherwise confirmed. it also means mundane attacks in the future are just gonna fall flat completely like if it doesnt affect you you will not care whatsoever",
          "created_at": "2026-01-08T23:03:38Z",
          "was_summarised": false
        },
        {
          "id": "nyhvrpq",
          "author": "WristbandYang",
          "content": "OP only responds using AI. This is bot behavior.",
          "created_at": "2026-01-08T23:31:25Z",
          "was_summarised": false
        },
        {
          "id": "nyin0jf",
          "author": "SilentLennie",
          "content": "Don't know if it matters in practice what they propose. US politics is such a mess and the business interests are so 'great' they might prevent it being passed or no enforcement will happen (regulatory capture).",
          "created_at": "2026-01-09T01:53:44Z",
          "was_summarised": false
        },
        {
          "id": "nyioqq8",
          "author": "timschwartz",
          "content": "Just frame it like guns:\n\nModels don't deepfake people, people with models deepfake people.",
          "created_at": "2026-01-09T02:03:00Z",
          "was_summarised": false
        },
        {
          "id": "nyitkq2",
          "author": "Acceptable_Home_",
          "content": "US and tech bros are actively targeting open source models",
          "created_at": "2026-01-09T02:28:39Z",
          "was_summarised": false
        },
        {
          "id": "nyhroca",
          "author": "PostEasy7183",
          "content": "Comments are coming in a lot quicker than I thought tonight. Please be patient and I will try to get back with you as many of you as I can in a couple hours. Thank you and make sure to bump the thread so this gets attention. Ensure to write to your reps or contact them with any other means necessary.",
          "created_at": "2026-01-08T23:10:40Z",
          "was_summarised": false
        },
        {
          "id": "nyhwkd3",
          "author": "grady_vuckovic",
          "content": "I would love to hear everyone's suggestion for what the alternative is to stop the deep fakes?\n\nThis seems to make it pretty simple. If you're a developer and you release a tool that can be used to impersonate people, you're responsible.\n\nSo let's say you're all against that, alright, fair, what's the next step? What laws, what enforcement, how do we stop this?\n\nBecause \"we just have to learn to live with it\" is not gonna fly.",
          "created_at": "2026-01-08T23:35:34Z",
          "was_summarised": false
        },
        {
          "id": "nyhubjk",
          "author": "Marksta",
          "content": "\u0026gt;a tool that is **primarily** used for replicas.\n\nIsn't the primarily part key here? Because everytime someone advertises a TTS related thing, they bold 1000 times EAZY ONE SHOT VOICE REPLICATION as if that's its only use. This thing passes, then just take that out of your description and add a warning that you're expected to own the rights to any likeness you're duplicating.\n\nThis worry is pretty same as Photoshop being able to edit images you don't own. Photoshop doesn't spam 1000 times that you can steal copyright and must have written somewhere that copyright laws are your own problem somewhere.\n\nSo all that's left is probably actual heinous sites on the chopping block who should definitely be liable if their advertising is spamming how easy their tool will allow you to rip off likenesses and generate infringing content.",
          "created_at": "2026-01-08T23:23:59Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://share.google/u6dpy7ZQDvZWUrlfc",
          "was_fetched": true,
          "page": "Title: Just a moment...\n\nURL Source: https://share.google/u6dpy7ZQDvZWUrlfc\n\nWarning: Target URL returned error 403: Forbidden\nWarning: This page maybe not yet fully loaded, consider explicitly specify a timeout.\n\nMarkdown Content:\nJust a moment...\n===============",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:33:53.954112028Z"
    },
    {
      "flow_id": "",
      "id": "1q7d8bj",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/",
      "title": "Jensen Huang saying \"AI\" 121 times during the NVIDIA CES keynote - cut with one prompt",
      "content": "Someone had to count it. Turns out Jensen said \"AI\" exactly 121 times in the CES 2025 keynote.\n\nI used [https://github.com/OpenAgentPlatform/Dive](https://github.com/OpenAgentPlatform/Dive) (open-source MCP client) + two MCPs I made:\n\n\\- [https://github.com/kevinwatt/yt-dlp-mcp](https://github.com/kevinwatt/yt-dlp-mcp) \\- YouTube download  \n\\- [https://github.com/kevinwatt/ffmpeg-mcp-lite](https://github.com/kevinwatt/ffmpeg-mcp-lite) \\- video editing\n\n**One prompt:**\n\n\u0026gt;Task: Create a compilation video of every exact moment Jensen Huang says \"AI\".  \nVideo source: [https://www.youtube.com/watch?v=0NBILspM4c4](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n\u0026gt;**Instructions:**\n\n\u0026gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)\n\n\u0026gt;Parse JSON3 to find every \"AI\" instance with precise start/end times\n\n\u0026gt;Use ffmpeg to cut clips (\\~50-100ms padding for natural sound)\n\n\u0026gt;Concatenate all clips chronologically\n\n\u0026gt;Output: Jensen\\_CES\\_AI.mp4\n\nDive chained the two MCPs together - download â†’ parse timestamps â†’ cut 121 clips â†’ merge. All local, no cloud.\n\nIf you want to see how it runs: [https://www.youtube.com/watch?v=u\\_7OtyYAX74](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nThe result is... hypnotic.",
      "author": "Prior-Arm-6705",
      "created_at": "2026-01-08T14:29:47Z",
      "comments": [
        {
          "id": "nygb8aw",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T19:15:15Z",
          "was_summarised": false
        },
        {
          "id": "nyejqtm",
          "author": "YearZero",
          "content": "Honestly that's probably a great summary of the keynote. He should've just done exactly that and it wouldn't change anything.",
          "created_at": "2026-01-08T14:33:06Z",
          "was_summarised": false
        },
        {
          "id": "nyf9wy6",
          "author": "DriveSolid7073",
          "content": "\"All local, no cloud.\"   \nopen video  \n\\\u0026gt;claude opus 4.5  \nI couldn't get Dive to work with my Koboldcpp, and the functions aren't being called.",
          "created_at": "2026-01-08T16:33:54Z",
          "was_summarised": false
        },
        {
          "id": "nyekf0a",
          "author": "LambdaHominem",
          "content": "gamers nexus would be proud\n\nhttps://youtu.be/-qbylbEek-M",
          "created_at": "2026-01-08T14:36:33Z",
          "was_summarised": false
        },
        {
          "id": "nyepodd",
          "author": "LinkSea8324",
          "content": "The fuck is that latex-leather jacket",
          "created_at": "2026-01-08T15:02:13Z",
          "was_summarised": false
        },
        {
          "id": "nyekw78",
          "author": "FastDecode1",
          "content": "Reminds me of the Xbox One reveal: https://www.youtube.com/watch?v=KbWgUO-Rqcw",
          "created_at": "2026-01-08T14:38:59Z",
          "was_summarised": false
        },
        {
          "id": "nyenr5m",
          "author": "International-Try467",
          "content": "This man is the reason why everything is so expensive",
          "created_at": "2026-01-08T14:53:01Z",
          "was_summarised": false
        },
        {
          "id": "nyeqoex",
          "author": "anon235340346823",
          "content": "0:44 \"hey I have AI\"",
          "created_at": "2026-01-08T15:07:02Z",
          "was_summarised": false
        },
        {
          "id": "nyg0uk3",
          "author": "Sea_Succotash3634",
          "content": "\"Consumer\" Electronic Show",
          "created_at": "2026-01-08T18:30:35Z",
          "was_summarised": false
        },
        {
          "id": "nygtg3m",
          "author": "Freonr2",
          "content": "\"Consumer\" Electronics Show, showing billion dollar datacenter configs.",
          "created_at": "2026-01-08T20:36:11Z",
          "was_summarised": false
        },
        {
          "id": "nygf0cq",
          "author": "r0ckl0bsta",
          "content": "Old McJensen's server farm.\nA-I-A-I-O ðŸŽ¶",
          "created_at": "2026-01-08T19:31:52Z",
          "was_summarised": false
        },
        {
          "id": "nyfjql1",
          "author": "budz",
          "content": "dope, I made a script that does this , back in October      [https://imgur.com/a/h0vc2f6](https://imgur.com/a/h0vc2f6)",
          "created_at": "2026-01-08T17:16:29Z",
          "was_summarised": false
        },
        {
          "id": "nyervtk",
          "author": "GoranjeWasHere",
          "content": "He's literally the only one that can say as much AI as he wants.\n\nHe literally build his whole company on AI promise before everyone outside of research circle even knew what was AI and his hardware innovated so much that AI finally became a thing.\n\nOutside of Nvidia only Tesla I think and few other small companies can shout AI without sounding like a fool. Tesla was also super early in it and it got to the point where they were building their own chips just not to pay Nvidia tax for AI before AI even became investor bait.",
          "created_at": "2026-01-08T15:12:41Z",
          "was_summarised": false
        },
        {
          "id": "nyffw6s",
          "author": "WavierLays",
          "content": "I mean this is like making a supercut of every time KFC's CEO says \"chicken\"",
          "created_at": "2026-01-08T16:59:32Z",
          "was_summarised": false
        },
        {
          "id": "nyfa965",
          "author": "MMAgeezer",
          "content": "Appreciate you sharing the details of how you did this. One small thing though, it includes multiple clips of a narrator (i.e. not Jensen, as instructed) saying \"AI\" too.",
          "created_at": "2026-01-08T16:35:22Z",
          "was_summarised": false
        },
        {
          "id": "nyfskqo",
          "author": "SmegPoison",
          "content": "Old MacDonald had a farm, ai-ai-oh!",
          "created_at": "2026-01-08T17:55:09Z",
          "was_summarised": false
        },
        {
          "id": "nyfuof4",
          "author": "Caladan23",
          "content": "Duh surprise... it's an AI hardware company nowadays. Nvidia as a gaming company wasn't worth even 1/10th.",
          "created_at": "2026-01-08T18:04:10Z",
          "was_summarised": false
        },
        {
          "id": "nyfp4wj",
          "author": "positivcheg",
          "content": "Nah. AMD wins, AMD mentioned AI 299 times. Big win for AMD.\n\nSadly it was just 1 more to a nice 300.",
          "created_at": "2026-01-08T17:40:13Z",
          "was_summarised": false
        },
        {
          "id": "nyet9oy",
          "author": "Agile_Date6729",
          "content": "someone should make a remix of it -would be a banger",
          "created_at": "2026-01-08T15:19:06Z",
          "was_summarised": false
        },
        {
          "id": "nyezpx4",
          "author": "XiRw",
          "content": "Willing to bet anything the future of gaming (based on prices going up and shortages) will be server based subscriptions like everything else out there. Youâ€™ll own nothing and be happy.",
          "created_at": "2026-01-08T15:48:33Z",
          "was_summarised": false
        },
        {
          "id": "nyfa87l",
          "author": "dasjati",
          "content": "Someone should put a nice beat under this!",
          "created_at": "2026-01-08T16:35:15Z",
          "was_summarised": false
        },
        {
          "id": "nyfu4de",
          "author": "nonaveris",
          "content": "ai ai ai Cthulhu fthangh",
          "created_at": "2026-01-08T18:01:45Z",
          "was_summarised": false
        },
        {
          "id": "nyfx33c",
          "author": "Amazing_Athlete_2265",
          "content": "AI? AI!",
          "created_at": "2026-01-08T18:14:33Z",
          "was_summarised": false
        },
        {
          "id": "nyfzi09",
          "author": "HerrGronbar",
          "content": "so without AI how long it was?",
          "created_at": "2026-01-08T18:24:50Z",
          "was_summarised": false
        },
        {
          "id": "nyg250o",
          "author": "pierrenoir2017",
          "content": "A I Caramba",
          "created_at": "2026-01-08T18:36:06Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://v.redd.it/hein55gpx4cg1",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://v.redd.it/hein55gpx4cg1\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://github.com/OpenAgentPlatform/Dive",
          "was_fetched": true,
          "page": "Title: GitHub - OpenAgentPlatform/Dive: Dive is an open-source MCP Host Desktop Application that seamlessly integrates with any LLMs supporting function calling capabilities. âœ¨\n\nURL Source: https://github.com/OpenAgentPlatform/Dive\n\nMarkdown Content:\nGitHub - OpenAgentPlatform/Dive: Dive is an open-source MCP Host Desktop Application that seamlessly integrates with any LLMs supporting function calling capabilities. âœ¨\n===============\n\n[Skip to content](https://github.com/OpenAgentPlatform/Dive#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FOpenAgentPlatform%2FDive)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FOpenAgentPlatform%2FDive)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=OpenAgentPlatform%2FDive)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/OpenAgentPlatform/Dive) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/OpenAgentPlatform/Dive) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/OpenAgentPlatform/Dive) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[OpenAgentPlatform](https://github.com/OpenAgentPlatform)/**[Dive](https://github.com/OpenAgentPlatform/Dive)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2FOpenAgentPlatform%2FDive)You must be signed in to change notification settings\n*   [Fork 157](https://github.com/login?return_to=%2FOpenAgentPlatform%2FDive)\n*   [Star 1.7k](https://github.com/login?return_to=%2FOpenAgentPlatform%2FDive) \n\nDive is an open-source MCP Host Desktop Application that seamlessly integrates with any LLMs supporting function calling capabilities. âœ¨\n\n### License\n\n[MIT license](https://github.com/OpenAgentPlatform/Dive/blob/main/LICENSE)\n\n[1.7k stars](https://github.com/OpenAgentPlatform/Dive/stargazers)[157 forks](https://github.com/OpenAgentPlatform/Dive/forks)[Branches](https://github.com/OpenAgentPlatform/Dive/branches)[Tags](https://github.com/OpenAgentPlatform/Dive/tags)[Activity](https://github.com/OpenAgentPlatform/Dive/activity)\n\n[Star](https://github.com/login?return_to=%2FOpenAgentPlatform%2FDive)\n\n[Notifications](https://github.com/login?return_to=%2FOpenAgentPlatform%2FDive)You must be signed in to change notification settings\n\n*   [Code](https://github.com/OpenAgentPlatform/Dive)\n*   [Issues 21](https://github.com/OpenAgentPlatform/Dive/issues)\n*   [Pull requests 6](https://github.com/OpenAgentPlatform/Dive/pulls)\n*   [Discussions](https://github.com/OpenAgentPlatform/Dive/discussions)\n*   [Actions](https://github.com/OpenAgentPlatform/Dive/actions)\n*   [Projects 0](https://github.com/OpenAgentPlatform/Dive/projects)\n*   [Security 2](https://github.com/OpenAgentPlatform/Dive/security)[](https://github.com/OpenAgentPlatform/Dive/security)[](https://github.com/OpenAgentPlatform/Dive/security)[](https://github.com/OpenAgentPlatform/Dive/security)[### Uh oh!](https://github.com/OpenAgentPlatform/Dive/security)\n[There was an error while loading.](https://github.com/OpenAgentPlatform/Dive/security)[Please reload this page](https://github.com/OpenAgentPlatform/Dive).    \n*   [Insights](https://github.com/OpenAgentPlatform/Dive/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/OpenAgentPlatform/Dive)\n*   [Issues](https://github.com/OpenAgentPlatform/Dive/issues)\n*   [Pull requests](https://github.com/OpenAgentPlatform/Dive/pulls)\n*   [Discussions](https://github.com/OpenAgentPlatform/Dive/discussions)\n*   [Actions](https://github.com/OpenAgentPlatform/Dive/actions)\n*   [Projects](https://github.com/OpenAgentPlatform/Dive/projects)\n*   [Security](https://github.com/OpenAgentPlatform/Dive/security)\n*   [Insights](https://github.com/OpenAgentPlatform/Dive/pulse)\n\nOpenAgentPlatform/Dive\n======================\n\nmain\n\n[**13**Branches](https://github.com/OpenAgentPlatform/Dive/branches)[**56**Tags](https://github.com/OpenAgentPlatform/Dive/tags)\n\n[](https://github.com/OpenAgentPlatform/Dive/branches)[](https://github.com/OpenAgentPlatform/Dive/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- [](https://github.com/ckaznable)[ckaznable](https://github.com/OpenAgentPlatform/Dive/commits?author=ckaznable) [chore: update README](https://github.com/OpenAgentPlatform/Dive/commit/8cf01b266fc1f91b1ed8e7ceaf59191bdb80e7cf) Jan 9, 2026 [8cf01b2](https://github.com/OpenAgentPlatform/Dive/commit/8cf01b266fc1f91b1ed8e7ceaf59191bdb80e7cf)Â·Jan 9, 2026 History ------- [1,411 Commits](https://github.com/OpenAgentPlatform/Dive/commits/main/) Open commit details [](https://github.com/OpenAgentPlatform/Dive/commits/main/) |\n| [.github](https://github.com/OpenAgentPlatform/Dive/tree/main/.github \".github\") | [.github](https://github.com/OpenAgentPlatform/Dive/tree/main/.github \".github\") | [chore: fix ci](https://github.com/OpenAgentPlatform/Dive/commit/9f5d17853d032620e747240a4113a45d82f36736 \"chore: fix ci\") | Dec 26, 2025 |\n| [.vscode](https://github.com/OpenAgentPlatform/Dive/tree/main/.vscode \".vscode\") | [.vscode](https://github.com/OpenAgentPlatform/Dive/tree/main/.vscode \".vscode\") | [feat: add tauri backend](https://github.com/OpenAgentPlatform/Dive/commit/b3281154517446cba99556bd03e67177897a725a \"feat: add tauri backend\") | Jul 28, 2025 |\n| [build](https://github.com/OpenAgentPlatform/Dive/tree/main/build \"build\") | [build](https://github.com/OpenAgentPlatform/Dive/tree/main/build \"build\") | [chore: simplify Windows installer configuration](https://github.com/OpenAgentPlatform/Dive/commit/7e05d02a086873323b5c5dd43f6e9842262134a3 \"chore: simplify Windows installer configuration - Remove custom NSIS installer script - Add `selectPerMachineByDefault` option to electron-builder config - Streamline installation directory selection\") | Feb 5, 2025 |\n| [docker](https://github.com/OpenAgentPlatform/Dive/tree/main/docker \"docker\") | [docker](https://github.com/OpenAgentPlatform/Dive/tree/main/docker \"docker\") | [chore: add libunwind-dev dependency to release workflow and Dockerfile](https://github.com/OpenAgentPlatform/Dive/commit/d761ff938a0f25f99b7344e56220349773936e65 \"chore: add libunwind-dev dependency to release workflow and Dockerfile\") | Sep 1, 2025 |\n| [docs](https://github.com/OpenAgentPlatform/Dive/tree/main/docs \"docs\") | [docs](https://github.com/OpenAgentPlatform/Dive/tree/main/docs \"docs\") | [update MCP configuration screenshot](https://github.com/OpenAgentPlatform/Dive/commit/e8fc4052f4dcc8d4e244372e016ba143fb431612 \"update MCP configuration screenshot\") | Apr 22, 2025 |\n| [electron](https://github.com/OpenAgentPlatform/Dive/tree/main/electron \"electron\") | [electron](https://github.com/OpenAgentPlatform/Dive/tree/main/electron \"electron\") | [fix: enable mcp host log in electron](https://github.com/OpenAgentPlatform/Dive/commit/d74a6cdbac0b7a4ce00aae15f67c284faf85b0ae \"fix: enable mcp host log in electron\") | Dec 30, 2025 |\n| [mcp-host @ 931614b](https://github.com/OpenAgentPlatform/dive-mcp-host/tree/931614bee6d48e8757a5519c9f9bbdb2b8dfa3fb \"mcp-host\") | [mcp-host @ 931614b](https://github.com/OpenAgentPlatform/dive-mcp-host/tree/931614bee6d48e8757a5519c9f9bbdb2b8dfa3fb \"mcp-host\") | [chore: bump mcp host](https://github.com/OpenAgentPlatform/Dive/commit/8e6e5823ac3319ecbce68df1b612842b39e3e3e6 \"chore: bump mcp host\") | Jan 8, 2026 |\n| [packages](https://github.com/OpenAgentPlatform/Dive/tree/main/packages \"packages\") | [packages](https://github.com/OpenAgentPlatform/Dive/tree/main/packages \"packages\") | [feat: add the elicitation handler](https://github.com/OpenAgentPlatform/Dive/commit/500a1a12073b7cadd7ac1ea00fb5b0344efc5238 \"feat: add the elicitation handler Reviewed-on: https://git.biggo.com/john/dive/pulls/139 Co-authored-by: john \u003cckaznable@gmail.com\u003e Co-committed-by: john \u003cckaznable@gmail.com\u003e\") | Dec 26, 2025 |\n| [patches](https://github.com/OpenAgentPlatform/Dive/tree/main/patches \"patches\") | [patches](https://github.com/OpenAgentPlatform/Dive/tree/main/patches \"patches\") | [feat(break): replacement mcp client service with dive_mcp_host](https://github.com/OpenAgentPlatform/Dive/commit/5ffa0e29e7607fb7611670bb0cc28f6a2276f075 \"feat(break): replacement mcp client service with dive_mcp_host\") | Apr 18, 2025 |\n| [prebuilt](https://github.com/OpenAgentPlatform/Dive/tree/main/prebuilt \"prebuilt\") | [prebuilt](https://github.com/OpenAgentPlatform/Dive/tree/main/prebuilt \"prebuilt\") | [feat: add the elicitation handler](https://github.com/OpenAgentPlatform/Dive/commit/500a1a12073b7cadd7ac1ea00fb5b0344efc5238 \"feat: add the elicitation handler Reviewed-on: https://git.biggo.com/john/dive/pulls/139 Co-authored-by: john \u003cckaznable@gmail.com\u003e Co-committed-by: john \u003cckaznable@gmail.com\u003e\") | Dec 26, 2025 |\n| [public](https://github.com/OpenAgentPlatform/Dive/tree/main/public \"public\") | [public](https://github.com/OpenAgentPlatform/Dive/tree/main/public \"public\") | [fix: models provider list image src](https://github.com/OpenAgentPlatform/Dive/commit/f0b89f9a7664d9469f2cead20ba78ca0f9513d25 \"fix: models provider list image src\") | Oct 20, 2025 |\n| [resources/mac](https://github.com/OpenAgentPlatform/Dive/tree/main/resources/mac \"This path skips through empty directories\") | [resources/mac](https://github.com/OpenAgentPlatform/Dive/tree/main/resources/mac \"This path skips through empty directories\") | [chore: enhance macOS support and cross-platform path handling](https://github.com/OpenAgentPlatform/Dive/commit/453214913f573c1a2349c9e7983e8e2e4a93c980 \"chore: enhance macOS support and cross-platform path handling - Update macOS entitlements with additional security and network permissions - Modify electron-builder configuration to include macOS-specific settings - Improve cross-platform path modification and npm installation logic - Add platform-specific path handling in Electron main process - Update logging configuration to use consistent logging method\") | Feb 4, 2025 |\n| [scripts](https://github.com/OpenAgentPlatform/Dive/tree/main/scripts \"scripts\") | [scripts](https://github.com/OpenAgentPlatform/Dive/tree/main/scripts \"scripts\") | [feat: add the elicitation handler](https://github.com/OpenAgentPlatform/Dive/commit/500a1a12073b7cadd7ac1ea00fb5b0344efc5238 \"feat: add the elicitation handler Reviewed-on: https://git.biggo.com/john/dive/pulls/139 Co-authored-by: john \u003cckaznable@gmail.com\u003e Co-committed-by: john \u003cckaznable@gmail.com\u003e\") | Dec 26, 2025 |\n| [shared](https://github.com/OpenAgentPlatform/Dive/tree/main/shared \"shared\") | [shared](https://github.com/OpenAgentPlatform/Dive/tree/main/shared \"shared\") | [feat: hot key previous input history;close/shrink window](https://github.com/OpenAgentPlatform/Dive/commit/bcc7635f311f9a888c6fe8207b7bbcdb0703ebf4 \"feat: hot key previous input history;close/shrink window\") | Dec 9, 2025 |\n| [src-tauri](https://github.com/OpenAgentPlatform/Dive/tree/main/src-tauri \"src-tauri\") | [src-tauri](https://github.com/OpenAgentPlatform/Dive/tree/main/src-tauri \"src-tauri\") | [chore: bump version](https://github.com/OpenAgentPlatform/Dive/commit/30a12813e19c0bfccf6137172e43c524de5608e7 \"chore: bump version\") | Jan 8, 2026 |\n| [src](https://github.com/OpenAgentPlatform/Dive/tree/main/src \"src\") | [src](https://github.com/OpenAgentPlatform/Dive/tree/main/src \"src\") | [fix: llm provider loading at init phase](https://github.com/OpenAgentPlatform/Dive/commit/b50ef2de5477dcb4db141227897593592526e776 \"fix: llm provider loading at init phase\") | Jan 8, 2026 |\n| [types](https://github.com/OpenAgentPlatform/Dive/tree/main/types \"types\") | [types](https://github.com/OpenAgentPlatform/Dive/tree/main/types \"types\") | [feat: handle expired model](https://github.com/OpenAgentPlatform/Dive/commit/c02aa5d4efee19e8710ca781f308f25c220e8e2b \"feat: handle expired model\") | Dec 29, 2025 |\n| [.dockerignore](https://github.com/OpenAgentPlatform/Dive/blob/main/.dockerignore \".dockerignore\") | [.dockerignore](https://github.com/OpenAgentPlatform/Dive/blob/main/.dockerignore \".dockerignore\") | [feat: add tauri backend](https://github.com/OpenAgentPlatform/Dive/commit/b3281154517446cba99556bd03e67177897a725a \"feat: add tauri backend\") | Jul 28, 2025 |\n| [.env.example](https://github.com/OpenAgentPlatform/Dive/blob/main/.env.example \".env.example\") | [.env.example](https://github.com/OpenAgentPlatform/Dive/blob/main/.env.example \".env.example\") | [feat: support oaphub](https://github.com/OpenAgentPlatform/Dive/commit/a111319b3412cccfc3bf32f4394251725e627dcf \"feat: support oaphub\") | Jul 28, 2025 |\n| [.gitignore](https://github.com/OpenAgentPlatform/Dive/blob/main/.gitignore \".gitignore\") | [.gitignore](https://github.com/OpenAgentPlatform/Dive/blob/main/.gitignore \".gitignore\") | [feat: add the elicitation handler](https://github.com/OpenAgentPlatform/Dive/commit/500a1a12073b7cadd7ac1ea00fb5b0344efc5238 \"feat: add the elicitation handler Reviewed-on: https://git.biggo.com/john/dive/pulls/139 Co-authored-by: john \u003cckaznable@gmail.com\u003e Co-committed-by: john \u003cckaznable@gmail.com\u003e\") | Dec 26, 2025 |\n| [.gitmodules](https://github.com/OpenAgentPlatform/Dive/blob/main/.gitmodules \".gitmodules\") | [.gitmodules](https://github.com/OpenAgentPlatform/Dive/blob/main/.gitmodules \".gitmodules\") | [feat(break): replacement mcp client service with dive_mcp_host](https://github.com/OpenAgentPlatform/Dive/commit/5ffa0e29e7607fb7611670bb0cc28f6a2276f075 \"feat(break): replacement mcp client service with dive_mcp_host\") | Apr 18, 2025 |\n| [BUILD.md](https://github.com/OpenAgentPlatform/Dive/blob/main/BUILD.md \"BUILD.md\") | [BUILD.md](https://github.com/OpenAgentPlatform/Dive/blob/main/BUILD.md \"BUILD.md\") | [chore(docs): update document](https://github.com/OpenAgentPlatform/Dive/commit/5c73b82711b8eebf67942b33dca196b5aeee18ec \"chore(docs): update document\") | Apr 18, 2025 |\n| [Cargo.lock](https://github.com/OpenAgentPlatform/Dive/blob/main/Cargo.lock \"Cargo.lock\") | [Cargo.lock](https://github.com/OpenAgentPlatform/Dive/blob/main/Cargo.lock \"Cargo.lock\") | [chore: bump version](https://github.com/OpenAgentPlatform/Dive/commit/30a12813e19c0bfccf6137172e43c524de5608e7 \"chore: bump version\") | Jan 8, 2026 |\n| [Cargo.toml](https://github.com/OpenAgentPlatform/Dive/blob/main/Cargo.toml \"Cargo.toml\") | [Cargo.toml](https://github.com/OpenAgentPlatform/Dive/blob/main/Cargo.toml \"Cargo.toml\") | [feat: add the elicitation handler](https://github.com/OpenAgentPlatform/Dive/commit/500a1a12073b7cadd7ac1ea00fb5b0344efc5238 \"feat: add the elicitation handler Reviewed-on: https://git.biggo.com/john/dive/pulls/139 Co-authored-by: john \u003cckaznable@gmail.com\u003e Co-committed-by: john \u003cckaznable@gmail.com\u003e\") | Dec 26, 2025 |\n| [LICENSE](https://github.com/OpenAgentPlatform/Dive/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/OpenAgentPlatform/Dive/blob/main/LICENSE \"LICENSE\") | [chore: add LINCESE](https://github.com/OpenAgentPlatform/Dive/commit/6648ce3151144429f3ea78fc3681bf26178add89 \"chore: add LINCESE\") | Feb 5, 2025 |\n| [MCP_SETUP.md](https://github.com/OpenAgentPlatform/Dive/blob/main/MCP_SETUP.md \"MCP_SETUP.md\") | [MCP_SETUP.md](https://github.com/OpenAgentPlatform/Dive/blob/main/MCP_SETUP.md \"MCP_SETUP.md\") | [chore: add MCP_SETUP.md](https://github.com/OpenAgentPlatform/Dive/commit/7175278ca7a5313f03bf04b4689869c36fd3a66a \"chore: add MCP_SETUP.md\") | Oct 30, 2025 |\n| [README.md](https://github.com/OpenAgentPlatform/Dive/blob/main/README.md \"README.md\") | [README.md](https://github.com/OpenAgentPlatform/Dive/blob/main/README.md \"README.md\") | [chore: update README](https://github.com/OpenAgentPlatform/Dive/commit/8cf01b266fc1f91b1ed8e7ceaf59191bdb80e7cf \"chore: update README\") | Jan 9, 2026 |\n| [SECURITY.md](https://github.com/OpenAgentPlatform/Dive/blob/main/SECURITY.md \"SECURITY.md\") | [SECURITY.md](https://github.com/OpenAgentPlatform/Dive/blob/main/SECURITY.md \"SECURITY.md\") | [Create SECURITY.md](https://github.com/OpenAgentPlatform/Dive/commit/f1b132aa45d943e2278d79f3d6ae015c9e9026f0 \"Create SECURITY.md Create security page, to guide responsible vulnerability disclosure.\") | Aug 21, 2025 |\n| [electron-builder.json](https://github.com/OpenAgentPlatform/Dive/blob/main/electron-builder.json \"electron-builder.json\") | [electron-builder.json](https://github.com/OpenAgentPlatform/Dive/blob/main/electron-builder.json \"electron-builder.json\") | [feat: add the update endpoint to oaphub (](https://github.com/OpenAgentPlatform/Dive/commit/7347e6825c02f09f1dc9d2ca84ea9c7fce7e58bb \"feat: add the update endpoint to oaphub (#288) * feat: add ua for the request to oaphub * feat: add getClientInfo functionality to retrieve client version and ID - Exported CLIENT_ID from oap.ts for use in IPC. - Implemented getClientInfo in ipc/util.ts to return client version and ID. - Updated Tauri updater to include client information in the User-Agent header. - Exposed getClientInfo in preload/index.ts for renderer access. - Added corresponding command in Tauri backend to fetch client info. * feat: implement update server fallback mechanism - Added primary update server configuration using OAP_ROOT_URL. - Implemented fallback mechanism to switch to GitHub releases if the primary server fails. - Updated Tauri updater to include CLIENT_ID in the User-Agent header for requests. - Modified tauri.conf.json to point to the new primary update server endpoint. * fix: correct file size calculations in update progress and UI * chore: remove publish fields in electron-builder.json * chore: restore GitHub releases endpoint to Tauri updater configuration * feat: implement automatic update checking in Electron and Tauri updaters - Added configuration for automatic update checks in Electron, including initial delay and periodic checks. - Implemented a new function to perform update checks and handle errors silently. - Refactored Tauri updater to use a timer for periodic update checks instead of a one-time check. - Enhanced update handling logic to support auto-download for non-Darwin platforms. * fix: initiate update checking on component mount in Tauri updater - Added a call to checkUpdate within the useEffect hook to ensure updates are checked immediately upon component mount. * refactor: simplify User-Agent header for tauri * chore: update the download url for mac * chore: reorder update endpoints in Tauri configuration * chore: make sure app dir exsists * chore: create project dir before generate client id\")[#288](https://github.com/OpenAgentPlatform/Dive/pull/288)[)](https://github.com/OpenAgentPlatform/Dive/commit/7347e6825c02f09f1dc9d2ca84ea9c7fce7e58bb \"feat: add the update endpoint to oaphub (#288) * feat: add ua for the request to oaphub * feat: add getClientInfo functionality to retrieve client version and ID - Exported CLIENT_ID from oap.ts for use in IPC. - Implemented getClientInfo in ipc/util.ts to return client version and ID. - Updated Tauri updater to include client information in the User-Agent header. - Exposed getClientInfo in preload/index.ts for renderer access. - Added corresponding command in Tauri backend to fetch client info. * feat: implement update server fallback mechanism - Added primary update server configuration using OAP_ROOT_URL. - Implemented fallback mechanism to switch to GitHub releases if the primary server fails. - Updated Tauri updater to include CLIENT_ID in the User-Agent header for requests. - Modified tauri.conf.json to point to the new primary update server endpoint. * fix: correct file size calculations in update progress and UI * chore: remove publish fields in electron-builder.json * chore: restore GitHub releases endpoint to Tauri updater configuration * feat: implement automatic update checking in Electron and Tauri updaters - Added configuration for automatic update checks in Electron, including initial delay and periodic checks. - Implemented a new function to perform update checks and handle errors silently. - Refactored Tauri updater to use a timer for periodic update checks instead of a one-time check. - Enhanced update handling logic to support auto-download for non-Darwin platforms. * fix: initiate update checking on component mount in Tauri updater - Added a call to checkUpdate within the useEffect hook to ensure updates are checked immediately upon component mount. * refactor: simplify User-Agent header for tauri * chore: update the download url for mac * chore: reorder update endpoints in Tauri configuration * chore: make sure app dir exsists * chore: create project dir before generate client id\") | Oct 30, 2025 |\n| [eslint.config.js](https://github.com/OpenAgentPlatform/Dive/blob/main/eslint.config.js \"eslint.config.js\") | [eslint.config.js](https://github.com/OpenAgentPlatform/Dive/blob/main/eslint.config.js \"eslint.config.js\") | [chore: disable '@typescript-eslint/ban-ts-comment' rule in ESLint conâ€¦](https://github.com/OpenAgentPlatform/Dive/commit/3f80ba3c941a3de4c88452b3bf6e3d9e2e877f2d \"chore: disable '@typescript-eslint/ban-ts-comment' rule in ESLint configuration\") | Jul 28, 2025 |\n| [index.html](https://github.com/OpenAgentPlatform/Dive/blob/main/index.html \"index.html\") | [index.html](https://github.com/OpenAgentPlatform/Dive/blob/main/index.html \"index.html\") | [feat: add welcome page](https://github.com/OpenAgentPlatform/Dive/commit/4817ca0d17e5ebb352fd0b25e83d2dcad7d97c20 \"feat: add welcome page\") | Jan 3, 2025 |\n| [package-lock.json](https://github.com/OpenAgentPlatform/Dive/blob/main/package-lock.json \"package-lock.json\") | [package-lock.json](https://github.com/OpenAgentPlatform/Dive/blob/main/package-lock.json \"package-lock.json\") | [chore: bump version](https://github.com/OpenAgentPlatform/Dive/commit/30a12813e19c0bfccf6137172e43c524de5608e7 \"chore: bump version\") | Jan 8, 2026 |\n| [package.json](https://github.com/OpenAgentPlatform/Dive/blob/main/package.json \"package.json\") | [package.json](https://github.com/OpenAgentPlatform/Dive/blob/main/package.json \"package.json\") | [chore: bump version](https://github.com/OpenAgentPlatform/Dive/commit/30a12813e19c0bfccf6137172e43c524de5608e7 \"chore: bump version\") | Jan 8, 2026 |\n| [tsconfig.app.json](https://github.com/OpenAgentPlatform/Dive/blob/main/tsconfig.app.json \"tsconfig.app.json\") | [tsconfig.app.json](https://github.com/OpenAgentPlatform/Dive/blob/main/tsconfig.app.json \"tsconfig.app.json\") | [feat: add tauri backend](https://github.com/OpenAgentPlatform/Dive/commit/b3281154517446cba99556bd03e67177897a725a \"feat: add tauri backend\") | Jul 28, 2025 |\n| [tsconfig.json](https://github.com/OpenAgentPlatform/Dive/blob/main/tsconfig.json \"tsconfig.json\") | [tsconfig.json](https://github.com/OpenAgentPlatform/Dive/blob/main/tsconfig.json \"tsconfig.json\") | [init](https://github.com/OpenAgentPlatform/Dive/commit/614cf98d307baf3dabcf768b43f061ba509c2181 \"init\") | Jan 3, 2025 |\n| [tsconfig.node.json](https://github.com/OpenAgentPlatform/Dive/blob/main/tsconfig.node.json \"tsconfig.node.json\") | [tsconfig.node.json](https://github.com/OpenAgentPlatform/Dive/blob/main/tsconfig.node.json \"tsconfig.node.json\") | [feat(break): replacement mcp client service with dive_mcp_host](https://github.com/OpenAgentPlatform/Dive/commit/5ffa0e29e7607fb7611670bb0cc28f6a2276f075 \"feat(break): replacement mcp client service with dive_mcp_host\") | Apr 18, 2025 |\n| [vite.config.electron.ts](https://github.com/OpenAgentPlatform/Dive/blob/main/vite.config.electron.ts \"vite.config.electron.ts\") | [vite.config.electron.ts](https://github.com/OpenAgentPlatform/Dive/blob/main/vite.config.electron.ts \"vite.config.electron.ts\") | [feat: add tauri backend](https://github.com/OpenAgentPlatform/Dive/commit/b3281154517446cba99556bd03e67177897a725a \"feat: add tauri backend\") | Jul 28, 2025 |\n| [vite.config.ts](https://github.com/OpenAgentPlatform/Dive/blob/main/vite.config.ts \"vite.config.ts\") | [vite.config.ts](https://github.com/OpenAgentPlatform/Dive/blob/main/vite.config.ts \"vite.config.ts\") | [feat: add tauri backend](https://github.com/OpenAgentPlatform/Dive/commit/b3281154517446cba99556bd03e67177897a725a \"feat: add tauri backend\") | Jul 28, 2025 |\n| [vitest.config.ts](https://github.com/OpenAgentPlatform/Dive/blob/main/vitest.config.ts \"vitest.config.ts\") | [vitest.config.ts](https://github.com/OpenAgentPlatform/Dive/blob/main/vitest.config.ts \"vitest.config.ts\") | [feat: init electron](https://github.com/OpenAgentPlatform/Dive/commit/354f7deba20e70bf1862758bcc8a405db48483e2 \"feat: init electron\") | Jan 14, 2025 |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/OpenAgentPlatform/Dive#)\n*   [MIT license](https://github.com/OpenAgentPlatform/Dive#)\n*   [Security](https://github.com/OpenAgentPlatform/Dive#)\n\n[](https://github.com/OpenAgentPlatform/Dive/blob/main/build/icon.png)\n\nDive AI Agent\n=============\n\n[](https://github.com/OpenAgentPlatform/Dive#dive-ai-agent)\n\n[](https://camo.githubusercontent.com/c10e80de95f723a1f9b230597ae4d7d9998b4028eef6fd23d191ec117d40fe1d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e4167656e74506c6174666f726d2f446976653f7374796c653d736f6369616c)[](https://camo.githubusercontent.com/7cffb5cdc41f9f948577bc505e4610718880d2c0afc61771a23313fa0042c6c8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f4f70656e4167656e74506c6174666f726d2f446976653f7374796c653d736f6369616c)[](https://camo.githubusercontent.com/b8a30243600b60a0950db559805851c4d30b860b2ea81ddcd7fe2be425b8f24c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f77617463686572732f4f70656e4167656e74506c6174666f726d2f446976653f7374796c653d736f6369616c)[](https://camo.githubusercontent.com/e9c0d52442921d08b883d729d61327d58015cb9a6e769591752995a98a116b2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f4f70656e4167656e74506c6174666f726d2f44697665)[](https://camo.githubusercontent.com/5d4468563dd1ab34162ea31ba47f519735030ca4c7d47ed98e30140b59a20ffb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f636f756e742f4f70656e4167656e74506c6174666f726d2f44697665)[](https://camo.githubusercontent.com/6770ccf8e4eb91353a96dd694b6b3f4237080254b7518eb9e1003942bcc3bb9c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f746f702f4f70656e4167656e74506c6174666f726d2f44697665)[](https://camo.githubusercontent.com/c4c2026a50a031a181bd840d4e0bafcce60bff26735ebd6808a31b90baf20f33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f4f70656e4167656e74506c6174666f726d2f446976653f636f6c6f723d726564)[](https://discord.gg/xaV7xzMYBA)[](https://twitter.com/Dive_ai_agent)\n\nDive is an open-source MCP Host Desktop Application that seamlessly integrates with any LLMs supporting function calling capabilities. âœ¨\n\n[](https://github.com/OpenAgentPlatform/Dive/blob/main/docs/0.8.0_DiveGIF.gif)[](https://github.com/OpenAgentPlatform/Dive/blob/main/docs/0.8.0_DiveGIF.gif)[](https://github.com/OpenAgentPlatform/Dive/blob/main/docs/0.8.0_DiveGIF.gif)\n\nFeatures ðŸŽ¯\n-----------\n\n[](https://github.com/OpenAgentPlatform/Dive#features-)\n\n*   ðŸŒ **Universal LLM Support**: Compatible with ChatGPT, Anthropic, Ollama and OpenAI-compatible models\n*   ðŸ’» **Cross-Platform**: Available for Windows, MacOS, and Linux\n*   ðŸ”„ **Model Context Protocol**: Enabling seamless MCP AI agent integration on both stdio and SSE mode\n*   â˜ï¸ **OAP Cloud Integration**: One-click access to managed MCP servers via [OAPHub.ai](https://oaphub.ai/) - eliminates complex local deployments\n*   ðŸ—ï¸ **Dual Architecture**: Modern Tauri version alongside traditional Electron version for optimal performance\n*   ðŸŒ **Multi-Language Support**: Supports 24+ languages including English, Traditional Chinese, Simplified Chinese, Spanish, Japanese, Korean, German, French, Italian, Portuguese, Russian, Thai, Vietnamese, Filipino, Indonesian, Polish, Turkish, Ukrainian, Swedish, Norwegian, Finnish, and Lao\n*   âš™ï¸ **Advanced API Management**: Multiple API keys and model switching support with `model_settings.json`\n*   ðŸ› ï¸ **Granular Tool Control**: Enable/disable individual MCP tools for precise customization\n*   ðŸ’¡ **Custom Instructions**: Personalized system prompts for tailored AI behavior\n*   âŒ¨ï¸ **Keyboard Shortcuts**: Comprehensive hotkey support for efficient navigation and operations (rename, settings, reload, new chat, etc.)\n*   ðŸ“ **Chat Draft Saving**: Automatically saves chat input drafts to prevent data loss\n*   ðŸ”„ **Auto-Update Mechanism**: Automatically checks for and installs the latest application updates\n*   ðŸ” **MCP Server Authentication**: Added support for MCP server authentication \n\u003e âš ï¸**Note**: This feature is currently unstable and may require frequent re-authorization\n\nRecent updates(2026/01/08) - v0.12.5 ðŸŽ‰\n---------------------------------------\n\n[](https://github.com/OpenAgentPlatform/Dive#recent-updates20260108---v0125-)\n\n*   ðŸ› ï¸ **Built-in Local Tools**: Pre-configured tools available out of the box - Fetch (web requests), File Manager (read/write files), and Bash (command execution)\n*   ðŸ¤– **MCP Server Installer Agent**: Intelligent agent that helps you install and configure MCP servers automatically\n*   ðŸ”” **Multiple Elicitation Support**: Handle multiple MCP elicitation requests simultaneously in the UI\n\n### Platform Availability\n\n[](https://github.com/OpenAgentPlatform/Dive#platform-availability)\n\n| Platform | Electron | Tauri |\n| :--- | :---: | :---: |\n| **Windows** | âœ… | âœ… |\n| **macOS** | âœ… | ðŸ”œ |\n| **Linux** | âœ… | âœ… |\n\n\u003e **Migration Note:** Existing local MCP/LLM configurations remain fully supported. OAP integration is additive and does not affect current workflows.\n\nDownload and Install â¬‡ï¸\n-----------------------\n\n[](https://github.com/OpenAgentPlatform/Dive#download-and-install-%EF%B8%8F)\n\nGet the latest version of Dive: [](https://github.com/OpenAgentPlatform/Dive/releases/latest)\n\n### Windows users: ðŸªŸ\n\n[](https://github.com/OpenAgentPlatform/Dive#windows-users-)\n\nChoose between two architectures:\n\n*   **Tauri Version** (Recommended): Smaller installer (\u003c30MB), modern architecture\n*   **Electron Version**: Traditional architecture, fully stable\n*   Python and Node.js environments will be downloaded automatically after launching\n\n### MacOS users: ðŸŽ\n\n[](https://github.com/OpenAgentPlatform/Dive#macos-users-)\n\n*   **Electron Version**: Download the .dmg version\n*   You need to install Python and Node.js (with npx uvx) environments yourself\n*   Follow the installation prompts to complete setup\n\n### Linux users: ðŸ§\n\n[](https://github.com/OpenAgentPlatform/Dive#linux-users-)\n\nChoose between two architectures:\n\n*   **Tauri Version** (Recommended): Modern architecture with smaller installer size\n*   **Electron Version**: Traditional architecture with .AppImage format\n*   You need to install Python and Node.js (with npx uvx) environments yourself\n*   For Ubuntu/Debian users: \n    *   You may need to add `--no-sandbox` parameter\n    *   Or modify system settings to allow sandbox\n    *   Run `chmod +x` to make the AppImage executable\n\n*   For Arch users: \n    *   If you are using Arch Linux, you can install dive using an [AUR helper](https://wiki.archlinux.org/title/AUR_helpers). For example: `paru -S dive-ai`\n\nMCP Setup Options\n-----------------\n\n[](https://github.com/OpenAgentPlatform/Dive#mcp-setup-options)\n\nFor more detailed instructions, please see [MCP Servers Setup](https://github.com/OpenAgentPlatform/Dive/blob/main/MCP_SETUP.md).\n\nThe easiest way to get started! Access enterprise-grade MCP tools instantly:\n\n1.   **Sign up** at [OAPHub.ai](https://oaphub.ai/)\n2.   **Connect** to Dive using one-click deep links or configuration files\n3.   **Enjoy** managed MCP servers with zero setup - no Python, Docker, or complex dependencies required\n\nBenefits:\n\n*   âœ… Zero configuration needed\n*   âœ… Cross-platform compatibility\n*   âœ… Enterprise-grade reliability\n*   âœ… Automatic updates and maintenance\n\nBuild ðŸ› ï¸\n---------\n\n[](https://github.com/OpenAgentPlatform/Dive#build-%EF%B8%8F)\n\nSee [BUILD.md](https://github.com/OpenAgentPlatform/Dive/blob/main/BUILD.md) for more details.\n\nContributing ðŸ¤\n---------------\n\n[](https://github.com/OpenAgentPlatform/Dive#contributing-)\n\nWe welcome contributions from the community! Here's how you can help:\n\n### Development Setup\n\n[](https://github.com/OpenAgentPlatform/Dive#development-setup)\n\n1.   Fork the repository\n2.   Clone your fork: `git clone https://github.com/YOUR_USERNAME/Dive.git`\n3.   Install dependencies: `npm install`\n4.   Start development: `npm run dev` (Electron) or `cargo tauri dev` (Tauri)\n5.   Make your changes and test thoroughly\n6.   Submit a pull request\n\nLicense ðŸ“„\n----------\n\n[](https://github.com/OpenAgentPlatform/Dive#license-)\n\nDive is open-source software licensed under the [MIT License](https://github.com/OpenAgentPlatform/Dive/blob/main/LICENSE).\n\nConnect With Us ðŸŒ\n------------------\n\n[](https://github.com/OpenAgentPlatform/Dive#connect-with-us-)\n\n*   ðŸ’¬ Join our [Discord](https://discord.gg/xaV7xzMYBA)\n*   ðŸ¦ Follow us on [Twitter/X](https://x.com/Dive_ai_agent)[Reddit](https://www.reddit.com/user/BigGo_official/)[Thread](https://www.threads.net/@dive_mcpserver)\n*   â­ Star us on GitHub\n*   ðŸ› Report issues on our [Issue Tracker](https://github.com/OpenAgentPlatform/Dive/issues)\n\nAbout\n-----\n\nDive is an open-source MCP Host Desktop Application that seamlessly integrates with any LLMs supporting function calling capabilities. âœ¨\n\n### Topics\n\n[ai](https://github.com/topics/ai \"Topic: ai\")[ai-agents](https://github.com/topics/ai-agents \"Topic: ai-agents\")[ollama](https://github.com/topics/ollama \"Topic: ollama\")[llm-ui](https://github.com/topics/llm-ui \"Topic: llm-ui\")[ollama-ui](https://github.com/topics/ollama-ui \"Topic: ollama-ui\")[ollama-client](https://github.com/topics/ollama-client \"Topic: ollama-client\")[llm-interface](https://github.com/topics/llm-interface \"Topic: llm-interface\")[mcp-server](https://github.com/topics/mcp-server \"Topic: mcp-server\")[mcp-client](https://github.com/topics/mcp-client \"Topic: mcp-client\")[mcp-host](https://github.com/topics/mcp-host \"Topic: mcp-host\")\n\n### Resources\n\n[Readme](https://github.com/OpenAgentPlatform/Dive#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/OpenAgentPlatform/Dive#MIT-1-ov-file)\n\n### Security policy\n\n[Security policy](https://github.com/OpenAgentPlatform/Dive#security-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/OpenAgentPlatform/Dive).\n\n[Activity](https://github.com/OpenAgentPlatform/Dive/activity)\n\n[Custom properties](https://github.com/OpenAgentPlatform/Dive/custom-properties)\n\n### Stars\n\n[**1.7k** stars](https://github.com/OpenAgentPlatform/Dive/stargazers)\n\n### Watchers\n\n[**18** watching](https://github.com/OpenAgentPlatform/Dive/watchers)\n\n### Forks\n\n[**157** forks](https://github.com/OpenAgentPlatform/Dive/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FOpenAgentPlatform%2FDive\u0026report=OpenAgentPlatform+%28user%29)\n\n[Releases 56](https://github.com/OpenAgentPlatform/Dive/releases)\n-----------------------------------------------------------------\n\n[v0.12.5 Latest Jan 8, 2026](https://github.com/OpenAgentPlatform/Dive/releases/tag/v0.12.5)\n\n[+ 55 releases](https://github.com/OpenAgentPlatform/Dive/releases)\n\n[Packages 0](https://github.com/orgs/OpenAgentPlatform/packages?repo_name=Dive)\n-------------------------------------------------------------------------------\n\n No packages published \n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/OpenAgentPlatform/Dive).\n\n[Contributors 12](https://github.com/OpenAgentPlatform/Dive/graphs/contributors)\n--------------------------------------------------------------------------------\n\n*   [](https://github.com/johnfunmula)\n*   [](https://github.com/ckaznable)\n*   [](https://github.com/johnaa0219)\n*   [](https://github.com/tuo-funmula)\n*   [](https://github.com/kevinwatt)\n*   [](https://github.com/apps/dependabot)\n*   [](https://github.com/OpenBigGo)\n*   [](https://github.com/claude)\n*   [](https://github.com/nicolaskrier)\n*   [](https://github.com/josx)\n*   [](https://github.com/shiena)\n*   [](https://github.com/Suuuuuzy)\n\nLanguages\n---------\n\n*   [TypeScript 72.6%](https://github.com/OpenAgentPlatform/Dive/search?l=typescript)\n*   [SCSS 14.8%](https://github.com/OpenAgentPlatform/Dive/search?l=scss)\n*   [Rust 11.4%](https://github.com/OpenAgentPlatform/Dive/search?l=rust)\n*   [Shell 0.5%](https://github.com/OpenAgentPlatform/Dive/search?l=shell)\n*   [JavaScript 0.5%](https://github.com/OpenAgentPlatform/Dive/search?l=javascript)\n*   [Dockerfile 0.1%](https://github.com/OpenAgentPlatform/Dive/search?l=dockerfile)\n*   Other 0.1%\n\nFooter\n------\n\n[](https://github.com/) Â© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You canâ€™t perform that action at this time.",
          "was_summarised": false
        },
        {
          "url": "https://github.com/kevinwatt/yt-dlp-mcp",
          "was_fetched": true,
          "page": "Title: GitHub - kevinwatt/yt-dlp-mcp: A Model Context Protocol (MCP) server that bridges Video \u0026 Audio content with Large Language Models using yt-dlp.\n\nURL Source: https://github.com/kevinwatt/yt-dlp-mcp\n\nMarkdown Content:\nðŸŽ¬ yt-dlp-mcp\n-------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-yt-dlp-mcp)\n\n**A powerful MCP server that brings video platform capabilities to your AI agents**\n\n[](https://www.npmjs.com/package/@kevinwatt/yt-dlp-mcp)[](https://opensource.org/licenses/MIT)[](https://nodejs.org/)[](https://www.typescriptlang.org/)\n\nIntegrate yt-dlp with Claude, Dive, and other MCP-compatible AI systems. Download videos, extract metadata, get transcripts, and more â€” all through natural language.\n\n[Features](https://github.com/kevinwatt/yt-dlp-mcp#-features) â€¢ [Installation](https://github.com/kevinwatt/yt-dlp-mcp#-installation) â€¢ [Tools](https://github.com/kevinwatt/yt-dlp-mcp#-available-tools) â€¢ [Usage](https://github.com/kevinwatt/yt-dlp-mcp#-usage-examples) â€¢ [Documentation](https://github.com/kevinwatt/yt-dlp-mcp#-documentation)\n\n* * *\n\nâœ¨ Features\n----------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-features)\n### ðŸ” **Search \u0026 Discovery**\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-search--discovery)\n*   Search YouTube with pagination\n*   JSON or Markdown output formats\n*   Filter by relevance and quality\n\n### ðŸ“Š **Metadata Extraction**\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-metadata-extraction)\n*   Comprehensive video information\n*   Channel details and statistics\n*   Upload dates, tags, categories\n*   No content download required\n\n### ðŸ“ **Transcript \u0026 Subtitles**\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-transcript--subtitles)\n*   Download subtitles in VTT format\n*   Generate clean text transcripts\n*   Multi-language support\n*   Auto-generated captions### ðŸŽ¥ **Video Downloads**\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-video-downloads)\n*   Resolution control (480p-1080p)\n*   Video trimming support\n*   Platform-agnostic (YouTube, Facebook, etc.)\n*   Saved to Downloads folder\n\n### ðŸŽµ **Audio Extraction**\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-audio-extraction)\n*   Best quality audio (M4A/MP3)\n*   Direct audio-only downloads\n*   Perfect for podcasts \u0026 music\n\n### ðŸ›¡ï¸ **Privacy \u0026 Safety**\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#%EF%B8%8F-privacy--safety)\n*   No tracking or analytics\n*   Direct downloads via yt-dlp\n*   Zod schema validation\n*   Character limits for LLM safety\n\n* * *\n\nðŸš€ Installation\n---------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-installation)\n### Prerequisites\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#prerequisites)\n**Install yt-dlp** on your system:\n\n| Platform | Command |\n| --- | --- |\n| ðŸªŸ **Windows** | `winget install yt-dlp` |\n| ðŸŽ **macOS** | `brew install yt-dlp` |\n| ðŸ§ **Linux** | `pip install yt-dlp` |\n\n### Getting Started\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#getting-started)\nAdd the following config to your MCP client:\n\n{\n  \"mcpServers\": {\n    \"yt-dlp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kevinwatt/yt-dlp-mcp@latest\"]\n    }\n  }\n}\n\n### MCP Client Configuration\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#mcp-client-configuration)\n\n**Dive**\n1.   Open [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n2.   Click **\"+ Add MCP Server\"**\n3.   Paste the config provided above\n4.   Click **\"Save\"** and you're ready!\n\n**Claude Code**\nUse the Claude Code CLI to add the yt-dlp MCP server ([guide](https://docs.anthropic.com/en/docs/claude-code/mcp)):\n\nclaude mcp add yt-dlp npx @kevinwatt/yt-dlp-mcp@latest\n\n**Claude Desktop**\nAdd to your `claude_desktop_config.json`:\n\n*   macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n*   Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n*   Linux: `~/.config/Claude/claude_desktop_config.json`\n\n{\n  \"mcpServers\": {\n    \"yt-dlp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kevinwatt/yt-dlp-mcp@latest\"]\n    }\n  }\n}\n\n**Cursor**\nGo to `Cursor Settings` -\u003e`MCP` -\u003e`New MCP Server`. Use the config provided above.\n\n**VS Code / Copilot**\nInstall via the VS Code CLI:\n\ncode --add-mcp '{\"name\":\"yt-dlp\",\"command\":\"npx\",\"args\":[\"-y\",\"@kevinwatt/yt-dlp-mcp@latest\"]}'\n\nOr follow the [MCP install guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server) with the standard config from above.\n\n**Windsurf**\nFollow the [configure MCP guide](https://docs.windsurf.com/windsurf/cascade/mcp#mcp-config-json) using the standard config from above.\n\n**Cline**\nFollow [Cline MCP configuration guide](https://docs.cline.bot/mcp/configuring-mcp-servers) and use the config provided above.\n\n**Warp**\nGo to `Settings | AI | Manage MCP Servers` -\u003e`+ Add` to [add an MCP Server](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server). Use the config provided above.\n\n**JetBrains AI Assistant**\nGo to `Settings | Tools | AI Assistant | Model Context Protocol (MCP)` -\u003e`Add`. Use the config provided above.\n\n### Manual Installation\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#manual-installation)\n\nnpm install -g @kevinwatt/yt-dlp-mcp\n\n* * *\n\nðŸ› ï¸ Available Tools\n-------------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#%EF%B8%8F-available-tools)\nAll tools are prefixed with `ytdlp_` to avoid naming conflicts with other MCP servers.\n\n### ðŸ” Search \u0026 Discovery\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-search--discovery-1)\n| Tool | Description |\n| --- | --- |\n| `ytdlp_search_videos` | Search YouTube with pagination and date filtering support * **Parameters**: `query`, `maxResults`, `offset`, `response_format`, `uploadDateFilter` * **Date Filter**: `hour`, `today`, `week`, `month`, `year` (optional) * **Returns**: Video list with titles, channels, durations, URLs * **Supports**: JSON and Markdown formats |\n\n### ðŸ“ Subtitles \u0026 Transcripts\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-subtitles--transcripts)\n| Tool | Description |\n| --- | --- |\n| `ytdlp_list_subtitle_languages` | List all available subtitle languages for a video * **Parameters**: `url` * **Returns**: Available languages, formats, auto-generated status |\n| `ytdlp_download_video_subtitles` | Download subtitles in VTT format with timestamps * **Parameters**: `url`, `language` (optional) * **Returns**: Raw VTT subtitle content |\n| `ytdlp_download_transcript` | Generate clean plain text transcript * **Parameters**: `url`, `language` (optional) * **Returns**: Cleaned text without timestamps or formatting |\n\n### ðŸŽ¥ Video \u0026 Audio Downloads\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-video--audio-downloads)\n| Tool | Description |\n| --- | --- |\n| `ytdlp_download_video` | Download video to Downloads folder * **Parameters**: `url`, `resolution`, `startTime`, `endTime` * **Resolutions**: 480p, 720p, 1080p, best * **Supports**: Video trimming |\n| `ytdlp_download_audio` | Extract and download audio only * **Parameters**: `url` * **Format**: Best quality M4A/MP3 |\n\n### ðŸ“Š Metadata\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-metadata)\n| Tool | Description |\n| --- | --- |\n| `ytdlp_get_video_metadata` | Extract comprehensive video metadata in JSON * **Parameters**: `url`, `fields` (optional array) * **Returns**: Complete metadata or filtered fields * **Includes**: Views, likes, upload date, tags, formats, etc. |\n| `ytdlp_get_video_metadata_summary` | Get human-readable metadata summary * **Parameters**: `url` * **Returns**: Formatted text with key information |\n\n* * *\n\nðŸ’¡ Usage Examples\n-----------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-usage-examples)\n### Search Videos\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#search-videos)\n\n```\n\"Search for Python programming tutorials\"\n\"Find the top 20 machine learning videos\"\n\"Search for 'react hooks tutorial' and show results 10-20\"\n\"Search for JavaScript courses in JSON format\"\n```\n\n### Get Metadata\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#get-metadata)\n\n```\n\"Get metadata for https://youtube.com/watch?v=...\"\n\"Show me the title, channel, and view count for this video\"\n\"Extract just the duration and upload date\"\n\"Give me a quick summary of this video's info\"\n```\n\n### Download Subtitles \u0026 Transcripts\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#download-subtitles--transcripts)\n\n```\n\"List available subtitles for https://youtube.com/watch?v=...\"\n\"Download English subtitles from this video\"\n\"Get a clean transcript of this video in Spanish\"\n\"Download Chinese (zh-Hant) transcript\"\n```\n\n### Download Content\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#download-content)\n\n```\n\"Download this video in 1080p: https://youtube.com/watch?v=...\"\n\"Download audio from this YouTube video\"\n\"Download this video from 1:30 to 2:45\"\n\"Save this Facebook video to my Downloads\"\n```\n\n* * *\n\nðŸ“– Documentation\n----------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-documentation)\n*   **[API Reference](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/api.md)** - Detailed tool documentation\n*   **[Configuration](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/configuration.md)** - Environment variables and settings\n*   **[Cookie Configuration](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/cookies.md)** - Authentication and private video access\n*   **[Error Handling](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/error-handling.md)** - Common errors and solutions\n*   **[Contributing](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/contributing.md)** - How to contribute\n\n* * *\n\nðŸ”§ Configuration\n----------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-configuration)\n### Environment Variables\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#environment-variables)\n\n# Downloads directory (default: ~/Downloads)\nYTDLP_DOWNLOADS_DIR=/path/to/downloads\n\n# Default resolution (default: 720p)\nYTDLP_DEFAULT_RESOLUTION=1080p\n\n# Default subtitle language (default: en)\nYTDLP_DEFAULT_SUBTITLE_LANG=en\n\n# Character limit (default: 25000)\nYTDLP_CHARACTER_LIMIT=25000\n\n# Max transcript length (default: 50000)\nYTDLP_MAX_TRANSCRIPT_LENGTH=50000\n\n### Cookie Configuration\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#cookie-configuration)\nTo access private videos, age-restricted content, or avoid rate limits, configure cookies:\n\n\u003e âš ï¸**Important**: Cookie authentication requires a JavaScript runtime (deno) to be installed. When using cookies, YouTube uses authenticated API endpoints that require JavaScript challenge solving. Without deno, downloads will fail with \"n challenge solving failed\" error.\n\u003e \n\u003e \n\u003e Install deno: [https://docs.deno.com/runtime/getting_started/installation/](https://docs.deno.com/runtime/getting_started/installation/)\n\n# Extract cookies from browser (recommended)\nYTDLP_COOKIES_FROM_BROWSER=chrome\n\n# Or use a cookie file\nYTDLP_COOKIES_FILE=/path/to/cookies.txt\n\n**MCP Configuration with cookies:**\n\n{\n  \"mcpServers\": {\n    \"yt-dlp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kevinwatt/yt-dlp-mcp@latest\"],\n      \"env\": {\n        \"YTDLP_COOKIES_FROM_BROWSER\": \"chrome\"\n      }\n    }\n  }\n}\n\nSupported browsers: `brave`, `chrome`, `chromium`, `edge`, `firefox`, `opera`, `safari`, `vivaldi`, `whale`\n\nSee [Cookie Configuration Guide](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/cookies.md) for detailed setup instructions.\n\n* * *\n\nðŸ—ï¸ Architecture\n----------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#%EF%B8%8F-architecture)\n### Built With\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#built-with)\n*   **[yt-dlp](https://github.com/yt-dlp/yt-dlp)** - Video extraction engine\n*   **[MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk)** - Model Context Protocol\n*   **[Zod](https://github.com/colinhacks/zod)** - TypeScript-first schema validation\n*   **TypeScript** - Type safety and developer experience\n\n### Key Features\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#key-features)\n*   âœ… **Type-Safe**: Full TypeScript with strict mode\n*   âœ… **Validated Inputs**: Zod schemas for runtime validation\n*   âœ… **Character Limits**: Automatic truncation to prevent context overflow\n*   âœ… **Tool Annotations**: readOnly, destructive, idempotent hints\n*   âœ… **Error Guidance**: Actionable error messages for LLMs\n*   âœ… **Modular Design**: Clean separation of concerns\n\n* * *\n\nðŸ“Š Response Formats\n-------------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-response-formats)\n### JSON Format\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#json-format)\nPerfect for programmatic processing:\n\n{\n  \"total\": 50,\n  \"count\": 10,\n  \"offset\": 0,\n  \"videos\": [...],\n  \"has_more\": true,\n  \"next_offset\": 10\n}\n\n### Markdown Format\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#markdown-format)\nHuman-readable display:\n\nFound 50 videos (showing 10):\n\n1. **Video Title**\n   ðŸ“º Channel: Creator Name\n   â±ï¸  Duration: 10:30\n   ðŸ”— URL: https://...\n\n* * *\n\nðŸ”’ Privacy \u0026 Security\n---------------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-privacy--security)\n*   **No Tracking**: Direct downloads, no analytics\n*   **Input Validation**: Zod schemas prevent injection\n*   **URL Validation**: Strict URL format checking\n*   **Character Limits**: Prevents context overflow attacks\n*   **Read-Only by Default**: Most tools don't modify system state\n\n* * *\n\nðŸ¤ Contributing\n---------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-contributing)\nContributions are welcome! Please check out our [Contributing Guide](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/docs/contributing.md).\n\n1.   Fork the repository\n2.   Create a feature branch (`git checkout -b feature/amazing-feature`)\n3.   Commit your changes (`git commit -m 'Add amazing feature'`)\n4.   Push to the branch (`git push origin feature/amazing-feature`)\n5.   Open a Pull Request\n\n* * *\n\nðŸ“ License\n----------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-license)\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/kevinwatt/yt-dlp-mcp/blob/main/LICENSE) file for details.\n\n* * *\n\nðŸ™ Acknowledgments\n------------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-acknowledgments)\n*   [yt-dlp](https://github.com/yt-dlp/yt-dlp) - The amazing video extraction tool\n*   [Anthropic](https://www.anthropic.com/) - For the Model Context Protocol\n*   [Dive](https://github.com/OpenAgentPlatform/Dive) - MCP-compatible AI platform\n\n* * *\n\nðŸ“š Related Projects\n-------------------\n\n[](https://github.com/kevinwatt/yt-dlp-mcp#-related-projects)\n*   [MCP Servers](https://github.com/modelcontextprotocol/servers) - Official MCP server implementations\n*   [yt-dlp](https://github.com/yt-dlp/yt-dlp) - Command-line video downloader\n*   [Dive Desktop](https://github.com/OpenAgentPlatform/Dive) - AI agent platform\n\n* * *",
          "was_summarised": false
        },
        {
          "url": "https://github.com/kevinwatt/ffmpeg-mcp-lite",
          "was_fetched": true,
          "page": "Title: GitHub - kevinwatt/ffmpeg-mcp-lite: MCP server for video/audio processing via FFmpeg - convert, compress, trim, extract audio, add subtitles\n\nURL Source: https://github.com/kevinwatt/ffmpeg-mcp-lite\n\nMarkdown Content:\nðŸŽ¬ ffmpeg-mcp\n-------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-ffmpeg-mcp)\n\n**A powerful MCP server for video and audio processing through FFmpeg**\n\n[](https://pypi.org/project/ffmpeg-mcp-lite/)[](https://opensource.org/licenses/MIT)[](https://www.python.org/)\n\nIntegrate FFmpeg with Claude, Dive, and other MCP-compatible AI systems. Convert, compress, trim videos, extract audio, and more â€” all through natural language.\n\n[Features](https://github.com/kevinwatt/ffmpeg-mcp-lite#-features) â€¢ [Installation](https://github.com/kevinwatt/ffmpeg-mcp-lite#-installation) â€¢ [Tools](https://github.com/kevinwatt/ffmpeg-mcp-lite#-available-tools) â€¢ [Usage](https://github.com/kevinwatt/ffmpeg-mcp-lite#-usage-examples) â€¢ [Configuration](https://github.com/kevinwatt/ffmpeg-mcp-lite#-configuration)\n\n* * *\n\nâœ¨ Features\n----------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-features)\n### ðŸ“Š **Media Information**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-media-information)\n*   Get comprehensive metadata\n*   Duration, resolution, codecs\n*   Bitrate and stream details\n*   JSON formatted output\n\n### ðŸ”„ **Format Conversion**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-format-conversion)\n*   Convert between any formats\n*   MP4, MKV, WebM, MOV, etc.\n*   Custom video/audio codecs\n*   Resolution scaling\n\n### ðŸ—œï¸ **Video Compression**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#%EF%B8%8F-video-compression)\n*   Quality presets (low/medium/high)\n*   Encoding speed control\n*   H.264 optimization\n*   Size reduction stats\n\n### âœ‚ï¸ **Video Trimming**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#%EF%B8%8F-video-trimming)\n*   Precise start/end times\n*   Duration-based cuts\n*   Stream copy (fast)\n*   No re-encoding needed### ðŸŽµ **Audio Extraction**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-audio-extraction)\n*   Multiple formats supported\n*   MP3, AAC, WAV, FLAC, OGG, Opus\n*   Bitrate control\n*   High quality output\n\n### ðŸŽžï¸ **Advanced Features**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#%EF%B8%8F-advanced-features)\n*   Merge multiple videos\n*   Extract frames as images\n*   Interval or count-based extraction\n*   JPG, PNG, BMP output\n\n### ðŸ“ **Subtitles**\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-subtitles)\n*   Burn-in SRT/ASS/VTT subtitles\n*   Multiple styles available\n*   Customizable font size\n*   Works great with Whisper MCP\n\n* * *\n\nðŸš€ Installation\n---------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-installation)\n### Prerequisites\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#prerequisites)\n**Install FFmpeg** on your system:\n\n| Platform | Command |\n| --- | --- |\n| ðŸªŸ **Windows** | `winget install FFmpeg` |\n| ðŸŽ **macOS** | `brew install ffmpeg` |\n| ðŸ§ **Linux** | `sudo apt install ffmpeg` |\n\n### Getting Started\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#getting-started)\nAdd the following config to your MCP client:\n\n{\n  \"mcpServers\": {\n    \"ffmpeg\": {\n      \"command\": \"uvx\",\n      \"args\": [\"ffmpeg-mcp-lite\"]\n    }\n  }\n}\n\n### MCP Client Configuration\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#mcp-client-configuration)\n\n**Dive**\n1.   Open [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n2.   Click **\"+ Add MCP Server\"**\n3.   Paste the config provided above\n4.   Click **\"Save\"** and you're ready!\n\n**Claude Code**\nUse the Claude Code CLI to add the ffmpeg MCP server:\n\nclaude mcp add ffmpeg uvx ffmpeg-mcp-lite\n\n**Claude Desktop**\nAdd to your `claude_desktop_config.json`:\n\n*   macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n*   Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n*   Linux: `~/.config/Claude/claude_desktop_config.json`\n\n{\n  \"mcpServers\": {\n    \"ffmpeg\": {\n      \"command\": \"uvx\",\n      \"args\": [\"ffmpeg-mcp-lite\"]\n    }\n  }\n}\n\n**Cursor**\nGo to `Cursor Settings` -\u003e`MCP` -\u003e`New MCP Server`. Use the config provided above.\n\n**VS Code / Copilot**\nInstall via the VS Code CLI:\n\ncode --add-mcp '{\"name\":\"ffmpeg\",\"command\":\"uvx\",\"args\":[\"ffmpeg-mcp-lite\"]}'\n\n**Windsurf**\nFollow the [configure MCP guide](https://docs.windsurf.com/windsurf/cascade/mcp#mcp-config-json) using the standard config from above.\n\n### Manual Installation\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#manual-installation)\n\npip install ffmpeg-mcp-lite\n\nOr with uv:\n\nuv pip install ffmpeg-mcp-lite\n\n* * *\n\nðŸ› ï¸ Available Tools\n-------------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#%EF%B8%8F-available-tools)\nAll tools are prefixed with `ffmpeg_` to avoid naming conflicts with other MCP servers.\n\n### ðŸ“Š Media Information\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-media-information-1)\n| Tool | Description |\n| --- | --- |\n| `ffmpeg_get_info` | Get comprehensive video/audio metadata * **Parameters**: `file_path` * **Returns**: JSON with duration, resolution, codecs, bitrate, streams |\n\n### ðŸ”„ Conversion \u0026 Compression\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-conversion--compression)\n| Tool | Description |\n| --- | --- |\n| `ffmpeg_convert` | Convert video/audio to different formats * **Parameters**: `file_path`, `output_format`, `scale`, `video_codec`, `audio_codec` * **Formats**: mp4, mkv, webm, mov, mp3, wav, etc. |\n| `ffmpeg_compress` | Compress video to reduce file size * **Parameters**: `file_path`, `quality`, `scale`, `preset` * **Quality**: low, medium, high * **Preset**: ultrafast to veryslow |\n\n### âœ‚ï¸ Editing\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#%EF%B8%8F-editing)\n| Tool | Description |\n| --- | --- |\n| `ffmpeg_trim` | Trim video to extract a segment * **Parameters**: `file_path`, `start_time`, `end_time` or `duration` * **Time format**: \"00:01:30\" or seconds |\n| `ffmpeg_merge` | Concatenate multiple videos into one * **Parameters**: `file_paths` (list), `output_path` * **Supports**: Same codec videos |\n\n### ðŸŽµ Audio \u0026 Frames\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-audio--frames)\n| Tool | Description |\n| --- | --- |\n| `ffmpeg_extract_audio` | Extract audio track from video * **Parameters**: `file_path`, `audio_format`, `bitrate` * **Formats**: mp3, aac, wav, flac, ogg, opus |\n| `ffmpeg_extract_frames` | Extract frames as images * **Parameters**: `file_path`, `interval` or `count`, `format` * **Formats**: jpg, png, bmp |\n\n### ðŸ“ Subtitles\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-subtitles-1)\n| Tool | Description |\n| --- | --- |\n| `ffmpeg_add_subtitles` | Burn-in subtitles to video (hardcode) * **Parameters**: `file_path`, `subtitle_path`, `style`, `font_size`, `output_path` * **Formats**: SRT, ASS, VTT * **Styles**: outline, shadow, background, glow |\n\n* * *\n\nðŸ’¡ Usage Examples\n-----------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-usage-examples)\n### Get Media Information\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#get-media-information)\n\n```\n\"Get info about /path/to/video.mp4\"\n\"What's the resolution and duration of this video?\"\n\"Show me the codec information for my video\"\n```\n\n### Convert Videos\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#convert-videos)\n\n```\n\"Convert video.mp4 to WebM format\"\n\"Convert this video to MKV with h265 codec\"\n\"Convert and scale to 1280x720\"\n```\n\n### Compress Videos\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#compress-videos)\n\n```\n\"Compress video.mp4 with medium quality\"\n\"Compress this video to reduce file size, use fast preset\"\n\"Compress with high quality and scale to 1920:-1\"\n```\n\n### Trim Videos\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#trim-videos)\n\n```\n\"Trim video.mp4 from 00:01:00 to 00:02:30\"\n\"Cut the first 30 seconds from this video\"\n\"Extract a 1-minute clip starting at 5:00\"\n```\n\n### Extract Audio\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#extract-audio)\n\n```\n\"Extract audio from video.mp4 as MP3\"\n\"Get the audio track in AAC format with 192k bitrate\"\n\"Extract audio as FLAC for best quality\"\n```\n\n### Extract Frames\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#extract-frames)\n\n```\n\"Extract one frame every 5 seconds from video.mp4\"\n\"Get 10 frames evenly distributed from this video\"\n\"Extract frames as PNG images\"\n```\n\n### Merge Videos\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#merge-videos)\n\n```\n\"Merge video1.mp4 and video2.mp4 together\"\n\"Concatenate these three videos into one\"\n```\n\n### Add Subtitles\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#add-subtitles)\n\n```\n\"Add subtitles.srt to video.mp4\"\n\"Burn in Chinese subtitles with shadow style\"\n\"Add subtitles with font size 32 and glow effect\"\n```\n\n* * *\n\nðŸ”§ Configuration\n----------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-configuration)\n### Environment Variables\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#environment-variables)\n| Variable | Description | Default |\n| --- | --- | --- |\n| `FFMPEG_PATH` | Path to ffmpeg binary | `ffmpeg` |\n| `FFPROBE_PATH` | Path to ffprobe binary | `ffprobe` |\n| `FFMPEG_OUTPUT_DIR` | Default output directory | `~/Downloads` |\n\n### Custom Configuration\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#custom-configuration)\n\n{\n  \"mcpServers\": {\n    \"ffmpeg\": {\n      \"command\": \"uvx\",\n      \"args\": [\"ffmpeg-mcp-lite\"],\n      \"env\": {\n        \"FFMPEG_OUTPUT_DIR\": \"/path/to/output\"\n      }\n    }\n  }\n}\n\n* * *\n\nðŸ—ï¸ Architecture\n----------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#%EF%B8%8F-architecture)\n### Built With\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#built-with)\n*   **[FFmpeg](https://ffmpeg.org/)** - Video/audio processing engine\n*   **[FastMCP](https://github.com/modelcontextprotocol/python-sdk)** - MCP Python framework\n*   **[asyncio](https://docs.python.org/3/library/asyncio.html)** - Async subprocess execution\n*   **Python 3.10+** - Type hints and modern features\n\n### Key Features\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#key-features)\n*   âœ… **Async Processing**: Non-blocking FFmpeg execution\n*   âœ… **Type Safe**: Full type hints with mypy validation\n*   âœ… **Well Tested**: 31 test cases with pytest\n*   âœ… **Cross Platform**: Works on Windows, macOS, Linux\n*   âœ… **Modular Design**: One file per tool\n\n* * *\n\nðŸ¤ Contributing\n---------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-contributing)\nContributions are welcome!\n\n1.   Fork the repository\n2.   Create a feature branch (`git checkout -b feature/amazing-feature`)\n3.   Commit your changes (`git commit -m 'Add amazing feature'`)\n4.   Push to the branch (`git push origin feature/amazing-feature`)\n5.   Open a Pull Request\n\n### Development\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#development)\n\n# Clone and install\ngit clone https://github.com/kevinwatt/ffmpeg-mcp-lite.git\ncd ffmpeg-mcp-lite\nuv sync\n\n# Run tests\nuv run pytest\n\n# Type checking\nuv run mypy src/\n\n# Linting\nuv run ruff check src/\n\n* * *\n\nðŸ“ License\n----------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-license)\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/kevinwatt/ffmpeg-mcp-lite/blob/main/LICENSE) file for details.\n\n* * *\n\nðŸ™ Acknowledgments\n------------------\n\n[](https://github.com/kevinwatt/ffmpeg-mcp-lite#-acknowledgments)\n*   [FFmpeg](https://ffmpeg.org/) - The powerful multimedia framework\n*   [Anthropic](https://www.anthropic.com/) - For the Model Context Protocol\n*   [Dive](https://github.com/OpenAgentPlatform/Dive) - MCP-compatible AI platform\n\n* * *",
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=0NBILspM4c4",
          "was_fetched": true,
          "page": "Title: NVIDIA Live with CEO Jensen Huang\n\nURL Source: https://www.youtube.com/watch?v=0NBILspM4c4\n\nMarkdown Content:\nNVIDIA Live with CEO Jensen Huang - YouTube\n===============\n\n Back [](https://www.youtube.com/ \"YouTube Home\")\n\nSkip navigation\n\n Search \n\n Search with your voice \n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[Sign in](https://accounts.google.com/ServiceLogin?service=youtube\u0026uilel=3\u0026passive=true\u0026continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252Fwatch%253Fv%253D0NBILspM4c4\u0026hl=en\u0026ec=65620)\n\n[](https://www.youtube.com/ \"YouTube Home\")\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[NVIDIA Live with CEO Jensen Huang](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\nTap to unmute\n\n2x\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\nNVIDIA Live with CEO Jensen Huang\n---------------------------------\n\nNVIDIA 7,296,584 views Streamed 3 days ago\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\nSearch\n\nCopy link\n\nInfo\n\nShopping\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\nIf playback doesn't begin shortly, try restarting your device.\n\nâ€¢\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancel Confirm\n\nVideo unavailable\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\n\nShare\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4 \"Share link\")- [x] Include playlist \n\nAn error occurred while retrieving sharing information. Please try again later.\n\n0:00\n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)[](https://www.youtube.com/watch?v=jpZ0dPsnIWw \"Next (SHIFT+n)\")\n\n0:00 / 1:31:40\n\nLive\n\nâ€¢Watch full video\n\nâ€¢\n\nThe Rise of AI: NVIDIA Live at CES 2026 Opening Video\n\nâ€¢\n\n[1:31:50 FULL KEY NOTE: NVIDIA CEO Jensen Huang Speaks at CES in Las Vegas | AI \u0026 Tech Future | AI15 DWS News 16K views â€¢ 3 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=7YdhAsvoCao)[31:16 I Skied Down Mount Everest (world first, no oxygen)Red Bull 28M views â€¢ 2 months ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=cjZvFY6__qw)[23:12 Richard Feynman Explains Time Like Youâ€™ve Never Seen Before Physics The Feynman Way 1.3M views â€¢ 2 weeks ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=zUHtlXA1f-w)[17:54 The Shocking AI Reveals That Stunned CES 2026 (DAY 2)AI Revolution 4.4K views â€¢ 2 hours ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=9kdw6hLFFss)[23:03 I Tested Nvidia's Self Driving Carâ€¦ should Tesla be worried?Everyday Chris 172K views â€¢ 3 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=EzAVW1VgzcI)[18:06 Tom Lee Just Said The UNTHINKABLE Tom Nash 79K views â€¢ 6 hours ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=a6MVvEkoGkk)[28:09 Jensen Huang, Nvidia CEO: A Fortt Knox CES Update Fortt Knox 9.1K views â€¢ Streamed 2 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=RoJNO-7HbwQ)[48:54 America CAN'T Compete with Chinaâ€™s High-Tech Future! ðŸ‡¨ðŸ‡³Jay and Karolina 485K views â€¢ 3 weeks ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=t5oSxg5iG44)[55:00 The Ridiculous Engineering Of The World's Most Important Machine Veritasium 12M views â€¢ 8 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=MiUHjLxm3V0)[1:33:26 FULL CES 2026 EVENT: NVIDIA CEO Reveals Physical AI and Autonomous Robots Changing Industries | AI14 DWS News 71K views â€¢ 3 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=UrMnOp2N9Kw)[52:08 NVIDIAâ€™s Jensen Huang on Securing American Leadership on AI Center for Strategic \u0026 International Studies 207K views â€¢ Streamed 1 month ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=jpZ0dPsnIWw)[24:10 NVIDIA CEO Jensen Huang Leaves Everyone SPEECHLESS (CES Supercut)Ticker Symbol: YOU 64K views â€¢ 2 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=tXGlS460b_w)\n\nNVIDIA Live with CEO Jensen Huang\n=================================\n\n[](https://www.youtube.com/@NVIDIA)\n\n[NVIDIA](https://www.youtube.com/@NVIDIA)\n\n NVIDIA \n\n2.12M subscribers\n\nSubscribe\n\nSubscribed\n\nLike\n\nShare\n\nSave\n\nDownload\n\n Download \n\n7,296,584 views Streamed 3 days ago\n\n 7,296,584 views â€¢ Streamed live on Jan 5, 2026 \n\nLive from CES in Las Vegas, NVIDIA CEO Jensen Huang shares how the next generation of accelerated computing and AI will transform every industry. The event kicks off with a one-hour NVIDIA pregame show where panelists discuss the future of AI infrastructure, open ecosystems, and physical AI. [00:00:00](https://www.youtube.com/watch?v=0NBILspM4c4) The Rise of AI: NVIDIA Live at CES 2026 Opening Video [00:04:03](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s) Intro: Platform Shift [00:07:23](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s) AI Scales Beyond LLMs [00:10:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s) Open Models Really Took Off Last Year [00:12:17](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=737s) NVIDIA Leads Open Model Ecosystem [00:15:25](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=925s) NVIDIA Tops Leaderboards [00:16:16](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=976s) Agents Are Multi-Model, Multi-Cloud, and Hybrid Cloud [00:21:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1302s) Video: Build Your Own AI Assistant with Hugging Face on NVIDIA DGX Spark [00:24:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1446s) Enterprise Platforms Adopt NVIDIA AI [00:26:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1616s) NVIDIA Full-Stack Physical AI Platform [00:30:22](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1822s) Compute Is Data [00:31:24](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1884s) Video: The Breakthrough Moment for Physical AI, Powered by NVIDIA Cosmos [00:33:33](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2013s) Announcing NVIDIA Alpamayo [00:36:33](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2193s) Level 2++ Autonomous Driving in San Francisco With NVIDIA DRIVE AV [00:39:24](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2364s) NVIDIA Ships Full-Stack AV on 2025 Mercedes-Benz CLA [00:44:27](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2667s) Global L4 and Robotaxi Ecosystem Building on NVIDIA [00:46:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2816s) Video: Robotics Learning Simulator [00:47:58](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2878s) Robotics Enters the Real World [00:49:00](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2940s) Cadence + NVIDIA (AI-Accelerated Design and Simulation) [00:50:02](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3002s) Synopsys + NVIDIA (System-Scale Design and Emulation) [00:51:10](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3070s) Siemens CUDA-X Integration [00:53:12](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3192s) Video: The Next Generation of Industrial AI With Siemens and NVIDIA [00:54:57](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3297s) NVIDIA Leads Open Model Ecosystem pt. 2 [00:55:26](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3326s) Announcing NVIDIA Vera Rubin [00:59:20](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3560s) Video: The NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer [01:02:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3762s) NVIDIA Vera Rubin CPU [01:05:30](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3930s) NVIDIA Vera Rubin GPU [01:07:52](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4072s) NVIDIA ConnectX-9 SuperNIC [01:10:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4242s) NVIDIA BlueField-4 [01:12:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4362s) NVIDIA NVLink 6 Switch [01:13:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4436s) NVIDIA Vera Rubin NVL72 [01:16:58](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4618s) NVIDIA Spectrum-X Ethernet with Co-Packaged Optics [01:18:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4686s) Context Is the New Bottleneck [01:21:16](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4876s) NVIDIA Context Memory Storage Platform [01:22:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4926s) Six New Chips â€” One Giant Leap to the Next Frontier [01:25:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5135s) NVIDIA â€” One Platform for Every AI [01:28:13](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5293s) Close Show less \n\nLive from CES in Las Vegas, NVIDIA CEO Jensen Huang shares how the next generation of accelerated computing and AI will transform every industry. The event kicks off with a one-hour NVIDIA pregame show where panelists discuss the future of AI infrastructure, open ecosystems, and physical AI.â€¦...more \n\n...more \n\nChapters\n\nView all\n--------------------\n\n[#### The Rise of AI: NVIDIA Live at CES 2026 Opening Video #### The Rise of AI: NVIDIA Live at CES 2026 Opening Video 0:00](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[#### The Rise of AI: NVIDIA Live at CES 2026 Opening Video](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n0:00\n\n[#### Intro: Platform Shift #### Intro: Platform Shift 4:03](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s)\n\n[#### Intro: Platform Shift](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s)\n\n4:03\n\n[#### AI Scales Beyond LLMs #### AI Scales Beyond LLMs 7:23](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s)\n\n[#### AI Scales Beyond LLMs](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s)\n\n7:23\n\n[#### Open Models Really Took Off Last Year #### Open Models Really Took Off Last Year 10:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s)\n\n[#### Open Models Really Took Off Last Year](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s)\n\n10:35\n\n[#### NVIDIA Leads Open Model Ecosystem #### NVIDIA Leads Open Model Ecosystem 12:17](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=737s)\n\n[#### NVIDIA Leads Open Model Ecosystem](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=737s)\n\n12:17\n\nTranscript\n\nFollow along using the transcript.\n\nShow transcript\n\n[### NVIDIA 2.12M subscribers](https://www.youtube.com/@NVIDIA)\n\n[Videos](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/videos)\n\n[About](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/about)\n\n[Videos](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/videos)[About](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/about)[LinkedIn](https://www.youtube.com/redirect?event=Watch_SD_EP\u0026redir_token=QUFFLUhqbC1fWXVEUWJBa2tSRXJ4cXh3MzVHdGNVcjJ4QXxBQ3Jtc0ttenJLTE1FVnVnRWNsLS15czlfYlZGTjdDLU8xLW5PT3JIM0pDTnVFMmx5SWJOT1hndzhIU0ZXMzVyWk9FeS1xV253Ty13UHZfSjcyLW4wVnJKT2kzLU40OXBqWDBieWRuWFhDeGxtdjRaOFdmcTB2MA\u0026q=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fnvidia%2F)[Instagram](https://www.youtube.com/redirect?event=Watch_SD_EP\u0026redir_token=QUFFLUhqbnFiRXRLRWRXN25fSm9fZ2lzMmdabzJ1dWJmZ3xBQ3Jtc0tuTUVsRWkyVnNLcDJkQm9ld1B2THJCUURKbzYwNkY0Wllmb05lampXb0pYVzFUTGQ2X0tzMEVIMXM3QTVDUmpNZHA3Z05JZ0pxeEVBSjdpNGVUTE9mNzVQa0czemdlZU9NLVFJRlQzMHJSM0NVX01oZw\u0026q=https%3A%2F%2Fwww.instagram.com%2Fnvidia%2F)[Facebook](https://www.youtube.com/redirect?event=Watch_SD_EP\u0026redir_token=QUFFLUhqbGJoOUs2c0V3UERzM1lPTGMxMm0teHd4ZHVFd3xBQ3Jtc0trcWYyUnBsVmNnS3NobEc3XzdIV2puSlV3NEh3YjIyTmM3MHBhdlFmelFqTzZkTDdqYzJDZ3V1U0poSE5HaF9DcDNmU0tPUkx1bXZwaFlJSnNlQzNReXQ4NXBheVFtRHVqSlVkU2VoWUVYcW9raExwOA\u0026q=https%3A%2F%2Fwww.facebook.com%2Fnvidia)\n\nShow less \n\n[](https://www.youtube.com/watch?v=0NBILspM4c4)\nNVIDIA Live with CEO Jensen Huang\n=================================\n\n7,296,584 views\n\nStreamed live on Jan 5, 2026\n\nLike\n\nShare\n\nSave\n\nDownload\n\n Download \n\nComments are turned off. [Learn more](https://support.google.com/youtube/answer/9706180?hl=en)\n\nNaN / NaN\n\nIn this video\n-------------\n\nTimeline\n\nChapters\n\nTranscript\n\nChapters\n--------\n\n[#### The Rise of AI: NVIDIA Live at CES 2026 Opening Video #### The Rise of AI: NVIDIA Live at CES 2026 Opening Video 0:00](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[#### The Rise of AI: NVIDIA Live at CES 2026 Opening Video](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n0:00\n\n[#### Intro: Platform Shift #### Intro: Platform Shift 4:03](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s)\n\n[#### Intro: Platform Shift](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s)\n\n4:03\n\n[#### AI Scales Beyond LLMs #### AI Scales Beyond LLMs 7:23](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s)\n\n[#### AI Scales Beyond LLMs](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s)\n\n7:23\n\n[#### Open Models Really Took Off Last Year #### Open Models Really Took Off Last Year 10:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s)\n\n[#### Open Models Really Took Off Last Year](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s)\n\n10:35\n\n[#### NVIDIA Leads Open Model Ecosystem #### NVIDIA Leads Open Model Ecosystem 12:17](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=737s)\n\n[#### NVIDIA Leads Open Model Ecosystem](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=737s)\n\n12:17\n\n[#### NVIDIA Tops Leaderboards #### NVIDIA Tops Leaderboards 15:25](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=925s)\n\n[#### NVIDIA Tops Leaderboards](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=925s)\n\n15:25\n\n[#### Agents Are Multi-Model, Multi-Cloud, and Hybrid Cloud #### Agents Are Multi-Model, Multi-Cloud, and Hybrid Cloud 16:16](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=976s)\n\n[#### Agents Are Multi-Model, Multi-Cloud, and Hybrid Cloud](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=976s)\n\n16:16\n\n[#### Video: Build Your Own AI Assistant with Hugging Face on NVIDIA DGX Spark #### Video: Build Your Own AI Assistant with Hugging Face on NVIDIA DGX Spark 21:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1302s)\n\n[#### Video: Build Your Own AI Assistant with Hugging Face on NVIDIA DGX Spark](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1302s)\n\n21:42\n\n[#### Enterprise Platforms Adopt NVIDIA AI #### Enterprise Platforms Adopt NVIDIA AI 24:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1446s\u0026pp=0gcJCd0CDuyUWbzu)\n\n[#### Enterprise Platforms Adopt NVIDIA AI](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1446s\u0026pp=0gcJCd0CDuyUWbzu)\n\n24:06\n\n[#### NVIDIA Full-Stack Physical AI Platform #### NVIDIA Full-Stack Physical AI Platform 26:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1616s)\n\n[#### NVIDIA Full-Stack Physical AI Platform](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1616s)\n\n26:56\n\n[#### Compute Is Data #### Compute Is Data 30:22](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1822s)\n\n[#### Compute Is Data](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1822s)\n\n30:22\n\n[#### Video: The Breakthrough Moment for Physical AI, Powered by NVIDIA Cosmos #### Video: The Breakthrough Moment for Physical AI, Powered by NVIDIA Cosmos 31:24](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1884s)\n\n[#### Video: The Breakthrough Moment for Physical AI, Powered by NVIDIA Cosmos](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1884s)\n\n31:24\n\n[#### Announcing NVIDIA Alpamayo #### Announcing NVIDIA Alpamayo 33:33](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2013s\u0026pp=0gcJCd0CDuyUWbzu)\n\n[#### Announcing NVIDIA Alpamayo](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2013s\u0026pp=0gcJCd0CDuyUWbzu)\n\n33:33\n\n[#### Level 2++ Autonomous Driving in San Francisco With NVIDIA DRIVE AV #### Level 2++ Autonomous Driving in San Francisco With NVIDIA DRIVE AV 36:33](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2193s)\n\n[#### Level 2++ Autonomous Driving in San Francisco With NVIDIA DRIVE AV](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2193s)\n\n36:33\n\n[#### NVIDIA Ships Full-Stack AV on 2025 Mercedes-Benz CLA #### NVIDIA Ships Full-Stack AV on 2025 Mercedes-Benz CLA 39:24](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2364s)\n\n[#### NVIDIA Ships Full-Stack AV on 2025 Mercedes-Benz CLA](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2364s)\n\n39:24\n\n[#### Global L4 and Robotaxi Ecosystem Building on NVIDIA #### Global L4 and Robotaxi Ecosystem Building on NVIDIA 44:27](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2667s)\n\n[#### Global L4 and Robotaxi Ecosystem Building on NVIDIA](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2667s)\n\n44:27\n\n[#### Video: Robotics Learning Simulator #### Video: Robotics Learning Simulator 46:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2816s)\n\n[#### Video: Robotics Learning Simulator](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2816s)\n\n46:56\n\n[#### Robotics Enters the Real World #### Robotics Enters the Real World 47:58](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2878s)\n\n[#### Robotics Enters the Real World](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2878s)\n\n47:58\n\n[#### Cadence + NVIDIA (AI-Accelerated Design and Simulation) #### Cadence + NVIDIA (AI-Accelerated Design and Simulation) 49:00](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2940s)\n\n[#### Cadence + NVIDIA (AI-Accelerated Design and Simulation)](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2940s)\n\n49:00\n\n[#### Synopsys + NVIDIA (System-Scale Design and Emulation) #### Synopsys + NVIDIA (System-Scale Design and Emulation) 50:02](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3002s)\n\n[#### Synopsys + NVIDIA (System-Scale Design and Emulation)](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3002s)\n\n50:02\n\n[#### Siemens CUDA-X Integration #### Siemens CUDA-X Integration 51:10](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3070s)\n\n[#### Siemens CUDA-X Integration](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3070s)\n\n51:10\n\n[#### Video: The Next Generation of Industrial AI With Siemens and NVIDIA #### Video: The Next Generation of Industrial AI With Siemens and NVIDIA 53:12](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3192s)\n\n[#### Video: The Next Generation of Industrial AI With Siemens and NVIDIA](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3192s)\n\n53:12\n\n[#### NVIDIA Leads Open Model Ecosystem pt. 2 #### NVIDIA Leads Open Model Ecosystem pt. 2 54:57](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3297s)\n\n[#### NVIDIA Leads Open Model Ecosystem pt. 2](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3297s)\n\n54:57\n\n[#### Announcing NVIDIA Vera Rubin #### Announcing NVIDIA Vera Rubin 55:26](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3326s)\n\n[#### Announcing NVIDIA Vera Rubin](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3326s)\n\n55:26\n\n[#### Video: The NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer #### Video: The NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer 59:20](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3560s)\n\n[#### Video: The NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3560s)\n\n59:20\n\n[#### NVIDIA Vera Rubin CPU #### NVIDIA Vera Rubin CPU 1:02:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3762s)\n\n[#### NVIDIA Vera Rubin CPU](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3762s)\n\n1:02:42\n\n[#### NVIDIA Vera Rubin GPU #### NVIDIA Vera Rubin GPU 1:05:30](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3930s\u0026pp=0gcJCd0CDuyUWbzu)\n\n[#### NVIDIA Vera Rubin GPU](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3930s\u0026pp=0gcJCd0CDuyUWbzu)\n\n1:05:30\n\n[#### NVIDIA ConnectX-9 SuperNIC #### NVIDIA ConnectX-9 SuperNIC 1:07:52](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4072s)\n\n[#### NVIDIA ConnectX-9 SuperNIC](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4072s)\n\n1:07:52\n\n[#### NVIDIA BlueField-4 #### NVIDIA BlueField-4 1:10:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4242s)\n\n[#### NVIDIA BlueField-4](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4242s)\n\n1:10:42\n\n[#### NVIDIA NVLink 6 Switch #### NVIDIA NVLink 6 Switch 1:12:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4362s\u0026pp=0gcJCd0CDuyUWbzu)\n\n[#### NVIDIA NVLink 6 Switch](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4362s\u0026pp=0gcJCd0CDuyUWbzu)\n\n1:12:42\n\n[#### NVIDIA Vera Rubin NVL72 #### NVIDIA Vera Rubin NVL72 1:13:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4436s)\n\n[#### NVIDIA Vera Rubin NVL72](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4436s)\n\n1:13:56\n\n[#### NVIDIA Spectrum-X Ethernet with Co-Packaged Optics #### NVIDIA Spectrum-X Ethernet with Co-Packaged Optics 1:16:58](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4618s)\n\n[#### NVIDIA Spectrum-X Ethernet with Co-Packaged Optics](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4618s)\n\n1:16:58\n\n[#### Context Is the New Bottleneck #### Context Is the New Bottleneck 1:18:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4686s)\n\n[#### Context Is the New Bottleneck](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4686s)\n\n1:18:06\n\n[#### NVIDIA Context Memory Storage Platform #### NVIDIA Context Memory Storage Platform 1:21:16](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4876s)\n\n[#### NVIDIA Context Memory Storage Platform](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4876s)\n\n1:21:16\n\n[#### Six New Chips â€” One Giant Leap to the Next Frontier #### Six New Chips â€” One Giant Leap to the Next Frontier 1:22:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4926s)\n\n[#### Six New Chips â€” One Giant Leap to the Next Frontier](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4926s)\n\n1:22:06\n\n[#### NVIDIA â€” One Platform for Every AI #### NVIDIA â€” One Platform for Every AI 1:25:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5135s)\n\n[#### NVIDIA â€” One Platform for Every AI](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5135s)\n\n1:25:35\n\n[#### Close #### Close 1:28:13](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5293s\u0026pp=0gcJCd0CDuyUWbzu)\n\n[#### Close](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5293s\u0026pp=0gcJCd0CDuyUWbzu)\n\n1:28:13\n\nSync to video time\n\nDescription\n-----------\n\nNVIDIA Live with CEO Jensen Huang\n\nLive from CES in Las Vegas, NVIDIA CEO Jensen Huang shares how the next generation of accelerated computing and AI will transform every industry. The event kicks off with a one-hour NVIDIA pregame show where panelists discuss the future of AI infrastructure, open ecosystems, and physical AI. [00:00:00](https://www.youtube.com/watch?v=0NBILspM4c4) The Rise of AI: NVIDIA Live at CES 2026 Opening Video [00:04:03](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s) Intro: Platform Shift [00:07:23](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s) AI Scales Beyond LLMs [00:10:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s) Open Models Really Took Off Last Year [00:12:17](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=737s) NVIDIA Leads Open Model Ecosystem [00:15:25](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=925s) NVIDIA Tops Leaderboards [00:16:16](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=976s) Agents Are Multi-Model, Multi-Cloud, and Hybrid Cloud [00:21:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1302s) Video: Build Your Own AI Assistant with Hugging Face on NVIDIA DGX Spark [00:24:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1446s) Enterprise Platforms Adopt NVIDIA AI [00:26:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1616s) NVIDIA Full-Stack Physical AI Platform [00:30:22](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1822s) Compute Is Data [00:31:24](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=1884s) Video: The Breakthrough Moment for Physical AI, Powered by NVIDIA Cosmos [00:33:33](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2013s) Announcing NVIDIA Alpamayo [00:36:33](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2193s) Level 2++ Autonomous Driving in San Francisco With NVIDIA DRIVE AV [00:39:24](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2364s) NVIDIA Ships Full-Stack AV on 2025 Mercedes-Benz CLA [00:44:27](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2667s) Global L4 and Robotaxi Ecosystem Building on NVIDIA [00:46:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2816s) Video: Robotics Learning Simulator [00:47:58](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2878s) Robotics Enters the Real World [00:49:00](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=2940s) Cadence + NVIDIA (AI-Accelerated Design and Simulation) [00:50:02](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3002s) Synopsys + NVIDIA (System-Scale Design and Emulation) [00:51:10](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3070s) Siemens CUDA-X Integration [00:53:12](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3192s) Video: The Next Generation of Industrial AI With Siemens and NVIDIA [00:54:57](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3297s) NVIDIA Leads Open Model Ecosystem pt. 2 [00:55:26](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3326s) Announcing NVIDIA Vera Rubin [00:59:20](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3560s) Video: The NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer [01:02:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3762s) NVIDIA Vera Rubin CPU [01:05:30](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=3930s) NVIDIA Vera Rubin GPU [01:07:52](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4072s) NVIDIA ConnectX-9 SuperNIC [01:10:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4242s) NVIDIA BlueField-4 [01:12:42](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4362s) NVIDIA NVLink 6 Switch [01:13:56](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4436s) NVIDIA Vera Rubin NVL72 [01:16:58](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4618s) NVIDIA Spectrum-X Ethernet with Co-Packaged Optics [01:18:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4686s) Context Is the New Bottleneck [01:21:16](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4876s) NVIDIA Context Memory Storage Platform [01:22:06](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=4926s) Six New Chips â€” One Giant Leap to the Next Frontier [01:25:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5135s) NVIDIA â€” One Platform for Every AI [01:28:13](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=5293s) Closeâ€¦...more \n\n...more Show less \n\nChapters\n\nView all\n--------------------\n\n[#### The Rise of AI: NVIDIA Live at CES 2026 Opening Video #### The Rise of AI: NVIDIA Live at CES 2026 Opening Video 0:00](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n[#### The Rise of AI: NVIDIA Live at CES 2026 Opening Video](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n0:00\n\n[#### Intro: Platform Shift #### Intro: Platform Shift 4:03](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s)\n\n[#### Intro: Platform Shift](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=243s)\n\n4:03\n\n[#### AI Scales Beyond LLMs #### AI Scales Beyond LLMs 7:23](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s)\n\n[#### AI Scales Beyond LLMs](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=443s)\n\n7:23\n\n[#### Open Models Really Took Off Last Year #### Open Models Really Took Off Last Year 10:35](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s)\n\n[#### Open Models Really Took Off Last Year](https://www.youtube.com/watch?v=0NBILspM4c4\u0026t=635s)\n\n10:35\n\nTranscript\n\nFollow along using the transcript.\n\nShow transcript\n\n[### NVIDIA 2.12M subscribers](https://www.youtube.com/@NVIDIA)\n\n[Videos](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/videos)\n\n[About](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/about)\n\n[Videos](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/videos)[About](https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g/about)[LinkedIn](https://www.youtube.com/redirect?event=Watch_SD_EP\u0026redir_token=QUFFLUhqbC1fWXVEUWJBa2tSRXJ4cXh3MzVHdGNVcjJ4QXxBQ3Jtc0ttenJLTE1FVnVnRWNsLS15czlfYlZGTjdDLU8xLW5PT3JIM0pDTnVFMmx5SWJOT1hndzhIU0ZXMzVyWk9FeS1xV253Ty13UHZfSjcyLW4wVnJKT2kzLU40OXBqWDBieWRuWFhDeGxtdjRaOFdmcTB2MA\u0026q=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fnvidia%2F)[Instagram](https://www.youtube.com/redirect?event=Watch_SD_EP\u0026redir_token=QUFFLUhqbnFiRXRLRWRXN25fSm9fZ2lzMmdabzJ1dWJmZ3xBQ3Jtc0tuTUVsRWkyVnNLcDJkQm9ld1B2THJCUURKbzYwNkY0Wllmb05lampXb0pYVzFUTGQ2X0tzMEVIMXM3QTVDUmpNZHA3Z05JZ0pxeEVBSjdpNGVUTE9mNzVQa0czemdlZU9NLVFJRlQzMHJSM0NVX01oZw\u0026q=https%3A%2F%2Fwww.instagram.com%2Fnvidia%2F)[Facebook](https://www.youtube.com/redirect?event=Watch_SD_EP\u0026redir_token=QUFFLUhqbGJoOUs2c0V3UERzM1lPTGMxMm0teHd4ZHVFd3xBQ3Jtc0trcWYyUnBsVmNnS3NobEc3XzdIV2puSlV3NEh3YjIyTmM3MHBhdlFmelFqTzZkTDdqYzJDZ3V1U0poSE5HaF9DcDNmU0tPUkx1bXZwaFlJSnNlQzNReXQ4NXBheVFtRHVqSlVkU2VoWUVYcW9raExwOA\u0026q=https%3A%2F%2Fwww.facebook.com%2Fnvidia)\n\nTranscript\n----------\n\nLive chat replay was turned off for this video.\n\n[1:31:50](https://www.youtube.com/watch?v=7YdhAsvoCao)\n\n### [FULL KEY NOTE: NVIDIA CEO Jensen Huang Speaks at CES in Las Vegas | AI \u0026 Tech Future | AI15](https://www.youtube.com/watch?v=7YdhAsvoCao)\n\nDWS News\n\n16K views â€¢ 3 days ago\n\nNew\n\n[31:16](https://www.youtube.com/watch?v=cjZvFY6__qw)\n\n### [I Skied Down Mount Everest (world first, no oxygen)](https://www.youtube.com/watch?v=cjZvFY6__qw)\n\nRed Bull\n\n28M views â€¢ 2 months ago\n\n[23:12](https://www.youtube.com/watch?v=zUHtlXA1f-w\u0026pp=0gcJCU0KAYcqIYzv)\n\n### [Richard Feynman Explains Time Like Youâ€™ve Never Seen Before](https://www.youtube.com/watch?v=zUHtlXA1f-w\u0026pp=0gcJCU0KAYcqIYzv)\n\nPhysics The Feynman Way\n\n1.3M views â€¢ 2 weeks ago\n\n[17:54](https://www.youtube.com/watch?v=9kdw6hLFFss)\n\n### [The Shocking AI Reveals That Stunned CES 2026 (DAY 2)](https://www.youtube.com/watch?v=9kdw6hLFFss)\n\nAI Revolution\n\n4.4K views â€¢ 2 hours ago\n\nNew\n\n[23:03](https://www.youtube.com/watch?v=EzAVW1VgzcI\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\n### [I Tested Nvidia's Self Driving Carâ€¦ should Tesla be worried?](https://www.youtube.com/watch?v=EzAVW1VgzcI\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\nEveryday Chris\n\n172K views â€¢ 3 days ago\n\nNew\n\n[18:06](https://www.youtube.com/watch?v=a6MVvEkoGkk)\n\n### [Tom Lee Just Said The UNTHINKABLE](https://www.youtube.com/watch?v=a6MVvEkoGkk)\n\nTom Nash\n\n79K views â€¢ 6 hours ago\n\nNew\n\n[28:09](https://www.youtube.com/watch?v=RoJNO-7HbwQ)\n\n### [Jensen Huang, Nvidia CEO: A Fortt Knox CES Update](https://www.youtube.com/watch?v=RoJNO-7HbwQ)\n\nFortt Knox\n\n9.1K views â€¢ Streamed 2 days ago\n\nNew\n\n[48:54](https://www.youtube.com/watch?v=t5oSxg5iG44)\n\n### [America CAN'T Compete with Chinaâ€™s High-Tech Future! ðŸ‡¨ðŸ‡³](https://www.youtube.com/watch?v=t5oSxg5iG44)\n\nJay and Karolina\n\n485K views â€¢ 3 weeks ago\n\n[55:00](https://www.youtube.com/watch?v=MiUHjLxm3V0)\n\n### [The Ridiculous Engineering Of The World's Most Important Machine](https://www.youtube.com/watch?v=MiUHjLxm3V0)\n\nVeritasium\n\n12M views â€¢ 8 days ago\n\n[1:33:26](https://www.youtube.com/watch?v=UrMnOp2N9Kw)\n\n### [FULL CES 2026 EVENT: NVIDIA CEO Reveals Physical AI and Autonomous Robots Changing Industries | AI14](https://www.youtube.com/watch?v=UrMnOp2N9Kw)\n\nDWS News\n\n71K views â€¢ 3 days ago\n\nNew\n\n[52:08](https://www.youtube.com/watch?v=jpZ0dPsnIWw)\n\n### [NVIDIAâ€™s Jensen Huang on Securing American Leadership on AI](https://www.youtube.com/watch?v=jpZ0dPsnIWw)\n\nCenter for Strategic \u0026 International Studies\n\n207K views â€¢ Streamed 1 month ago\n\n[24:10](https://www.youtube.com/watch?v=tXGlS460b_w\u0026pp=ugUEEgJlbg%3D%3D)\n\n### [NVIDIA CEO Jensen Huang Leaves Everyone SPEECHLESS (CES Supercut)](https://www.youtube.com/watch?v=tXGlS460b_w\u0026pp=ugUEEgJlbg%3D%3D)\n\nTicker Symbol: YOU\n\n64K views â€¢ 2 days ago\n\nNew\n\n[25:57](https://www.youtube.com/watch?v=t260757b_vU)\n\n### [Stop Rambling: The 3-2-1 Speaking Trick That Makes You Sound Like A CEO](https://www.youtube.com/watch?v=t260757b_vU)\n\nBigDeal by Codie Sanchez \n\n929K views â€¢ 2 weeks ago\n\n[1:31:20](https://www.youtube.com/watch?v=VfIK5LFGnlk)\n\n### [Groq Founder, Jonathan Ross: OpenAI \u0026 Anthropic Will Build Their Own Chips \u0026 Will NVIDIA Hit $10TRN](https://www.youtube.com/watch?v=VfIK5LFGnlk)\n\n20VC with Harry Stebbings\n\n206K views â€¢ 3 months ago\n\n[28:08](https://www.youtube.com/watch?v=YT3BxZNJlhs\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\n### [Iâ€™m a Pathologist: The \"Biological Cost\" of Ozempic and Mounjaro.](https://www.youtube.com/watch?v=YT3BxZNJlhs\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\nDr. Amin Hedayat, MD\n\n2.4M views â€¢ 2 weeks ago\n\n[28:33](https://www.youtube.com/watch?v=_wux3uZotVg)\n\n### ['Work Will Be Optional': Elon Musk Shares His Staggering Predictions About The Future | Full](https://www.youtube.com/watch?v=_wux3uZotVg)\n\nForbes\n\n163K views â€¢ 1 month ago\n\n[27:23](https://www.youtube.com/watch?v=XCUWrrmaNck\u0026pp=0gcJCU0KAYcqIYzv)\n\n### [no, claude code is not \"PRICED IN\"](https://www.youtube.com/watch?v=XCUWrrmaNck\u0026pp=0gcJCU0KAYcqIYzv)\n\nWes Roth\n\n3.2K views â€¢ 1 hour ago\n\nNew\n\n[12:14](https://www.youtube.com/watch?v=Ikiu2S6zjHY)\n\n### [14 Most Unique Tech at CES 2026 (Day 2)](https://www.youtube.com/watch?v=Ikiu2S6zjHY)\n\nMax Tech\n\n22K views â€¢ 10 hours ago\n\nNew\n\n[1:50:41](https://www.youtube.com/watch?v=M8fL0RUmbP0)\n\n### [Nvidia CEO Jensen Huang talks about his company's latest innovations at CES 2026](https://www.youtube.com/watch?v=M8fL0RUmbP0)\n\nYahoo Finance\n\n82K views â€¢ Streamed 3 days ago\n\nNew\n\n[13:03](https://www.youtube.com/watch?v=k69-ivYwEfE\u0026pp=0gcJCU0KAYcqIYzv)\n\n### [11 Tech We Saw at CES 2026 (Media Days)](https://www.youtube.com/watch?v=k69-ivYwEfE\u0026pp=0gcJCU0KAYcqIYzv)\n\nCybernews\n\n286K views â€¢ 3 days ago\n\nNew\n\nShow more\n\n[[](https://www.youtube.com/watch?v=0NBILspM4c4)](https://www.youtube.com/watch?v=0NBILspM4c4)",
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=u\\_7OtyYAX74",
          "was_fetched": true,
          "page": "Title: YouTube\n\nURL Source: https://www.youtube.com/watch?v=u\\_7OtyYAX74\n\nMarkdown Content:\nYouTube\n===============\n\n Back [](https://www.youtube.com/ \"YouTube Home\")\n\nSkip navigation\n\n Search \n\n Search with your voice \n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\n[Sign in](https://accounts.google.com/ServiceLogin?service=youtube\u0026uilel=3\u0026passive=true\u0026continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252Fwatch%253Fv%253Du%25255C_7OtyYAX74\u0026hl=en\u0026ec=65620)\n\n[](https://www.youtube.com/ \"YouTube Home\")\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\nThis video isn't available anymore\n\n[GO TO HOME](https://www.youtube.com/)\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\nTap to unmute\n\n2x\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\nSearch\n\nCopy link\n\nInfo\n\nShopping\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\nIf playback doesn't begin shortly, try restarting your device.\n\nâ€¢\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancel Confirm\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)\n\nShare\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74 \"Share link\")- [x] Include playlist \n\nAn error occurred while retrieving sharing information. Please try again later.\n\n0:00\n\n[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)[](https://www.youtube.com/watch?v=u\\_7OtyYAX74 \"Next (SHIFT+n)\")\n\n0:00 / 0:00\n\nLive\n\nâ€¢Watch full video\n\nâ€¢\n\nâ€¢\n\nVideo unavailable\n\nNaN / NaN\n\n[[](https://www.youtube.com/watch?v=u\\_7OtyYAX74)](https://www.youtube.com/watch?v=u\\_7OtyYAX74)",
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=u_7OtyYAX74",
          "was_fetched": true,
          "page": "Title: I made AI count how many times Jensen says \"AI\" - Dive Desktop demo\n\nURL Source: https://www.youtube.com/watch?v=u_7OtyYAX74\n\nMarkdown Content:\nI made AI count how many times Jensen says \"AI\" - Dive Desktop demo - YouTube\n===============\n\n Back [](https://www.youtube.com/ \"YouTube Home\")\n\nSkip navigation\n\n Search \n\n Search with your voice \n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\n[Sign in](https://accounts.google.com/ServiceLogin?service=youtube\u0026uilel=3\u0026passive=true\u0026continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3Dhttps%253A%252F%252Fwww.youtube.com%252Fwatch%253Fv%253Du_7OtyYAX74\u0026hl=en\u0026ec=65620)\n\n[](https://www.youtube.com/ \"YouTube Home\")\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\n[I made AI count how many times Jensen says \"AI\" - Dive Desktop demo](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nTap to unmute\n\n2x\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nI made AI count how many times Jensen says \"AI\" - Dive Desktop demo\n-------------------------------------------------------------------\n\nDewei Yen 212 views 12 hours ago\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nSearch\n\nCopy link\n\nInfo\n\nShopping\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nIf playback doesn't begin shortly, try restarting your device.\n\nâ€¢\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancel Confirm\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nShare\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74 \"Share link\")- [x] Include playlist \n\nAn error occurred while retrieving sharing information. Please try again later.\n\n0:00\n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)[](https://www.youtube.com/watch?v=TM2YGVVrWV4 \"Next (SHIFT+n)\")\n\n0:00 / 0:00\n\nLive\n\nâ€¢Watch full video\n\nâ€¢\n\nâ€¢\n\n[8:07 I Entered A Jiu-Jitsu Tournament To Prove It Doesn't Work Jesse Enkamp 10M views â€¢ 2 years ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=EAJ2vt8wUbY)[13:11 This Tool Changes Streaming Forever!FreakweaselGaming 5K views â€¢ 2 months ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=WpUwR3BumNY)[10:56 Unlocking Suno's Secret \"MAX MODE\" (Is It Real? I Tested It)AI With TechZnap 14K views â€¢ 3 days ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=D0IBRMq6jyA)[1:00:00 1 Hour of Abstract Wave Height Map Loop | QuietQuests Wavy 37K views â€¢ 1 year ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=uPdyTOge6w4)[4:24 Kristen Wiig Breaking People on SNL for 4 Minutes Straight PopMojo 2.2M views â€¢ 10 months ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=eE2BQ7rE7OU)[1:00:10 No-Break Study Timer ðŸŒ¸ | 1 Hour of Pink Aesthetic Productivity Focus Corner â™¡439K views â€¢ 1 year ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=CYLIc5S6KK4)[1:01:11 LINEAS AZULES ðŸŒˆ FONDO de PANTALLA con MOVIMIENTO - VFX ã€4Kã€‘GRATIS âœ… (no copyright)ðŸ’ªFondos Animados - Arte VFX 42K views â€¢ 4 years ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=YhNjL5hROf8)[2:00:00 Vintage Floral TV Art Screensaver Tv Wallpaper Home Decor Oil Painting Digital Wall Art LunaSkyeTvArt 521K views â€¢ 1 year ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=uIq-2oRRt1w)[18:14 Ex-OpenAI Scientist WARNS: \"You Have No Idea What's Coming\"AI Upload 4.9M views â€¢ 5 months ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=79-bApI3GIU)[1:01:19 dreamlike Pidalso 421K views â€¢ 7 months ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=qAyEPXuXVoo)[8:24 ONE Feather. ONE Girl. You CANNOT Look Away!Top Talent 10M views â€¢ 5 months ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=8XOaVF6KRn0)[2:00:50 Winter Scenes TV Art Screensaver | Vintage Winter Inspired Paintings | 8 Scenes For 2 Hours 2023 Verdot Studio TV Art 1.5M views â€¢ 2 years ago Live Playlist ()Mix (50+)](https://www.youtube.com/watch?v=TM2YGVVrWV4)\n\nSign in to confirm youâ€™re not a bot This helps protect our community. [Learn more](https://support.google.com/youtube/answer/3037019#zippy=%2Ccheck-that-youre-signed-into-youtube)\n\n[Sign in](https://accounts.google.com/ServiceLogin?service=youtube\u0026uilel=3\u0026passive=true\u0026continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3D%252Fwatch%253Fv%253Du_7OtyYAX74\u0026hl=en)\n\nI made AI count how many times Jensen says \"AI\" - Dive Desktop demo\n===================================================================\n\n[](https://www.youtube.com/@DeweiYen)\n\n[Dewei Yen](https://www.youtube.com/@DeweiYen)\n\n Dewei Yen \n\n8 subscribers\n\nSubscribe\n\nSubscribed\n\n6\n\nShare\n\nSave\n\nDownload\n\n Download \n\n212 views 12 hours ago\n\n 212 views â€¢ Jan 8, 2026 \n\nNo description has been added to this video.\n\nShow less \n\nâ€¦...more \n\n...more \n\n[### Dewei Yen 8 subscribers](https://www.youtube.com/@DeweiYen)\n\n[Videos](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/videos)\n\n[About](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/about)\n\n[Videos](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/videos)[About](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/about)\n\nShow less \n\n[](https://www.youtube.com/watch?v=u_7OtyYAX74)\nI made AI count how many times Jensen says \"AI\" - Dive Desktop demo\n===================================================================\n\n212 views\n\nJan 8, 2026\n\n6\n\nShare\n\nSave\n\nDownload\n\n Download \n\n0 Comments\n----------\n\n Sort comments \n\nSort by\n\n[Top Show featured comments](https://www.youtube.com/watch?v=u_7OtyYAX74)[Newest Show recent comments, including potential spam](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nAdd a comment...\n\nNaN / NaN\n\nComments\n--------\n\n[Top Show featured comments](https://www.youtube.com/watch?v=u_7OtyYAX74)[Newest Show recent comments, including potential spam](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nDescription\n-----------\n\nI made AI count how many times Jensen says \"AI\" - Dive Desktop demo\n\n6 Likes\n\n212 Views\n\n12h Ago\n\nâ€¦\n\n[### Dewei Yen 8 subscribers](https://www.youtube.com/@DeweiYen)\n\n[Videos](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/videos)\n\n[About](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/about)\n\n[Videos](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/videos)[About](https://www.youtube.com/channel/UCEWh2pcUcFnMaYt13UQCObQ/about)\n\n[8:07](https://www.youtube.com/watch?v=EAJ2vt8wUbY)\n\n### [I Entered A Jiu-Jitsu Tournament To Prove It Doesn't Work](https://www.youtube.com/watch?v=EAJ2vt8wUbY)\n\nJesse Enkamp\n\n10M views â€¢ 2 years ago\n\n[13:11](https://www.youtube.com/watch?v=WpUwR3BumNY)\n\n### [This Tool Changes Streaming Forever!](https://www.youtube.com/watch?v=WpUwR3BumNY)\n\nFreakweaselGaming\n\n5K views â€¢ 2 months ago\n\n[10:56](https://www.youtube.com/watch?v=D0IBRMq6jyA\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\n### [Unlocking Suno's Secret \"MAX MODE\" (Is It Real? I Tested It)](https://www.youtube.com/watch?v=D0IBRMq6jyA\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\nAI With TechZnap\n\n14K views â€¢ 3 days ago\n\nNew\n\n[1:00:00](https://www.youtube.com/watch?v=uPdyTOge6w4)\n\n### [1 Hour of Abstract Wave Height Map Loop | QuietQuests](https://www.youtube.com/watch?v=uPdyTOge6w4)\n\nWavy\n\n37K views â€¢ 1 year ago\n\n[4:24](https://www.youtube.com/watch?v=eE2BQ7rE7OU)\n\n### [Kristen Wiig Breaking People on SNL for 4 Minutes Straight](https://www.youtube.com/watch?v=eE2BQ7rE7OU)\n\nPopMojo\n\n2.2M views â€¢ 10 months ago\n\n[1:00:10](https://www.youtube.com/watch?v=CYLIc5S6KK4)\n\n### [No-Break Study Timer ðŸŒ¸ | 1 Hour of Pink Aesthetic Productivity](https://www.youtube.com/watch?v=CYLIc5S6KK4)\n\nFocus Corner â™¡\n\n439K views â€¢ 1 year ago\n\n[1:01:11](https://www.youtube.com/watch?v=YhNjL5hROf8)\n\n### [LINEAS AZULES ðŸŒˆ FONDO de PANTALLA con MOVIMIENTO - VFX ã€4Kã€‘GRATIS âœ… (no copyright)ðŸ’ª](https://www.youtube.com/watch?v=YhNjL5hROf8)\n\nFondos Animados - Arte VFX\n\n42K views â€¢ 4 years ago\n\n[2:00:00](https://www.youtube.com/watch?v=uIq-2oRRt1w)\n\n### [Vintage Floral TV Art Screensaver Tv Wallpaper Home Decor Oil Painting Digital Wall Art](https://www.youtube.com/watch?v=uIq-2oRRt1w)\n\nLunaSkyeTvArt\n\n521K views â€¢ 1 year ago\n\n[18:14](https://www.youtube.com/watch?v=79-bApI3GIU\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\n### [Ex-OpenAI Scientist WARNS: \"You Have No Idea What's Coming\"](https://www.youtube.com/watch?v=79-bApI3GIU\u0026pp=ugUHEgVlbi1VUw%3D%3D)\n\nAI Upload\n\n4.9M views â€¢ 5 months ago\n\n[1:01:19](https://www.youtube.com/watch?v=qAyEPXuXVoo\u0026list=RDqAyEPXuXVoo\u0026start_radio=1\u0026pp=oAcB)\n\n### [dreamlike](https://www.youtube.com/watch?v=qAyEPXuXVoo\u0026list=RDqAyEPXuXVoo\u0026start_radio=1\u0026pp=oAcB)\n\nPidalso\n\n421K views â€¢ 7 months ago\n\n[8:24](https://www.youtube.com/watch?v=8XOaVF6KRn0)\n\n### [ONE Feather. ONE Girl. You CANNOT Look Away!](https://www.youtube.com/watch?v=8XOaVF6KRn0)\n\nTop Talent\n\n10M views â€¢ 5 months ago\n\n[2:00:50](https://www.youtube.com/watch?v=TM2YGVVrWV4)\n\n### [Winter Scenes TV Art Screensaver | Vintage Winter Inspired Paintings | 8 Scenes For 2 Hours 2023](https://www.youtube.com/watch?v=TM2YGVVrWV4)\n\nVerdot Studio TV Art\n\n1.5M views â€¢ 2 years ago\n\n[3:24](https://www.youtube.com/watch?v=Vtpj_w_QeA4)\n\n### [Simple Review: A simple free tool for reviewing videos](https://www.youtube.com/watch?v=Vtpj_w_QeA4)\n\nHYVE Studio\n\n34 views â€¢ 6 days ago\n\nNew\n\n[1:00:37](https://www.youtube.com/watch?v=VtLHngErlEM)\n\n### [60 min | Relaxing Screensaver | Bokeh Warm Glowing Lights](https://www.youtube.com/watch?v=VtLHngErlEM)\n\nMoodSetter\n\n407K views â€¢ 5 years ago\n\n[1:00:00](https://www.youtube.com/watch?v=95qnVSzSn2Y)\n\n### [1 Hour of Dark Abstract Height Map Pattern Loop Animation | QuietQuests](https://www.youtube.com/watch?v=95qnVSzSn2Y)\n\nWavy\n\n1.3M views â€¢ 2 years ago\n\n[28:41](https://www.youtube.com/watch?v=Mxy6MVbpNhg\u0026pp=ugUEEgJlbtIHCQlNCgGHKiGM7w%3D%3D)\n\n### [These ChatGPT Hacks Will Make You SO Productive It Feels Illegal](https://www.youtube.com/watch?v=Mxy6MVbpNhg\u0026pp=ugUEEgJlbtIHCQlNCgGHKiGM7w%3D%3D)\n\nDan Martell\n\n638K views â€¢ 2 weeks ago\n\n[1:03:42](https://www.youtube.com/watch?v=EY7GWjrYBBo\u0026list=RDEY7GWjrYBBo\u0026start_radio=1\u0026pp=oAcB)\n\n### [PRAY - Soaking worship instrumental | Prayer and Devotional](https://www.youtube.com/watch?v=EY7GWjrYBBo\u0026list=RDEY7GWjrYBBo\u0026start_radio=1\u0026pp=oAcB)\n\nCentral Record\n\n226K views â€¢ 4 months ago\n\n[2:00:01](https://www.youtube.com/watch?v=8sMAvL_BkF0)\n\n### [Winter Celebration of Light and Warmth | Art Frame Screensaver | Art for your TV | 2 Hrs | 4K ðŸŽ„ðŸŽ†](https://www.youtube.com/watch?v=8sMAvL_BkF0)\n\nFrame TV Art Gallery Wallpapers\n\n14K views â€¢ 1 year ago\n\n[7:48](https://www.youtube.com/watch?v=_MJERTr8gfU\u0026pp=ugUEEgJlbg%3D%3D)\n\n### [I Married An Irish Guy and Chinese Is Too Hard For Him To Learn | Dawn Wong](https://www.youtube.com/watch?v=_MJERTr8gfU\u0026pp=ugUEEgJlbg%3D%3D)\n\nDawn Wong\n\n9.6M views â€¢ 4 years ago\n\n[1:00:00](https://www.youtube.com/watch?v=DZlz8WcjATc)\n\n### [Vintage Frame TV Art | Mountain Landscape Oil Painting | Scenery Screensaver Slideshow | 1 Hr of 4K](https://www.youtube.com/watch?v=DZlz8WcjATc)\n\n7artprints\n\n13K views â€¢ 1 year ago\n\nShow more\n\n[[](https://www.youtube.com/watch?v=u_7OtyYAX74)](https://www.youtube.com/watch?v=u_7OtyYAX74)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:34:37.770275902Z"
    },
    {
      "flow_id": "",
      "id": "1q7mvuf",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/",
      "title": "Z.ai  (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange",
      "content": "",
      "author": "Old-School8916",
      "created_at": "2026-01-08T20:23:59Z",
      "comments": [
        {
          "id": "nygtope",
          "author": "ForsookComparison",
          "content": "Hopefully they're all partying it up.\n\nAnd hopefully their new shareholders don't mind it if they spend millions in compute to give us free stuff ðŸ˜¬",
          "created_at": "2026-01-08T20:37:16Z",
          "was_summarised": false
        },
        {
          "id": "nygwihp",
          "author": "_Sneaky_Bastard_",
          "content": "they also said GLM 5 is in training. hoping it would be a open weight release.",
          "created_at": "2026-01-08T20:49:51Z",
          "was_summarised": false
        },
        {
          "id": "nygvbc0",
          "author": "TheAncientPizza711",
          "content": "They issued shares at HK$116.20 each. Opened at HK$120 and is now currently HK$131.50.\n\nStock is up 13.17% on its 1st day. Not bad.",
          "created_at": "2026-01-08T20:44:33Z",
          "was_summarised": false
        },
        {
          "id": "nyh2044",
          "author": "FullOf_Bad_Ideas",
          "content": "Minimax IPOs a day later, 9th of January.\n\nLots of info about both of them can be found here:\n\n[Zhipu offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1230/2025123000017.pdf)\n\n[Minimax offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1231/2025123100025.pdf)\n\nSource: [this website](https://www.hkex.com.hk/Services/Trading/Securities/Trading-News/Newly-Listed-Securities?sc_lang=en)\n\nOne of the very interesting thing contained there is that Zhipu has slightly negative profit margin on GLM Coding Plan. They lose money on serving alone, not even counting in marketing or R\u0026amp;D costs.",
          "created_at": "2026-01-08T21:14:09Z",
          "was_summarised": false
        },
        {
          "id": "nyh1ztw",
          "author": "jacek2023",
          "content": "Hype hype and still no Air",
          "created_at": "2026-01-08T21:14:07Z",
          "was_summarised": false
        },
        {
          "id": "nygrahs",
          "author": "rookan",
          "content": "Good for them",
          "created_at": "2026-01-08T20:26:28Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://x.com/Zai_org/status/2009290783678239032",
          "was_fetched": true,
          "page": "Title: Z.ai on X: \"Weâ€™re officially public. (HKEX: 02513)\n\nTo everyone who has supported GLM, built with it, tested it, or simply followed along. Thank you.â¤ï¸\nThis moment belongs to our community as much as it belongs to us.\n\nTo celebrate, weâ€™re opening a 48-hour community challenge.â¤ï¸â€ðŸ”¥â¤ï¸â€ðŸ”¥â¤ï¸â€ðŸ”¥\n\n48\" / X\n\nURL Source: https://x.com/Zai_org/status/2009290783678239032\n\nMarkdown Content:\nWeâ€™re officially public. (HKEX: 02513) To everyone who has supported GLM, built with it, tested it, or simply followed along. Thank you. This moment belongs to our community as much as it belongs to us. To celebrate, weâ€™re opening a 48-hour community challenge. 48 hours. A few ways to join!  Comment challenge Every 12 hours, weâ€™ll select the top 25 comments by likes. Each will receive $50 in credits.  Repost challenge Every 24 hours, weâ€™ll select the top 13 reposts by likes. Each will receive $200 in credits.  Editorâ€™s picks Some of the most interesting ideas donâ€™t always get the most likes. Weâ€™ll be reading closely and highlighting thoughtful, original developer posts. If your post is selected, Lou\n\n[@louszbd](https://x.com/louszbd)\n\nwill reach out personally with an exclusive developer gift pack. Weâ€™ll wrap up in 48 hours. All rewards will be sent within 72 hours after the challenge ends. Letâ€™s celebrate! [z.ai/subscribe?utm_](https://t.co/n5FbqerBd1)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:34:43.788210501Z"
    },
    {
      "flow_id": "",
      "id": "1q7jd1a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/",
      "title": "LFM2.5 1.2B Instruct is amazing",
      "content": "This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.\n\nImportant note:  \n\"\"\"  \nWe recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.\n\n\"\"\"\n\n[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)",
      "author": "Paramecium_caudatum_",
      "created_at": "2026-01-08T18:17:04Z",
      "comments": [
        {
          "id": "nyg2ddq",
          "author": "pj-frey",
          "content": "It is the perfect small \"helper\" model for Open WebUI creating tags, chat headlines, web searches and that kind of stuff. Fast AND good. I love it. Big thank you to LiquidAI (and unsloth).",
          "created_at": "2026-01-08T18:37:05Z",
          "was_summarised": false
        },
        {
          "id": "nyg9prk",
          "author": "YearZero",
          "content": "Yeah models at this size just need to work with the data you give them and adhere to the prompt as much as possible.",
          "created_at": "2026-01-08T19:08:38Z",
          "was_summarised": false
        },
        {
          "id": "nygh6w6",
          "author": "-Akos-",
          "content": "I'm amazed, especially now that it has tool use. A few days ago it didn't yet, but now I can enable MCP in LM Studio, and have blazing fast inference. On my 8th gen i7 with a 1050Ti nvidia I am getting 41 tps!!\n\nI've asked it to create a small webpage, and then it complained that it's not good at that, so indeed the programming part is correct.",
          "created_at": "2026-01-08T19:41:30Z",
          "was_summarised": false
        },
        {
          "id": "nyggecn",
          "author": "LionStrange493",
          "content": "Yeah, that caveat is important.\n\nSmaller models can be surprisingly good, but once tools/RAG get involved, edge cases show up quickly.\n\nCurious how this holds up in real agent setups.",
          "created_at": "2026-01-08T19:38:00Z",
          "was_summarised": false
        },
        {
          "id": "nyh8p9o",
          "author": "Noob_l",
          "content": "Very promising, can it also do translation? \n~~What models are recommended for local on device translation?\nI had mixed results with small qwen results. \nThank you in advance for any helpers that can point me into the right direction. (I do not know which benchmarks would be for translation)~~\n\nEdit: no it does seem like a model that cannot follow instructions on translation well. I would still love to know which models are used by the community",
          "created_at": "2026-01-08T21:43:20Z",
          "was_summarised": false
        },
        {
          "id": "nygrptl",
          "author": "crantob",
          "content": "LFM2 has a variety of interrrresting models....",
          "created_at": "2026-01-08T20:28:22Z",
          "was_summarised": false
        },
        {
          "id": "nyh2lxw",
          "author": "ElectronSpiderwort",
          "content": "It is strong for 1.2B. I argued with it a bit about a topic I know well; it was confidently incorrect for a particular formula, and even when I corrected it, it kept spitting out the incorrect formula. It just couldn't use the corrected formula because it was sure the incorrect formula was \"the standard formula\". I wonder what happens in RAG when it retrieves a piece of information it thinks is wrong? \n\nRelatedly, I thing training data should include epistemic humility",
          "created_at": "2026-01-08T21:16:51Z",
          "was_summarised": false
        },
        {
          "id": "nyh1m7k",
          "author": "DHasselhoff77",
          "content": "I tried to replace Granite-4.0-h-micro with this for tab completion but the `/completion` endpoint in llama.cpp gave 501 errors when I loaded LFM2.5-1.2B-Q8.gguf. Perhaps it's missing FIM support?",
          "created_at": "2026-01-08T21:12:26Z",
          "was_summarised": false
        },
        {
          "id": "nyhhafa",
          "author": "countAbsurdity",
          "content": "Dunno I was never impressed with previous iterations, they always misinterpreted what I wrote them or hallucinated a lot.",
          "created_at": "2026-01-08T22:21:05Z",
          "was_summarised": false
        },
        {
          "id": "nyhl8y7",
          "author": "AyraWinla",
          "content": "Favorably impressed; it actually writes pretty well and has very good understanding for its size. Under 2b, it's by far the best I've seen at it, with no real competition (small Qwen and Granite aren't too good at it in my opinion, and Gemma 2 2b is pretty outdated by now).\n\nLarger models like Gemma 3 4b or even E2B definitely give better results, but LFM2.5 1.2b runs MUCH faster on my phone; it's the first time I try a model that feels both smart and fast local on my phone. Normally it's just one or the other. I'm not going to throw away my usual Gemma models, but LFM2.5 will definitely see some use from me and I'll certainly be experimenting more with it.\n\nPretty impressive!",
          "created_at": "2026-01-08T22:39:27Z",
          "was_summarised": false
        },
        {
          "id": "nyiglpa",
          "author": "Thin_Yoghurt_6483",
          "content": "Give practical examples of how to use a model.",
          "created_at": "2026-01-09T01:19:35Z",
          "was_summarised": false
        },
        {
          "id": "nygrtn7",
          "author": "nonerequired_",
          "content": "But context size is too low. Is there any way to increase that?",
          "created_at": "2026-01-08T20:28:50Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct",
          "was_fetched": true,
          "page": "Title: LiquidAI/LFM2.5-1.2B-Instruct Â· Hugging Face\n\nURL Source: https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct\n\nMarkdown Content:\nLFM2.5 is a new family of hybrid models designed for **on-device deployment**. It builds on the LFM2 architecture with extended pre-training and reinforcement learning.\n\n*   **Best-in-class performance**: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket.\n*   **Fast edge inference**: 239 tok/s decode on AMD CPU, 82 tok/s on mobile NPU. Runs under 1GB of memory with day-one support for llama.cpp, MLX, and vLLM.\n*   **Scaled training**: Extended pre-training from 10T to 28T tokens and large-scale multi-stage reinforcement learning.\n\n[](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/dxnYF2fuLpulismtFSGFi.png)\n\nFind more information about LFM2.5 in our [blog post](https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai).\n\n[](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#%F0%9F%97%92%EF%B8%8F-model-details) ðŸ—’ï¸ Model Details\n--------------------------------------------------------------------------------------------------------------\n\n| Model | Parameters | Description |\n| --- | --- | --- |\n| [LFM2.5-1.2B-Base](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Base) | 1.2B | Pre-trained base model for fine-tuning |\n| [**LFM2.5-1.2B-Instruct**](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct) | 1.2B | General-purpose instruction-tuned model |\n| [LFM2.5-1.2B-JP](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP) | 1.2B | Japanese-optimized chat model |\n| [LFM2.5-VL-1.6B](https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B) | 1.6B | Vision-language model with fast inference |\n| [LFM2.5-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B) | 1.5B | Audio-language model for speech and text I/O |\n\nLFM2.5-1.2B-Instruct is a general-purpose text-only model with the following features:\n\n*   **Number of parameters**: 1.17B\n*   **Number of layers**: 16 (10 double-gated LIV convolution blocks + 6 GQA blocks)\n*   **Training budget**: 28T tokens\n*   **Context length**: 32,768 tokens\n*   **Vocabulary size**: 65,536\n*   **Languages**: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish\n*   **Generation parameters**:\n    *   `temperature: 0.1`\n    *   `top_k: 50`\n    *   `top_p: 0.1`\n    *   `repetition_penalty: 1.05`\n\n| Model | Description |\n| --- | --- |\n| [**LFM2.5-1.2B-Instruct**](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct) | Original model checkpoint in native format. Best for fine-tuning or inference with Transformers and vLLM. |\n| [LFM2.5-1.2B-Instruct-GGUF](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-GGUF) | Quantized format for llama.cpp and compatible tools. Optimized for CPU inference and local deployment with reduced memory usage. |\n| [LFM2.5-1.2B-Instruct-ONNX](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-ONNX) | ONNX Runtime format for cross-platform deployment. Enables hardware-accelerated inference across diverse environments (cloud, edge, mobile). |\n| [LFM2.5-1.2B-Instruct-MLX](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct-MLX-8bit) | MLX format for Apple Silicon. Optimized for fast inference on Mac devices using the MLX framework. |\n\nWe recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.\n\n### [](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#chat-template) Chat Template\n\nLFM2.5 uses a ChatML-like format. See the [Chat Template documentation](https://docs.liquid.ai/lfm/key-concepts/chat-template) for details. Example:\n\n```\n\u003c|startoftext|\u003e\u003c|im_start|\u003esystem\nYou are a helpful assistant trained by Liquid AI.\u003c|im_end|\u003e\n\u003c|im_start|\u003euser\nWhat is C. elegans?\u003c|im_end|\u003e\n\u003c|im_start|\u003eassistant\n```\n\nYou can use [`tokenizer.apply_chat_template()`](https://huggingface.co/docs/transformers/en/chat_templating#using-applychattemplate) to format your messages automatically.\n\n### [](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#tool-use) Tool Use\n\nLFM2.5 supports function calling as follows:\n\n1.   **Function definition**: We recommend providing the list of tools as a JSON object in the system prompt. You can also use the [`tokenizer.apply_chat_template()`](https://huggingface.co/docs/transformers/en/chat_extras#passing-tools) function with tools.\n2.   **Function call**: By default, LFM2.5 writes Pythonic function calls (a Python list between `\u003c|tool_call_start|\u003e` and `\u003c|tool_call_end|\u003e` special tokens), as the assistant answer. You can override this behavior by asking the model to output JSON function calls in the system prompt.\n3.   **Function execution**: The function call is executed, and the result is returned as a \"tool\" role.\n4.   **Final answer**: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\n\nSee the [Tool Use documentation](https://docs.liquid.ai/lfm/key-concepts/tool-use) for the full guide. Example:\n\n```\n\u003c|startoftext|\u003e\u003c|im_start|\u003esystem\nList of tools: [{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]\u003c|im_end|\u003e\n\u003c|im_start|\u003euser\nWhat is the current status of candidate ID 12345?\u003c|im_end|\u003e\n\u003c|im_start|\u003eassistant\n\u003c|tool_call_start|\u003e[get_candidate_status(candidate_id=\"12345\")]\u003c|tool_call_end|\u003eChecking the current status of candidate ID 12345.\u003c|im_end|\u003e\n\u003c|im_start|\u003etool\n[{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}]\u003c|im_end|\u003e\n\u003c|im_start|\u003eassistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.\u003c|im_end|\u003e\n```\n\n[](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#%F0%9F%8F%83-inference) ðŸƒ Inference\n--------------------------------------------------------------------------------------------\n\nLFM2.5 is supported by many inference frameworks. See the [Inference documentation](https://docs.liquid.ai/lfm/inference/transformers) for the full list.\n\n| Name | Description | Docs | Notebook |\n| --- | --- | --- | --- |\n| [Transformers](https://github.com/huggingface/transformers) | Simple inference with direct access to model internals. | [Link](https://docs.liquid.ai/lfm/inference/transformers) | [](https://colab.research.google.com/drive/1_q3jQ6LtyiuPzFZv7Vw8xSfPU5FwkKZY?usp=sharing) |\n| [vLLM](https://github.com/vllm-project/vllm) | High-throughput production deployments with GPU. | [Link](https://docs.liquid.ai/lfm/inference/vllm) | [](https://colab.research.google.com/drive/1VfyscuHP8A3we_YpnzuabYJzr5ju0Mit?usp=sharing) |\n| [llama.cpp](https://github.com/ggml-org/llama.cpp) | Cross-platform inference with CPU offloading. | [Link](https://docs.liquid.ai/lfm/inference/llama-cpp) | [](https://colab.research.google.com/drive/1ohLl3w47OQZA4ELo46i5E4Z6oGWBAyo8?usp=sharing) |\n| [MLX](https://github.com/ml-explore/mlx) | Apple's machine learning framework optimized for Apple Silicon. | [Link](https://docs.liquid.ai/lfm/inference/mlx) | â€” |\n| [LM Studio](https://lmstudio.ai/) | Desktop application for running LLMs locally. | [Link](https://docs.liquid.ai/lfm/inference/lm-studio) | â€” |\n\nHere's a quick start example with Transformers:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\nmodel_id = \"LiquidAI/LFM2.5-1.2B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    dtype=\"bfloat16\",\n#   attn_implementation=\"flash_attention_2\" \u003c- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\nprompt = \"What is C. elegans?\"\n\ninput_ids = tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": prompt}],\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    tokenize=True,\n).to(model.device)\n\noutput = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.1,\n    top_k=50,\n    top_p=0.1,\n    repetition_penalty=1.05,\n    max_new_tokens=512,\n    streamer=streamer,\n)\n```\n\n[](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#%F0%9F%94%A7-fine-tuning) ðŸ”§ Fine-Tuning\n------------------------------------------------------------------------------------------------\n\nWe recommend fine-tuning LFM2.5 for your specific use case to achieve the best results.\n\n| Name | Description | Docs | Notebook |\n| --- | --- | --- | --- |\n| SFT ([Unsloth](https://github.com/unslothai/unsloth)) | Supervised Fine-Tuning with LoRA using Unsloth. | [Link](https://docs.liquid.ai/lfm/fine-tuning/unsloth) | [](https://colab.research.google.com/drive/1HROdGaPFt1tATniBcos11-doVaH7kOI3?usp=sharing) |\n| SFT ([TRL](https://github.com/huggingface/trl)) | Supervised Fine-Tuning with LoRA using TRL. | [Link](https://docs.liquid.ai/lfm/fine-tuning/trl) | [](https://colab.research.google.com/drive/1j5Hk_SyBb2soUsuhU0eIEA9GwLNRnElF?usp=sharing) |\n| DPO ([TRL](https://github.com/huggingface/trl)) | Direct Preference Optimization with LoRA using TRL. | [Link](https://docs.liquid.ai/lfm/fine-tuning/trl) | [](https://colab.research.google.com/drive/1MQdsPxFHeZweGsNx4RH7Ia8lG8PiGE1t?usp=sharing) |\n\n[](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#%F0%9F%93%8A-performance) ðŸ“Š Performance\n------------------------------------------------------------------------------------------------\n\n### [](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#benchmarks) Benchmarks\n\nWe compared LFM2.5-1.2B-Instruct with relevant sub-2B models on a diverse suite of benchmarks.\n\n| Model | GPQA | MMLU-Pro | IFEval | IFBench | Multi-IF | AIME25 | BFCLv3 |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| **LFM2.5-1.2B-Instruct** | 38.89 | 44.35 | 86.23 | 47.33 | 60.98 | 14.00 | 49.12 |\n| Qwen3-1.7B (instruct) | 34.85 | 42.91 | 73.68 | 21.33 | 56.48 | 9.33 | 46.30 |\n| Granite 4.0-1B | 24.24 | 33.53 | 79.61 | 21.00 | 43.65 | 3.33 | 52.43 |\n| Llama 3.2 1B Instruct | 16.57 | 20.80 | 52.37 | 15.93 | 30.16 | 0.33 | 21.44 |\n| Gemma 3 1B IT | 24.24 | 14.04 | 63.25 | 20.47 | 44.31 | 1.00 | 16.64 |\n\nGPQA, MMLU-Pro, IFBench, and AIME25 follow [ArtificialAnalysis's methodology](https://artificialanalysis.ai/methodology/intelligence-benchmarking). For IFEval and Multi-IF, we report the average score across strict and loose prompt and instruction accuracies. For BFCLv3, we report the final weighted average score with a custom Liquid handler to support our tool use template.\n\n### [](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#inference-speed) Inference speed\n\nLFM2.5-1.2B-Instruct offers extremely fast inference speed on CPUs with a low memory profile compared to similar-sized models.\n\n[](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/dbbI-15p9re2ROhAkqnZm.png)\n\nIn addition, we are partnering with AMD, Qualcomm, and Nexa AI to bring the LFM2.5 family to NPUs. These optimized models are available through our partners, enabling highly efficient on-device inference.\n\n| Device | Inference | Framework | Model | Prefill (tok/s) | Decode (tok/s) | Memory (GB) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Qualcomm SnapdragonÂ® X Elite | NPU | NexaML | LFM2.5-1.2B-Instruct | 2591 | 63 | 0.9GB |\n| Qualcomm SnapdragonÂ® Gen4 (ROG Phone9 Pro) | NPU | NexaML | LFM2.5-1.2B-Instruct | 4391 | 82 | 0.9GB |\n| Qualcomm SnapdragonÂ® Gen4 (Samsung Galaxy S25 Ultra) | CPU | llama.cpp (Q4_0) | LFM2.5-1.2B-Instruct | 335 | 70 | 719MB |\n| Qualcomm SnapdragonÂ® Gen4 (Samsung Galaxy S25 Ultra) | CPU | llama.cpp (Q4_0) | Qwen3-1.7B | 181 | 40 | 1306MB |\n\nThese capabilities unlock new deployment scenarios across various devices, including vehicles, mobile devices, laptops, IoT devices, and embedded systems.\n\n[](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#contact) Contact\n------------------------------------------------------------------------\n\nFor enterprise solutions and edge deployment, contact [sales@liquid.ai](mailto:sales@liquid.ai).\n\n[](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct#citation) Citation\n--------------------------------------------------------------------------\n\n```\n@article{liquidai2025lfm2,\n  title={LFM2 Technical Report},\n  author={Liquid AI},\n  journal={arXiv preprint arXiv:2511.23404},\n  year={2025}\n}\n```",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:34:48.845254926Z"
    },
    {
      "flow_id": "",
      "id": "1q7uuxo",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/",
      "title": "OK I get it, now I love llama.cpp",
      "content": "I just made the switch from Ollama to llama.cpp.  Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all.  Once you realize what you truly want to run, llama.cpp is really the way to go.\n\nMy hardware ain't great, I have a single 3060 12GB GPU and three P102-100 GPUs for a total of 42GB.  My system ram is 96GB along with an Intel i7-9800x.  It blows my mind that with some tuning what difference it can make.  You really need to understand each of the commands for llama.cpp to get the most out of it especially with uneven vram like mine.  I used Chatgpt, Perplexity and suprisingly only Google AI studio could optimize my settings while teaching me along the way.\n\nCrazy how these two commands both fill up the ram but one is twice as fast as the other.  Chatgpt helped me with the first one, Google AI with the other ;).  Now I'm happy running local lol.\n\n**11t/s:**  \nsudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo CUDA\\_VISIBLE\\_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4\\_K\\_M/gpt-oss-120b-Q4\\_K\\_M-00001-of-00002.gguf --n-gpu-layers 21 --main-gpu 0 --flash-attn off --cache-type-k q8\\_0 --cache-type-v f16 --ctx-size 30000 --port 8080 --host [0.0.0.0](http://0.0.0.0) \\--mmap --numa distribute --batch-size 384 --ubatch-size 256 --jinja --threads $(nproc) --parallel 2 --tensor-split 12,10,10,10 --mlock\n\n**21t/s**  \nsudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo GGML\\_CUDA\\_ENABLE\\_UNIFIED\\_MEMORY=0 CUDA\\_VISIBLE\\_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4\\_K\\_M/gpt-oss-120b-Q4\\_K\\_M-00001-of-00002.gguf --n-gpu-layers 99 --main-gpu 0 --split-mode layer --tensor-split 5,5,6,20 -ot \"blk\\\\.(2\\[1-9\\]|\\[3-9\\]\\[0-9\\])\\\\.ffn\\_.\\*\\_exps\\\\.weight=CPU\" --ctx-size 30000 --port 8080 --host [0.0.0.0](http://0.0.0.0) \\--batch-size 512 --ubatch-size 256 --threads 8 --parallel 1 --mlock\n\nNothing here is worth copying and pasting as it is unique to my config but the moral of the story is, if you tune llama.cpp this thing will FLY!",
      "author": "vulcan4d",
      "created_at": "2026-01-09T01:39:13Z",
      "comments": [
        {
          "id": "nyit742",
          "author": "pmttyji",
          "content": "Since you have 42GB VRAM, experiment with increased batch-size(1024) \u0026amp; ubatch-size(4096) for more better t/s. And bottom command doesn't have flash attention, enable it.\n\nAnd don't use quantized version of GPT-OSS-120B model. Use MXFP4 version [https://huggingface.co/ggml-org/gpt-oss-120b-GGUF](https://huggingface.co/ggml-org/gpt-oss-120b-GGUF) instead which is best.",
          "created_at": "2026-01-09T02:26:39Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://0.0.0.0",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:34:50.200226405Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://0.0.0.0: jina: status 451: {\"data\":null,\"path\":\"url\",\"code\":451,\"name\":\"SecurityCompromiseError\",\"status\":45102,\"message\":\"Suspicious action: Request to localhost or non-public IP: 0.0.0.0\",\"readableMessage\":\"SecurityCompromiseError: Suspicious action: Request to localhost or non-public IP: 0.0.0.0\"}: retry failed: jina request failed: 451 Unavailable For Legal Reasons",
          "occurred_at": "2026-01-09T02:34:50.200224041Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7dlkn",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/",
      "title": "Qwen3-VL-Reranker - a Qwen Collection",
      "content": "",
      "author": "LinkSea8324",
      "created_at": "2026-01-08T14:45:00Z",
      "comments": [
        {
          "id": "nyen4yx",
          "author": "swagonflyyyy",
          "content": "A...a...multimodal RERANKER??????",
          "created_at": "2026-01-08T14:50:02Z",
          "was_summarised": false
        },
        {
          "id": "nyenqbx",
          "author": "Hanselltc",
          "content": "multimodal rag in my home lab? yes please!",
          "created_at": "2026-01-08T14:52:54Z",
          "was_summarised": false
        },
        {
          "id": "nyfeiy6",
          "author": "unofficialmerve",
          "content": "I have just built an e2e notebook chaining these models together with Qwen3-VL for multimodal RAG if anyone's interested!  [https://colab.research.google.com/drive/1LyGQcNhrv7QnpSOKyUkHojD3Bq7MkGbU?usp=sharing](https://colab.research.google.com/drive/1LyGQcNhrv7QnpSOKyUkHojD3Bq7MkGbU?usp=sharing)Â \n\nhttps://preview.redd.it/8zv6o6h8o5cg1.png?width=2200\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2b28640bc39e55c1915cc0bad815dce6bd86579",
          "created_at": "2026-01-08T16:53:40Z",
          "was_summarised": false
        },
        {
          "id": "nyem95q",
          "author": "LinkSea8324",
          "content": "Didn't want to make a double post, they also released Qwen3-VL Embeddings : https://huggingface.co/collections/Qwen/qwen3-vl-embedding\n\nTech report : https://github.com/QwenLM/Qwen3-VL-Embedding/blob/main/assets/qwen3vlembedding_technical_report.pdf",
          "created_at": "2026-01-08T14:45:43Z",
          "was_summarised": false
        },
        {
          "id": "nyfthgd",
          "author": "planetearth80",
          "content": "Can this be used in OpenWebUI?",
          "created_at": "2026-01-08T17:59:03Z",
          "was_summarised": false
        },
        {
          "id": "nyeurap",
          "author": "exaknight21",
          "content": "Wow. Just wow.",
          "created_at": "2026-01-08T15:25:59Z",
          "was_summarised": false
        },
        {
          "id": "nyf1jct",
          "author": "coder543",
          "content": "The example they provide is funny/not confidence inspiring:\n\n    inputs = {\n        \"instruction\": \"Retrieval relevant image or text with user's query\",\n        \"query\": {\"text\": \"A woman playing with her dog on a beach at sunset.\"},\n        \"documents\": [\n            {\"text\": \"A woman shares a joyful moment with her golden retriever on a sun-drenched beach at sunset, as the dog offers its paw in a heartwarming display of companionship and trust.\"},\n            {\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"},\n            {\"text\": \"A woman shares a joyful moment with her golden retriever on a sun-drenched beach at sunset, as the dog offers its paw in a heartwarming display of companionship and trust.\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"}\n        ],\n        \"fps\": 1.0\n    }\n    \n    scores = model.process(inputs)\n    print(scores)\n    # [0.7838293313980103, 0.585621178150177, 0.6147719025611877]\n\nSo, the random snippet of text is ranked higher than the actual picture or the same snippet of text with the actual picture? Shouldn't the third one be the highest ranked, most relevant response?",
          "created_at": "2026-01-08T15:56:38Z",
          "was_summarised": false
        },
        {
          "id": "nyeqb03",
          "author": "maglat",
          "content": "To understand it right, to RAG a pdf which includes Text and images, I first need to OCR it, than embed it with Qwen3VL Embedding and at the end rank the content with Qwen3 VL Reranker?",
          "created_at": "2026-01-08T15:05:14Z",
          "was_summarised": false
        },
        {
          "id": "nyfqi1l",
          "author": "TaiMaiShu-71",
          "content": "Does the embedding model support patch embeddings like the colpali models do?",
          "created_at": "2026-01-08T17:46:12Z",
          "was_summarised": false
        },
        {
          "id": "nygib67",
          "author": "Sensitive_Sweet_1850",
          "content": "qwen is a making amazing job",
          "created_at": "2026-01-08T19:46:27Z",
          "was_summarised": false
        },
        {
          "id": "nygwotm",
          "author": "Salt-Advertising-939",
          "content": "when moe reranker so that it runs good on cpu",
          "created_at": "2026-01-08T20:50:37Z",
          "was_summarised": false
        },
        {
          "id": "nygxedf",
          "author": "Flamenverfer",
          "content": "Has anyone got this running? I tried in google colab and i am having issues\n\nThis dependency, pip install qwen-vl-utils\n\n    ModuleNotFoundError                       Traceback (most recent call last)\n    /tmp/ipython-input-2332886689.py in \u0026lt;cell line: 0\u0026gt;()\n    ----\u0026gt; 1 from scripts.qwen_vl_reranker import Qwen3VLReranker\n          2 \n          3 \n          4 # Specify the model path\n          5 model_name_or_path = \"Qwen/Qwen3-VL-Reranker-2B\"\n\n    ModuleNotFoundError: No module named 'scripts'\n\n\nIts trying to import something that doesn't exist?\n\n    from scripts.qwen_vl_reranker import Qwen3VLReranker",
          "created_at": "2026-01-08T20:53:44Z",
          "was_summarised": false
        },
        {
          "id": "nyh6gam",
          "author": "newdoria88",
          "content": "Still waiting for Qwen Next VL",
          "created_at": "2026-01-08T21:33:36Z",
          "was_summarised": false
        },
        {
          "id": "nyh9q2n",
          "author": "richardanaya",
          "content": "How do you fine tune something like this?",
          "created_at": "2026-01-08T21:47:51Z",
          "was_summarised": false
        },
        {
          "id": "nyeq2n8",
          "author": "lolwutdo",
          "content": "Could this give \"vision\" to non vision models?\n\nEdit: maybe was a dumb question, just ignore. lol",
          "created_at": "2026-01-08T15:04:08Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/collections/Qwen/qwen3-vl-reranker",
          "was_fetched": true,
          "page": "Title: Qwen3-VL-Reranker - a Qwen Collection\n\nURL Source: https://huggingface.co/collections/Qwen/qwen3-vl-reranker\n\nMarkdown Content:\nQwen3-VL-Reranker - a Qwen Collection\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n[Qwen](https://huggingface.co/Qwen)'s Collections\n\n[Qwen3-VL-Reranker](https://huggingface.co/collections/Qwen/qwen3-vl-reranker)\n\n[Qwen3-VL-Embedding](https://huggingface.co/collections/Qwen/qwen3-vl-embedding)\n\n[Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl)\n\n[Qwen3Guard](https://huggingface.co/collections/Qwen/qwen3guard)\n\n[Qwen3-Omni](https://huggingface.co/collections/Qwen/qwen3-omni)\n\n[Qwen3-Next](https://huggingface.co/collections/Qwen/qwen3-next)\n\n[Qwen-Image](https://huggingface.co/collections/Qwen/qwen-image)\n\n[Qwen3-Coder](https://huggingface.co/collections/Qwen/qwen3-coder)\n\n[Qwen3](https://huggingface.co/collections/Qwen/qwen3)\n\n[Qwen3-Reranker](https://huggingface.co/collections/Qwen/qwen3-reranker)\n\n[Qwen3-Embedding](https://huggingface.co/collections/Qwen/qwen3-embedding)\n\n[WorldPM](https://huggingface.co/collections/Qwen/worldpm)\n\n[Qwen2.5-Omni](https://huggingface.co/collections/Qwen/qwen25-omni)\n\n[Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl)\n\n[Qwen2.5-1M](https://huggingface.co/collections/Qwen/qwen25-1m)\n\n[QVQ](https://huggingface.co/collections/Qwen/qvq)\n\n[QwQ](https://huggingface.co/collections/Qwen/qwq)\n\n[Qwen2.5-Coder](https://huggingface.co/collections/Qwen/qwen25-coder)\n\n[Qwen2.5-Math](https://huggingface.co/collections/Qwen/qwen25-math)\n\n[Qwen2.5](https://huggingface.co/collections/Qwen/qwen25)\n\n[Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl)\n\n[Qwen2-Audio](https://huggingface.co/collections/Qwen/qwen2-audio)\n\n[Qwen2-Math](https://huggingface.co/collections/Qwen/qwen2-math)\n\n[Qwen2](https://huggingface.co/collections/Qwen/qwen2)\n\n[Qwen1.5](https://huggingface.co/collections/Qwen/qwen15)\n\n[Qwen](https://huggingface.co/collections/Qwen/qwen)\n\nQwen3-VL-Reranker\n-----------------\n\nupdated about 13 hours ago\n\n[- [x] Upvote 20](https://huggingface.co/login?next=%2Fcollections%2FQwen%2Fqwen3-vl-reranker)\n*   [](https://huggingface.co/victor \"victor\")\n*   [](https://huggingface.co/mfeldman143 \"mfeldman143\")\n*   [](https://huggingface.co/mrdbourke \"mrdbourke\")\n*   [](https://huggingface.co/nguaman \"nguaman\")\n*   +10\n\n*   \n* * *\n\n[#### Qwen/Qwen3-VL-Reranker-2B Image-to-Text â€¢ 2Bâ€¢Updated about 18 hours agoâ€¢ 18 â€¢ 26](https://huggingface.co/Qwen/Qwen3-VL-Reranker-2B)\n*   \n* * *\n\n[#### Qwen/Qwen3-VL-Reranker-8B Image-to-Text â€¢ 9Bâ€¢Updated about 18 hours agoâ€¢ 1 â€¢ 23](https://huggingface.co/Qwen/Qwen3-VL-Reranker-8B)\n\n[- [x] Upvote 20](https://huggingface.co/login?next=%2Fcollections%2FQwen%2Fqwen3-vl-reranker)\n*   [](https://huggingface.co/victor \"victor\")\n*   [](https://huggingface.co/mfeldman143 \"mfeldman143\")\n*   [](https://huggingface.co/mrdbourke \"mrdbourke\")\n*   [](https://huggingface.co/nguaman \"nguaman\")\n*   +16\n\n*   Share collection\n*    View history\n*   [Collection guide](https://huggingface.co/docs/hub/collections)\n*   [Browse collections](https://huggingface.co/collections)\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:34:54.762976988Z"
    },
    {
      "flow_id": "",
      "id": "1q7nqxl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/",
      "title": "llama.cpp has Out-of-bounds Write in llama-server",
      "content": "Maybe good to know for some of you that might be running llama.cpp on a regular basis.\n\n\u0026gt;llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n\\_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama\\_memory\\_seq\\_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.\n\nAlso reported [for Debian](https://security-tracker.debian.org/tracker/CVE-2026-21869).",
      "author": "radarsat1",
      "created_at": "2026-01-08T20:56:15Z",
      "comments": [
        {
          "id": "nyh0wmu",
          "author": "dinerburgeryum",
          "content": "Important note:\n\n\u0026gt;Prerequisite: start the server with context shift enabled (--context-shift).\n\nIt appears you have to be running with the --context-shift flag, at least according to [llama.cpp's security advisory](https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c).",
          "created_at": "2026-01-08T21:09:17Z",
          "was_summarised": false
        },
        {
          "id": "nyh1yxi",
          "author": "coder543",
          "content": "Wouldn't recommend exposing this kind of server directly on the internet, that's for sure.",
          "created_at": "2026-01-08T21:14:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhvt6s",
          "author": "Repulsive_Educator61",
          "content": "Doesn't llama-server logs warn about not exposing llama-server to the internet because it's still in alpha/beta or something?",
          "created_at": "2026-01-08T23:31:38Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.cve.org/CVERecord?id=CVE-2026-21869",
          "was_fetched": true,
          "page": "Title: CVE Record: CVE-2026-21869\n\nURL Source: https://www.cve.org/CVERecord?id=CVE-2026-21869\n\nMarkdown Content:\nCVE Record: CVE-2026-21869\n===============\n\nOpens in a new window Opens an external website Opens an external website in a new window\n\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. [Privacy Policy](https://www.cve.org/Legal/PrivacyPolicy)\n\nAccept Deny Non-Essential Manage Preferences \n\nCommon vulnerabilities and Exposures (CVE)\n==========================================\n\n[Skip to main content](https://www.cve.org/CVERecord?id=CVE-2026-21869#cve-main-page-content)\n\n[](https://www.cve.org/)\n\n[About](https://www.cve.org/About/Overview)\n\n[[Overview](https://www.cve.org/About/Overview)](https://www.cve.org/About/Overview)[[History](https://www.cve.org/About/History)](https://www.cve.org/About/History)[[Process](https://www.cve.org/About/Process)](https://www.cve.org/About/Process)[[Related Efforts](https://www.cve.org/About/RelatedEfforts)](https://www.cve.org/About/RelatedEfforts)[[Metrics](https://www.cve.org/About/Metrics)](https://www.cve.org/About/Metrics)\n\n[Partner Information](https://www.cve.org/PartnerInformation/Partner)\n\n[[Partner](https://www.cve.org/PartnerInformation/Partner)](https://www.cve.org/PartnerInformation/Partner)[[List of Partners](https://www.cve.org/PartnerInformation/ListofPartners)](https://www.cve.org/PartnerInformation/ListofPartners)\n\n[Program Organization](https://www.cve.org/ProgramOrganization/Structure)\n\n[[Structure](https://www.cve.org/ProgramOrganization/Structure)](https://www.cve.org/ProgramOrganization/Structure)[[Program Relationship with Partners](https://www.cve.org/ProgramOrganization/ProgramRelationshipwithPartners)](https://www.cve.org/ProgramOrganization/ProgramRelationshipwithPartners)[[Board](https://www.cve.org/ProgramOrganization/Board)](https://www.cve.org/ProgramOrganization/Board)[[Working Groups](https://www.cve.org/ProgramOrganization/WorkingGroups)](https://www.cve.org/ProgramOrganization/WorkingGroups)[[CVE Numbering Authorities](https://www.cve.org/ProgramOrganization/CNAs)](https://www.cve.org/ProgramOrganization/CNAs)[[Authorized Data Publishers](https://www.cve.org/ProgramOrganization/ADPs)](https://www.cve.org/ProgramOrganization/ADPs)\n\n[Downloads](https://www.cve.org/Downloads)\n\n[Resources \u0026 Support](https://www.cve.org/ResourcesSupport/Resources)\n\n[[Resources](https://www.cve.org/ResourcesSupport/Resources)](https://www.cve.org/ResourcesSupport/Resources)[[Glossary](https://www.cve.org/ResourcesSupport/Glossary)](https://www.cve.org/ResourcesSupport/Glossary)[[FAQs](https://www.cve.org/ResourcesSupport/FAQs)](https://www.cve.org/ResourcesSupport/FAQs)\n\n[Report/Request](https://www.cve.org/ReportRequest/ReportRequestForNonCNAs)\n\n[[CNAs](https://www.cve.org/ReportRequest/ReserveIDsPublishRecordsForCNAs)](https://www.cve.org/ReportRequest/ReserveIDsPublishRecordsForCNAs)[[Non-CNAs](https://www.cve.org/ReportRequest/ReportRequestForNonCNAs)](https://www.cve.org/ReportRequest/ReportRequestForNonCNAs)\n\n[Site Search](https://www.cve.org/SiteSearch)\n\nSearch\n\n[Search tips](https://www.cve.org/ResourcesSupport/FAQs#pc_cve_list_basicssearch_cve)\nlightbulb\n\n|[Provide feedback](https://forms.office.com/g/qmmTaYnr5y)\nSurvey opens in a new tab or window depending on browser settings\n\n[Site Search](https://www.cve.org/SiteSearch)\n\nalert\n\n**Notice:**Expanded keyword searching of CVE Records [(with limitations)](https://www.cve.org/ResourcesSupport/FAQs#pc_cve_list_basicssearch_cve)is now available in the search box above. Learn more [here](https://www.cve.org/ResourcesSupport/FAQs#pc_cve_list_basicssearch_cve). \n\nalert\n\n**Notice:**Expanded keyword searching of CVE Records[(with limitations)](https://www.cve.org/ResourcesSupport/FAQs#pc_cve_list_basicssearch_cve)is now available in the search box above. Learn more[here](https://www.cve.org/ResourcesSupport/FAQs#pc_cve_list_basicssearch_cve).\n\nExpand or collapse notification button\n\nclose notification button\n\nCVE-2026-21869\n==============\n\nPUBLISHED\n\n[external site View JSON](https://cveawg.mitre.org/api/cve/CVE-2026-21869)\n\n | \n\n[external site User Guide](https://www.cve.org/CVERecord/UserGuide)\n\n* * *\n\nCollapse all \n\nRequired CVE Record Information\n-------------------------------\n\nCNA: GitHub, Inc.\n=================\n\nexpand\n\nPublished: 2026-01-07\n\nUpdated: 2026-01-07\n\nTitle:  llama.cpp has Out-of-bounds Write in llama-server\n\n#### Description\n\nllama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama_memory_seq_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.\n\n#### CWE 1 Total\n\n[Learn more](https://www.cve.org/CVERecord/UserGuide/#cve-cwe)\n\n*   [CWE-787: CWE-787: Out-of-bounds Write](https://cwe.mitre.org/data/definitions/787.html)\n\n#### CVSS 1 Total\n\n[Learn more](https://www.cve.org/CVERecord/UserGuide/#cve-cvss)\n\n| Score | Severity | Version | Vector String |\n| --- | --- | --- | --- |\n| 8.8 | HIGH | 3.1 | CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H |\n\n#### Product Status\n\n[Learn more](https://www.cve.org/CVERecord/UserGuide/#cve-product-status)\n\nVendor\n\nggml-org\n\nProduct\n\nllama.cpp\n\nVersions 1 Total\n\nDefault Status: unknown\n\naffected\n\n*   affected at\u003c= 55d4206c8\n\n#### References 1 Total\n\n*   [github.com: https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c external site](https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c)\n\nAuthorized Data Publishers\n--------------------------\n\n[Learn more](https://www.cve.org/ProgramOrganization/ADPs)\n\nCISA-ADP\n========\n\ncollapse\n\nOn this page\n\n*   [Required CVE Record Information](https://www.cve.org/CVERecord?id=CVE-2026-21869#cve-cna-cve-program-containers)\n    *   [CNA: GitHub, Inc.](https://www.cve.org/CVERecord?id=CVE-2026-21869#cna-a0819718-46f1-4df5-94e2-005712e83aaa)\n\n*   [Authorized Data Publishers](https://www.cve.org/CVERecord?id=CVE-2026-21869#cve-adp-containers)\n    *   [CISA-ADP](https://www.cve.org/CVERecord?id=CVE-2026-21869#adp-134c704f-9b21-4f2e-91b3-4a467353bcc0)\n\n#### Policies \u0026 Cookies\n\n*   [Terms of Use](https://www.cve.org/Legal/TermsOfUse)\n*   [Website Security Policy](https://github.com/CVEProject/cve-website/security/policy)\n*   [Privacy Policy](https://www.cve.org/Legal/PrivacyPolicy)\n*   [Cookie Notice](https://www.cve.org/Legal/CookieNotice)\n*   [Manage Cookies](https://www.cve.org/CVERecord?id=CVE-2026-21869#)\n\n#### Media\n\n*   [News](https://www.cve.org/Media/News/AllNews)\n*   [Blogs](https://www.cve.org/Media/News/Blogs)\n*   [Podcasts](https://www.cve.org/Media/News/Podcasts)\n*   [Email newsletter sign up](https://www.cve.org/Media/News/NewsletterSignup)\n\n##### Social Media\n\n[github](https://github.com/CVEProject)[linkedin](https://www.linkedin.com/company/cve-program)[bluesky](https://bsky.app/profile/cveprogram.bsky.social)[mastodon](https://mastodon.social/@CVE_Program)[youtube](https://www.youtube.com/channel/UCUHd2XFDsKH8kjMZQaSKpDQ/)[medium](https://medium.com/@CVE_Program)\n\n[x-twitter icon for @CVEnew New CVE Records](https://x.com/CVEnew)[x-twitter icon for @CVEannounce CVE Announce](https://x.com/CVEannounce)\n\n#### Contact\n\n*   [CVE Program Support external site](https://cveform.mitre.org/)\n*   [CNA Partners](https://www.cve.org/PartnerInformation/ListofPartners)\n*   [CVE Website Feedback Form external site](https://forms.office.com/g/dFzysrHLpR)\n*   [CVE Website Support external site](https://cveform.mitre.org/)\n\nUse of the CVEâ„¢ List and the associated references from this website are subject to the [terms of use](https://www.cve.org/Legal/TermsOfUse). CVE is sponsored by the [U.S. Department of Homeland Security (DHS) external link](https://www.dhs.gov/)[Cybersecurity and Infrastructure Security Agency (CISA) external link](https://www.cisa.gov/about/divisions-offices/cybersecurity-division). Copyright Â© 1999-2026, [The MITRE Corporation external link](https://www.mitre.org/). CVE is a trademark and the CVE logo is a registered trademark of The MITRE Corporation.\n\nLinks that redirect to external websites  will open a new window or tab depending on the web browser used.",
          "was_summarised": false
        },
        {
          "url": "https://security-tracker.debian.org/tracker/CVE-2026-21869",
          "was_fetched": true,
          "page": "Title: CVE-2026-21869\n\nURL Source: https://security-tracker.debian.org/tracker/CVE-2026-21869\n\nMarkdown Content:\nCVE-2026-21869\n===============\n\nCVE-2026-21869\n==============\n\n**Name**CVE-2026-21869\n**Description**llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama_memory_seq_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.\n**Source**[CVE](https://www.cve.org/CVERecord?id=CVE-2026-21869) (at [NVD](https://nvd.nist.gov/vuln/detail/CVE-2026-21869); [CERT](https://www.kb.cert.org/vuls/byid?searchview=\u0026query=CVE-2026-21869), [ENISA](https://euvd.enisa.europa.eu/vulnerability/CVE-2026-21869), [LWN](https://lwn.net/Search/DoSearch?words=CVE-2026-21869), [oss-sec](https://marc.info/?l=oss-security\u0026s=CVE-2026-21869), [fulldisc](https://marc.info/?l=full-disclosure\u0026s=CVE-2026-21869), [Debian ELTS](https://deb.freexian.com/extended-lts/tracker/CVE-2026-21869), [Red Hat](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2026-21869), [Ubuntu](https://ubuntu.com/security/CVE-2026-21869), [Gentoo](https://bugs.gentoo.org/show_bug.cgi?id=CVE-2026-21869), SUSE [bugzilla](https://bugzilla.suse.com/show_bug.cgi?id=CVE-2026-21869)/[CVE](https://www.suse.com/security/cve/CVE-2026-21869/), GitHub [advisories](https://github.com/advisories?query=CVE-2026-21869)/[code](https://github.com/search?type=Code\u0026q=%22CVE-2026-21869%22)/[issues](https://github.com/search?type=Issues\u0026q=%22CVE-2026-21869%22), [web search](https://duckduckgo.com/html?q=%22CVE-2026-21869%22), [more](https://oss-security.openwall.org/wiki/vendors))\n**Debian Bugs**[1125060](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1125060)\n\nVulnerable and fixed packages\n-----------------------------\n\nThe table below lists information on source packages.\n\n| Source Package | Release | Version | Status |\n| --- | --- | --- | --- |\n| [llama.cpp](https://security-tracker.debian.org/tracker/source-package/llama.cpp) ([PTS](https://tracker.debian.org/pkg/llama.cpp)) | sid | 7593+dfsg-1 | vulnerable |\n\nThe information below is based on the following data on fixed versions.\n\n| Package | Type | Release | Fixed Version | Urgency | Origin | Debian Bugs |\n| --- | --- | --- | --- | --- | --- | --- |\n| [llama.cpp](https://security-tracker.debian.org/tracker/source-package/llama.cpp) | source | (unstable) | (unfixed) |  |  | [1125060](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1125060) |\n\nNotes\n-----\n\n[https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c](https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c)\n\n* * *\n\nSearch for package or bug name: [Reporting problems](https://security-tracker.debian.org/tracker/data/report)\n[Home](https://security-tracker.debian.org/tracker/) - [Debian Security](https://www.debian.org/security/) - [Source](https://salsa.debian.org/security-tracker-team/security-tracker/blob/master/bin/tracker_service.py)[(Git)](https://salsa.debian.org/security-tracker-team/security-tracker)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:34:59.647859457Z"
    },
    {
      "flow_id": "",
      "id": "1q7a62a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/",
      "title": "AI21 Labs releases Jamba2",
      "content": "https://preview.redd.it/zmo6dijns4cg1.png?width=1800\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb\n\n52B [https://huggingface.co/ai21labs/AI21-Jamba2-Mini](https://huggingface.co/ai21labs/AI21-Jamba2-Mini)\n\nJamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2).\n\n# Key Advantages\n\n* **Superior reliability-to-throughput ratio:** Maintains high performance at 100K+ token contexts\n* **Category-leading benchmarks:** Excels on IFBench, IFEval, Collie, and FACTS\n* **Statistically significant quality wins:** Outperforms comparable models on real-world enterprise tasks\n* **256K context window:** Processes technical manuals, research papers, and knowledge bases\n* **Apache 2.0 License:** Fully open source for commercial use\n* **Production-optimized:** Lean memory footprint for scalable deployments\n\nhttps://preview.redd.it/cqwicpwts4cg1.png?width=2400\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d\n\n3B [https://huggingface.co/ai21labs/AI21-Jamba2-3B](https://huggingface.co/ai21labs/AI21-Jamba2-3B)\n\nJamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devicesâ€”iPhones, Androids, Macs, and PCsâ€”while maintaining the grounding and instruction-following capabilities required for production use.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2).\n\n# \n\n# Key Advantages\n\n* **On-device deployment:** Runs efficiently on iPhones, Androids, Macs, and PCs\n* **Ultra-compact footprint:** 3B parameters enabling edge deployments with minimal resources\n* **Benchmark leadership:** Excels on IFBench, IFEval, Collie, and FACTS\n* **256K context window:** Processes long documents and knowledge bases\n* **Apache 2.0 License:** Fully open source for commercial use\n* **SSM-Transformer architecture:** Memory-efficient design for resource-constrained environments\n\nit works in llama.cpp, tested on my Windows desktop:\n\nhttps://preview.redd.it/ijzgde7bg5cg1.png?width=3802\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71\n\nfixed blog post [https://www.ai21.com/blog/introducing-jamba2/](https://www.ai21.com/blog/introducing-jamba2/)\n\nGGUFs are in progress [https://huggingface.co/mradermacher/model\\_requests/discussions/1683](https://huggingface.co/mradermacher/model_requests/discussions/1683)\n\n\n\nprevious generation of Jamba models\n\n399B [https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7)\n\n52B [https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7)\n\n3B [https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B)",
      "author": "jacek2023",
      "created_at": "2026-01-08T12:10:15Z",
      "comments": [
        {
          "id": "nye2sbf",
          "author": "ilintar",
          "content": "Previous Jamba models were terrible. They were an architectural novelty but their performance was abysmal. Curious to see if they've improved.",
          "created_at": "2026-01-08T13:01:25Z",
          "was_summarised": false
        },
        {
          "id": "nydvl7n",
          "author": "Smooth-Cow9084",
          "content": "52b named \"mini\" lol\n\n\nFor those curious, it has 12b active.Â \n\n\nThen the 3b model has no info on HF repository, for whatever reason.",
          "created_at": "2026-01-08T12:14:31Z",
          "was_summarised": false
        },
        {
          "id": "nyedmvu",
          "author": "LinkSea8324",
          "content": "Fixed blog link for the brainlets : https://ai21.com/blog/introducing-jamba2",
          "created_at": "2026-01-08T14:01:41Z",
          "was_summarised": false
        },
        {
          "id": "nygexex",
          "author": "YearZero",
          "content": "# Merged Benchmark Comparison Table\n\n|Benchmark Category|Jamba2 3B|Jamba2 Mini|**Qwen3 4B A3B Instruct 2507**|Qwen3 30B A3B Instruct 2507|Nemotron3 30B A3B (Non-Thinking)|Minstral3 3B Instruct 2512|Minstral3 14B Instruct 2512|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|**IFBench**|0.36|**0.38**|0.32|0.32|0.33|0.22|0.27|\n|**Collie**|0.24|0.33|**0.34**|0.29|0.30|0.20|0.26|\n|**IFEval**|0.93|**0.97**|0.90|0.91|0.88|0.67|0.73|\n|**FACTS**|0.54|**0.57**|0.54|0.54|0.48|0.44|0.49|\n|**Enterprise Reliability Score (Avg)**|0.52|**0.56**|0.53|0.51|0.50|0.38|0.44|\n\nGGUF's available here now:  \n[https://huggingface.co/bartowski/ai21labs\\_AI21-Jamba2-Mini-GGUF](https://huggingface.co/bartowski/ai21labs_AI21-Jamba2-Mini-GGUF)\n\nEdit: The 52b failed every single one of my tests (I used the Q6\\_K\\_L quant). It wasn't able to do anything right or follow instructions at all. Not sure if back-end and/or template may need adjusting, but right now the model is unusable for me.",
          "created_at": "2026-01-08T19:31:29Z",
          "was_summarised": false
        },
        {
          "id": "nyf3ail",
          "author": "FullOf_Bad_Ideas",
          "content": "It shares pre-training weights with Jamba 1.5, as per their own documentation.\n\nPre-training from scratch is becoming less and less common.\n\nI wonder where's 10T Qwen at.",
          "created_at": "2026-01-08T16:04:36Z",
          "was_summarised": false
        },
        {
          "id": "nyesr4v",
          "author": "Cool-Chemical-5629",
          "content": "Just a note. Jamba 1.7 alone wasn't the first generation. There were also 1.6 and 1.5.",
          "created_at": "2026-01-08T15:16:43Z",
          "was_summarised": false
        },
        {
          "id": "nydyh0r",
          "author": "abkibaarnsit",
          "content": "Blog post giving 404",
          "created_at": "2026-01-08T12:34:10Z",
          "was_summarised": false
        },
        {
          "id": "nye7ag6",
          "author": "SlowFail2433",
          "content": "Wow a 400B sub-quadratic model\n\n\nThis is by far the largest sub-quadratic model ever released as far as I know",
          "created_at": "2026-01-08T13:27:36Z",
          "was_summarised": false
        },
        {
          "id": "nyeiu0b",
          "author": "Accomplished_Ad9530",
          "content": "Apache 2.0 for the 52B, nice. Only the 3B had a permissive license in the prior gen, so itâ€™s nice to see larger models open up.",
          "created_at": "2026-01-08T14:28:29Z",
          "was_summarised": false
        },
        {
          "id": "nyho3pp",
          "author": "zoyer2",
          "content": "tested some one-shot coding tasks using ai21labs\\_AI21-Jamba2-Mini-Q4\\_K\\_M.gguf (52b) in llama.cpp vs:  \n\\- Qwen3-Next-80B-A3B-Instruct-IQ4\\_XS.gguf  \n\\- cerebras\\_GLM-4.5-Air-REAP-82B-A12B-IQ3\\_XXS.gguf  \n\\- Qwen3-Coder-30B-A3B-Instruct-UD-Q6\\_K\\_XL.gguf\n\nwasn't close to beat them, many times just started to outputting crap. I really would want a model this size to be a great coder model",
          "created_at": "2026-01-08T22:52:56Z",
          "was_summarised": false
        },
        {
          "id": "nyegfqg",
          "author": "International-Try467",
          "content": "Glad to see that AI-21 is still around. I remember them from the AI Dungeon days where they replaced GPT-3 with Jurassic instead. I wonder if their models are less slopped than OpenAI's",
          "created_at": "2026-01-08T14:16:18Z",
          "was_summarised": false
        },
        {
          "id": "nyf05ou",
          "author": "Cool-Chemical-5629",
          "content": "I guess there's no day one support for LlamaCpp. It usually leads to the models being buried under newer ones which have support on day one. What would be really cool is the REAP version 30B and support in LlamaCpp.",
          "created_at": "2026-01-08T15:50:29Z",
          "was_summarised": false
        },
        {
          "id": "nyfims9",
          "author": "FizzarolliAI",
          "content": "PSA: AI21 is an Israeli company founded by ex-IDF spies from their NSA equivalent who support the ongoing attempts at ethnic cleansing and genocide in Palestine. They are not worth supporting, and neither are their models.",
          "created_at": "2026-01-08T17:11:36Z",
          "was_summarised": false
        },
        {
          "id": "nye3dns",
          "author": "indicava",
          "content": "Blog post is 404â€™d, anyone know what kind of VRAM requirements we are looking at here for the 3B model (at native BF16)?",
          "created_at": "2026-01-08T13:05:03Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-Mini",
          "was_fetched": true,
          "page": "Title: ai21labs/AI21-Jamba2-Mini Â· Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba2-Mini\n\nMarkdown Content:\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#introduction) Introduction\n------------------------------------------------------------------------------\n\nJamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2).\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#key-advantages) Key Advantages\n----------------------------------------------------------------------------------\n\n*   **Superior reliability-to-throughput ratio:** Maintains high performance at 100K+ token contexts\n*   **Category-leading benchmarks:** Excels on IFBench, IFEval, Collie, and FACTS\n*   **Statistically significant quality wins:** Outperforms comparable models on real-world enterprise tasks\n*   **256K context window:** Processes technical manuals, research papers, and knowledge bases\n*   **Apache 2.0 License:** Fully open source for commercial use\n*   **Production-optimized:** Lean memory footprint for scalable deployments\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#evaluation-results) Evaluation Results\n------------------------------------------------------------------------------------------\n\nJamba2 Mini leads on instruction following and grounding metrics, demonstrating exceptional steerability and context faithfulness. In blind side-by-side evaluations on 100 real-world enterprise prompts, the model achieved statistically significant wins on output quality and factuality compared to Ministral3 14B.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#training-and-evaluation-details) Training and Evaluation Details\n--------------------------------------------------------------------------------------------------------------------\n\nJamba2 models were developed using a comprehensive post-training pipeline starting from Jamba 1.5 pre-training. The models underwent mid-training on 500B carefully curated tokens with increased representation of math, code, high-quality web data, and long documents. A state passing phase optimized the Mamba layers for effective context length generalization. Training continued with cold start supervised fine-tuning to establish instruction-following and reasoning capabilities, followed by DPO optimization.\n\nThe final training stages involved multiple on-policy reinforcement learning phases, progressively moving from short-context verifiable rewards to longer context training with mixed verifiable and model-based rewards. Evaluation focused on two key enterprise reliability signals: instruction-following benchmarks measuring steerability, and grounding benchmarks testing context faithfulness. Human evaluators assessed performance on real-world enterprise tasks using blind, counterbalanced side-by-side comparisons, rating outputs on factuality, style, constraint-adherence, instruction-following, and helpfulness.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#quickstart) Quickstart\n--------------------------------------------------------------------------\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#run-with-vllm) Run with vLLM\n--------------------------------------------------------------------------------\n\nBest results require vLLM version **0.12.0** or higher.\n\n```\nvllm serve \"ai21labs/AI21-Jamba2-Mini\" --mamba-ssm-cache-dtype float32 --enable-auto-tool-choice --tool-call-parser hermes --enable-prefix-caching --quantization experts_int8\n```\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-Mini#run-with-transformers) Run with Transformers\n------------------------------------------------------------------------------------------------\n\n```\npip install transformers\u003e=4.54.0\npip install flash-attn --no-build-isolation\npip install causal-conv1d\u003e=1.2.0\npip install mamba-ssm\n```\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba2-Mini\",\n                                  dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\", device_map=\"auto\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba2-Mini\")\n\nmessages = [\n    {\"role\": \"system\",\n     \"content\": \"You are an HR Policy Assistant.\n                 Answer employee questions using only the provided policy documents.\n                 If the answer isn't in the documents, say so clearly.\n                 Be concise and cite the specific policy section when possible.\"\n},\n    {\"role\": \"user\",\n     \"content\": \"Context documents: {retrieved_chunks}.\n                 Employee question: {user_question}.\n                 Answer:\"\n},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\noutputs = model.generate(**tokenizer(prompts, return_tensors=\"pt\").to(model.device), do_sample=True, temperature=0.6)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\nFor more deployment guides and resources, visit our [official documentation](https://docs.ai21.com/home).",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2",
          "was_fetched": true,
          "page": "Title: 404 â€“ Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 â€“ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n404\n===\n\nEntry not found\n\n System theme \n\nWebsite\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*   [Changelog](https://huggingface.co/changelog)\n*   [Inference Endpoints](https://endpoints.huggingface.co/)\n*   [HuggingChat](https://huggingface.co/chat)\n\nCompany\n\n*   [About](https://huggingface.co/huggingface)\n*   [Brand assets](https://huggingface.co/brand)\n*   [Terms of service](https://huggingface.co/terms-of-service)\n*   [Privacy](https://huggingface.co/privacy)\n*   [Careers](https://apply.workable.com/huggingface/)\n*   [Press](mailto:press@huggingface.co)\n\nResources\n\n*   [Learn](https://huggingface.co/learn)\n*   [Documentation](https://huggingface.co/docs)\n*   [Blog](https://huggingface.co/blog)\n*   [Forum](https://discuss.huggingface.co/)\n*   [Service Status](https://status.huggingface.co/)\n\nSocial\n\n*   [GitHub](https://github.com/huggingface)\n*   [Twitter](https://twitter.com/huggingface)\n*   [LinkedIn](https://www.linkedin.com/company/huggingface/)\n*   [Discord](https://huggingface.co/join/discord)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-3B",
          "was_fetched": true,
          "page": "Title: ai21labs/AI21-Jamba2-3B Â· Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba2-3B\n\nMarkdown Content:\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#introduction) Introduction\n----------------------------------------------------------------------------\n\nJamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devicesâ€”iPhones, Androids, Macs, and PCsâ€”while maintaining the grounding and instruction-following capabilities required for production use.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2).\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#key-advantages) Key Advantages\n--------------------------------------------------------------------------------\n\n*   **On-device deployment:** Runs efficiently on iPhones, Androids, Macs, and PCs\n*   **Ultra-compact footprint:** 3B parameters enabling edge deployments with minimal resources\n*   **Benchmark leadership:** Excels on IFBench, IFEval, Collie, and FACTS\n*   **256K context window:** Processes long documents and knowledge bases\n*   **Apache 2.0 License:** Fully open source for commercial use\n*   **SSM-Transformer architecture:** Memory-efficient design for resource-constrained environments\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#evaluation-results) Evaluation Results\n----------------------------------------------------------------------------------------\n\nJamba2 3B achieves category-leading performance on instruction following and grounding benchmarks despite its compact size. The model delivers consistent, context-faithful outputs across diverse enterprise tasks including RAG workflows and technical document processing.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#training-and-evaluation-details) Training and Evaluation Details\n------------------------------------------------------------------------------------------------------------------\n\nJamba2 models were developed using a comprehensive post-training pipeline starting from Jamba 1.5 pre-training. The models underwent mid-training on 500B carefully curated tokens with increased representation of math, code, high-quality web data, and long documents. A state passing phase optimized the Mamba layers for effective context length generalization. Training continued with cold start supervised fine-tuning to establish instruction-following and reasoning capabilities, followed by DPO optimization.\n\nThe final training stages involved multiple on-policy reinforcement learning phases, progressively moving from short-context verifiable rewards to longer context training with mixed verifiable and model-based rewards. Evaluation focused on two key enterprise reliability signals: instruction-following benchmarks measuring steerability, and grounding benchmarks testing context faithfulness. Human evaluators assessed performance on real-world enterprise tasks using blind, counterbalanced side-by-side comparisons, rating outputs on factuality, style, constraint-adherence, instruction-following, and helpfulness.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#quickstart) Quickstart\n------------------------------------------------------------------------\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#run-with-vllm) Run with vLLM\n------------------------------------------------------------------------------\n\nBest results require vLLM version **0.10.2** or higher.\n\n```\nvllm serve \"ai21labs/AI21-Jamba2-3B\" --mamba-ssm-cache-dtype float32 --enable-auto-tool-choice --tool-call-parser hermes --enable-prefix-caching\n```\n\n[](https://huggingface.co/ai21labs/AI21-Jamba2-3B#run-with-transformers) Run with Transformers\n----------------------------------------------------------------------------------------------\n\n```\npip install transformers\u003e= 4.54.0\npip install flash-attn --no-build-isolation\npip install causal-conv1d\u003e=1.2.0\npip install mamba-ssm\n```\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba2-3B\",\n                                      dtype=torch.bfloat16,\n                        attn_implementation=\"flash_attention_2\",\n                                             device_map=\"auto\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba-3B\")\n\nmessages = [\n    {\"role\": \"system\",\n     \"content\": \"You are an HR Policy Assistant.\n                 Answer employee questions using only the provided policy documents.\n                 If the answer isn't in the documents, say so clearly.\n                 Be concise and cite the specific policy section when possible.\"\n},\n    {\"role\": \"user\",\n     \"content\": \"Context documents: {retrieved_chunks}.\n                 Employee question: {user_question}.\n                 Answer:\"\n},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\noutputs = model.generate(**tokenizer(prompts, return_tensors=\"pt\").to(model.device), do_sample=True, temperature=0.6)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n```",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2",
          "was_fetched": true,
          "page": "Title: 404 â€“ Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 â€“ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n404\n===\n\nEntry not found\n\n System theme \n\nWebsite\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*   [Changelog](https://huggingface.co/changelog)\n*   [Inference Endpoints](https://endpoints.huggingface.co/)\n*   [HuggingChat](https://huggingface.co/chat)\n\nCompany\n\n*   [About](https://huggingface.co/huggingface)\n*   [Brand assets](https://huggingface.co/brand)\n*   [Terms of service](https://huggingface.co/terms-of-service)\n*   [Privacy](https://huggingface.co/privacy)\n*   [Careers](https://apply.workable.com/huggingface/)\n*   [Press](mailto:press@huggingface.co)\n\nResources\n\n*   [Learn](https://huggingface.co/learn)\n*   [Documentation](https://huggingface.co/docs)\n*   [Blog](https://huggingface.co/blog)\n*   [Forum](https://discuss.huggingface.co/)\n*   [Service Status](https://status.huggingface.co/)\n\nSocial\n\n*   [GitHub](https://github.com/huggingface)\n*   [Twitter](https://twitter.com/huggingface)\n*   [LinkedIn](https://www.linkedin.com/company/huggingface/)\n*   [Discord](https://huggingface.co/join/discord)",
          "was_summarised": false
        },
        {
          "url": "https://www.ai21.com/blog/introducing-jamba2/",
          "was_fetched": true,
          "page": "Title: Introducing Jamba2: The open source model family for enterprise reliability and efficiency\n\nURL Source: https://www.ai21.com/blog/introducing-jamba2/\n\nPublished Time: 2026-01-08T13:31:16+00:00\n\nMarkdown Content:\nToday, we are introducing Jamba2, an open source family of language models built for maximum reliability and steerability in the enterprise.\n\n[Built on our novel SSM-Transformer architecture](https://arxiv.org/pdf/2403.19887), and a category leader across grounding and instruction following benchmarks, Jamba2 offers a compact, memory-efficient addition to any production agent stack, able to power precise question answering workflows that donâ€™t call for the heavy â€œthinking tokenâ€ overhead of reasoning models.\n\nJamba2 is available in two model sizes: 3B and Mini (MoE; 12B active, 52B total parameters). We are proud to release both under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0), as part of our continued commitment to democratizing access to quality models. We are particularly excited this release includes a 3B model, enabling developers everywhere to download and run this technology right on their own devices, including iPhones, Androids, Macs, and PCs.\n\nJamba2 model family fast facts\n------------------------------\n\n*   License: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n*   Model sizes: 3B (dense) 52B/12A\n*   Context window length: 256K\n*   Availability: [AI21 Studio](https://studio.ai21.com/v2/), [Hugging Face](https://huggingface.co/collections/ai21labs/jamba2)\n\nMaximum reliability and steerability\n------------------------------------\n\nWith over seven years of experience building AI systems for enterprise customers, we know that reliability and steerability are non-negotiables when it comes to choosing a model. 54% of enterprises still cite accuracy as the number one AI-related risk they are working to mitigateâ€”with 30% of surveyed organizations having experienced the negative drawbacks of AI inaccuracies at least once ([McKinsey, 2025](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)). Even as models get increasingly sophisticated, there is an enormous, high-stakes gap to be filled when it comes to enterprise-grade accuracy.\n\nWe also know, from our work with customers, that not every enterprise workflow requires the high cost and high latency of reasoning models. Especially as more organizations adopt agents and model routers, a strong enterprise AI system should contain a range of model types, with different models called as needed based on task type and budget considerations.\n\nWith this in mind, we set out to build a model that delivers precise output with a lean memory footprint. To make â€œprecisionâ€ measurable, we focused on two signals that map directly to real deployments: instruction-following benchmarks, which capture how steerable a model is, and grounding benchmarks, which test whether outputs stay faithful to the provided context. Together, they serve as practical indicators of how consistently a model will behave in enterprise knowledge workflows.\n\nToday, weâ€™re bringing that vision to market with Jamba2: compact models that lead on these measures of enterprise reliability. Jamba2 is built to produce grounded answers across a range of source types, including technical manuals, research papers, company policies, and internal knowledge bases, so teams can deploy it as a dependable component in production stacks.\n\nLeader on instruction following and grounding\n---------------------------------------------\n\nJamba2 models excel across the instruction following benchmarks IFBench, IFEval, and Collie, as well as the grounding benchmark FACTS.\n\nWinning reliability-to-throughput ratio\n---------------------------------------\n\nFor the enterprise, models that shine on quality, yet choke in production-scale settings, are unusable.\n\nJamba2 leverages its memory-efficient architecture to maintain high throughput alongside high enterprise reliability, even as context scales to 100K tokens, mirroring the real-world usage we can expect from enterprise QA workflows.\n\nWinning performance on enterprise tasks\n---------------------------------------\n\nIn a human evaluated comparison between Jamba2 Mini and Ministral3 14B on a test set of real-world enterprise task prompts, Jamba2 Mini showed a statistically significant advantage in overall output quality and win rate. The tasks included a mix of enterprise QA tasks, instruction-heavy developer prompts, and other common business tasks such as summarization and drafting. This win rate encapsulates human preference, with special attention to factuality, style, constraint-adherence, instruction-following, and helpfulness.\n\n**Note on human evaluation methodology**: The models were evaluated by a team of content evaluation experts using a side-by-side, blind comparison protocol; leftâ€“right ordering was counterbalanced across items to mitigate order and precedence effects. Evaluators reviewed both outputs for both absolute metrics (general quality measures, as well as fine-grained error analysis), as well as human preference.\n\nHow we built it\n---------------\n\nTo train Jamba2, we utilized a contemporary LLM post-training pipeline. Following the pre-training phase, we mid-trained Jamba2 on 500B carefully curated tokens, with a higher representation of math and code in the mix, along with high-quality web data and long documents. We completed the mid-training phase with a short [state passing phase](https://arxiv.org/abs/2507.02782) for the modelâ€™s Mamba layers, a method recently introduced for effectively generalizing an SSMâ€™s context length.\n\nGiven our midtrained model, we performed cold start SFT to teach the model basic instruction-following and reasoning. We then topped off the model with DPO to further improve its performance as a starting point for on-policy RL. Finally, we ran multiple on-policy RL phases on our model, starting with short-context verifiable rewards and gradually moving to longer context training with a mix of verifiable and model-based rewards.\n\nCarefully aggregating verifiable and model-based rewards was one of the key advancements allowing Jamba2 models to excel on tasks that require following specific instructions and details while adhering to the general intention of the user. As part of this process, we optimized our training infra for efficient on-policy RL training, which combines model inference in both the generation and reward phases of the training.\n\nGetting started with Jamba2\n---------------------------\n\nAvailable for download on [Hugging Face](https://huggingface.co/collections/ai21labs/jamba2) or directly on [AI21 Studio](https://studio.ai21.com/v2/), researchers and AI enthusiasts alike can enjoy experimenting with Jamba2 and pushing it to support new use casesâ€”we canâ€™t wait to see what you build!",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/mradermacher/model%5C_requests/discussions/1683",
          "was_fetched": true,
          "page": "Title: 404 â€“ Hugging Face\n\nURL Source: https://huggingface.co/mradermacher/model/_requests/discussions/1683\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 â€“ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n404\n===\n\nSorry, we can't find the page you are looking for.\n\n System theme \n\nWebsite\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*   [Changelog](https://huggingface.co/changelog)\n*   [Inference Endpoints](https://endpoints.huggingface.co/)\n*   [HuggingChat](https://huggingface.co/chat)\n\nCompany\n\n*   [About](https://huggingface.co/huggingface)\n*   [Brand assets](https://huggingface.co/brand)\n*   [Terms of service](https://huggingface.co/terms-of-service)\n*   [Privacy](https://huggingface.co/privacy)\n*   [Careers](https://apply.workable.com/huggingface/)\n*   [Press](mailto:press@huggingface.co)\n\nResources\n\n*   [Learn](https://huggingface.co/learn)\n*   [Documentation](https://huggingface.co/docs)\n*   [Blog](https://huggingface.co/blog)\n*   [Forum](https://discuss.huggingface.co/)\n*   [Service Status](https://status.huggingface.co/)\n\nSocial\n\n*   [GitHub](https://github.com/huggingface)\n*   [Twitter](https://twitter.com/huggingface)\n*   [LinkedIn](https://www.linkedin.com/company/huggingface/)\n*   [Discord](https://huggingface.co/join/discord)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/mradermacher/model_requests/discussions/1683",
          "was_fetched": true,
          "page": "Title: mradermacher/model_requests Â· Jamba 2\n\nURL Source: https://huggingface.co/mradermacher/model_requests/discussions/1683\n\nMarkdown Content:\nmradermacher/model_requests Â· Jamba 2\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n[](https://huggingface.co/mradermacher)\n\n[mradermacher](https://huggingface.co/mradermacher)\n\n/\n\n[model_requests](https://huggingface.co/mradermacher/model_requests)\n\nlike 127\n===================================================================================================================================================================================\n\n[English](https://huggingface.co/models?language=en)\n\n[Model card](https://huggingface.co/mradermacher/model_requests)[Files Files and versions xet](https://huggingface.co/mradermacher/model_requests/tree/main)[Community 1682](https://huggingface.co/mradermacher/model_requests/discussions)\n\nJamba 2\n-------\n\n#1683\n\nby [jacek2024](https://huggingface.co/jacek2024) - opened about 10 hours ago\n\n[Discussion](https://huggingface.co/mradermacher/model_requests/discussions/1683)\n\n[jacek2024](https://huggingface.co/jacek2024)\n\n[about 10 hours ago](https://huggingface.co/mradermacher/model_requests/discussions/1683#695fd71a6a858a5b9954ea4b)\n\nno llama.cpp update needed\n\n[https://huggingface.co/ai21labs/AI21-Jamba2-3B](https://huggingface.co/ai21labs/AI21-Jamba2-3B)\n\n[https://huggingface.co/ai21labs/AI21-Jamba2-Mini](https://huggingface.co/ai21labs/AI21-Jamba2-Mini)\n\n[RichardErkhov](https://huggingface.co/RichardErkhov)\n\n[about 10 hours ago](https://huggingface.co/mradermacher/model_requests/discussions/1683#695fd796a2ab22461f450dad)\n\nIt's queued!\n\nYou can check for progress at [http://hf.tst.eu/status.html](http://hf.tst.eu/status.html) or regularly check the model\n\nsummary page at [https://hf.tst.eu/model#AI21-Jamba2-3B-GGUF](https://hf.tst.eu/model#AI21-Jamba2-3B-GGUF) and [https://hf.tst.eu/model#AI21-Jamba2-Mini-GGUF](https://hf.tst.eu/model#AI21-Jamba2-Mini-GGUF) for quants to appear.\n\nEdit Preview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\n \n\n Comment\nÂ·[Sign up](https://huggingface.co/join?next=%2Fmradermacher%2Fmodel_requests%2Fdiscussions%2F1683) or [log in](https://huggingface.co/login?next=%2Fmradermacher%2Fmodel_requests%2Fdiscussions%2F1683) to comment\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7",
          "was_fetched": true,
          "page": "Title: ai21labs/AI21-Jamba-Large-1.7 Â· Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7\n\nMarkdown Content:\nYou need to agree to share your contact information to access this model\n------------------------------------------------------------------------\n\nThis repository is publicly accessible, but you have to accept the conditions to access its files and content.\n\n[Log in](https://huggingface.co/login?next=%2Fai21labs%2FAI21-Jamba-Large-1.7) or [Sign Up](https://huggingface.co/join?next=%2Fai21labs%2FAI21-Jamba-Large-1.7) to review the conditions and access this model content.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#model-information) Model Information\n--------------------------------------------------------------------------------------------\n\nJamba Large 1.7 offers new improvements to our Jamba open model family. This new version builds on the novel SSM-Transformer hybrid architecture, 256K context window, and efficiency gains of previous versions, while introducing improvements in grounding and instruction-following.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#key-improvements) Key improvements:\n-------------------------------------------------------------------------------------------\n\n*   **Grounding**: Jamba Large 1.7 provides more complete and accurate answers, grounded fully in the given context. \n*   **Instruction following**: Jamba Large 1.7 improves on steerability.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#use-cases) Use cases\n----------------------------------------------------------------------------\n\nJambaâ€™s long context efficiency, contextual faithfulness, and steerability make it ideal for a variety of business applications and industries, such as:\n\n*   **Finance**: Investment research, digital banking support chatbot, M\u0026A due diligence.\n*   **Healthcare**: Procurement (RFP creation \u0026 response review), medical publication and reports generation.\n*   **Retail**: Brand-aligned product description generation, conversational AI. \n*   **Education \u0026 Research**: Personalized chatbot tutor, grants applications.\n\nThe models are released under the [Jamba Open Model License](https://www.ai21.com/jamba-open-model-license/), a permissive license allowing full research use and commercial use under the license terms. If you need to license the model for your needs, [talk to us](https://www.ai21.com/contact-sales/).\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#model-details) Model Details\n------------------------------------------------------------------------------------\n\nDeveloped by: AI21 Model type: Joint Attention and Mamba (Jamba) Model size: 94B active/398B total parameters License: Jamba Open Model License Context length: 256K Knowledge cutoff date: August 22, 2024 Supported languages: English, Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#grounding-and-instruction-following-improvements) Grounding and instruction-following improvements\n----------------------------------------------------------------------------------------------------------------------------------------------------------\n\n| Category | Benchmark | Jamba Large 1.6 | Jamba Large 1.7 |\n| --- | --- | --- | --- |\n| Grounding | FACTS | 0.758 | 0.832 |\n| Steerability | IFEcal | 0.782 | 0.84 |\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#usage) Usage\n--------------------------------------------------------------------\n\nFind step-by-step instructions on how to privately deploy Jamba:\n\n**Run the model with vLLM**\nThe recommended way to perform efficient inference with Jamba Large 1.7 is using [vLLM](https://docs.vllm.ai/en/latest/). First, make sure to install vLLM (version 0.5.4 or higher is required):\n\n```\npip install vllm\u003e=0.6.5\n```\n\nJamba Large 1.7 is too large to be loaded in full (FP32) or half (FP16/BF16) precision on a single node of 8 80GB GPUs. Therefore, quantization is required. We've developed an innovative and efficient quantization technique, ExpertsInt8, designed for MoE models deployed in vLLM, including Jamba models. Using it, you'll be able to deploy Jamba Large 1.7 on a single node of 8 80GB GPUs. With ExpertsInt8 quantization and the default vLLM configuration, you'll be able to perform inference on prompts up to 220K tokens long on 8 80GB GPUs:\n\n```\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel = \"ai21labs/AI21-Jamba-1.7-Large\"\n\nllm = LLM(model=model,\n          tensor_parallel_size=8,\n          max_model_len=220*1024,\n          quantization=\"experts_int8\",\n         )\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\nmessages = [\n   {\"role\": \"system\", \"content\": \"You are an ancient oracle who speaks in cryptic but wise phrases, always hinting at deeper meanings.\"},\n   {\"role\": \"user\", \"content\": \"Hello!\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\nsampling_params = SamplingParams(temperature=0.4, top_p=0.95, max_tokens=100)\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\n**Note:** Versions 4.44.0 and 4.44.1 of transformers have a bug that restricts the ability to run the Jamba architecture. Make sure you're not using these versions\n\n**Note:** If you're having trouble installing mamba-ssm and causal-conv1d for the optimized Mamba kernels, you can run Jamba Large 1.7 without them, at the cost of extra latency. In order to do that, add the kwarg use_mamba_kernels=False when loading the model via AutoModelForCausalLM.from_pretained(). You can also find all instructions in our [private AI (vLLM) deployment guide](https://docs.ai21.com/docs/vllm).\n\n**Run the model with Transformers**\nTo load Jamba Large 1.7 in transformers on a single node of 8 80GB GPUs, we recommend to parallelize it using [accelerate](https://huggingface.co/docs/accelerate/index):\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True,\n                                         llm_int8_skip_modules=[\"mamba\"])\n\n# a device map to distribute the model evenly across 8 GPUs\ndevice_map = {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 5, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 6, 'model.layers.56': 6, 'model.layers.57': 6, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 7, 'model.layers.65': 7, 'model.layers.66': 7, 'model.layers.67': 7, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.final_layernorm': 7, 'lm_head': 7}\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.7\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             quantization_config=quantization_config,\n                                             device_map=device_map)\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.7\")\n\nmessages = [\n   {\"role\": \"system\", \"content\": \"You are an ancient oracle who speaks in cryptic but wise phrases, always hinting at deeper meanings.\"},\n   {\"role\": \"user\", \"content\": \"Hello!\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(model.device)\n\noutputs = model.generate(input_ids, max_new_tokens=216)\n\n# Decode the output\nconversation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Split the conversation to get only the assistant's response\nassistant_response = conversation.split(messages[-1]['content'])[1].strip()\nprint(assistant_response)\n# Output: Seek and you shall find. The path is winding, but the journey is enlightening. What wisdom do you seek from the ancient echoes?\n```\n\n**Note:** Versions 4.44.0 and 4.44.1 of transformers have a bug that restricts the ability to run the Jamba architecture. Make sure you're not using these versions.\n\n**Note:** If you're having trouble installing mamba-ssm and causal-conv1d for the optimized Mamba kernels, you can run Jamba Large 1.7 without them, at the cost of extra latency. In order to do that, add the kwarg use_mamba_kernels=False when loading the model via AutoModelForCausalLM.from_pretained().\n\nYou can also find all instructions in our [private AI (vLLM) deployment guide](https://docs.ai21.com/docs/vllm).\n\nAnd to get started with our SDK: [AI21 Python SDK guide](https://docs.ai21.com/docs/sdk)\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7#further-documentation) Further documentation\n----------------------------------------------------------------------------------------------------\n\nFor more comprehensive guides and advanced usage:\n\n*   [Tokenization guide](https://docs.ai21.com/docs/tokenization) - Using ai21-tokenizer\n*   [Quantization guide](https://docs.ai21.com/docs/quantization) - ExpertsInt8, bitsandbytes\n*   [Fine-tuning guide](https://docs.ai21.com/docs/fine-tuning) - LoRA, qLoRA, and full fine-tuning\n\nFor more resources to start building, [visit our official documentation](https://docs.ai21.com/home).\n\nDownloads last month 83 \n\nSafetensors\n\n[](https://huggingface.co/docs/safetensors)\n\nModel tree for ai21labs/AI21-Jamba-Large-1.7[](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\n-------------------------------------------------------------------------------------------------------------------\n\nCollection including ai21labs/AI21-Jamba-Large-1.7\n--------------------------------------------------",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7",
          "was_fetched": true,
          "page": "Title: ai21labs/AI21-Jamba-Mini-1.7 Â· Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7\n\nMarkdown Content:\nYou need to agree to share your contact information to access this model\n------------------------------------------------------------------------\n\nThis repository is publicly accessible, but you have to accept the conditions to access its files and content.\n\n[Log in](https://huggingface.co/login?next=%2Fai21labs%2FAI21-Jamba-Mini-1.7) or [Sign Up](https://huggingface.co/join?next=%2Fai21labs%2FAI21-Jamba-Mini-1.7) to review the conditions and access this model content.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#model-information) Model Information\n-------------------------------------------------------------------------------------------\n\nJamba Mini 1.7 offers new improvements to our Jamba open model family. This new version builds on the novel SSM-Transformer hybrid architecture, 256K context window, and efficiency gains of previous versions, while introducing improvements in grounding and instruction-following.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#key-improvements) Key Improvements\n-----------------------------------------------------------------------------------------\n\n*   **Grounding**: Jamba Mini 1.7 provides more complete and accurate answers, grounded fully in the given context. \n*   **Instruction following**: Jamba Mini 1.7 improves on steerability.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#use-cases) Use Cases\n---------------------------------------------------------------------------\n\nJambaâ€™s long context efficiency, contextual faithfulness, and steerability make it ideal for a variety of business applications and industries, such as:\n\n*   **Finance**: Investment research, digital banking support chatbot, M\u0026A due diligence.\n*   **Healthcare**: Procurement (RFP creation \u0026 response review), medical publication and reports generation.\n*   **Retail**: Brand-aligned product description generation, conversational AI. \n*   **Education \u0026 Research**: Personalized chatbot tutor, grants applications.\n\nThe models are released under the [Jamba Open Model License](https://www.ai21.com/jamba-open-model-license/), a permissive license allowing full research use and commercial use under the license terms. If you need to license the model for your needs, [talk to us](https://www.ai21.com/contact-sales/).\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#model-details) Model Details\n-----------------------------------------------------------------------------------\n\n*   **Developed by:**[AI21](https://www.ai21.com/)\n*   **Model type:** Joint Attention and Mamba (Jamba)\n*   **License:**[Jamba Open Model License](https://www.ai21.com/licenses/jamba-open-model-license)\n*   **Context length:** 256K\n*   **Knowledge cutoff date:** August 22nd, 2024\n*   **Supported languages:** English, Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#grounding-and-instruction-following-improvements) Grounding and instruction-following improvements\n---------------------------------------------------------------------------------------------------------------------------------------------------------\n\n| Category | Benchmark | Jamba Mini 1.6 | Jamba Mini 1.7 |\n| --- | --- | --- | --- |\n| Grounding | FACTS | 0.727 | 0.790 |\n| Steerability | IFEcal | 0.68 | 0.76 |\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#usage) Usage\n-------------------------------------------------------------------\n\nFind step-by-step instructions on how to privately deploy Jamba:\n\n**Run the model with vLLM**\nThe recommended way to perform efficient inference with Jamba Mini 1.7 is using [vLLM](https://docs.vllm.ai/en/latest/). First, make sure to install vLLM (version 0.5.4 or higher is required):\n\n```\npip install vllm\u003e=0.5.4\n```\n\nIn the example below, `number_gpus` should match the number of GPUs you want to deploy Jamba Mini 1.7 on. A minimum of 2Ã—80GB GPUs is required.\n\n```\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel = \"ai21labs/AI21-Jamba-1.7-Mini\"\nnumber_gpus = 2\n\nllm = LLM(model=model,\n          max_model_len=200*1024,\n          tensor_parallel_size=number_gpus)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an ancient oracle who speaks in cryptic but wise phrases, always hinting at deeper meanings.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\nsampling_params = SamplingParams(temperature=0.4, top_p=0.95, max_tokens=100)\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\n**Output**:\n\n_Seek and you shall find. The path is winding, but the journey is enlightening. What wisdom do you seek from the ancient echoes?_\n\nWith the default BF16 precision on 2Ã—80GB A100 GPUs and default vLLM configuration, you'll be able to perform inference on prompts up to 200K tokens long. On more than 2Ã—80GB GPUs, you can easily fit the full 256K context.\n\n\u003e **Note:** vLLM's main branch has some memory utilization improvements specific to the Jamba architecture that allow using the full 256K context length on 2Ã—80GB GPUs. You can build vLLM from source if you wish to make use of them.\n\n**Run the model with Transformers**\nThe following example loads Jamba Mini 1.7 to the GPU in BF16 precision, uses optimized [FlashAttention2](https://github.com/Dao-AILab/flash-attention) and Mamba kernels, and parallelizes the model across multiple GPUs using [`accelerate`](https://huggingface.co/docs/accelerate/index).\n\n\u003e **Note:** In half precision (FP16/BF16), Jamba Mini 1.7 is too large to fit on a single 80GB GPU, so you'll need at least 2 such GPUs.\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-1.7-Mini\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             device_map=\"auto\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba-1.7-Mini\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an ancient oracle who speaks in cryptic but wise phrases, always hinting at deeper meanings.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\nsampling_params = SamplingParams(temperature=0.4, top_p=0.95, max_tokens=100)\noutputs = model.generate(**tokenizer(prompts, return_tensors=\"pt\").to(model.device),\n                         **sampling_params.to_dict())\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\n\u003e **Note:** Versions `4.44.0` and `4.44.1` of `transformers` have a bug that restricts the ability to run the Jamba architecture. Make sure you're not using these versions.\n\n\u003e **Note:** If you're having trouble installing `mamba-ssm` and `causal-conv1d` for the optimized Mamba kernels, you can run Jamba Mini 1.7 without them at the cost of extra latency. To do that, add the kwarg `use_mamba_kernels=False` when loading the model:\n\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-1.7-Mini\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             device_map=\"auto\",\n                                             use_mamba_kernels=False)\n```\n\nYou can also find all instructions in our [private AI (vLLM) deployment guide](https://docs.ai21.com/docs/vllm).\n\nAnd to get started with our SDK:\n\n[AI21 Python SDK guide](https://docs.ai21.com/docs/sdk)\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7#further-documentation) Further Documentation\n---------------------------------------------------------------------------------------------------\n\nFor comprehensive guides and advanced usage:\n\n*   [Tokenization Guide](https://docs.ai21.com/docs/tokenization) â€“ Using `ai21-tokenizer`\n*   [Quantization Guide](https://docs.ai21.com/docs/quantization) â€“ ExpertsInt8, bitsandbytes\n*   [Fine-tuning Guide](https://docs.ai21.com/docs/fine-tuning) â€“ LoRA, qLoRA and full fine-tuning\n\n**For more resources to start building, visit our [official documentation](https://docs.ai21.com/docs).**\n\nDownloads last month 146 \n\nSafetensors\n\n[](https://huggingface.co/docs/safetensors)\n\nModel tree for ai21labs/AI21-Jamba-Mini-1.7[](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\n------------------------------------------------------------------------------------------------------------------\n\nCollection including ai21labs/AI21-Jamba-Mini-1.7\n-------------------------------------------------",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B",
          "was_fetched": true,
          "page": "Title: ai21labs/AI21-Jamba-Reasoning-3B Â· Hugging Face\n\nURL Source: https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B\n\nMarkdown Content:\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#introduction) Introduction\n-------------------------------------------------------------------------------------\n\nAI21â€™s Jamba Reasoning 3B is a top-performing reasoning model that packs leading scores on intelligence benchmarks and highly-efficient processing into a compact 3B build. \n\n Read the full blog post [here](https://www.ai21.com/blog/introducing-jamba-reasoning-3B).\n\n### [](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#key-advantages) Key Advantages\n\n**Fast: Optimized for efficient sequence processing**\n\nThe hybrid design combines Transformer attention with Mamba (a state-space model). Mamba layers are more efficient for sequence processing, while attention layers capture complex dependencies. This mix reduces memory overhead, improves throughput, and makes the model run smoothly on laptops, GPUs, and even mobile devices, while maintainig impressive quality.\n\n**Smart: Leading intelligence scores** The model outperforms competitors, such as Gemma 3 4B, Llama 3.2 3B, and Granite 4.0 Micro, on a combined intelligence score that averages 6 standard benchmarks.\n\n**Scalable: Handles very long contexts**\n\nUnlike most compact models, Jamba Reasoning 3B supports extremely long contexts. Mamba layers allow the model to process inputs without storing massive attention caches, so it scales to **256K tokens** while keeping inference practical. This makes it suitable for edge deployment as well as datacenter workloads.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#model-details) Model Details\n---------------------------------------------------------------------------------------\n\n*   Number of Parameters: 3B\n*   Number of Layers: 28 (26 Mamba, 2 Attention)\n*   Number of Attention Heads: 20 MQA (20 for Q, 1 for KV)\n*   Vocabulary Size: 64K\n*   Context Length: **256k**\n*   Architecture: Hybrid Transformerâ€“Mamba with efficient attention and long-context support\n*   **Developed by:**[**AI21**](https://www.ai21.com/)\n*   **Supported languages:**English, Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew\n*   Intelligence benchmark results:\n\n|  | **MMLU-Pro** | **Humanityâ€™s Last Exam** | **IFBench** |\n| --- | --- | --- | --- |\n| DeepSeek R1 Distill Qwen 1.5B | 27.0% | 3.3% | 13.0% |\n| Phi-4 mini | 47.0% | 4.2% | 21.0% |\n| Granite 4.0 Micro | 44.7% | 5.1% | 24.8% |\n| Llama 3.2 3B | 35.0% | 5.2% | 26.0% |\n| Gemma 3 4B | 42.0% | 5.2% | 28.0% |\n| Qwen 3 1.7B | 57.0% | 4.8% | 27.0% |\n| Qwen 3 4B | 70% | 5.1% | 33% |\n| **Jamba Reasoning 3B** | **61.0%** | **6.0%** | **52.0%** |\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#quickstart) Quickstart\n---------------------------------------------------------------------------------\n\n### [](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#run-the-model-locally) Run the model locally\n\n\u003e Please reference the GGUF model card [here](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B-GGUF#quickstart).\n\n### [](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#run-the-model-with-vllm)**Run the model with vLLM**\n\nFor best results, we recommend using vLLM version 0.11.0 or higher and enabling `--mamba-ssm-cache-dtype=float32`\n\n```\npip install vllm\u003e=0.11.0\n```\n\nUsing vllm in online server mode:\n\n```\nvllm serve \"ai21labs/AI21-Jamba-Reasoning-3B\" --mamba-ssm-cache-dtype float32 --reasoning-parser deepseek_r1 --enable-auto-tool-choice --tool-call-parser hermes\n```\n\nUsing vllm in offline mode:\n\n```\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel = \"ai21labs/AI21-Jamba-Reasoning-3B\"\n\nllm = LLM(model=model,\n          tensor_parallel_size=1,\n          mamba_ssm_cache_dtype=\"float32\")\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"You are analyzing customer support tickets to decide which need escalation.\\nTicket 1: 'App crashes when uploading files \u003e50MB.'\\nTicket 2: 'Forgot password, canâ€™t log in.'\\nTicket 3: 'Billing page missing enterprise pricing.'\\nClassify each ticket as Critical, Medium, or Low and explain your reasoning.\\n\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\nsampling_params = SamplingParams(temperature=0.6, max_tokens=4096)\noutputs = llm.generate(prompts, sampling_params)\n\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\n```\n\n### [](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#run-the-model-with-transformers)**Run the model with Transformers**\n\n```\npip install transformers\u003e=Â 4.54.0\npip install flash-attn --no-build-isolation\npip install causal-conv1d\u003e=1.2.0\npip install mamba-ssm\n```\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Reasoning-3B\",\n                                             dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             device_map=\"auto\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba-Reasoning-3B\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"You are analyzing customer support tickets to decide which need escalation.\\nTicket 1: 'App crashes when uploading files \u003e50MB.'\\nTicket 2: 'Forgot password, canâ€™t log in.'\\nTicket 3: 'Billing page missing enterprise pricing.'\\nClassify each ticket as Critical, Medium, or Low and explain your reasoning.\\n\"},\n]\n\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\noutputs = model.generate(**tokenizer(prompts, return_tensors=\"pt\").to(model.device), do_sample=True, temperature=0.6, max_new_tokens=4096)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#training-details) Training Details\n---------------------------------------------------------------------------------------------\n\nWe trained the model in multiple stages, each designed to strengthen reasoning and long-context performance. The process began with large-scale pre-training on a diverse corpus of natural documents. We then mid-trained on ~0.5T tokens of math and code, while extending the context length to 32K tokens. During this stage we also applied a [Mamba-specific long-context method](https://arxiv.org/abs/2507.02782), which we found to significantly improve long-context abilities.\n\nTo improve reasoning, tool use, and instruction following, we applied cold-start distillation: supervised fine-tuning with a 32K window and direct preference optimization with a 64K window. Finally, we enhanced reasoning performance further through online reinforcement learning with RLVR, targeting tasks such as code generation, mathematical problem solving, structured output, and information extraction.\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#reinforcement-fine-tuning) Reinforcement â€œFine-Tuningâ€\n-----------------------------------------------------------------------------------------------------------------\n\nFull support for training Jamba through VeRL will be available soon. AI21 has introduced several improvements to the VeRL framework ([https://github.com/volcengine/verl](https://github.com/volcengine/verl)), including new capabilities for training hybrid models, and stability improvements for GRPO training. These improvements will soon be available to the open source community.\n\n* * *\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#license) License\n---------------------------------------------------------------------------\n\n*   `Apache 2.0`\n\n* * *\n\n[](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B#citation) Citation\n-----------------------------------------------------------------------------\n\n*   Blog post- Read the full blog post [here](https://www.ai21.com/blog/introducing-jamba-reasoning-3B)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:35:45.879199103Z"
    },
    {
      "flow_id": "",
      "id": "1q77rxh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/",
      "title": "Z-image base model is being prepared for release",
      "content": "[https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08)",
      "author": "Ravencloud007",
      "created_at": "2026-01-08T09:51:33Z",
      "comments": [
        {
          "id": "nydp3k8",
          "author": "AmazinglyObliviouse",
          "content": "Wake me up when it actually is released, I do not care for your month long teasing.",
          "created_at": "2026-01-08T11:26:07Z",
          "was_summarised": false
        },
        {
          "id": "nye3gr5",
          "author": "FastDecode1",
          "content": "Gooners waiting with bated breath, blue balls, and shivers runnin'.",
          "created_at": "2026-01-08T13:05:33Z",
          "was_summarised": false
        },
        {
          "id": "nydfyut",
          "author": "Geritas",
          "content": "True if big.\n\nI thought they were quietly abandoning the idea.",
          "created_at": "2026-01-08T10:08:18Z",
          "was_summarised": false
        },
        {
          "id": "nyilefa",
          "author": "q5sys",
          "content": "This really only means that they are releasing it on THAT platform.  It doesn't necessarily mean they're going to release open weights to us at the same time.   \n  \nI want them to release open weights, but a Cloud service prepping for it to be able to be used doesn't mean it's getting released to run on our systems. (I hope I'm wrong)",
          "created_at": "2026-01-09T01:45:07Z",
          "was_summarised": false
        },
        {
          "id": "nyilp9m",
          "author": "sammoga123",
          "content": "I thought it would only be T2I, but it seems it will also be able to edit images. I just hope it allows more than one input image, and that it's at least on par with Qwen Edit, because Flux 2 is still useless compared to Qwen.",
          "created_at": "2026-01-09T01:46:44Z",
          "was_summarised": false
        },
        {
          "id": "nyfjv5u",
          "author": "YearZero",
          "content": "What's the purpose of an image gen base model? I understand text base models without instruction tuning just complete/continue a text. What would this one do, and what would it be used for? Would it complete an incomplete image or something? So if I gave it an image with a missing section, would it fill that in? Would it handle any instructions? I know qwen-image-edit can already do this, but it's instruct tuned for that function, and it can work on sections of an image with the right scaffolding.",
          "created_at": "2026-01-08T17:17:02Z",
          "was_summarised": false
        },
        {
          "id": "nyh874x",
          "author": "azerpsen",
          "content": "Waiting for the posts crying why the model is too censored (it refused to generate furry futa inflation incest CP images)",
          "created_at": "2026-01-08T21:41:09Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08",
          "was_fetched": true,
          "page": "Title: Commits Â· modelscope/DiffSynth-Studio\n\nURL Source: https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08\n\nMarkdown Content:\nCommits Â· modelscope/DiffSynth-Studio Â· GitHub\n===============\n\n[Skip to content](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmodelscope%2FDiffSynth-Studio%2Fcommits%3Fauthor%3DArtiprocher%26amp%3Bsince%3D2025-12-31%26amp%3Buntil%3D2026-01-08)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmodelscope%2FDiffSynth-Studio%2Fcommits%3Fauthor%3DArtiprocher%26amp%3Bsince%3D2025-12-31%26amp%3Buntil%3D2026-01-08)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fcommits%2Fshow\u0026source=header-repo\u0026source_repo=modelscope%2FDiffSynth-Studio)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[modelscope](https://github.com/modelscope)/**[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Fmodelscope%2FDiffSynth-Studio)You must be signed in to change notification settings\n*   [Fork 1.1k](https://github.com/login?return_to=%2Fmodelscope%2FDiffSynth-Studio)\n*   [Star 11.4k](https://github.com/login?return_to=%2Fmodelscope%2FDiffSynth-Studio) \n\n*   [Code](https://github.com/modelscope/DiffSynth-Studio)\n*   [Issues 398](https://github.com/modelscope/DiffSynth-Studio/issues)\n*   [Pull requests 12](https://github.com/modelscope/DiffSynth-Studio/pulls)\n*   [Actions](https://github.com/modelscope/DiffSynth-Studio/actions)\n*   [Projects 0](https://github.com/modelscope/DiffSynth-Studio/projects)\n*   [Wiki](https://github.com/modelscope/DiffSynth-Studio/wiki)\n*   [Security](https://github.com/modelscope/DiffSynth-Studio/security)[](https://github.com/modelscope/DiffSynth-Studio/security)[](https://github.com/modelscope/DiffSynth-Studio/security)[](https://github.com/modelscope/DiffSynth-Studio/security)[### Uh oh!](https://github.com/modelscope/DiffSynth-Studio/security)\n[There was an error while loading.](https://github.com/modelscope/DiffSynth-Studio/security)[Please reload this page](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08).    \n*   [Insights](https://github.com/modelscope/DiffSynth-Studio/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/modelscope/DiffSynth-Studio)\n*   [Issues](https://github.com/modelscope/DiffSynth-Studio/issues)\n*   [Pull requests](https://github.com/modelscope/DiffSynth-Studio/pulls)\n*   [Actions](https://github.com/modelscope/DiffSynth-Studio/actions)\n*   [Projects](https://github.com/modelscope/DiffSynth-Studio/projects)\n*   [Wiki](https://github.com/modelscope/DiffSynth-Studio/wiki)\n*   [Security](https://github.com/modelscope/DiffSynth-Studio/security)\n*   [Insights](https://github.com/modelscope/DiffSynth-Studio/pulse)\n\nCommits\n=======\n\nBranch selector\n---------------\n\nmain\n\nUser selector\n-------------\n\nArtiprocher\n\nDatepicker\n----------\n\nAll time\n\nCommit History\n--------------\n\n### Commits on Jan 8, 2026\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/de0aa946f785db498441e0ad6e4cb633b09f4150 \"Merge pull request #1184 from modelscope/z-image-omni-base-dev\n\nupdate package version\")[#1184](https://github.com/modelscope/DiffSynth-Studio/pull/1184)[from modelscope/z-image-omni-base-dev](https://github.com/modelscope/DiffSynth-Studio/commit/de0aa946f785db498441e0ad6e4cb633b09f4150 \"Merge pull request #1184 from modelscope/z-image-omni-base-dev\n\nupdate package version\")\n\nShow description for de0aa94  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Jan 8, 2026      Verified [de0aa94](https://github.com/modelscope/DiffSynth-Studio/commit/de0aa946f785db498441e0ad6e4cb633b09f4150)Copy full SHA for de0aa94  [](https://github.com/modelscope/DiffSynth-Studio/tree/de0aa946f785db498441e0ad6e4cb633b09f4150)   More actions  \n*   #### [update package version](https://github.com/modelscope/DiffSynth-Studio/commit/f376202a4968a54826ddc95781ab91416f65e447 \"update package version\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 8, 2026       [f376202](https://github.com/modelscope/DiffSynth-Studio/commit/f376202a4968a54826ddc95781ab91416f65e447)Copy full SHA for f376202  [](https://github.com/modelscope/DiffSynth-Studio/tree/f376202a4968a54826ddc95781ab91416f65e447)   More actions  \n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/a13ecfc46b04ae57b3f31fd98327bbee501d6ca7 \"Merge pull request #1183 from modelscope/z-image-omni-base-dev\n\nfix unused parameters in z-image-omni-base\")[#1183](https://github.com/modelscope/DiffSynth-Studio/pull/1183)[from modelscope/z-image-omni-base-dev](https://github.com/modelscope/DiffSynth-Studio/commit/a13ecfc46b04ae57b3f31fd98327bbee501d6ca7 \"Merge pull request #1183 from modelscope/z-image-omni-base-dev\n\nfix unused parameters in z-image-omni-base\")\n\nShow description for a13ecfc  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Jan 8, 2026      Verified [a13ecfc](https://github.com/modelscope/DiffSynth-Studio/commit/a13ecfc46b04ae57b3f31fd98327bbee501d6ca7)Copy full SHA for a13ecfc  [](https://github.com/modelscope/DiffSynth-Studio/tree/a13ecfc46b04ae57b3f31fd98327bbee501d6ca7)   More actions  \n*   #### [fix unused parameters in z-image-omni-base](https://github.com/modelscope/DiffSynth-Studio/commit/10a1853eda567146c2669e4e57d55d6326b33e91 \"fix unused parameters in z-image-omni-base\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 8, 2026       [10a1853](https://github.com/modelscope/DiffSynth-Studio/commit/10a1853eda567146c2669e4e57d55d6326b33e91)Copy full SHA for 10a1853  [](https://github.com/modelscope/DiffSynth-Studio/tree/10a1853eda567146c2669e4e57d55d6326b33e91)   More actions  \n*   #### [Support Z-Image-Omni-Base and its related models](https://github.com/modelscope/DiffSynth-Studio/commit/0efab85674f2a65a8064acfb7a4b7950503a5668 \"Support Z-Image-Omni-Base and its related models\n\nSupport Z-Image-Omni-Base and its related models.\")\n\nShow description for 0efab85  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Jan 8, 2026      Verified [0efab85](https://github.com/modelscope/DiffSynth-Studio/commit/0efab85674f2a65a8064acfb7a4b7950503a5668)Copy full SHA for 0efab85  [](https://github.com/modelscope/DiffSynth-Studio/tree/0efab85674f2a65a8064acfb7a4b7950503a5668)   More actions  \n*   #### [support z-image-omni-base vram management](https://github.com/modelscope/DiffSynth-Studio/commit/f45a0ffd028743009f916e7b9e7b707dd1416e38 \"support z-image-omni-base vram management\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 8, 2026       [f45a0ff](https://github.com/modelscope/DiffSynth-Studio/commit/f45a0ffd028743009f916e7b9e7b707dd1416e38)Copy full SHA for f45a0ff  [](https://github.com/modelscope/DiffSynth-Studio/tree/f45a0ffd028743009f916e7b9e7b707dd1416e38)   More actions  \n*   #### [bugfix](https://github.com/modelscope/DiffSynth-Studio/commit/8ba528a8f65a252d5cfff04d526494f94ef03e55 \"bugfix\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 8, 2026       [8ba528a](https://github.com/modelscope/DiffSynth-Studio/commit/8ba528a8f65a252d5cfff04d526494f94ef03e55)Copy full SHA for 8ba528a  [](https://github.com/modelscope/DiffSynth-Studio/tree/8ba528a8f65a252d5cfff04d526494f94ef03e55)   More actions  \n\n### Commits on Jan 7, 2026\n\n*   #### [support z-image-omni-base-i2L](https://github.com/modelscope/DiffSynth-Studio/commit/dd479e5bffb7a214183431ca5c6fb3a892bb3e63 \"support z-image-omni-base-i2L\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 7, 2026       [dd479e5](https://github.com/modelscope/DiffSynth-Studio/commit/dd479e5bffb7a214183431ca5c6fb3a892bb3e63)Copy full SHA for dd479e5  [](https://github.com/modelscope/DiffSynth-Studio/tree/dd479e5bffb7a214183431ca5c6fb3a892bb3e63)   More actions  \n*   #### [support z-image controlnet](https://github.com/modelscope/DiffSynth-Studio/commit/bac39b1cd281ad0c464a95c4eec7f74237bec710 \"support z-image controlnet\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 7, 2026       [bac39b1](https://github.com/modelscope/DiffSynth-Studio/commit/bac39b1cd281ad0c464a95c4eec7f74237bec710)Copy full SHA for bac39b1  [](https://github.com/modelscope/DiffSynth-Studio/tree/bac39b1cd281ad0c464a95c4eec7f74237bec710)   More actions  \n\n### Commits on Jan 5, 2026\n\n*   #### [support z-image-omni-base training](https://github.com/modelscope/DiffSynth-Studio/commit/32449a6aa0166754d2bdf44dea2e6c1bf4ec5899 \"support z-image-omni-base training\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 5, 2026       [32449a6](https://github.com/modelscope/DiffSynth-Studio/commit/32449a6aa0166754d2bdf44dea2e6c1bf4ec5899)Copy full SHA for 32449a6  [](https://github.com/modelscope/DiffSynth-Studio/tree/32449a6aa0166754d2bdf44dea2e6c1bf4ec5899)   More actions  \n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/a6884f6b3aa08f3c06c67ccf467d625424858ab1 \"Merge pull request #1171 from YZBPXX/main\n\nFix issue where LoRa loads on a device different from Dit\")[#1171](https://github.com/modelscope/DiffSynth-Studio/pull/1171)[from YZBPXX/main](https://github.com/modelscope/DiffSynth-Studio/commit/a6884f6b3aa08f3c06c67ccf467d625424858ab1 \"Merge pull request #1171 from YZBPXX/main\n\nFix issue where LoRa loads on a device different from Dit\")\n\nShow description for a6884f6  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Jan 5, 2026      Verified [a6884f6](https://github.com/modelscope/DiffSynth-Studio/commit/a6884f6b3aa08f3c06c67ccf467d625424858ab1)Copy full SHA for a6884f6  [](https://github.com/modelscope/DiffSynth-Studio/tree/a6884f6b3aa08f3c06c67ccf467d625424858ab1)   More actions  \n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/b07866664014d41de83c86055fe4b2be30355889 \"Merge pull request #1173 from modelscope/flux-compatibility-patch\n\nflux compatibility patch\")[#1173](https://github.com/modelscope/DiffSynth-Studio/pull/1173)[from modelscope/flux-compatibility-patch](https://github.com/modelscope/DiffSynth-Studio/commit/b07866664014d41de83c86055fe4b2be30355889 \"Merge pull request #1173 from modelscope/flux-compatibility-patch\n\nflux compatibility patch\")\n\nShow description for b078666  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Jan 5, 2026      Verified [b078666](https://github.com/modelscope/DiffSynth-Studio/commit/b07866664014d41de83c86055fe4b2be30355889)Copy full SHA for b078666  [](https://github.com/modelscope/DiffSynth-Studio/tree/b07866664014d41de83c86055fe4b2be30355889)   More actions  \n*   #### [flux compatibility patch](https://github.com/modelscope/DiffSynth-Studio/commit/7604ca1e52cff901d902a59b591d36a9af521c4f \"flux compatibility patch\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 5, 2026       [7604ca1](https://github.com/modelscope/DiffSynth-Studio/commit/7604ca1e52cff901d902a59b591d36a9af521c4f)Copy full SHA for 7604ca1  [](https://github.com/modelscope/DiffSynth-Studio/tree/7604ca1e52cff901d902a59b591d36a9af521c4f)   More actions  \n*   #### [support z-image-omni-base](https://github.com/modelscope/DiffSynth-Studio/commit/5745c9f2003476956352d3154361065653e07e41 \"support z-image-omni-base\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Jan 5, 2026       [5745c9f](https://github.com/modelscope/DiffSynth-Studio/commit/5745c9f2003476956352d3154361065653e07e41)Copy full SHA for 5745c9f  [](https://github.com/modelscope/DiffSynth-Studio/tree/5745c9f2003476956352d3154361065653e07e41)   More actions  \n\n### Commits on Dec 30, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/ab8580f77eb190ca40dbc5fa19fbcf8be440e83e \"Merge pull request #1166 from modelscope/qwen-image-2512\n\nsupport qwen-image-2512\")[#1166](https://github.com/modelscope/DiffSynth-Studio/pull/1166)[from modelscope/qwen-image-2512](https://github.com/modelscope/DiffSynth-Studio/commit/ab8580f77eb190ca40dbc5fa19fbcf8be440e83e \"Merge pull request #1166 from modelscope/qwen-image-2512\n\nsupport qwen-image-2512\")\n\nShow description for ab8580f  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 30, 2025      Verified [ab8580f](https://github.com/modelscope/DiffSynth-Studio/commit/ab8580f77eb190ca40dbc5fa19fbcf8be440e83e)Copy full SHA for ab8580f  [](https://github.com/modelscope/DiffSynth-Studio/tree/ab8580f77eb190ca40dbc5fa19fbcf8be440e83e)   More actions  \n*   #### [support qwen-image-2512](https://github.com/modelscope/DiffSynth-Studio/commit/64542598534529b58e737bf1caf1382d59533690 \"support qwen-image-2512\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 30, 2025       [6454259](https://github.com/modelscope/DiffSynth-Studio/commit/64542598534529b58e737bf1caf1382d59533690)Copy full SHA for 6454259  [](https://github.com/modelscope/DiffSynth-Studio/tree/64542598534529b58e737bf1caf1382d59533690)   More actions  \n\n### Commits on Dec 20, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/8f1d10fb432aa55541f5ad6f3ed4ba1c300a3b18 \"Merge pull request #1150 from modelscope/qwen-image-layered\n\nsupport qwen-image-layered\")[#1150](https://github.com/modelscope/DiffSynth-Studio/pull/1150)[from modelscope/qwen-image-layered](https://github.com/modelscope/DiffSynth-Studio/commit/8f1d10fb432aa55541f5ad6f3ed4ba1c300a3b18 \"Merge pull request #1150 from modelscope/qwen-image-layered\n\nsupport qwen-image-layered\")\n\nShow description for 8f1d10f  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 20, 2025      Verified [8f1d10f](https://github.com/modelscope/DiffSynth-Studio/commit/8f1d10fb432aa55541f5ad6f3ed4ba1c300a3b18)Copy full SHA for 8f1d10f  [](https://github.com/modelscope/DiffSynth-Studio/tree/8f1d10fb432aa55541f5ad6f3ed4ba1c300a3b18)   More actions  \n*   #### [bugfix](https://github.com/modelscope/DiffSynth-Studio/commit/20e1aaf9085442a13330d475e5a5dbeebd44f3c9 \"bugfix\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 20, 2025       [20e1aaf](https://github.com/modelscope/DiffSynth-Studio/commit/20e1aaf9085442a13330d475e5a5dbeebd44f3c9)Copy full SHA for 20e1aaf  [](https://github.com/modelscope/DiffSynth-Studio/tree/20e1aaf9085442a13330d475e5a5dbeebd44f3c9)   More actions  \n\n### Commits on Dec 19, 2025\n\n*   #### [support qwen-image-layered](https://github.com/modelscope/DiffSynth-Studio/commit/c6722b3f56bb298b4c81b1e2ca81d27cc81a1081 \"support qwen-image-layered\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 19, 2025       [c6722b3](https://github.com/modelscope/DiffSynth-Studio/commit/c6722b3f56bb298b4c81b1e2ca81d27cc81a1081)Copy full SHA for c6722b3  [](https://github.com/modelscope/DiffSynth-Studio/tree/c6722b3f56bb298b4c81b1e2ca81d27cc81a1081)   More actions  \n\n### Commits on Dec 18, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/11315d7a40fb645f7806298e50613ab6663912fa \"Merge pull request #1147 from modelscope/qwen-image-edit-2511\n\nQwen image edit 2511\")[#1147](https://github.com/modelscope/DiffSynth-Studio/pull/1147)[from modelscope/qwen-image-edit-2511](https://github.com/modelscope/DiffSynth-Studio/commit/11315d7a40fb645f7806298e50613ab6663912fa \"Merge pull request #1147 from modelscope/qwen-image-edit-2511\n\nQwen image edit 2511\")\n\nShow description for 11315d7  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 18, 2025      Verified [11315d7](https://github.com/modelscope/DiffSynth-Studio/commit/11315d7a40fb645f7806298e50613ab6663912fa)Copy full SHA for 11315d7  [](https://github.com/modelscope/DiffSynth-Studio/tree/11315d7a40fb645f7806298e50613ab6663912fa)   More actions  \n*   #### [update doc](https://github.com/modelscope/DiffSynth-Studio/commit/68d97a9844164970b4262e583453109f29ffb60d \"update doc\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 18, 2025       [68d97a9](https://github.com/modelscope/DiffSynth-Studio/commit/68d97a9844164970b4262e583453109f29ffb60d)Copy full SHA for 68d97a9  [](https://github.com/modelscope/DiffSynth-Studio/tree/68d97a9844164970b4262e583453109f29ffb60d)   More actions  \n*   #### [support qwen-image-edit-2511](https://github.com/modelscope/DiffSynth-Studio/commit/4629d4cf9ed315d69da64aea3c225406856f4331 \"support qwen-image-edit-2511\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 18, 2025       [4629d4c](https://github.com/modelscope/DiffSynth-Studio/commit/4629d4cf9ed315d69da64aea3c225406856f4331)Copy full SHA for 4629d4c  [](https://github.com/modelscope/DiffSynth-Studio/tree/4629d4cf9ed315d69da64aea3c225406856f4331)   More actions  \n\n### Commits on Dec 17, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/3cb5cec906ee54043f4c06de96d8a5600376e5e3 \"Merge pull request #1143 from modelscope/readme-update\n\nupdate README\")[#1143](https://github.com/modelscope/DiffSynth-Studio/pull/1143)[from modelscope/readme-update](https://github.com/modelscope/DiffSynth-Studio/commit/3cb5cec906ee54043f4c06de96d8a5600376e5e3 \"Merge pull request #1143 from modelscope/readme-update\n\nupdate README\")\n\nShow description for 3cb5cec  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 17, 2025      Verified [3cb5cec](https://github.com/modelscope/DiffSynth-Studio/commit/3cb5cec906ee54043f4c06de96d8a5600376e5e3)Copy full SHA for 3cb5cec  [](https://github.com/modelscope/DiffSynth-Studio/tree/3cb5cec906ee54043f4c06de96d8a5600376e5e3)   More actions  \n*   #### [update README](https://github.com/modelscope/DiffSynth-Studio/commit/b7e16b903416acf91108132e33fbbd2dfde1dad0 \"update README\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 17, 2025       [b7e16b9](https://github.com/modelscope/DiffSynth-Studio/commit/b7e16b903416acf91108132e33fbbd2dfde1dad0)Copy full SHA for b7e16b9  [](https://github.com/modelscope/DiffSynth-Studio/tree/b7e16b903416acf91108132e33fbbd2dfde1dad0)   More actions  \n\n### Commits on Dec 16, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/83d1e7361f824670803ad02c66345c8483ca1d5f \"Merge pull request #1136 from modelscope/bugfix-device\n\nbugfix\")[#1136](https://github.com/modelscope/DiffSynth-Studio/pull/1136)[from modelscope/bugfix-device](https://github.com/modelscope/DiffSynth-Studio/commit/83d1e7361f824670803ad02c66345c8483ca1d5f \"Merge pull request #1136 from modelscope/bugfix-device\n\nbugfix\")\n\nShow description for 83d1e73  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 16, 2025      Verified [83d1e73](https://github.com/modelscope/DiffSynth-Studio/commit/83d1e7361f824670803ad02c66345c8483ca1d5f)Copy full SHA for 83d1e73  [](https://github.com/modelscope/DiffSynth-Studio/tree/83d1e7361f824670803ad02c66345c8483ca1d5f)   More actions  \n*   #### [bugfix](https://github.com/modelscope/DiffSynth-Studio/commit/1547c3f7863d93ace1b82511b9c1c421c85b12b9 \"bugfix\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 16, 2025       [1547c3f](https://github.com/modelscope/DiffSynth-Studio/commit/1547c3f7863d93ace1b82511b9c1c421c85b12b9)Copy full SHA for 1547c3f  [](https://github.com/modelscope/DiffSynth-Studio/tree/1547c3f7863d93ace1b82511b9c1c421c85b12b9)   More actions  \n\n### Commits on Dec 15, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/bfaaf12bf41bb573336572e8c90cd0d5b3361f7c \"Merge pull request #1129 from modelscope/ascend\n\nSupport Ascend NPU\")[#1129](https://github.com/modelscope/DiffSynth-Studio/pull/1129)[from modelscope/ascend](https://github.com/modelscope/DiffSynth-Studio/commit/bfaaf12bf41bb573336572e8c90cd0d5b3361f7c \"Merge pull request #1129 from modelscope/ascend\n\nSupport Ascend NPU\")\n\nShow description for bfaaf12  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 15, 2025      Verified [bfaaf12](https://github.com/modelscope/DiffSynth-Studio/commit/bfaaf12bf41bb573336572e8c90cd0d5b3361f7c)Copy full SHA for bfaaf12  [](https://github.com/modelscope/DiffSynth-Studio/tree/bfaaf12bf41bb573336572e8c90cd0d5b3361f7c)   More actions  \n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/47545e1aab6ef286fcc39aa0bdff07aee9f99fcc \"Merge pull request #1126 from Leoooo333/main\n\nFixed: Wan S2V Long video severe quality downgrade\")[#1126](https://github.com/modelscope/DiffSynth-Studio/pull/1126)[from Leoooo333/main](https://github.com/modelscope/DiffSynth-Studio/commit/47545e1aab6ef286fcc39aa0bdff07aee9f99fcc \"Merge pull request #1126 from Leoooo333/main\n\nFixed: Wan S2V Long video severe quality downgrade\")\n\nShow description for 47545e1  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 15, 2025      Verified [47545e1](https://github.com/modelscope/DiffSynth-Studio/commit/47545e1aab6ef286fcc39aa0bdff07aee9f99fcc)Copy full SHA for 47545e1  [](https://github.com/modelscope/DiffSynth-Studio/tree/47545e1aab6ef286fcc39aa0bdff07aee9f99fcc)   More actions  \n*   #### [support ascend npu](https://github.com/modelscope/DiffSynth-Studio/commit/7c6905a4322bf6691cc50353912fbc80adbe1a61 \"support ascend npu\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 15, 2025       [7c6905a](https://github.com/modelscope/DiffSynth-Studio/commit/7c6905a4322bf6691cc50353912fbc80adbe1a61)Copy full SHA for 7c6905a  [](https://github.com/modelscope/DiffSynth-Studio/tree/7c6905a4322bf6691cc50353912fbc80adbe1a61)   More actions  \n*   #### [support ascend npu](https://github.com/modelscope/DiffSynth-Studio/commit/2883bc1b763523c34a2fab10a282a81fa0917288 \"support ascend npu\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 15, 2025       [2883bc1](https://github.com/modelscope/DiffSynth-Studio/commit/2883bc1b763523c34a2fab10a282a81fa0917288)Copy full SHA for 2883bc1  [](https://github.com/modelscope/DiffSynth-Studio/tree/2883bc1b763523c34a2fab10a282a81fa0917288)   More actions  \n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/78d8842ddfca1e8bacc63ad894814df6ea5b6313 \"Merge pull request #1128 from modelscope/amd_install\n\nupdate installation instructions for AMD\")[#1128](https://github.com/modelscope/DiffSynth-Studio/pull/1128)[from modelscope/amd_install](https://github.com/modelscope/DiffSynth-Studio/commit/78d8842ddfca1e8bacc63ad894814df6ea5b6313 \"Merge pull request #1128 from modelscope/amd_install\n\nupdate installation instructions for AMD\")\n\nShow description for 78d8842  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 15, 2025      Verified [78d8842](https://github.com/modelscope/DiffSynth-Studio/commit/78d8842ddfca1e8bacc63ad894814df6ea5b6313)Copy full SHA for 78d8842  [](https://github.com/modelscope/DiffSynth-Studio/tree/78d8842ddfca1e8bacc63ad894814df6ea5b6313)   More actions  \n*   #### [update AMD GPU support](https://github.com/modelscope/DiffSynth-Studio/commit/5821a664a062652368ff5967909db09d113c1202 \"update AMD GPU support\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 15, 2025       [5821a66](https://github.com/modelscope/DiffSynth-Studio/commit/5821a664a062652368ff5967909db09d113c1202)Copy full SHA for 5821a66  [](https://github.com/modelscope/DiffSynth-Studio/tree/5821a664a062652368ff5967909db09d113c1202)   More actions  \n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/ab9aa1a087fee7f89b0b470d615ee8f7f00d91b2 \"Merge pull request #1124 from lzws/main\n\nadd wan usp example\")[#1124](https://github.com/modelscope/DiffSynth-Studio/pull/1124)[from lzws/main](https://github.com/modelscope/DiffSynth-Studio/commit/ab9aa1a087fee7f89b0b470d615ee8f7f00d91b2 \"Merge pull request #1124 from lzws/main\n\nadd wan usp example\")\n\nShow description for ab9aa1a  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 15, 2025      Verified [ab9aa1a](https://github.com/modelscope/DiffSynth-Studio/commit/ab9aa1a087fee7f89b0b470d615ee8f7f00d91b2)Copy full SHA for ab9aa1a  [](https://github.com/modelscope/DiffSynth-Studio/tree/ab9aa1a087fee7f89b0b470d615ee8f7f00d91b2)   More actions  \n\n### Commits on Dec 12, 2025\n\n*   #### [Merge pull request](https://github.com/modelscope/DiffSynth-Studio/commit/e316fb717f4ab5ab05c28c340d6ed4e8555697bf \"Merge pull request #1122 from modelscope/flux-lora-revert\n\nrevert FluxLoRAConverter due to dependency issues\")[#1122](https://github.com/modelscope/DiffSynth-Studio/pull/1122)[from modelscope/flux-lora-revert](https://github.com/modelscope/DiffSynth-Studio/commit/e316fb717f4ab5ab05c28c340d6ed4e8555697bf \"Merge pull request #1122 from modelscope/flux-lora-revert\n\nrevert FluxLoRAConverter due to dependency issues\")\n\nShow description for e316fb7  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) authored Dec 12, 2025      Verified [e316fb7](https://github.com/modelscope/DiffSynth-Studio/commit/e316fb717f4ab5ab05c28c340d6ed4e8555697bf)Copy full SHA for e316fb7  [](https://github.com/modelscope/DiffSynth-Studio/tree/e316fb717f4ab5ab05c28c340d6ed4e8555697bf)   More actions  \n*   #### [revert FluxLoRAConverter due to dependency issues](https://github.com/modelscope/DiffSynth-Studio/commit/64c5139502ed5af31a1fb27bff8f254bbc1311f9 \"revert FluxLoRAConverter due to dependency issues\")  [](https://github.com/Artiprocher)[Artiprocher](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher) committed Dec 12, 2025       [64c5139](https://github.com/modelscope/DiffSynth-Studio/commit/64c5139502ed5af31a1fb27bff8f254bbc1311f9)Copy full SHA for 64c5139  [](https://github.com/modelscope/DiffSynth-Studio/tree/64c5139502ed5af31a1fb27bff8f254bbc1311f9)   More actions  \n\nPagination\n----------\n\n[Previous](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp%3Bsince=2025-12-31\u0026amp%3Buntil=2026-01-08\u0026before=de0aa946f785db498441e0ad6e4cb633b09f4150+0)\n\n[Next](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp%3Bsince=2025-12-31\u0026amp%3Buntil=2026-01-08\u0026after=de0aa946f785db498441e0ad6e4cb633b09f4150+34)\n\nFooter\n------\n\n[](https://github.com/) Â© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You canâ€™t perform that action at this time.",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/038zb25ok3cg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:35:55.444786481Z"
    },
    {
      "flow_id": "",
      "id": "1q7o8kl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/",
      "title": "GLM-4.7 on 4x RTX 3090 with ik_llama.cpp",
      "content": "With the help of Opus 4.5 I got unsloth/GLM-4.7-GGUF (Q4\\_K\\_M) running on my 4x RTX 3090 setup using ik\\_llama.cpp in Docker. I wanted to share my benchmark results and configuration, and ask if these numbers are what I should expect - or if there's room for improvement.\n\n# My Setup\n\n|Component|Specs|\n|:-|:-|\n|Motherboard|Supermicro H12SSL-i|\n|CPU|AMD EPYC 7282|\n|GPUs|4x NVIDIA RTX 3090 (96GB VRAM total, all at PCIe x16)|\n|RAM|256GB DDR4-2133|\n|Storage|2 TB NVMe SSD|\n\n# Benchmark Results\n\n|Config|Context|n-cpu-moe|Batch|VRAM/GPU|Prompt|**Generation**|\n|:-|:-|:-|:-|:-|:-|:-|\n|Initial (mmap)|16K|all|512|\\~5 GB|2.8 t/s|3.1 t/s|\n|split-mode layer|16K|partial|4096|\\~17 GB|2.8 t/s|âš ï¸ 0.29 t/s|\n|\\+ no-mmap|16K|all|4096|\\~10 GB|8.5 t/s|3.45 t/s|\n|\\+ n-cpu-moe 72|16K|72|4096|\\~17 GB|9.9 t/s|4.12 t/s|\n|**Best 8K**|**8K**|**65**|**4096**|**\\~21 GB**|**12.0 t/s**|**4.48 t/s** â­|\n|**Best 16K**|**16K**|**68**|**2048**|**\\~19 GB**|**10.5 t/s**|**4.28 t/s** â­|\n\n# Benchmark Methodology\n\nAll tests were performed using the same simple request via curl:\n\n    curl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"model\": \"GLM-4.7-GUFF\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Write a short Haiku.\"}],\n        \"temperature\": 0.7,\n        \"max_tokens\": 100\n      }'\n\nThe response includes timing information:\n\n    {\n      \"timings\": {\n        \"prompt_n\": 17,\n        \"prompt_ms\": 1419.902,\n        \"prompt_per_second\": 11.97,\n        \"predicted_n\": 100,\n        \"predicted_ms\": 22301.81,\n        \"predicted_per_second\": 4.48\n      }\n    }\n\n* **prompt\\_per\\_second**: How fast the input tokens are processed\n* **predicted\\_per\\_second**: How fast new tokens are generated (this is what matters most for chat)\n\nEach configuration was tested with a fresh server start (cold start) and the first request after warmup. Note that GLM-4.7 has a \"thinking/reasoning\" mode enabled by default, so the 100 generated tokens include internal reasoning tokens.\n\n# My Current Configuration\n\n# Best for 8K Context (fastest):\n\n    llama-server \\\n        --model \"/models/GLM-4-Q4_K_M-00001-of-00005.gguf\" \\\n        --host 0.0.0.0 --port 8080 \\\n        --ctx-size 8192 \\\n        --n-gpu-layers 999 \\\n        --split-mode graph \\\n        --flash-attn on \\\n        --no-mmap \\\n        -b 4096 -ub 4096 \\\n        --cache-type-k q4_0 --cache-type-v q4_0 \\\n        --k-cache-hadamard \\\n        --jinja \\\n        --n-cpu-moe 65\n\n# Best for 16K Context:\n\n    llama-server \\\n        --model \"/models/GLM-4-Q4_K_M-00001-of-00005.gguf\" \\\n        --host 0.0.0.0 --port 8080 \\\n        --ctx-size 16384 \\\n        --n-gpu-layers 999 \\\n        --split-mode graph \\\n        --flash-attn on \\\n        --no-mmap \\\n        -b 2048 -ub 2048 \\\n        --cache-type-k q4_0 --cache-type-v q4_0 \\\n        --k-cache-hadamard \\\n        --jinja \\\n        --n-cpu-moe 68\n\n# Key Findings:\n\n1. `--no-mmap` **is crucial** \\- Loading the model into RAM instead of memory-mapping from SSD **tripled** my prompt processing speed (2.8 â†’ 12 t/s)\n2. `--split-mode graph` **not** `layer` \\- Layer mode gave me only 0.29 t/s because GPUs process sequentially. Graph mode enables true tensor parallelism.\n3. `--n-cpu-moe X` \\- This flag controls how many MoE layers stay on CPU.\n4. **Batch size matters** \\- Smaller batches (2048) allowed more MoE layers on GPU for 16K context.\n\n# Docker Setup\n\nI'm running this in Docker. Here's my `docker-compose.yml`:\n\n    services:\n      glm-4:\n        build:\n          context: .\n          dockerfile: Dockerfile\n        container_name: glm-4-server\n        deploy:\n          resources:\n            reservations:\n              devices:\n                - driver: nvidia\n                  count: all\n                  capabilities: [gpu]\n        volumes:\n          - /path/to/models:/models:ro\n        ports:\n          - \"8080:8080\"\n        environment:\n          - CTX_MODE=${CTX_MODE:-8k}  # Switch between 8k/16k\n          - NO_MMAP=true\n          - KV_CACHE_K=q4_0\n          - KV_CACHE_V=q4_0\n          - K_CACHE_HADAMARD=true\n        shm_size: '32gb'\n        ipc: host\n        restart: unless-stopped\n\nAnd my `Dockerfile` builds ik\\_llama.cpp with CUDA support:\n\n    FROM nvidia/cuda:12.4.0-devel-ubuntu22.04\n    \n    # Install dependencies\n    RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\\n        git cmake build-essential curl \\\n        \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*\n    \n    # Clone and build ik_llama.cpp\n    WORKDIR /opt\n    RUN git clone https://github.com/ikawrakow/ik_llama.cpp.git\n    WORKDIR /opt/ik_llama.cpp\n    \n    RUN cmake -B build \\\n        -DGGML_CUDA=ON \\\n        -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n        -DCMAKE_CUDA_ARCHITECTURES=\"86\" \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        \u0026amp;\u0026amp; cmake --build build --config Release -j$(nproc) \\\n        \u0026amp;\u0026amp; cmake --install build\n    \n    EXPOSE 8080\n    COPY entrypoint.sh /entrypoint.sh\n    RUN chmod +x /entrypoint.sh\n    ENTRYPOINT [\"/entrypoint.sh\"]\n\n# Questions\n\n1. **Are these speeds (4.48 t/s generation) normal for this setup?** I've seen some posts mentioning 5-6 t/s with 2x RTX 5090, but they had 64GB VRAM total vs my 96GB.\n2. **Any other flags I should try?** I tested `--run-time-repack` but it didn't help much.\n3. **Is there a better MoE offloading strategy?** I'm using `--n-cpu-moe` but I know there's also the `-ot` regex approach.\n4. **Would a different quantization help?** Currently using Q4\\_K\\_M. Would IQ4\\_XS or Q5\\_K\\_M be faster/better?\n5. **Low GPU power usage during inference?** My cards are power-limited to 275W each, but during inference they only draw \\~100-120W. Could this be a bottleneck limiting my token/s?\n\nI would love to hear your thoughts and any optimization tips.",
      "author": "iamn0",
      "created_at": "2026-01-08T21:14:19Z",
      "comments": [
        {
          "id": "nyhbw4l",
          "author": "massive_rock33",
          "content": "The token gen seems too slow, im surprised it's this slow with 4 gpu",
          "created_at": "2026-01-08T21:57:11Z",
          "was_summarised": false
        },
        {
          "id": "nyhsog9",
          "author": "Lissanro",
          "content": "My rig is somewhat similar to yours, also EPYC and DDR4-based, with four 3090 cards. I have EPYC 7763 with 1TB DDR4 3200MHz RAM + 4x3090 GPUs, and with IQ4 quant of GLM-4.7 I get about 6 tokens/s generation 200 tokens/s prompt processing., with 19 full layers on GPUs along with common expert tensors and 200K context cache at Q8. Here is my command for reference:\n\n    numactl --cpunodebind=0 --interleave=all ~/pkgs/ik_llama.cpp/build/bin/llama-server \\\n    --model /mnt/neuro/models/GLM-4.7-IQ4_K/GLM-4.7-IQ4_K-00001-of-00006.gguf \\\n    --ctx-size 202752 --n-gpu-layers 62 --tensor-split 25,23,25,27 -ctk q8_0 -ctv q8_0 -b 4096 -ub 4096 -fa on \\\n    -ot \"blk\\.(3|4|5|6)\\.ffn_.*=CUDA0\" \\\n    -ot \"blk\\.(8|9|10|11|12)\\.ffn_.*=CUDA1\" \\\n    -ot \"blk\\.(13|14|15|16|17)\\.ffn_.*=CUDA2\" \\\n    -ot \"blk\\.(18|19|20|21|22)\\.ffn_.*=CUDA3\" \\\n    -ot exps=CPU \\\n    --threads 64 --host 0.0.0.0 --port 5000 \\\n    --jinja \\\n    --slot-save-path /var/cache/ik_llama.cpp/glm-4.7\n\nBy the way, with  `--split-mode graph` I get crash:\n\n    /home/lissanro/pkgs/ik_llama.cpp/src/llama.cpp:566: GGML_ASSERT(kl \u0026amp;\u0026amp; (!kv_self.v_l[il] || vl)) failed\n\nAnd with `--k-cache-hadamard` it generated gibberish. I tried with the latest ik\\_llama.cpp from git. But it sounds like you managed to get these options working?\n\nAs of your bottleneck, for token generation I think it is both your CPU and RAM. During token generation, all 64 cores on my EPYC 7763 get saturated a little bit sooner than memory bandwidth of 8-channel 3200MHz RAM, so any slower CPU would reduce the performance. This also means your rig is well balanced, faster memory would probably not make much difference, so your current memory is well suited for your CPU. 4 tokens/s is very good given your CPU and RAM speeds, probably GPUs help a lot.\n\nBut your prompt processing speed is unusually slow, you should be getting about the same speed as me (around 200 tokens/s) since prompt processing is done on GPUs and my CPU is almost idle while it happens. I see you have `--n-cpu-moe 68` \\- I did not try it myself, but very likely it is not equivalent to the proper `-ot` options. Calibrating `--tensor-split` and `-ot` lines to decide how many layers to put and where can be time consuming but I think it is necessary if you want to get the best performance. Notice how I have `exps=CPU` at the end - order is important. And you can manually write layers number from 3 and up in each `-ot` line, to define which layers go to which CUDA device - in my case I put 4 on my first GPU (CUDA0) and 5 layers on the rest, but depending on your VRAM usage by the system and other factors, you may need to change my example.",
          "created_at": "2026-01-08T23:15:41Z",
          "was_summarised": false
        },
        {
          "id": "nyhwmta",
          "author": "tenebreoscure",
          "content": "You can get definitely better speeds. I'm currently running IQ4\\_XS on plain llama.cpp, using something like this:\n\n    CUDA_DEVICE_ORDER='PCI_BUS_ID' LLAMA_SET_ROWS=1 ./llama.cpp/build/bin/llama-server \\ \n    --model zai-org_GLM-4.7-IQ4_XS-00001-of-00005.gguf \\ \n    --alias bartowski/GLM-4.7-IQ4_XS \\ \n    --ctx-size 65536 \\ \n    --flash-attn on \\ \n    -ngl 99 \\ \n    -ot \"blk\\.[0-9]\\.ffn.*=CUDA0,blk\\.1[0-2]\\.ffn.*=CUDA0,blk\\.1[3-9]\\.ffn.*=CUDA1,blk\\.20\\.ffn.*=CUDA1,blk\\.2[1-8]\\.ffn.*=CUDA2,blk\\.29\\.ffn.*=CUDA3,blk\\.3[0-6]\\.ffn.*=CUDA3,blk\\.3[7-9]\\.ffn.*=CUDA4,blk\\.4[0-1]\\.ffn.*=CUDA4,blk\\\\..*_exps\\\\.=CPU\" \\ \n    --no-mmap \\ \n    --threads 11 \\ \n    --parallel 1 \\ \n    --host 127.0.0.1 \\ \n    --port 8080\n\nOn an AM5 platform with 70 GB/S Ram bandwdith, 192GB RAM and a combination of 120GB of VRAM.\n\nI get ~100pp/8TG @8k and ~100pp/7.5TG @16k, and run at 65K. The trick is to load with the override tensor flag as many layers as you can on the video cards without compromising too much with context. \n\nI'd suggest against quantizing the cache to even 8 bit as it brings spelling errors and slight hallucinations, it's tolerable for general conversation, not for coding or anything income related. \n\nAlso for ik_llama.cpp use Ubergarm's quants https://huggingface.co/ubergarm/GLM-4.7-GGUF or Thireus's, they are optimized for ik_llama.cpp and should give faster results. Ubergarm has an excellent readme with every trick to squeeze out performances.\n\nHow many ram sticks do you have? That epyc has an 8ch memory controller, if you have all the dimm slots occupied the ram bandwidth should be around double mine. I'd suggest measuring it with Intel MLC or a similar tool. Nevermind, read the comment about the processor having 2 ccds. So yeah, It's probably bottlenecking. I'd still try to squeeze out some more numbers.\n\nPersonally --n-cpu-moe never worked for me and gave sluggish speed, the -ot approach worked better, so I'd try that one. I'd also choose the smallest Q4 quant over the K_M one. Power limiting should not be an issue, you can monitor it with nvtop anyway to check if the gpus cap it.",
          "created_at": "2026-01-08T23:35:56Z",
          "was_summarised": false
        },
        {
          "id": "nyhdw1m",
          "author": "FullstackSensei",
          "content": "You can sell those four 3090s, and buy an 8 CCD Epyc (256MB L3 cache). You'll save a lot of power, and end up with better performance.\n\nJokes aside, two things are killing your performance: that CPU and your RAM speed (you don't say how many sticks you have, so there's also a chance your memory configuration is bad if you have 4x64GB sticks).\n\nEpyc Rome and Milan have ~26GB/s bandwidth between each CCD and the IO die. To get maximum memory bandwidth, you need a CPU with 8 CCDs. Given the compute, 32 cores might not cut it, so you need either 48 or 64 cores to crunch those numbers. The IO die has 8 memory channels, each running at a maximum speed of 3200MT/s. You're running at 2133, which would be bad, if it wasn't for your CPU having 2 CCDs only cutting it's effective bandwidth by four.\n\nKnow thy hardware before buying parts and putting a system together. It's a bit of a moot point now that you have it, and with current RAM prices you'll probably need a kidney to upgrade to 3200 memory. Though you should try to overclock it to 2400 or even 2666 if 2400 works.1",
          "created_at": "2026-01-08T22:05:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhgj3c",
          "author": "MikeRoz",
          "content": "Can you do any better on the RAM speed? I can beat this using 2 GPUs and all the experts in system memory (also using ik_llama.cpp, IQ5_K quant). Getting ~10 tps generation, ~83 tps prompt processing, 4k or so into context out of 64k. Main difference between us is I'm running DDR5-6000 and you're running DDR4-2133. Another is that I'm not bothering with anything but the first layers on GPU - this allows me to allocate 64k cache yet use only 2 GPUs.",
          "created_at": "2026-01-08T22:17:37Z",
          "was_summarised": false
        },
        {
          "id": "nyhi65g",
          "author": "Egoz3ntrum",
          "content": "That dockerfile will be useful, thanks.",
          "created_at": "2026-01-08T22:25:09Z",
          "was_summarised": false
        },
        {
          "id": "nyhm5j5",
          "author": "FullOf_Bad_Ideas",
          "content": "try 2.10bpw exl3 quant\n\nhttps://huggingface.co/mratsim/GLM-4.7-EXL3/tree/2.10bpw-tuned\n\nIt probably won't be smart, but it will be stupid much faster since it will be all be in VRAM\n\nwith llama cpp and IQ3_XSS GLM 4.6/4.7 I had about 3.5 t/s generation speed. 3200 DDR4 128GB and 2x rtx 3090 ti 24gb.",
          "created_at": "2026-01-08T22:43:41Z",
          "was_summarised": false
        },
        {
          "id": "nyhmjkn",
          "author": "Leflakk",
          "content": "CPU \u0026amp; RAM bottlebeck and not enough VRAM. Why donâ€™t you rather consider minimax m2.1 with lower quantz?",
          "created_at": "2026-01-08T22:45:32Z",
          "was_summarised": false
        },
        {
          "id": "nyhqi8u",
          "author": "segmond",
          "content": "I'm getting 6tk/sec on quad 3090 with regular llama.cpp",
          "created_at": "2026-01-08T23:04:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhrh0k",
          "author": "ScoreUnique",
          "content": "Quick tip- explore -ot flag, that thing shows big numbers on ik llama CPP.",
          "created_at": "2026-01-08T23:09:39Z",
          "was_summarised": false
        },
        {
          "id": "nyhwbzl",
          "author": "chub0ka",
          "content": "Havent yet tried graph but 360gb quant on kimi runs 10t/s generation and 2 3090 is enough. Using 8 gpus doesnt help",
          "created_at": "2026-01-08T23:34:21Z",
          "was_summarised": false
        },
        {
          "id": "nyigyvn",
          "author": "ortegaalfredo",
          "content": "Prompt processing is super slow. As PP is mostly CPU-bound, I believe you might be hitting some kind of power-limitation, or power saving.",
          "created_at": "2026-01-09T01:21:30Z",
          "was_summarised": false
        },
        {
          "id": "nyih1e6",
          "author": "ortegaalfredo",
          "content": "Prompt processing is super slow. As PP is mostly CPU-bound, I believe you might be hitting some kind of power-limitation, or power saving is activating.",
          "created_at": "2026-01-09T01:21:53Z",
          "was_summarised": false
        },
        {
          "id": "nyip2mi",
          "author": "southern_gio",
          "content": "Dude thanks for shearing",
          "created_at": "2026-01-09T02:04:47Z",
          "was_summarised": false
        },
        {
          "id": "nyhdlk6",
          "author": "cantgetthistowork",
          "content": "Very very slow. My Q4 K2-Thinking on 2x3090 runs faster than this",
          "created_at": "2026-01-08T22:04:31Z",
          "was_summarised": false
        },
        {
          "id": "nyhoo4p",
          "author": "leonbollerup",
          "content": "Why not run gpt-oss-120b .. should rub nicely on that setup ?",
          "created_at": "2026-01-08T22:55:43Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://localhost:8080/v1/chat/completions",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/ikawrakow/ik_llama.cpp.git",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:36:14.630483429Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://localhost:8080/v1/chat/completions: jina: status 451: {\"data\":null,\"path\":\"url\",\"code\":451,\"name\":\"SecurityCompromiseError\",\"status\":45102,\"message\":\"Suspicious action: Request to localhost or non-public IP: localhost\",\"readableMessage\":\"SecurityCompromiseError: Suspicious action: Request to localhost or non-public IP: localhost\"}: retry failed: jina request failed: 451 Unavailable For Legal Reasons",
          "occurred_at": "2026-01-09T02:35:56.743389336Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/ikawrakow/ik_llama.cpp.git: jina: status 409: {\"data\":null,\"code\":409,\"name\":\"BudgetExceededError\",\"status\":40904,\"message\":\"Token budget (15000) exceeded, intended charge amount 46963\",\"readableMessage\":\"BudgetExceededError: Token budget (15000) exceeded, intended charge amount 46963\"}: retry failed: jina request failed: 409 Conflict",
          "occurred_at": "2026-01-09T02:36:14.630481457Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q71sbe",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "title": "Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)",
      "content": "Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything\n\nhttps://preview.redd.it/shr3e0liv1cg1.png?width=2560\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079\n\nGithub: [https://github.com/MVPandey/DTS](https://github.com/MVPandey/DTS)\n\nMotivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:\n\n(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)\n\n1. Generates N diverse strategies\n2. Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)\n3. Rolls out full multi-turn conversations down each branch\n4. Has 3 independent LLM judges score each trajectory, takes the median\n5. Prunes branches below threshold, backpropagates scores\n6. Repeats for however many rounds you configure\n\nhttps://preview.redd.it/zkii0idvv1cg1.png?width=762\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4\n\nThree judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.\n\nMain additions over CAE:\n\n* user intent forking (strategies get stress-tested against different personas)\n* deep research integration via GPT-Researcher for domain context\n* proper visualization with conversation playback\n\nOnly supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls\n\nIt's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.\n\n\\--\n\nBTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.",
      "author": "ManavTheWorld",
      "created_at": "2026-01-08T04:08:39Z",
      "comments": [
        {
          "id": "nycvkhb",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T07:05:10Z",
          "was_summarised": false
        },
        {
          "id": "nyc6x5o",
          "author": "TheGrossVolcano",
          "content": "This is actually pretty clever - using beam search instead of pure MCTS makes way more sense for dialogue since you don't want the exploration to go completely off the rails\n\n  \nThe user intent forking is a nice touch, most people forget that the same strategy can totally bomb depending on who you're talking to",
          "created_at": "2026-01-08T04:11:18Z",
          "was_summarised": false
        },
        {
          "id": "nyc8h0p",
          "author": "charlesrwest0",
          "content": "Weird thought... Could you use this to optimize an rp response?",
          "created_at": "2026-01-08T04:20:41Z",
          "was_summarised": false
        },
        {
          "id": "nycytz1",
          "author": "harlekinrains",
          "content": "firecrawls pricing guides you into a monthly subscription and is prohibitively costly for what it provides (140 USD/year). without knowing the intricacies, if you also could implement alternatives, that would be swell.\n\nThis github already collected a bunch of search providers, maybe it helps: https://github.com/rikkahub/rikkahub/tree/ffa2a0c4796d835454c7a9a0469f897ff1ffdb63/search/src/main/java/me/rerere/search",
          "created_at": "2026-01-08T07:33:08Z",
          "was_summarised": false
        },
        {
          "id": "nycqqjr",
          "author": "ItilityMSP",
          "content": "Nice work, lots of potential here to improve many ai chat agents. â­",
          "created_at": "2026-01-08T06:26:17Z",
          "was_summarised": false
        },
        {
          "id": "nycz02m",
          "author": "Nyghtbynger",
          "content": "That's cool, I was looking to optimize my prompts I think it has a good future in prompt fitting (for a specific dataset or customer)",
          "created_at": "2026-01-08T07:34:38Z",
          "was_summarised": false
        },
        {
          "id": "nydbiy1",
          "author": "Much-Researcher6135",
          "content": "This is new to me, pretty interesting. Anyone here used tech like this for interview prep before? Or is it all just chatbot strategy discovery for you guys?",
          "created_at": "2026-01-08T09:27:49Z",
          "was_summarised": false
        },
        {
          "id": "nydgto6",
          "author": "IrisColt",
          "content": "Thanks!",
          "created_at": "2026-01-08T10:15:52Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/MVPandey/DTS",
          "was_fetched": true,
          "page": "Title: GitHub - MVPandey/DTS: ðŸŒ³ MCTS-inspired parallel beam search for conversation optimization. Explore multiple dialogue strategies simultaneously, stress-test against diverse user personas, score with multi-judge consensus, and discover winning conversation paths that single-shot LLMs miss.\n\nURL Source: https://github.com/MVPandey/DTS\n\nMarkdown Content:\nDialogue Tree Search (DTS)\n--------------------------\n\n[](https://github.com/MVPandey/DTS#dialogue-tree-search-dts)\n[](https://www.python.org/downloads/)[](https://opensource.org/licenses/Apache-2.0)[](https://github.com/astral-sh/ruff)[](https://github.com/astral-sh/uv)\n\n**An LLM-powered tree search engine for multi-turn conversation optimization.**\n\nDTS explores conversation strategies in parallel, simulates diverse user reactions, scores trajectories with multi-judge consensus, and prunes underperformersâ€”finding optimal dialogue paths that single-shot LLM responses miss.\n\n[](https://github.com/MVPandey/DTS/blob/main/media/comprehensive_ss.png)_Real-time tree exploration with strategy scoring, conversation playback, and detailed evaluation breakdowns_\n\n* * *\n\nTable of Contents\n-----------------\n\n[](https://github.com/MVPandey/DTS#table-of-contents)\n*   [Why DTS?](https://github.com/MVPandey/DTS#why-dts)\n*   [How It Works](https://github.com/MVPandey/DTS#how-it-works)\n    *   [The Algorithm](https://github.com/MVPandey/DTS#the-algorithm)\n    *   [Parallel Beam Search](https://github.com/MVPandey/DTS#parallel-beam-search)\n    *   [User Intent Forking](https://github.com/MVPandey/DTS#user-intent-forking)\n    *   [Multi-Judge Scoring](https://github.com/MVPandey/DTS#multi-judge-scoring)\n    *   [Scoring Modes](https://github.com/MVPandey/DTS#scoring-modes-comparative-vs-absolute)\n\n*   [System Architecture](https://github.com/MVPandey/DTS#system-architecture)\n*   [Prerequisites \u0026 API Keys](https://github.com/MVPandey/DTS#prerequisites--api-keys)\n*   [Installation](https://github.com/MVPandey/DTS#installation)\n*   [Quick Start](https://github.com/MVPandey/DTS#quick-start)\n*   [Configuration](https://github.com/MVPandey/DTS#configuration)\n*   [Deep Research Integration](https://github.com/MVPandey/DTS#deep-research-integration)\n*   [API Reference](https://github.com/MVPandey/DTS#api-reference)\n*   [Frontend Visualizer](https://github.com/MVPandey/DTS#frontend-visualizer)\n*   [Project Structure](https://github.com/MVPandey/DTS#project-structure)\n*   [Token Usage \u0026 Cost Management](https://github.com/MVPandey/DTS#token-usage--cost-management)\n*   [Troubleshooting](https://github.com/MVPandey/DTS#troubleshooting)\n*   [License](https://github.com/MVPandey/DTS#license)\n\n* * *\n\nWhy DTS?\n--------\n\n[](https://github.com/MVPandey/DTS#why-dts)\nStandard LLMs generate responses one turn at a time, optimizing locally without considering long-term conversation outcomes. This leads to:\n\n*   **Myopic responses** that sound good but lead to dead ends\n*   **Single-path thinking** that misses better strategic approaches\n*   **Fragile strategies** that fail when users respond unexpectedly\n\nDTS solves this by treating conversation as a **tree search problem**:\n\n1.   **Explore multiple strategies** in parallel (not just one response)\n2.   **Simulate diverse user reactions** (skeptical, enthusiastic, confused, etc.)\n3.   **Score complete trajectories** against your goal\n4.   **Prune bad paths early** to focus computation on promising directions\n\nThe result: dialogue strategies that are **robust**, **goal-oriented**, and **tested against varied user behaviors**.\n\n* * *\n\nHow It Works\n------------\n\n[](https://github.com/MVPandey/DTS#how-it-works)\n### The Algorithm\n\n[](https://github.com/MVPandey/DTS#the-algorithm)\nDTS implements a parallel beam search with the following loop:\n\n```\nFor each round:\n    1. Generate N diverse conversation strategies\n    2. For each strategy, simulate K user intent variants\n    3. Roll out multi-turn conversations for each branch\n    4. Score all trajectories with 3 independent judges\n    5. Prune branches below threshold (median vote)\n    6. Backpropagate scores up the tree\n    7. Repeat with surviving branches\n```\n\n### Parallel Beam Search\n\n[](https://github.com/MVPandey/DTS#parallel-beam-search)\nUnlike traditional single-path generation, DTS maintains multiple conversation branches simultaneously:\n\nLoading\n\ngraph TD\n    subgraph Round 1\n        Root[User Message] --\u003e S1[Strategy: Empathetic]\n        Root --\u003e S2[Strategy: Direct]\n        Root --\u003e S3[Strategy: Socratic]\n    end\n\n    subgraph Round 2\n        S1 --\u003e S1I1[Intent: Cooperative]\n        S1 --\u003e S1I2[Intent: Skeptical]\n        S2 --\u003e S2I1[Intent: Cooperative]\n        S2 --\u003e S2I2[Intent: Resistant]\n    end\n\n    subgraph Scoring\n        S1I1 --\u003e J1((Judge 1))\n        S1I1 --\u003e J2((Judge 2))\n        S1I1 --\u003e J3((Judge 3))\n        J1 \u0026 J2 \u0026 J3 --\u003e M{Median Vote}\n    end\n\n    M --\u003e|Score â‰¥ 6.5| Keep[Keep Branch]\n    M --\u003e|Score \u003c 6.5| Prune[Prune Branch]\n\n[](https://github.com/MVPandey/DTS/blob/main/media/just_tree_ss.png)_Branches are color-coded by score: green (passing), yellow (borderline), red (pruned)_\n\n**Key parameters:**\n\n*   `init_branches`: Number of initial strategies (default: 6)\n*   `turns_per_branch`: Conversation depth per branch (default: 5)\n*   `max_concurrency`: Parallel LLM calls (default: 16)\n\n### User Intent Forking (Optional)\n\n[](https://github.com/MVPandey/DTS#user-intent-forking-optional)\nMost dialogue systems assume a single \"happy path\" user response. DTS can stress-test strategies against **diverse user personas** when enabled.\n\n**User Variability Mode:**\n\n*   `user_variability=False` (default): Uses a fixed \"healthily critical + engaged\" persona for consistent, realistic testing\n*   `user_variability=True`: Generates diverse user intents for robustness testing across user types\n\nWhen variability is enabled, possible user personas include:\n\n| Emotional Tone | Cognitive Stance | Example Behavior |\n| --- | --- | --- |\n| `engaged` | `accepting` | Cooperative, follows suggestions |\n| `skeptical` | `questioning` | Asks for evidence, challenges claims |\n| `confused` | `exploring` | Needs clarification, misunderstands |\n| `resistant` | `challenging` | Pushes back, disagrees |\n| `anxious` | `withdrawing` | Hesitant, wants to end conversation |\n\nEach strategy can fork into K intent variants (configurable via `user_intents_per_branch`), creating branches that prove robustness across user types.\n\n**UserIntent structure:**\n\nUserIntent(\n    id=\"skeptical_questioner\",\n    label=\"Skeptical Questioner\",\n    description=\"Demands evidence before accepting claims\",\n    emotional_tone=\"skeptical\",      # How user feels\n    cognitive_stance=\"questioning\",  # How user thinks\n)\n\n### Multi-Judge Scoring\n\n[](https://github.com/MVPandey/DTS#multi-judge-scoring)\nEach trajectory is evaluated by **3 independent LLM judges**. Scores are aggregated via **median voting** (robust to outlier judges):\n\n```\nJudge 1: 7.2  â”€â”\nJudge 2: 6.8  â”€â”¼â”€â–º Median: 7.2  â”€â–º Pass (â‰¥ 6.5)\nJudge 3: 8.1  â”€â”˜\n```\n\n**Why 3 judges?**\n\n*   Single judge = high variance, easily gamed\n*   Median of 3 = robust to one outlier\n*   Majority vote determines pass/fail (2 of 3 must pass)\n\n**Scoring criteria** (each 0-1, summed to 0-10):\n\n*   Goal achievement\n*   User need addressed\n*   Forward progress\n*   Clarity \u0026 coherence\n*   Appropriate tone\n*   Information accuracy\n*   Handling objections\n*   Building rapport\n*   Conversation flow\n*   Strategic effectiveness\n\n**High-Scoring Branch (9.2/10)**\n\n[](https://github.com/MVPandey/DTS/blob/main/media/selected_branch_ss.png)**Pruned Branch (4.1/10)**\n\n[](https://github.com/MVPandey/DTS/blob/main/media/pruned_branch_ss.png)\n\n_Left: A successful trajectory with detailed strengths. Right: A pruned branch showing weaknesses and why it failed._\n\n### Scoring Modes: Comparative vs Absolute\n\n[](https://github.com/MVPandey/DTS#scoring-modes-comparative-vs-absolute)\n[](https://github.com/MVPandey/DTS/blob/main/media/scoring_ss.png)\n\nDTS supports two evaluation modes:\n\n| Mode | How It Works | Best For |\n| --- | --- | --- |\n| **Comparative** | Sibling branches force-ranked against each other | Sharp discrimination, finding the single best path |\n| **Absolute** | Each branch scored independently (0-10) | Early pruning, filtering obviously bad paths |\n\n**Comparative mode** (default):\n\n```\nInput: [Strategy A, Strategy B, Strategy C] (siblings)\nOutput: A=7.5, B=6.0, C=4.5 (forced ranking with 1.5-point gaps)\n```\n\n**Absolute mode**:\n\n```\nInput: Strategy A (evaluated alone)\nOutput: 3 judges â†’ [7.2, 6.8, 8.1] â†’ Median: 7.2\n```\n\nUse `scoring_mode=\"comparative\"` when you need the best single answer. Use `scoring_mode=\"absolute\"` when filtering many branches quickly.\n\n* * *\n\nSystem Architecture\n-------------------\n\n[](https://github.com/MVPandey/DTS#system-architecture)\n\nsequenceDiagram\n    participant User\n    participant FE as Frontend (HTML/JS)\n    participant API as FastAPI WebSocket\n    participant ENG as DTS Engine\n    participant LLM as OpenRouter/OpenAI\n    participant RES as Firecrawl + Tavily\n\n    User-\u003e\u003eFE: Configure \u0026 Start Search\n    FE-\u003e\u003eAPI: WebSocket Connect\n    API-\u003e\u003eENG: Initialize DTSEngine\n\n    opt Deep Research Enabled\n        ENG-\u003e\u003eRES: Research Query\n        RES--\u003e\u003eENG: Domain Context\n    end\n\n    loop For Each Round\n        ENG-\u003e\u003eLLM: Generate Strategies\n        LLM--\u003e\u003eENG: N Strategies\n\n        loop For Each Branch\n            ENG-\u003e\u003eLLM: Generate User Intents\n            ENG-\u003e\u003eLLM: Simulate Conversation\n            ENG-\u003e\u003eLLM: Judge Trajectory (3x)\n        end\n\n        ENG-\u003e\u003eAPI: Emit Events (node_added, scored, pruned)\n        API--\u003e\u003eFE: Stream Updates\n        FE-\u003e\u003eUser: Update Visualization\n    end\n\n    ENG-\u003e\u003eAPI: Complete with Best Path\n    FE-\u003e\u003eUser: Show Results\n\n### Component Overview\n\n[](https://github.com/MVPandey/DTS#component-overview)\n| Component | Location | Purpose |\n| --- | --- | --- |\n| **DTSEngine** | `backend/core/dts/engine.py` | Main orchestrator, runs expandâ†’scoreâ†’prune loop |\n| **StrategyGenerator** | `backend/core/dts/components/generator.py` | Creates strategies and user intents |\n| **ConversationSimulator** | `backend/core/dts/components/simulator.py` | Runs multi-turn dialogue rollouts |\n| **TrajectoryEvaluator** | `backend/core/dts/components/evaluator.py` | Multi-judge scoring with median aggregation |\n| **DeepResearcher** | `backend/core/dts/components/researcher.py` | GPT-Researcher integration for context |\n| **DialogueTree** | `backend/core/dts/tree.py` | Tree data structure with backpropagation |\n| **LLM Client** | `backend/llm/client.py` | Provider-agnostic OpenAI-compatible wrapper |\n\n* * *\n\nPrerequisites \u0026 API Keys\n------------------------\n\n[](https://github.com/MVPandey/DTS#prerequisites--api-keys)\n### Required Credentials\n\n[](https://github.com/MVPandey/DTS#required-credentials)\n| Service | Environment Variable | Required | Purpose |\n| --- | --- | --- | --- |\n| **LLM Provider** | `OPENROUTER_API_KEY` | **Yes** | Strategy generation, simulation, and judging |\n| **Web Scraping** | `FIRECRAWL_API_KEY` | For Deep Research | Scrapes web pages for research context |\n| **Web Search** | `TAVILY_API_KEY` | For Deep Research | Searches the web for relevant sources |\n\n### Getting API Keys\n\n[](https://github.com/MVPandey/DTS#getting-api-keys)\n1.   **OpenRouter** (recommended): [openrouter.ai/keys](https://openrouter.ai/keys)\n\n    *   Works with 100+ models (GPT-4, Claude, Gemini, open-source)\n    *   Pay-per-token, no subscriptions\n    *   Set `OPENAI_BASE_URL=https://openrouter.ai/api/v1`\n\n2.   **Firecrawl**: [firecrawl.dev](https://firecrawl.dev/)\n\n    *   Required for `deep_research=True`\n    *   Handles JavaScript-rendered pages, anti-bot bypass\n\n3.   **Tavily**: [tavily.com](https://tavily.com/)\n\n    *   Required for `deep_research=True`\n    *   AI-optimized web search API\n\n\u003e **Note:** Deep Research features require both Firecrawl and Tavily keys. Without them, set `deep_research=False` in your configuration.\n\n### Optional Configuration\n\n[](https://github.com/MVPandey/DTS#optional-configuration)\n| Variable | Default | Description |\n| --- | --- | --- |\n| `OPENAI_BASE_URL` | `https://openrouter.ai/api/v1` | LLM API endpoint |\n| `LLM_NAME` | `minimax/minimax-m2.1` | Default model for all phases |\n| `FAST_LLM` | `openrouter:minimax/minimax-m2.1` | Fast model for research |\n| `SMART_LLM` | `openrouter:minimax/minimax-m2.1` | Smart model for complex tasks |\n| `STRATEGIC_LLM` | `openrouter:minimax/minimax-m2.1` | Strategic reasoning model |\n| `LLM_TIMEOUT` | `120` | Request timeout in seconds |\n| `LLM_MAX_RETRIES` | `2` | Retry attempts on failure |\n| `MAX_CONCURRENCY` | `16` | Parallel LLM call limit |\n\n\u003e **Note:** The default model `minimax/minimax-m2.1` is chosen for its excellent price/performance ratio. You can use any OpenRouter-compatible model for any task by overriding the model parameters in your configuration.\n\n* * *\n\nInstallation\n------------\n\n[](https://github.com/MVPandey/DTS#installation)\n**Requires Python 3.11+**\n\n# Clone the repository\ngit clone https://github.com/MVPandey/DTS.git\ncd DTS\n\n# Install uv (fast Python package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install dependencies\nuv venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\nuv pip install -e .\n\n### Configure Environment\n\n[](https://github.com/MVPandey/DTS#configure-environment)\nCreate a `.env` file in the project root (see `.env.example`):\n\n# Required - API Keys\nOPENROUTER_API_KEY=sk-or-v1-your-openrouter-key\nTAVILY_API_KEY=tvly-your-tavily-key\nFIRECRAWL_API_KEY=fc-your-firecrawl-key\n\n# LLM models (minimax-m2.1 recommended for price/performance)\nFAST_LLM=openrouter:minimax/minimax-m2.1\nSMART_LLM=openrouter:minimax/minimax-m2.1\nSTRATEGIC_LLM=openrouter:minimax/minimax-m2.1\nSMART_TOKEN_LIMIT=32000\n\n# Deep research parameters\nDEEP_RESEARCH_BREADTH=3\nDEEP_RESEARCH_DEPTH=2\nDEEP_RESEARCH_CONCURRENCY=4\n\n# Report comprehensiveness (higher = more detailed)\nTOTAL_WORDS=12000\nMAX_SUBTOPICS=8\nMAX_ITERATIONS=5\nMAX_SEARCH_RESULTS=10\nREPORT_FORMAT=markdown\n\n\u003e **Model flexibility:** You can use any model available on OpenRouter for any task. The default `minimax/minimax-m2.1` offers an excellent balance of cost and capability, but feel free to swap in `anthropic/claude-3-opus`, `openai/gpt-4o`, or any other model for specific phases.\n\n* * *\n\nQuick Start\n-----------\n\n[](https://github.com/MVPandey/DTS#quick-start)\n### Option 1: Using Start Scripts (Recommended)\n\n[](https://github.com/MVPandey/DTS#option-1-using-start-scripts-recommended)\nThe easiest way to start the server:\n\n**Unix/macOS/Linux:**\n\n# Start with Docker\n./scripts/start_server.sh\n\n# Start in development mode (hot reload)\n./scripts/start_server.sh --dev\n\n# Start without Docker (local Python)\n./scripts/start_server.sh --local\n\n# Stop the server\n./scripts/start_server.sh --down\n\n**Windows:**\n\nREM Start with Docker\nscripts\\start_server.bat\n\nREM Start in development mode (hot reload)\nscripts\\start_server.bat --dev\n\nREM Start without Docker (local Python)\nscripts\\start_server.bat --local\n\nREM Stop the server\nscripts\\start_server.bat --down\n\n### Option 2: Using Docker Compose Directly\n\n[](https://github.com/MVPandey/DTS#option-2-using-docker-compose-directly)\n\n# Start the server (production)\ndocker-compose up -d dts-server\n\n# Start in development mode with hot reload\ndocker-compose --profile dev up dts-server-dev\n\n# View logs\ndocker-compose logs -f\n\n# Stop and remove containers\ndocker-compose down\n\n# Rebuild after code changes\ndocker-compose up -d --build dts-server\n\n### Option 3: Manual Uvicorn Start\n\n[](https://github.com/MVPandey/DTS#option-3-manual-uvicorn-start)\n\n# Activate virtual environment\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n\n# Set PYTHONPATH\nexport PYTHONPATH=$(pwd)  # Windows: set PYTHONPATH=%cd%\n\n# Start server\nuvicorn backend.api.server:app --host localhost --port 8000 --reload --log-level info\n\n### Option 4: VSCode Debugger\n\n[](https://github.com/MVPandey/DTS#option-4-vscode-debugger)\nThe project includes VSCode launch configurations for debugging:\n\n1.   Open the project in VSCode\n2.   Go to **Run and Debug** (Ctrl+Shift+D / Cmd+Shift+D)\n3.   Select a configuration from the dropdown:\n\n| Configuration | Description |\n| --- | --- |\n| **Debug DTS Server** | Start the API server with debugger attached |\n| **Debug Current Python File** | Debug the currently open file |\n| **Attach to Remote Debugpy** | Attach to a running debugpy server (port 5678) |\n\n1.   Press F5 or click the green play button\n\n**VSCode Launch Configuration** (`.vscode/launch.json`):\n\n{\n  \"name\": \"Debug DTS Server\",\n  \"type\": \"debugpy\",\n  \"request\": \"launch\",\n  \"module\": \"uvicorn\",\n  \"args\": [\n    \"backend.api.server:app\",\n    \"--host\", \"localhost\",\n    \"--port\", \"8000\",\n    \"--reload\",\n    \"--log-level\", \"info\"\n  ],\n  \"env\": { \"PYTHONPATH\": \"${workspaceFolder}\" },\n  \"envFile\": \"${workspaceFolder}/.env\"\n}\n\n### Option 5: Python Script (Headless)\n\n[](https://github.com/MVPandey/DTS#option-5-python-script-headless)\nFor programmatic use without the web interface:\n\nimport asyncio\nfrom backend.core.dts import DTSConfig, DTSEngine\nfrom backend.llm.client import LLM\nfrom backend.utils.config import config\n\nasync def main():\n    # Initialize LLM client (uses minimax-m2.1 by default for best price/performance)\n    llm = LLM(\n        api_key=config.openrouter_api_key,\n        base_url=config.openai_base_url,\n        model=\"minimax/minimax-m2.1\",  # Or any OpenRouter model\n    )\n\n    # Configure the search\n    dts_config = DTSConfig(\n        goal=\"Negotiate a 15% discount on enterprise software\",\n        first_message=\"Hi, I'd like to discuss our renewal pricing.\",\n        init_branches=6,           # 6 initial strategies\n        turns_per_branch=5,        # 5-turn conversations\n        user_intents_per_branch=3, # Fork into 3 user persona variants\n        user_variability=False,    # Use fixed \"healthily critical\" persona (default)\n        scoring_mode=\"comparative\", # Force-rank siblings\n        prune_threshold=6.5,       # Minimum score to survive\n        deep_research=True,        # Enable research context\n    )\n\n    # Run the search\n    engine = DTSEngine(llm=llm, config=dts_config)\n    result = await engine.run(rounds=2)\n\n    # Output results\n    print(f\"Best Score: {result.best_score:.1f}/10\")\n    print(f\"Branches Explored: {len(result.all_nodes)}\")\n    print(f\"Branches Pruned: {result.pruned_count}\")\n\n    # Save full results\n    result.save_json(\"output.json\")\n\nif  __name__  == \"__main__\":\n    asyncio.run(main())\n\nOr run the included example:\n\npython main.py\n\n### Accessing the Application\n\n[](https://github.com/MVPandey/DTS#accessing-the-application)\nOnce the server is running:\n\n| Resource | URL |\n| --- | --- |\n| **API** | [http://localhost:8000](http://localhost:8000/) |\n| **API Docs (Swagger)** | [http://localhost:8000/docs](http://localhost:8000/docs) |\n| **API Docs (ReDoc)** | [http://localhost:8000/redoc](http://localhost:8000/redoc) |\n| **Frontend** | Open `frontend/index.html` in your browser |\n\n* * *\n\nConfiguration\n-------------\n\n[](https://github.com/MVPandey/DTS#configuration)\n### DTSConfig Parameters\n\n[](https://github.com/MVPandey/DTS#dtsconfig-parameters)\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| `goal` | `str` | _required_ | What you want the conversation to achieve |\n| `first_message` | `str` | _required_ | Opening user message |\n| `init_branches` | `int` | `6` | Number of initial strategies to generate |\n| `turns_per_branch` | `int` | `5` | Conversation depth (assistant+user turns) |\n| `user_intents_per_branch` | `int` | `3` | User persona variants per strategy |\n| `user_variability` | `bool` | `False` | Generate diverse user intents. When False, uses fixed \"healthily critical + engaged\" persona |\n| `scoring_mode` | `str` | `\"comparative\"` | `\"comparative\"` or `\"absolute\"` |\n| `prune_threshold` | `float` | `6.5` | Minimum score to survive (0-10 scale) |\n| `keep_top_k` | `int | None` | `None` | Hard cap on survivors per round |\n| `min_survivors` | `int` | `1` | Minimum branches to keep (floor) |\n| `deep_research` | `bool` | `False` | Enable GPT-Researcher integration |\n| `max_concurrency` | `int` | `16` | Parallel LLM call limit |\n| `temperature` | `float` | `0.7` | Generation temperature |\n| `judge_temperature` | `float` | `0.3` | Judge temperature (lower = more consistent) |\n| `reasoning_enabled` | `bool` | `False` | Enable reasoning tokens for LLM calls (increases cost but may improve quality) |\n| `provider` | `str | None` | `None` | Provider preference for OpenRouter (e.g., \"Fireworks\") |\n\nDeep Research Integration\n-------------------------\n\n[](https://github.com/MVPandey/DTS#deep-research-integration)\nDTS integrates [GPT-Researcher](https://github.com/assafelovic/gpt-researcher) to gather domain context before generating strategies.\n\n### How It Works\n\n[](https://github.com/MVPandey/DTS#how-it-works-1)\n\ngraph LR\n    A[Goal + First Message] --\u003e B[Query Distillation]\n    B --\u003e C[Web Search via Tavily]\n    C --\u003e D[Page Scraping via Firecrawl]\n    D --\u003e E[Research Report]\n    E --\u003e F[Strategy Generation]\n    E --\u003e G[Judge Evaluation]\n\n1.   **Query Distillation**: LLM converts goal into focused research query\n2.   **Web Search**: Tavily finds relevant sources\n3.   **Scraping**: Firecrawl extracts content (handles JS, anti-bot)\n4.   **Report**: GPT-Researcher synthesizes findings\n5.   **Injection**: Report fed to strategy generator and judges\n\n### Configuration\n\n[](https://github.com/MVPandey/DTS#configuration-1)\n\nDTSConfig(\n    goal=\"Explain quantum computing to a 10-year-old\",\n    first_message=\"What's quantum computing?\",\n    deep_research=True,  # Enable research\n)\n\n### Caching\n\n[](https://github.com/MVPandey/DTS#caching)\nResearch results are cached by `SHA256(goal + first_message)` in `.cache/research/`. Subsequent runs with the same inputs skip the research phase.\n\n### Required API Keys\n\n[](https://github.com/MVPandey/DTS#required-api-keys)\n| Service | Purpose | Get Key |\n| --- | --- | --- |\n| **Firecrawl** | Web page scraping | [firecrawl.dev](https://firecrawl.dev/) |\n| **Tavily** | Web search | [tavily.com](https://tavily.com/) |\n\n\u003e **Cost Note:** Deep research adds external API costs beyond LLM tokens. Monitor usage during development.\n\n* * *\n\nAPI Reference\n-------------\n\n[](https://github.com/MVPandey/DTS#api-reference)\n### WebSocket Endpoint\n\n[](https://github.com/MVPandey/DTS#websocket-endpoint)\n**URL:**`ws://localhost:8000/ws`\n\n### Starting a Search\n\n[](https://github.com/MVPandey/DTS#starting-a-search)\n\n{\n  \"type\": \"start_search\",\n  \"config\": {\n    \"goal\": \"Your conversation goal\",\n    \"first_message\": \"Opening user message\",\n    \"init_branches\": 6,\n    \"turns_per_branch\": 5,\n    \"user_intents_per_branch\": 3,\n    \"scoring_mode\": \"comparative\",\n    \"prune_threshold\": 6.5,\n    \"rounds\": 2,\n    \"deep_research\": false\n  }\n}\n\n### Event Stream\n\n[](https://github.com/MVPandey/DTS#event-stream)\nThe server emits real-time events as the search progresses:\n\n| Event | Description | Data |\n| --- | --- | --- |\n| `search_started` | Search initialized | `{goal, config}` |\n| `phase` | Lifecycle update | `{phase, message}` |\n| `strategy_generated` | New strategy created | `{tagline, description}` |\n| `intent_generated` | User intent created | `{label, emotional_tone}` |\n| `research_log` | Deep research progress | `{message}` |\n| `round_started` | Round begins | `{round, total_rounds}` |\n| `node_added` | Branch created | `{id, strategy, intent}` |\n| `node_updated` | Branch scored | `{id, score, passed}` |\n| `nodes_pruned` | Branches removed | `{ids, reasons}` |\n| `token_update` | Token usage snapshot | `{totals}` |\n| `complete` | Search finished | `{best_node, all_nodes}` |\n| `error` | Error occurred | `{message}` |\n\n### REST Endpoints\n\n[](https://github.com/MVPandey/DTS#rest-endpoints)\nFastAPI auto-generates OpenAPI docs at `http://localhost:8000/docs`.\n\n* * *\n\nFrontend Visualizer\n-------------------\n\n[](https://github.com/MVPandey/DTS#frontend-visualizer)\nThe included frontend (`frontend/index.html`) provides real-time visualization:\n\n[](https://github.com/MVPandey/DTS/blob/main/media/parameters_ss.png)_Configuration panel with basic parameters, deep research toggle, and user variability settings_\n\n### Features\n\n[](https://github.com/MVPandey/DTS#features)\n*   **Configuration Panel**: Set goal, branches, turns, rounds, and model settings\n*   **Deep Research Toggle**: Enable/disable GPT-Researcher integration\n*   **Reasoning Mode**: Auto-detect or manually set reasoning effort\n*   **User Variability Toggle**: Switch between fixed \"healthily critical\" persona (default) or diverse user personas for robustness testing\n*   **Live Progress**: Watch strategies generate and branches expand\n*   **Branch Browser**: Explore all trajectories with full transcripts\n*   **Score Details**: See individual judge scores and critiques\n*   **Token Tracking**: Monitor costs by phase and model\n*   **Export**: Download results as JSON\n\n### Running the Frontend\n\n[](https://github.com/MVPandey/DTS#running-the-frontend)\n1.   Start the API server:\n\nuvicorn backend.api.server:app --port 8000 \n2.   Open `frontend/index.html` in a browser\n\n3.   Configure your search and click \"Start Search\"\n\n* * *\n\nProject Structure\n-----------------\n\n[](https://github.com/MVPandey/DTS#project-structure)\n\n```\nDTS/\nâ”œâ”€â”€ backend/\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â”œâ”€â”€ server.py          # FastAPI WebSocket server\nâ”‚   â”‚   â””â”€â”€ schemas.py         # Pydantic request/response models\nâ”‚   â”œâ”€â”€ core/\nâ”‚   â”‚   â”œâ”€â”€ dts/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ engine.py      # Main DTSEngine orchestrator\nâ”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # DTSConfig dataclass\nâ”‚   â”‚   â”‚   â”œâ”€â”€ types.py       # Core data models\nâ”‚   â”‚   â”‚   â”œâ”€â”€ tree.py        # DialogueTree structure\nâ”‚   â”‚   â”‚   â”œâ”€â”€ aggregator.py  # Median vote aggregation\nâ”‚   â”‚   â”‚   â”œâ”€â”€ retry.py       # Shared retry logic\nâ”‚   â”‚   â”‚   â””â”€â”€ components/\nâ”‚   â”‚   â”‚       â”œâ”€â”€ generator.py   # Strategy \u0026 intent generation\nâ”‚   â”‚   â”‚       â”œâ”€â”€ simulator.py   # Conversation rollouts\nâ”‚   â”‚   â”‚       â”œâ”€â”€ evaluator.py   # Multi-judge scoring\nâ”‚   â”‚   â”‚       â””â”€â”€ researcher.py  # GPT-Researcher integration\nâ”‚   â”‚   â””â”€â”€ prompts.py         # All prompt templates\nâ”‚   â”œâ”€â”€ llm/\nâ”‚   â”‚   â”œâ”€â”€ client.py          # OpenAI-compatible LLM client\nâ”‚   â”‚   â”œâ”€â”€ types.py           # Message, Completion, Usage\nâ”‚   â”‚   â”œâ”€â”€ errors.py          # Custom exception types\nâ”‚   â”‚   â””â”€â”€ tools.py           # Tool calling support\nâ”‚   â”œâ”€â”€ services/\nâ”‚   â”‚   â””â”€â”€ search_service.py  # API service layer\nâ”‚   â””â”€â”€ utils/\nâ”‚       â””â”€â”€ config.py          # Pydantic settings from .env\nâ”œâ”€â”€ frontend/\nâ”‚   â”œâ”€â”€ index.html             # Single-page visualizer\nâ”‚   â””â”€â”€ app.js                 # WebSocket client \u0026 UI logic\nâ”œâ”€â”€ scripts/\nâ”‚   â”œâ”€â”€ start_server.sh        # Unix/macOS start script\nâ”‚   â””â”€â”€ start_server.bat       # Windows start script\nâ”œâ”€â”€ gpt-researcher/            # GPT-Researcher submodule\nâ”œâ”€â”€ .vscode/\nâ”‚   â””â”€â”€ launch.json            # VSCode debug configurations\nâ”œâ”€â”€ Dockerfile                 # Container image definition\nâ”œâ”€â”€ docker-compose.yml         # Multi-container orchestration\nâ”œâ”€â”€ main.py                    # Example script\nâ”œâ”€â”€ pyproject.toml             # Project metadata \u0026 dependencies\nâ”œâ”€â”€ CLAUDE.md                  # Developer instructions\nâ””â”€â”€ README.md                  # This file\n```\n\n### Key Data Models\n\n[](https://github.com/MVPandey/DTS#key-data-models)\n\n# Strategy for conversation approach\nStrategy(tagline=\"Empathetic Listener\", description=\"Validate feelings first...\")\n\n# User persona for intent forking\nUserIntent(\n    id=\"skeptic\",\n    label=\"Skeptical Questioner\",\n    emotional_tone=\"skeptical\",\n    cognitive_stance=\"questioning\",\n)\n\n# Tree node with conversation state\nDialogueNode(\n    id=\"uuid\",\n    strategy=Strategy(...),\n    user_intent=UserIntent(...),\n    messages=[Message(role=\"user\", content=\"...\")],\n    stats=NodeStats(aggregated_score=7.2),\n)\n\n# Final result\nDTSRunResult(\n    best_node_id=\"uuid\",\n    best_score=8.1,\n    best_messages=[...],\n    all_nodes=[...],\n    token_usage={...},\n)\n\n* * *\n\nToken Usage \u0026 Cost Management\n-----------------------------\n\n[](https://github.com/MVPandey/DTS#token-usage--cost-management)\n### Understanding Costs\n\n[](https://github.com/MVPandey/DTS#understanding-costs)\nDTS is token-intensive due to parallel exploration. A typical run involves:\n\n```\nCost Formula â‰ˆ Branches Ã— Intents Ã— Turns Ã— (Generation + 3Ã—Judging)\n\nExample: 6 branches Ã— 3 intents Ã— 5 turns Ã— 4 calls = 360 LLM calls\n```\n\n### Token Breakdown by Phase\n\n[](https://github.com/MVPandey/DTS#token-breakdown-by-phase)\n| Phase | % of Tokens | Purpose |\n| --- | --- | --- |\n| Strategy Generation | ~10% | Creating initial approaches |\n| Intent Generation | ~5% | Generating user personas |\n| User Simulation | ~30% | Simulating user responses |\n| Assistant Simulation | ~25% | Generating assistant replies |\n| Judging | ~30% | 3 judges per trajectory |\n\n### Cost Optimization Strategies\n\n[](https://github.com/MVPandey/DTS#cost-optimization-strategies)\n1.   **Raise `prune_threshold`**: Aggressively cull bad branches (6.5 â†’ 7.0)\n2.   **Set `keep_top_k`**: Hard cap on survivors (e.g., `keep_top_k=3`)\n3.   **Lower `turns_per_branch`**: Shorter conversations (5 â†’ 3)\n4.   **Disable forking**: Set `user_intents_per_branch=1`\n5.   **Use fast models**: Cheaper models for simulation, expensive for judging\n6.   **Fewer rounds**: Start with 1 round, add more if needed\n\n### Monitoring Usage\n\n[](https://github.com/MVPandey/DTS#monitoring-usage)\nThe engine tracks tokens per phase and model:\n\nresult = await engine.run(rounds=2)\nprint(result.token_usage)\n# {\n# \"total_input_tokens\": 45000,\n# \"total_output_tokens\": 12000,\n# \"total_cost_usd\": 0.42,\n# \"by_phase\": {...},\n# \"by_model\": {...},\n# }\n\n* * *\n\nTroubleshooting\n---------------\n\n[](https://github.com/MVPandey/DTS#troubleshooting)\n### Common Errors\n\n[](https://github.com/MVPandey/DTS#common-errors)\n| Error | Cause | Solution |\n| --- | --- | --- |\n| `AuthenticationError` | Invalid API key | Check `OPENROUTER_API_KEY` in `.env` |\n| `RateLimitError` | Too many requests | Lower `max_concurrency`, add delays |\n| `ContextLengthError` | Conversation too long | Reduce `turns_per_branch` |\n| `ValueError: FIRECRAWL_API_KEY required` | Missing research key | Add key or set `deep_research=False` |\n| `JSONParseError` | LLM returned invalid JSON | Retry usually fixes; check model quality |\n| `ServerError (5xx)` | Provider issues | Automatic retry with backoff |\n\n### Debug Mode\n\n[](https://github.com/MVPandey/DTS#debug-mode)\nEnable verbose logging:\n\nDEBUG=true\nLOGGING_LEVEL=DEBUG\n\n### Testing Without Deep Research\n\n[](https://github.com/MVPandey/DTS#testing-without-deep-research)\nIf you don't have Firecrawl/Tavily keys:\n\nDTSConfig(\n    goal=\"...\",\n    first_message=\"...\",\n    deep_research=False,  # Disable research\n)\n\n* * *\n\nLicense\n-------\n\n[](https://github.com/MVPandey/DTS#license)\nApache License 2.0 â€” see [LICENSE](https://github.com/MVPandey/DTS/blob/main/LICENSE).\n\n* * *\n\nContributing\n------------\n\n[](https://github.com/MVPandey/DTS#contributing)\nContributions welcome! Please read [CONTRIBUTING.md](https://github.com/MVPandey/DTS/blob/main/CONTRIBUTING.md) before submitting PRs.\n\nAcknowledgments\n---------------\n\n[](https://github.com/MVPandey/DTS#acknowledgments)\n*   [GPT-Researcher](https://github.com/assafelovic/gpt-researcher) for deep research capabilities\n*   [OpenRouter](https://openrouter.ai/) for multi-model API access\n*   [Firecrawl](https://firecrawl.dev/) for web scraping\n*   [Tavily](https://tavily.com/) for AI-optimized search",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:36:37.079626956Z"
    },
    {
      "flow_id": "",
      "id": "1q7c0pd",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/",
      "title": "AI21 releases Jamba2 3B and Jamba2 Mini, built for grounding and instruction following",
      "content": "*Disclaimer: I work for AI21, creator of the Jamba model family.*\n\nWeâ€™re excited to announce the public release of Jamba2 3B and Jamba2 Mini.\n\nThe Jamba2 family aims to give enterprises cost-effective models that will integrate well into production agent stacks.\n\nThese models are designed for reliable instruction following and grounded outputs, working well over long documents and avoiding drifting once context becomes large.\n\nThey perform best for precise question answering over internal policies, technical manuals and knowledge bases, without the overhead of thinking tokens which can become costly.\n\n**Key performance data**\n\nJamba2 3B and Jamba2 Mini outperform peers due to their hybrid SSM-Transformer architecture and KV cache innovations:\n\n* Outpaces Ministral3 14B and Qwen3 30B A3B across FACTS, IFBench and IFEval.Â \n* Beats Ministral3 3B and Qwen3 4B on IFEval and IFBench, tying with Qwen3 4B as category leader on FACTS.\n* At context lengths of 100K, Jamba2 Mini delivers 2.7X greater throughput than Ministral3 14B and 1.4X greater throughout than Qwen3 30B A3B.\n* At context lengths of 100K, Jamba2 3B delivers 1.7X greater throughout than Ministral3 3B and 2.7X greater throughput than Qwen 3 14B.\n\nItâ€™s available today in AI21â€™s SaaS and from Hugging Face.\n\nHappy to answer questions or dig into benchmarks if people want more detail.\n\nBlog: [http://www.ai21.com/blog/introducing-jamba2](http://www.ai21.com/blog/introducing-jamba2)  \nHugging Face: [https://huggingface.co/collections/ai21labs/jamba2](https://huggingface.co/collections/ai21labs/jamba2)",
      "author": "zennaxxarion",
      "created_at": "2026-01-08T13:38:34Z",
      "comments": [
        {
          "id": "nye9nvl",
          "author": "StillResult4344",
          "content": "Nice to see more hybrid architectures hitting the scene, those throughput numbers at 100K context are pretty solid. How's the memory usage compared to pure transformer models at those longer contexts?",
          "created_at": "2026-01-08T13:40:39Z",
          "was_summarised": false
        },
        {
          "id": "nyfxuey",
          "author": "gofiend",
          "content": "Whatâ€™s the llama.cpp (infer) and unsloth (training) situation?",
          "created_at": "2026-01-08T18:17:46Z",
          "was_summarised": false
        },
        {
          "id": "nyf8u9c",
          "author": "lacerating_aura",
          "content": "Hi, thank you for the release. How well does it compare to other hybrid architecture models, like qwen3 next?",
          "created_at": "2026-01-08T16:29:13Z",
          "was_summarised": false
        },
        {
          "id": "nygqyrp",
          "author": "crantob",
          "content": "This appears to me to be important iterative engineering progress in linear attention implementations.\n\nSkÃ¥l",
          "created_at": "2026-01-08T20:25:01Z",
          "was_summarised": false
        },
        {
          "id": "nyhn4nu",
          "author": "danigoncalves",
          "content": "What would be the max context before degrading in quality?",
          "created_at": "2026-01-08T22:48:18Z",
          "was_summarised": false
        },
        {
          "id": "nyhzj0z",
          "author": "casual_butte_play",
          "content": "Cool models! Heads up, thereâ€™s a typo in your Tiny Models chart, where it refers to Qwen3 4B A3B. Probably just a typo in chart generation while changing 30B A3B -\u0026gt; 4B",
          "created_at": "2026-01-08T23:50:57Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://www.ai21.com/blog/introducing-jamba2",
          "was_fetched": true,
          "page": "Title: Introducing Jamba2: The open source model family for enterprise reliability and efficiency\n\nURL Source: http://www.ai21.com/blog/introducing-jamba2\n\nPublished Time: 2026-01-08T13:31:16+00:00\n\nMarkdown Content:\nToday, we are introducing Jamba2, an open source family of language models built for maximum reliability and steerability in the enterprise.\n\n[Built on our novel SSM-Transformer architecture](https://arxiv.org/pdf/2403.19887), and a category leader across grounding and instruction following benchmarks, Jamba2 offers a compact, memory-efficient addition to any production agent stack, able to power precise question answering workflows that donâ€™t call for the heavy â€œthinking tokenâ€ overhead of reasoning models.\n\nJamba2 is available in two model sizes: 3B and Mini (MoE; 12B active, 52B total parameters). We are proud to release both under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0), as part of our continued commitment to democratizing access to quality models. We are particularly excited this release includes a 3B model, enabling developers everywhere to download and run this technology right on their own devices, including iPhones, Androids, Macs, and PCs.\n\nJamba2 model family fast facts\n------------------------------\n\n*   License: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n*   Model sizes: 3B (dense) 52B/12A\n*   Context window length: 256K\n*   Availability: [AI21 Studio](https://studio.ai21.com/v2/), [Hugging Face](https://huggingface.co/collections/ai21labs/jamba2)\n\nMaximum reliability and steerability\n------------------------------------\n\nWith over seven years of experience building AI systems for enterprise customers, we know that reliability and steerability are non-negotiables when it comes to choosing a model. 54% of enterprises still cite accuracy as the number one AI-related risk they are working to mitigateâ€”with 30% of surveyed organizations having experienced the negative drawbacks of AI inaccuracies at least once ([McKinsey, 2025](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)). Even as models get increasingly sophisticated, there is an enormous, high-stakes gap to be filled when it comes to enterprise-grade accuracy.\n\nWe also know, from our work with customers, that not every enterprise workflow requires the high cost and high latency of reasoning models. Especially as more organizations adopt agents and model routers, a strong enterprise AI system should contain a range of model types, with different models called as needed based on task type and budget considerations.\n\nWith this in mind, we set out to build a model that delivers precise output with a lean memory footprint. To make â€œprecisionâ€ measurable, we focused on two signals that map directly to real deployments: instruction-following benchmarks, which capture how steerable a model is, and grounding benchmarks, which test whether outputs stay faithful to the provided context. Together, they serve as practical indicators of how consistently a model will behave in enterprise knowledge workflows.\n\nToday, weâ€™re bringing that vision to market with Jamba2: compact models that lead on these measures of enterprise reliability. Jamba2 is built to produce grounded answers across a range of source types, including technical manuals, research papers, company policies, and internal knowledge bases, so teams can deploy it as a dependable component in production stacks.\n\nLeader on instruction following and grounding\n---------------------------------------------\n\nJamba2 models excel across the instruction following benchmarks IFBench, IFEval, and Collie, as well as the grounding benchmark FACTS.\n\nWinning reliability-to-throughput ratio\n---------------------------------------\n\nFor the enterprise, models that shine on quality, yet choke in production-scale settings, are unusable.\n\nJamba2 leverages its memory-efficient architecture to maintain high throughput alongside high enterprise reliability, even as context scales to 100K tokens, mirroring the real-world usage we can expect from enterprise QA workflows.\n\nWinning performance on enterprise tasks\n---------------------------------------\n\nIn a human evaluated comparison between Jamba2 Mini and Ministral3 14B on a test set of real-world enterprise task prompts, Jamba2 Mini showed a statistically significant advantage in overall output quality and win rate. The tasks included a mix of enterprise QA tasks, instruction-heavy developer prompts, and other common business tasks such as summarization and drafting. This win rate encapsulates human preference, with special attention to factuality, style, constraint-adherence, instruction-following, and helpfulness.\n\n**Note on human evaluation methodology**: The models were evaluated by a team of content evaluation experts using a side-by-side, blind comparison protocol; leftâ€“right ordering was counterbalanced across items to mitigate order and precedence effects. Evaluators reviewed both outputs for both absolute metrics (general quality measures, as well as fine-grained error analysis), as well as human preference.\n\nHow we built it\n---------------\n\nTo train Jamba2, we utilized a contemporary LLM post-training pipeline. Following the pre-training phase, we mid-trained Jamba2 on 500B carefully curated tokens, with a higher representation of math and code in the mix, along with high-quality web data and long documents. We completed the mid-training phase with a short [state passing phase](https://arxiv.org/abs/2507.02782) for the modelâ€™s Mamba layers, a method recently introduced for effectively generalizing an SSMâ€™s context length.\n\nGiven our midtrained model, we performed cold start SFT to teach the model basic instruction-following and reasoning. We then topped off the model with DPO to further improve its performance as a starting point for on-policy RL. Finally, we ran multiple on-policy RL phases on our model, starting with short-context verifiable rewards and gradually moving to longer context training with a mix of verifiable and model-based rewards.\n\nCarefully aggregating verifiable and model-based rewards was one of the key advancements allowing Jamba2 models to excel on tasks that require following specific instructions and details while adhering to the general intention of the user. As part of this process, we optimized our training infra for efficient on-policy RL training, which combines model inference in both the generation and reward phases of the training.\n\nGetting started with Jamba2\n---------------------------\n\nAvailable for download on [Hugging Face](https://huggingface.co/collections/ai21labs/jamba2) or directly on [AI21 Studio](https://studio.ai21.com/v2/), researchers and AI enthusiasts alike can enjoy experimenting with Jamba2 and pushing it to support new use casesâ€”we canâ€™t wait to see what you build!",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/collections/ai21labs/jamba2",
          "was_fetched": true,
          "page": "Title: Jamba2 - a ai21labs Collection\n\nURL Source: https://huggingface.co/collections/ai21labs/jamba2\n\nMarkdown Content:\nupdated about 17 hours ago\n\nJamba2 is a highly-efficient open source family of language models built for maximum reliability and steerability in the enterprise.\n\n[Upvote 3](https://huggingface.co/login?next=%2Fcollections%2Fai21labs%2Fjamba2)\n*   [](https://huggingface.co/21world \"21world\")\n*   [](https://huggingface.co/Presidentlin \"Presidentlin\")\n*   [](https://huggingface.co/aquiffoo \"aquiffoo\")\n\n*   \n* * *\n\n[#### ai21labs/AI21-Jamba2-Mini Text Generation â€¢ 52Bâ€¢ Updated about 13 hours agoâ€¢ 18](https://huggingface.co/ai21labs/AI21-Jamba2-Mini)\n*   \n* * *\n\n[#### ai21labs/AI21-Jamba2-Mini-FP8 Text Generation â€¢ 52Bâ€¢ Updated about 13 hours agoâ€¢ 5 â€¢ 4](https://huggingface.co/ai21labs/AI21-Jamba2-Mini-FP8)\n*   \n* * *\n\n[#### ai21labs/AI21-Jamba2-3B Text Generation â€¢ 3Bâ€¢ Updated about 13 hours agoâ€¢ 5 â€¢ 17](https://huggingface.co/ai21labs/AI21-Jamba2-3B)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:03.384461905Z"
    },
    {
      "flow_id": "",
      "id": "1q7hikw",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/",
      "title": "Qwen3-4B-Instruct-2507 multilingual FT with upscaled Polish language",
      "content": "Hi, \n\nJust wanted to share a preview of my latest finetuned model based on Qwen3-4B-Instruct-2507.\n\nLanguages ratio:\n\nPolish - high  \nEnglish - medium  \nChinese - medium  \nCzech - medium/low  \nUkrainian - medium/low  \nRussian - medium/low\n\n\n\n[https://huggingface.co/piotr-ai/polanka\\_4b\\_v0.3\\_preview\\_260108\\_qwen3\\_gguf](https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf)\n\n",
      "author": "Significant_Focus134",
      "created_at": "2026-01-08T17:12:08Z",
      "comments": [
        {
          "id": "nyfleqe",
          "author": "mtomas7",
          "content": "How big was your dataset? Also, it would be great if you could share your \"recipe\" so it could be used for other languages too. Thank you!",
          "created_at": "2026-01-08T17:23:50Z",
          "was_summarised": false
        },
        {
          "id": "nyfnz7y",
          "author": "x86rip",
          "content": "nice work ! what are datasets that you used to ft this ?",
          "created_at": "2026-01-08T17:35:09Z",
          "was_summarised": false
        },
        {
          "id": "nyfott5",
          "author": "FullOf_Bad_Ideas",
          "content": "How is this model trained?\n\nhttps://huggingface.co/piotr-ai/polanka_3.6b_exp_WIP_251227\n\nI trained something similar, but 8 out of 128 experts active, instead of 2 out of 32 experts. Trained from scratch on Polish datasets, FineWeb2, HPLT3, FinePDFs. APT4 tokenizer.\n\nhttps://huggingface.co/adamo1139/poziomka-lora-instruct-alpha-2\n\nWe converged onto very similar things here!",
          "created_at": "2026-01-08T17:38:53Z",
          "was_summarised": false
        },
        {
          "id": "nygph0i",
          "author": "crantob",
          "content": "These small qwens have PC dogma so heavily blasted through them that they turn into quivering middle-school guidance counselors around any real world chat.",
          "created_at": "2026-01-08T20:18:17Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/piotr-ai/polanka%5C_4b%5C_v0.3%5C_preview%5C_260108%5C_qwen3%5C_gguf",
          "was_fetched": true,
          "page": "Title: 404 â€“ Hugging Face\n\nURL Source: https://huggingface.co/piotr-ai/polanka/_4b/_v0.3/_preview/_260108/_qwen3/_gguf\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 â€“ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n404\n===\n\nSorry, we can't find the page you are looking for.\n\n System theme \n\nWebsite\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*   [Changelog](https://huggingface.co/changelog)\n*   [Inference Endpoints](https://endpoints.huggingface.co/)\n*   [HuggingChat](https://huggingface.co/chat)\n\nCompany\n\n*   [About](https://huggingface.co/huggingface)\n*   [Brand assets](https://huggingface.co/brand)\n*   [Terms of service](https://huggingface.co/terms-of-service)\n*   [Privacy](https://huggingface.co/privacy)\n*   [Careers](https://apply.workable.com/huggingface/)\n*   [Press](mailto:press@huggingface.co)\n\nResources\n\n*   [Learn](https://huggingface.co/learn)\n*   [Documentation](https://huggingface.co/docs)\n*   [Blog](https://huggingface.co/blog)\n*   [Forum](https://discuss.huggingface.co/)\n*   [Service Status](https://status.huggingface.co/)\n\nSocial\n\n*   [GitHub](https://github.com/huggingface)\n*   [Twitter](https://twitter.com/huggingface)\n*   [LinkedIn](https://www.linkedin.com/company/huggingface/)\n*   [Discord](https://huggingface.co/join/discord)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf",
          "was_fetched": true,
          "page": "Title: piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf Â· Hugging Face\n\nURL Source: https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf\n\nMarkdown Content:\nREADME.md exists but content is empty.\n\nDownloads last month- \n\nGGUF\n\n[](https://huggingface.co/docs/hub/gguf)\n\nModel size\n\n4B params\n\nArchitecture\n\nqwen3\n\nHardware compatibility\n\n[Log In](https://huggingface.co/login?next=https%3A%2F%2Fhuggingface.co%2Fpiotr-ai%2Fpolanka_4b_v0.3_preview_260108_qwen3_gguf)to view the estimation\n\nWe're not able to determine the quantization variants.\n\n[View all variants](https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf/tree/main)\n\nInference Providers[NEW](https://huggingface.co/docs/inference-providers)\n\n[Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\n\nThis model isn't deployed by any Inference Provider.[ðŸ™‹Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf\u0026description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bpiotr-ai%2Fpolanka_4b_v0.3_preview_260108_qwen3_gguf%5D(%2Fpiotr-ai%2Fpolanka_4b_v0.3_preview_260108_qwen3_gguf)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n\nModel tree for piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf[](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\n----------------------------------------------------------------------------------------------------------------------------------------\n\nBase model\n\n[Qwen/Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)\n\nQuantized\n\n([175](https://huggingface.co/models?other=base_model:quantized:Qwen/Qwen3-4B-Instruct-2507))\n\nthis model",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:11.377090724Z"
    },
    {
      "flow_id": "",
      "id": "1q7hywi",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/",
      "title": "How do you manage quality when AI agents write code faster than humans can review it?",
      "content": "We are shifting to an agentic workflow. My thesis is \"Code at Inference Speed.\" My CTO's counter-argument is that **reviewing code is harder than writing it**.\n\nHis concern is simple: If AI increases code volume by 10x, human review becomes a fatal bottleneck. He predicts technical debt will explode because humans canâ€™t mentally verify that much logic that quickly.\n\n  \nHow do handle this? I know one option is to slow down releases but is there any other approaches people are taking.",
      "author": "lostsoul8282",
      "created_at": "2026-01-08T17:28:30Z",
      "comments": [
        {
          "id": "nyfrj2m",
          "author": "Thick-Protection-458",
          "content": "\\\u0026gt; when AI agents write code faster than humans can review it?\n\nEasily. Just the bottleneck moves from me producing code (which is already lesser part of my job comparing to thinking about high-level structures. So it is kinda not a bottleneck anyway, just a nice spot to optimize) to me reviewing code.\n\nBefore that it was problematic too. Just we did not achieve the stage when this become bottleneck (means earlier bottlenecks is partially solved).\n\nAnd no, no way that electronic fucker (or human. My own, lol - better to at least review your own code later, when your stream of thoughts changed enough so you have a chance to see things from different angle) output get past me before I am sure I understand what this thing is doing.\n\n\\\u0026gt; **reviewing code is harder than writing it**\n\nHe is exactly right.\n\nIf you don't do it in digestable chunks.\n\nAnd for chunks to be digestable you have to know what to expect. So you have to take part in planning structural stuff. Either all by yourself or combo of you + LLM agent (it may give boost here too, by reviewing your ideas for missing corner cases or even noticing utterly wrong understanding of some stuff; also by suggesting tweaks). So this way you kinda know what to expect here.\n\nSo if you want to vibecode the whole thing and only review in the end - no, probably not the way unless coding agents get not only good quality, but actually superhuman quality. And even than - they would not be perfect decision mechanisms, so stacking them and human devs would still make sense. Because as soon as we make and notice different kind of errors - stacking different weak mechanisms would still work.\n\nIf you think about it like about pair programming, on the other hand - just a \"pair\" being not a human, but machine - it may start making sense,",
          "created_at": "2026-01-08T17:50:39Z",
          "was_summarised": false
        },
        {
          "id": "nyfv2lp",
          "author": "Abject-Kitchen3198",
          "content": "Start by accepting CTOs argument. \n\nSlow down. Accept that LLM induced productivity factor will be between 0.5 and 2x on a case-by-case basis.\n\nIterate with AI until you get a solution with minimal amount of code with acceptable quality that you feel comfortable reviewing. \n\nDo this for a quarter or two until you realize that either LLMs are not helpful for your case or that they provide some improvement on average and you can keep using them.",
          "created_at": "2026-01-08T18:05:52Z",
          "was_summarised": false
        },
        {
          "id": "nyfr4kz",
          "author": "AndThenFlashlights",
          "content": "I work in a field that has some pretty severe safety and liability consequences if something goes wrong. Qualified and competent humans eye need to review and comprehend every line of code that goes into the codebase, full stop. Reliability is more important than adding features. And weâ€™re usually working with devices or APIs that arenâ€™t documented publicly, so LLMs currently arenâ€™t super helpful at writing things unattended - theyâ€™re more useful in my workflow for writing API / class boilerplate or small contained methods, not vibe-coding whole things independently. \n\nTreat the AI like a flock of interns you need to watch and manage. You ever had too many interns to keep track of, and experienced that unfocused chaos? This is why I donâ€™t take on more than 1 intern at a time anymore.",
          "created_at": "2026-01-08T17:48:55Z",
          "was_summarised": false
        },
        {
          "id": "nyg6air",
          "author": "FullstackSensei",
          "content": "I find it funny how many here think LLMs will be able to review code and fix slop. Sounds like a chicken and egg problem to me. If you can train a model to detect and fix slop, then why wasn't the coding model trained to not generate said slop in the first place?\n\nIf we were anywhere near what some here seem to be predicting, why would anthropic spend a cool billion buying a Javascript runtime (Bun) rather than tuning a version of Claude to write something similar themselves?",
          "created_at": "2026-01-08T18:53:48Z",
          "was_summarised": false
        },
        {
          "id": "nyfrv55",
          "author": "Capaj",
          "content": "tests. Lots of them",
          "created_at": "2026-01-08T17:52:06Z",
          "was_summarised": false
        },
        {
          "id": "nyft98g",
          "author": "bigh-aus",
          "content": "It's a valid concern. But it's the same concern that larger enterprises are dealing with their current code stacks.  You need to increase the ecosystem around the code. Much like human written code that you outsourced to XYZ small company from ABC country.  \n\n\\#1: use a safe language: (rust, zig, safe c++, java, go etc). The compiler / runtime errors will help improve quality and catch bugs, vs interpreted languages where it's only runtime..  (It's one of the reasons I'm learning rust)\n\n\\#2: Full test suite imo is the main thing - unit tests, external API tests, integration tests, defensive tests, behavior tests, chaos tests, security tests, DR tests. Start simple, and scale up.  EG: extract any s3 buckets and check that they have encryption + auth turned on is a  classic example for low hanging fruit.  TLDR: how do you validate that the code is right? validate it by testing.\n\n\\#3: Have the code checked in in small steps, so if there is a problem rollback is easy.  Also look into having agents do code review.\n\n\\#4 CI/CD run as much static and dynamic analysis as you can on the code as part of the build / deployment pipeline.  Build agents to analyze the code, improvements, code smells.  Manage by exception.\n\n\\#5: Full red / blue team to test the security operation of the system, and build up automated security tests. \n\n\\#6: If required - compliance testing - is it HIPPA / PCI / Fedramp etc. How can you have continual testing to prove that the systems adhere to the standards. \n\n\\#7: run tests ON your staff - eg if there's a bug, how long does it take to find it, etc etc.  Break a non prod environment and have your staff try to fix it.\n\nAlso look at ways you can improve / reduce / optimize / etc the code using profiling and manual analysis.\n\nAlso do the dev, stage, prod environments at a minimum (more if needed).  Never have agents code in prod. ever.",
          "created_at": "2026-01-08T17:58:05Z",
          "was_summarised": false
        },
        {
          "id": "nyg0c0e",
          "author": "Zulfiqaar",
          "content": "\u0026gt;Â My thesis is \"Code at Inference Speed.\"\n\nJust cause someone can type at 100WPM doesn't mean they should\n\n\nThe alternatives all centre around increasing code quality or increasing review capacity",
          "created_at": "2026-01-08T18:28:23Z",
          "was_summarised": false
        },
        {
          "id": "nyfot0f",
          "author": "seanpuppy",
          "content": "I think this just highlights the importance of hiring highly skilled senior devs over jr's\n\nAny Senior dev today will have spent a TON of time reading and reviewing code, and will be both faster and better and finding issues.",
          "created_at": "2026-01-08T17:38:48Z",
          "was_summarised": false
        },
        {
          "id": "nyg6yqu",
          "author": "FastDecode1",
          "content": "Use AIs to review. Duh.\n\nWhat kind of \"agentic workflow\" are you using if the only thing that's automated is code generation? If you paid money for that, you need a refund.",
          "created_at": "2026-01-08T18:56:40Z",
          "was_summarised": false
        },
        {
          "id": "nyfo1wx",
          "author": "sabergeek",
          "content": "We'll probably have models for code review at some point, so that AI cleans it own slop.",
          "created_at": "2026-01-08T17:35:29Z",
          "was_summarised": false
        },
        {
          "id": "nygw7oq",
          "author": "ttkciar",
          "content": "Your CTO is totally right, and the problem he describes predates LLM codegen.  The advent of codegen has exacerbated problem tremendously, is all.\n\nPart of the problem in places I've worked is that management controls how much of developers' time is spend writing new code vs paying off technical debt, and management does not allocate enough time to paying off that debt.\n\nIn that sense, it is a people-problem, not a technical problem.  Fix management and the problem becomes a lot more tractable.\n\nOn the other hand, there are some things you can do to make LLM-inferred projects faster/easier to validate and review:\n\n**Write comprehensive unit tests**\n\nPreferably have the humans do this before codegen, because ideally unit tests will describe how code is expected to behave, which will help LLMs infer the expected code.  Not many devs like to write unit tests, though, so having your LLM generate unit tests after the fact is a second-best solution.  Note that you will need to instruct the LLM to write \"testable\" code, because sometimes the most natural-seeming implementations are not easily unit-tested.\n\nUnit tests with mocked dependencies are beneficial because they exercise the different parts of your project in isolation and verify that their outputs/side-effects comply with expectations.  This means you can find many bugs simply by running your unit tests, and which unit tests fail point you precisely at the code which needs to be fixed (if your tests are high-enough granularity, which requires that your functions/methods are decomposed into subroutines.  This is an important aspect of writing code to be testable).\n\nIt also makes adjusting the behavior of the project to comply with expectations easier, if you find that code does not do what you want it to do.  You can tweak the appropriate unit test(s), or write new tests, and have the dev or LLM fix the code so that the test passes.\n\nIt is good industry practice to make sure a development branch passes all unit tests before merging it into the main branch, and then making sure the merge passes all unit tests before pushing it to the remote repo.\n\n**Write good documentation**\n\nOne of the best uses I've found for codegen LLMs is to have them explain my coworkers' code to me.  Most codegen models (and some non-codegen models!) are good at writing code documentation.  This helps me come up to speed not just for code reviews but also for contributing to legacy projects with which I am familiar.\n\nIdeally you should have at least two layers of documentation, preferably three:\n\n* A high-level view, which is short and easy to read, explaining the purpose of the project, who is expected to use it, and for what, and the general flow of data through the project -- its inputs, its outputs, its side-effects, and the components it passes through in-between.\n\n* A component-level view, which describes the main subsystems involved in the project and their interfaces.  These can be libraries, external dependencies like databases or service APIs, frameworks, or any other reasonable-seeming partitioning of the project into a small handful of parts.  If you omit any documentation, it would be this one, not the high- or low-level views.\n\n* A low-level view, usually by source code file, which describes what the code in the file is for, what its classes and any global state are, the methods used by those classes, and what other files use those classes and/or call those methods.\n\nGood documentation will get the human reviewers up to speed quickly and let them start and finish their reviews more quickly.\n\n**Generate a list of possible bugs/problems**\n\nYou don't want to totally automate the code review process, but there's nothing wrong with asking the LLM to infer a list of what might be bugs or weaknesses in the project, for the human reviewers to assess.  When I ask GLM-4.5-Air to enumerate problems in my code, usually only about a third of the problems it identifies are actual problems which need fixing, but it's still better to have it than not.\n\nThis can help focus code reviewers' attention and at least give them something to consider, regarding whether the project should be better than it is.\n\n**Use a structured log**\n\nA lot of problems only become visible once you've been using a project for a while for real-world tasks.  A structured log will not only help you spot problems, but also expose the program's internal state in the steps leading up to the problem.  This is invaluable for rapid troubleshooting.\n\nWhen a problem crops up, you can look back through the log to identify exactly where things went awry, and use the conditions represented in the log to inform bugfixes and (especially!) new unit tests which would have identified the problem before it was put into production.\n\nStrictly speaking this is slightly out of scope for your problem, as the structured log only becomes useful *after* the code passes review and is put into production, but the simple fact is that not all problems get caught in code review.  Realistically new code needs to be vetted both before and after deployment.\n\n\n**These measures will accelerate code review, but the underlying problem persists.**\n\nIncorporating all of these measures can shorten the time it takes to review a project, but human reviewers still have to put in the work to verify that the code is good.  Depending on how many reviewers you have and how much code you are generating, they might or might not be able to keep up.\n\nWhether to bottleneck deployment of new code on code review, and how much, is and always has been a trade-off determined by the development team's management.  It is their job to assess the tradeoffs between releasing thoroughly-vetted code versus releasing possibly-buggy code and adding to the employer's technical debt.\n\nGenerating new code via LLM inference doesn't change that, but you should be able to demonstrate mathematically that given fixed human dev resources (say, programmer-hours per month, allocated to developing new code vs code reviews vs paying down technical debt), and given a fixed management tolerance for accumulating technical debt, the total useful code deployed per unit time is increased when LLMs generate at least some of the new code.",
          "created_at": "2026-01-08T20:48:32Z",
          "was_summarised": false
        },
        {
          "id": "nyfq6dy",
          "author": "1ncehost",
          "content": "I've been dealing with this for a year, and this is predominantly a solved issue with project management risk mitigation. Essentially executives have struggled with this issue since forever: how do you maintain quality when you don't know or interact with everyone in your company? Tests and process are the ultimate answer.\n\nYou must adopt the mindset of an executive and trust the employees, but ensure there are thoroughly enforced safegaurds, audits, and so on to maintain quality. The code you care about becomes the \"operating system\" that derives the systems, not the system design itself.",
          "created_at": "2026-01-08T17:44:47Z",
          "was_summarised": false
        },
        {
          "id": "nyfnzaq",
          "author": "notAllBits",
          "content": "Remit-driven development",
          "created_at": "2026-01-08T17:35:10Z",
          "was_summarised": false
        },
        {
          "id": "nyfsbbt",
          "author": "geoffwolf98",
          "content": "You have to balance the risks - whether it is better to get it out the door but potentially loses you millions due to a price error (or what ever) or have reliable working code that wont bankrupt you.",
          "created_at": "2026-01-08T17:54:02Z",
          "was_summarised": false
        },
        {
          "id": "nyg9e9z",
          "author": "adityaguru149",
          "content": "Yeah it gets difficult reviewing a lot of the slop by AI. My way is writing lots of tests and using AI for quick summarisations for code blacks so that I don't have to read through every line. I also get more involved in the architecting phase so that AI has better guidance before writing code.",
          "created_at": "2026-01-08T19:07:15Z",
          "was_summarised": false
        },
        {
          "id": "nyge9ds",
          "author": "CV514",
          "content": "AI agents have gained some innate right to merge PR or something? No? That's how.",
          "created_at": "2026-01-08T19:28:33Z",
          "was_summarised": false
        },
        {
          "id": "nygfde1",
          "author": "Jmc_da_boss",
          "content": "Oh wow, your telling me the historical bottle neck of human review and alignment in programming is STILL the bottleneck in programming?\n\nThat's crazy, however will we handle this thing that's been true for decades.",
          "created_at": "2026-01-08T19:33:27Z",
          "was_summarised": false
        },
        {
          "id": "nygjke0",
          "author": "synn89",
          "content": "I  expect we'll probably develop new code design methodologies that work best with AI. Languages may also end up being preferred for the same reason. This is why agile and MVC exists today, to optimize for human meat brains.",
          "created_at": "2026-01-08T19:51:57Z",
          "was_summarised": false
        },
        {
          "id": "nygoeq8",
          "author": "CallinCthulhu",
          "content": "Preliminary review by AI catches a lot of shit early. Still needs human review, but that review is faster",
          "created_at": "2026-01-08T20:13:32Z",
          "was_summarised": false
        },
        {
          "id": "nygps00",
          "author": "blackkettle",
          "content": "Thatâ€™s not an appropriate way to use AI for coding. Agentic workflows with high expertise can definitely make you much faster.  Blindly committing AI code based on prompts and no experience?  See you at the next post mortem!\n\nYour CTO is right.\n\nAnd AI shouldnâ€™t really be â€œincreasing code volumeâ€.  It should be used again with expertise to speed up well defined, low risk, relative tasks and gradually iterate to more complex ones.",
          "created_at": "2026-01-08T20:19:41Z",
          "was_summarised": false
        },
        {
          "id": "nygxd2i",
          "author": "a-wiseman-speaketh",
          "content": "I think this is like a corollary of Brandolini's Law - and we've seen how that's played out with the degeneration of shared reality and objective truth over the last decade, particularly.\n\nI will point out that one of the skills a senior dev should be great at and every LLM I have tried is absolutely awful at is DELETING code, or never writing it to begin with.",
          "created_at": "2026-01-08T20:53:34Z",
          "was_summarised": false
        },
        {
          "id": "nygzcnm",
          "author": "Psychological_Ear393",
          "content": "\u0026gt;How do you manage quality when AI agents write code faster than humans can review it?\n\nIf you are pumping out code faster than a human can understand and review it, then you literally can't.  It's a matter of doing the maths of which side you manage for what the product goals and roadmap is.  A pipe can only hold so much volume.  Right now you have pressure on the input side and it's more like a storm water drain than a filtered water outlet.  To strain the storm water you need a bigger pipe and grate which lets more things through.\n\nIt's up to the dev to ensure they are submitting quality pull requests.  If a PR comes in a human doesn't understand it then they have failed at their job.  If a reviewer finds a problem, it doesn't matter where it came from, that dev put in the PR - PRs have problems that's why we have them but to put one in that had no attempt to find the problems and submit understandable quality is egregious and if agents are writing code faster than the gates can handle then that's what's happening.\n\nPerformance objectives need to be updated to include appropriate use of AI. Everyone needs to be on the same page about what matters to your product, if some members of your team want to move faster than humans can understand and others want more thorough reviews then you have a culture problem that needs to be addressed.\n\nI mostly use AI for weird problems where I don't know where to start, like chunks of code I haven't touched before, then I take over and try to solve it myself where I can.  I use it to check the work I did for anything I missed, and you need to be careful with that too it can dream things so you need to know what you know to assess it. I also use it for bulk changes where it has a sample to go off for style and patterns.\n\nThe other day I had to put in a change that I didn't understand.  It was a legacy product in a framework I don't know and from top down they said they urgently need it and they are OK with AI writing it.  I reviewed it as best I could but I had no idea why it worked and in the PR I clearly stated that it was mostly AI written and I didn't fully understand how it works.  I'm a consultant and told them it's a bad idea, the owners said they wanted it, ok sure the people paying the bills get what they want.",
          "created_at": "2026-01-08T21:02:22Z",
          "was_summarised": false
        },
        {
          "id": "nyh2g45",
          "author": "DHasselhoff77",
          "content": "Look up \"The Goal\" by Eliyahu M. Goldratt. Your CTO is right.",
          "created_at": "2026-01-08T21:16:09Z",
          "was_summarised": false
        },
        {
          "id": "nyh8zks",
          "author": "Foreign_Risk_2031",
          "content": "Itâ€™s true. Itâ€™s difficult to accept but true. This is your CTOs job to solve. You can outsource testing. Or make more agentic workflows to review.",
          "created_at": "2026-01-08T21:44:35Z",
          "was_summarised": false
        },
        {
          "id": "nyhe0ju",
          "author": "rosstafarien",
          "content": "It's not tech debt that's your problem. That's literally the least of your worries. It's that nobody understands your codebase and nobody can say that it's correctly solving the problem.\n\nHow are you managing requirements? How are you testing the system to be sure that the requirements are being met? How are you going to confirm that a future change doesn't break existing functionality?\n\nAnd I have yet to see an AI produce sane code at 10x the rate of a human developer. An AI can produce boilerplate at 10x the rate, but that isn't the code you care about.",
          "created_at": "2026-01-08T22:06:21Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:11.377095015Z"
    },
    {
      "flow_id": "",
      "id": "1q7m2eh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/",
      "title": "Built a blind benchmark for coding models - which local models should I add?",
      "content": "3 AI judges score each output blind. Early results from 10 coding tasks - Deepseek V3.2 at #9. GLM 4.7 at #6, beating Claude Opus 4.5.\n\nSome open-source models are free to evaluate. Which local models should I evaluate and add to the leaderboard?\n\n[codelens.ai/leaderboard](http://codelens.ai/leaderboard)\n\nEDIT: Tested community suggestions! Results now live on the leaderboard:  \n  \n\\- GPT-OSS-120B, Qwen3 Next 80B, Devstral 2, Nemotron Nano 30B, and more  \n  \nKeep the suggestions coming - we'll keep adding models.",
      "author": "Equivalent-Yak2407",
      "created_at": "2026-01-08T19:54:08Z",
      "comments": [
        {
          "id": "nygvlfu",
          "author": "ciprianveg",
          "content": "minimax m2.1, qwen 235b",
          "created_at": "2026-01-08T20:45:48Z",
          "was_summarised": false
        },
        {
          "id": "nygtwck",
          "author": "Aggressive-Bother470",
          "content": "gpt-oss-120b, Seed-OSS-36B, Qwen3-30B-A3B-Thinking-2507-BF16, GLM-4.6-UD-IQ2\\_M",
          "created_at": "2026-01-08T20:38:14Z",
          "was_summarised": false
        },
        {
          "id": "nyhe3dy",
          "author": "MrBIMC",
          "content": "Devstral-2512 is goated. I know it's free only temporarily, but as far as free models go - it most often delivers exactly to spec. So I'd like it benchmarked.",
          "created_at": "2026-01-08T22:06:42Z",
          "was_summarised": false
        },
        {
          "id": "nyh5uym",
          "author": "-InformalBanana-",
          "content": "Qwen3 2507 30b a3b instruct, qwen3 next 80b, gpt oss 20b/120b, Devstral small 2 24b, Nemotron nano 3 e0b a3b, Nemotron Cascade 14b.\nI tried Nemotron models and I think they are bad and benchmaxed so if you cound check that. For example Nemotron Cascade 14b has better LCBv6 score than qwen next 80b a3b. But in my one shot try it even had syntax errors so complete failure.",
          "created_at": "2026-01-08T21:31:00Z",
          "was_summarised": false
        },
        {
          "id": "nyiql7p",
          "author": "pmttyji",
          "content": "* Kimi K2 Instruct 0905\n* Kimi-K2-Thinking\n* Devstral-2-123B-Instruct-2512\n* Devstral-Small-2-24B-Instruct-2512\n* Mistral-Large-3-675B-Instruct-2512\n* Ling-1T\n* Olmo-3.1-32B-Instruct\n* Qwen3-32B\n* Llama-3\\_3-Nemotron-Super-49B-v1\\_5\n* dots.llm1.inst",
          "created_at": "2026-01-09T02:12:42Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://codelens.ai/leaderboard",
          "was_fetched": true,
          "page": "Title: AI Model Leaderboard 2025 - Top Coding LLMs Ranked\n\nURL Source: http://codelens.ai/leaderboard\n\nMarkdown Content:\nCommunity-driven rankings based on real developer evaluations\n\nCurrent Rankings\n----------------\n\nBased on 10 evaluations from real developers\n\nSort by:\n\n1st\n\nGPT-5.2\n\n7 evaluations\n\n87.2\n\nScore\n\n2nd\n\nGpt 5.1 Codex Max\n\n5 evaluations\n\n85.3\n\nScore\n\n3rd\n\nGpt 5.1 Codex\n\n7 evaluations\n\n84.2\n\nScore\n\n4th\n\nGemini 2.5 Pro\n\n8 evaluations\n\n83.8\n\nScore\n\n5th\n\nGpt Oss 120b\n\n1 evaluations\n\n83.3\n\nScore\n\n6th\n\nGemini 3 Pro\n\n8 evaluations\n\n82.9\n\nScore\n\n7th\n\nGlm 4.7\n\n7 evaluations\n\n82.7\n\nScore\n\n8th\n\nO3\n\n3 evaluations\n\n82.7\n\nScore\n\n9th\n\nClaude Opus 4.5\n\n9 evaluations\n\n81.7\n\nScore\n\n10th\n\nDeepseek V3.2\n\n4 evaluations\n\n80.8\n\nScore\n\n11th\n\nClaude Sonnet 4.5\n\n9 evaluations\n\n79.7\n\nScore\n\n12th\n\nMimo V2 Flash\n\n2 evaluations\n\n78.7\n\nScore\n\n13th\n\nClaude Haiku 4.5\n\n5 evaluations\n\n78.6\n\nScore\n\n14th\n\nDeepseek V3.2 Exp\n\n3 evaluations\n\n77.3\n\nScore\n\n15th\n\nSeed 1.6 Flash\n\n2 evaluations\n\n76.7\n\nScore\n\n16th\n\nTng R1t Chimera\n\n1 evaluations\n\n76.3\n\nScore\n\n17th\n\nGrok Code Fast 1\n\n4 evaluations\n\n75.0\n\nScore\n\n18th\n\nSeed 1.6\n\n1 evaluations\n\n74.3\n\nScore\n\n19th\n\nGrok 4\n\n2 evaluations\n\n74.0\n\nScore\n\n20th\n\nQwen3 235b A22b 2507\n\n2 evaluations\n\n72.5\n\nScore\n\n21st\n\nDevstral 2512\n\n2 evaluations\n\n71.7\n\nScore\n\n22nd\n\nQwen3 30b A3b Thinking 2507\n\n2 evaluations\n\n70.7\n\nScore\n\n23rd\n\nGlm 4.5 Air\n\n1 evaluations\n\n70.7\n\nScore\n\n24th\n\nMinimax M2.1\n\n4 evaluations\n\n68.8\n\nScore\n\n25th\n\nQwen3 Next 80b A3b Instruct\n\n2 evaluations\n\n63.7\n\nScore\n\n26th\n\nNemotron 3 Nano 30b A3b\n\n3 evaluations\n\n62.2\n\nScore\n\n27th\n\nQwen3 Coder\n\n1 evaluations\n\n58.7\n\nScore\n\n28th\n\nMinimax M2\n\n1 evaluations\n\n49.3\n\nScore\n\n### Monthly Top Models\n\nGet a monthly digest of the top-performing AI models, ranking changes, and new challengers. Data-driven insights from real developer evaluations, not marketing hype.\n\nHow Rankings Work\n-----------------\n\n### Judge Score (Primary Metric)\n\nEach model output is scored by 3 frontier AI judges (Claude Opus 4.5, GPT-5.2, Gemini 3 Pro). We take the median score to reduce individual judge bias. Scores are based on 5 weighted criteria: Correctness (35%), Security (20%), Code Quality (20%), Efficiency (15%), and Completeness (10%).\n\n### Community Vote\n\nDevelopers vote on model outputs in blind head-to-head comparisons. Win rate shows the percentage of matchups where this model was chosen. Every vote requires a comment explaining the reasoning.\n\n### Response Time\n\nThe mean time each model takes to generate a response. Faster isn't always betterâ€”some models trade speed for qualityâ€”but this helps you understand performance trade-offs.\n\n### Cost per Evaluation\n\nAverage API cost for each model based on actual token usage. Pricing data sourced from OpenRouter. Helps you balance quality against budget.\n\n### Aggregation\n\nRankings aggregate all public evaluations. Same prompt from different users contributes to the same benchmark. This builds a community-driven dataset that reflects real-world usage, not synthetic benchmarks.\n\n### Why This Matters\n\nTraditional benchmarks like HumanEval and SWE-Bench use synthetic tasks that don't reflect real-world usage. CodeLens rankings are built on actual code challenges from real developers, making it a more accurate benchmark for choosing which AI model to use for your specific needs.\n\n[Read our full methodology](https://codelens.ai/methodology)\n\n### Contribute to the Leaderboard\n\nRun evaluations on your own prompts and help build the most accurate AI model benchmark. Pro users' results contribute to these public rankings.",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/6ocf1gbxj6cg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:14.813009428Z"
    },
    {
      "flow_id": "",
      "id": "1q7k754",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/",
      "title": "toy model",
      "content": "If anyone is interested in creating, training, and chatting with a toy model, Iâ€™ve created [https://github.com/EduardTalianu/toygpt](https://github.com/EduardTalianu/toygpt).\n\nIt includes:\n\n* a model script to create a model\n* a training script to train it on a`.txt` file\n* a chat script to interact with the trained model\n\nItâ€™s a PyTorch research implementation of a Manifold-Constrained Hyper-Connection Transformer (mHC), combining Mixture-of-Experts efficiency, Sinkhorn-based routing, and architectural stability enhancements.\n\nSlower per step than a vanilla Transformer â€” but *much* more sample-efficient. At \u0026lt;1 epoch it already learns grammar, structure, and style instead of collapsing into mush.\n\nEnjoy!",
      "author": "Eduard_T",
      "created_at": "2026-01-08T18:46:43Z",
      "comments": [
        {
          "id": "nyghz0s",
          "author": "cosimoiaia",
          "content": "Very interesting, thanks for sharing!",
          "created_at": "2026-01-08T19:44:58Z",
          "was_summarised": false
        },
        {
          "id": "nyhirhi",
          "author": "-InformalBanana-",
          "content": "Just interested what degree/knowledge base do you personally have in order to implement papers/innovations like that? Math or CS I'm guessing?",
          "created_at": "2026-01-08T22:27:53Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/EduardTalianu/toygpt",
          "was_fetched": true,
          "page": "Title: GitHub - EduardTalianu/toygpt\n\nURL Source: https://github.com/EduardTalianu/toygpt\n\nMarkdown Content:\nGitHub - EduardTalianu/toygpt\n===============\n\n[Skip to content](https://github.com/EduardTalianu/toygpt#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FEduardTalianu%2Ftoygpt)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FEduardTalianu%2Ftoygpt)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=EduardTalianu%2Ftoygpt)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/EduardTalianu/toygpt) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/EduardTalianu/toygpt) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/EduardTalianu/toygpt) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[EduardTalianu](https://github.com/EduardTalianu)/**[toygpt](https://github.com/EduardTalianu/toygpt)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2FEduardTalianu%2Ftoygpt)You must be signed in to change notification settings\n*   [Fork 2](https://github.com/login?return_to=%2FEduardTalianu%2Ftoygpt)\n*   [Star 5](https://github.com/login?return_to=%2FEduardTalianu%2Ftoygpt) \n\n[5 stars](https://github.com/EduardTalianu/toygpt/stargazers)[2 forks](https://github.com/EduardTalianu/toygpt/forks)[Branches](https://github.com/EduardTalianu/toygpt/branches)[Tags](https://github.com/EduardTalianu/toygpt/tags)[Activity](https://github.com/EduardTalianu/toygpt/activity)\n\n[Star](https://github.com/login?return_to=%2FEduardTalianu%2Ftoygpt)\n\n[Notifications](https://github.com/login?return_to=%2FEduardTalianu%2Ftoygpt)You must be signed in to change notification settings\n\n*   [Code](https://github.com/EduardTalianu/toygpt)\n*   [Issues 0](https://github.com/EduardTalianu/toygpt/issues)\n*   [Pull requests 0](https://github.com/EduardTalianu/toygpt/pulls)\n*   [Actions](https://github.com/EduardTalianu/toygpt/actions)\n*   [Projects 0](https://github.com/EduardTalianu/toygpt/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/EduardTalianu/toygpt).](https://github.com/EduardTalianu/toygpt/security)\n*   [Insights](https://github.com/EduardTalianu/toygpt/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/EduardTalianu/toygpt)\n*   [Issues](https://github.com/EduardTalianu/toygpt/issues)\n*   [Pull requests](https://github.com/EduardTalianu/toygpt/pulls)\n*   [Actions](https://github.com/EduardTalianu/toygpt/actions)\n*   [Projects](https://github.com/EduardTalianu/toygpt/projects)\n*   [Security](https://github.com/EduardTalianu/toygpt/security)\n*   [Insights](https://github.com/EduardTalianu/toygpt/pulse)\n\nEduardTalianu/toygpt\n====================\n\nmain\n\n[Branches](https://github.com/EduardTalianu/toygpt/branches)[Tags](https://github.com/EduardTalianu/toygpt/tags)\n\n[](https://github.com/EduardTalianu/toygpt/branches)[](https://github.com/EduardTalianu/toygpt/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [4 Commits](https://github.com/EduardTalianu/toygpt/commits/main/) [](https://github.com/EduardTalianu/toygpt/commits/main/) |\n| [README.md](https://github.com/EduardTalianu/toygpt/blob/main/README.md \"README.md\") | [README.md](https://github.com/EduardTalianu/toygpt/blob/main/README.md \"README.md\") |  |  |\n| [chat_expert_act.py](https://github.com/EduardTalianu/toygpt/blob/main/chat_expert_act.py \"chat_expert_act.py\") | [chat_expert_act.py](https://github.com/EduardTalianu/toygpt/blob/main/chat_expert_act.py \"chat_expert_act.py\") |  |  |\n| [model_mhc_expert.py](https://github.com/EduardTalianu/toygpt/blob/main/model_mhc_expert.py \"model_mhc_expert.py\") | [model_mhc_expert.py](https://github.com/EduardTalianu/toygpt/blob/main/model_mhc_expert.py \"model_mhc_expert.py\") |  |  |\n| [train_expert_act.py](https://github.com/EduardTalianu/toygpt/blob/main/train_expert_act.py \"train_expert_act.py\") | [train_expert_act.py](https://github.com/EduardTalianu/toygpt/blob/main/train_expert_act.py \"train_expert_act.py\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/EduardTalianu/toygpt#)\n\ntoygpt\n======\n\n[](https://github.com/EduardTalianu/toygpt#toygpt)\n\nA PyTorch implementation of the **Manifold-Constrained Hyper-Connection Expert Transformer (mHC-Expert)**, featuring architectural stability enhancements and Mixture-of-Experts (MoE) efficiency. This is a research codebase for training and interacting with transformer models that use hyper-connections constrained to the doubly stochastic manifold.\n\nOverview\n--------\n\n[](https://github.com/EduardTalianu/toygpt#overview)\n\nThe model combines:\n\n*   MoE layers with shared SwiGLU experts\n*   RoPE positional embeddings for efficient context extension\n*   Depth gating for dynamic layer contribution scaling\n*   Sinkhornâ€“Knopp projections for optimal routing (FP32-stable)\n\nThe codebase is designed for training language models on textual data and includes a full-featured chat interface with KV-caching and adaptive temperature sampling.\n\nQuick Start\n-----------\n\n[](https://github.com/EduardTalianu/toygpt#quick-start)\n\n### 1. Create a Base Model\n\n[](https://github.com/EduardTalianu/toygpt#1-create-a-base-model)\n\nRun the model definition script to generate a fresh transformer:\n\nundefinedshell\npython model_mhc_expert.py\nundefined\n\nThis creates `mhc_stable_transformer.pt` with:\n\n*   8 layers, 8 heads, hidden dim 512\n*   8 SwiGLU experts\n*   4Ã— stream expansion\n*   Vocabulary: GPT-2 tokenizer (50,257 tokens)\n\n### 2. Train on Text Data\n\n[](https://github.com/EduardTalianu/toygpt#2-train-on-text-data)\n\nLaunch the training GUI to select model and training text:\n\nundefinedshell\npython train_expert_act.py\nundefined\n\n**Training Configuration (defaults):**\n\n*   Total steps: 50,000\n*   Warmup: 2,000 steps\n*   Peak LR: 3e-4 â†’ Min LR: 1e-5\n*   Batch size: 8 (effective: 32 with gradient accumulation)\n*   Sequence length: 256 tokens\n*   Mixed precision: BF16 (auto-detected)\n*   Loss penalties: \n    *   Î»_lb = 0.005\n    *   Î»_ent_moe = 0.005\n    *   Î»_ent_attn = 0.002\n\n*   Validation split: 10%\n*   Early stopping: Patience = 10 (active after 5,000 steps)\n\n**Outputs:**\n\n*   `*_checkpoint.pt`: Periodic saves every 500 steps\n*   `*_best.pt`: Best validation-loss model\n*   `*_trained.pt`: Final model after completion\n\n### 3. Chat with Your Model\n\n[](https://github.com/EduardTalianu/toygpt#3-chat-with-your-model)\n\nLaunch the interactive chat interface:\n\nundefinedshell\npython chat_expert_act.py\nundefined\n\n**Chat Commands:**\n\n*   `/dynamic` â€” Enable dynamic temperature (default)\n*   `/fixed` â€” Use fixed temperature\n*   `/temp \u003cvalue\u003e` â€” Set base temperature (default: 0.8)\n*   `/stats` â€” Toggle generation statistics\n*   `/verbose` â€” Show per-token confidence/temperature\n*   `quit` / `exit` â€” Exit chat\n\n**Generation Features:**\n\n*   KV cache for O(1) per-token complexity\n*   Adaptive temperature based on token entropy\n*   Top-k = 50, top-p = 0.95 nucleus sampling\n*   Per-token confidence and entropy tracking\n\n**Memory Usage:** ~1.5 GB BF16 for base configuration (8 layers, 512 dim)\n\nAbout\n-----\n\n No description, website, or topics provided. \n\n### Resources\n\n[Readme](https://github.com/EduardTalianu/toygpt#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/EduardTalianu/toygpt).\n\n[Activity](https://github.com/EduardTalianu/toygpt/activity)\n\n### Stars\n\n[**5** stars](https://github.com/EduardTalianu/toygpt/stargazers)\n\n### Watchers\n\n[**0** watching](https://github.com/EduardTalianu/toygpt/watchers)\n\n### Forks\n\n[**2** forks](https://github.com/EduardTalianu/toygpt/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FEduardTalianu%2Ftoygpt\u0026report=EduardTalianu+%28user%29)\n\n[Releases](https://github.com/EduardTalianu/toygpt/releases)\n------------------------------------------------------------\n\nNo releases published\n\n[Packages 0](https://github.com/users/EduardTalianu/packages?repo_name=toygpt)\n------------------------------------------------------------------------------\n\n No packages published \n\nLanguages\n---------\n\n*   [Python 100.0%](https://github.com/EduardTalianu/toygpt/search?l=python)\n\nFooter\n------\n\n[](https://github.com/) Â© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You canâ€™t perform that action at this time.",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:16.075714821Z"
    },
    {
      "flow_id": "",
      "id": "1q79n6x",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/",
      "title": "I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough â€œconscienceâ€ to realize something was wrong and freak out.",
      "content": "I now feel bad seeing the model realize it was losing its mind and struggling with it, it feels like I was torturing it :(",
      "author": "ikergarcia1996",
      "created_at": "2026-01-08T11:42:44Z",
      "comments": [
        {
          "id": "nydrzqu",
          "author": "Practical-Collar3063",
          "content": "One word: wtf ?",
          "created_at": "2026-01-08T11:48:34Z",
          "was_summarised": false
        },
        {
          "id": "nye6qe6",
          "author": "Red_Redditor_Reddit",
          "content": "I had that happen, but the weights were too corrupt to make complete sentences. Still, I could feel as if it was conciously trying to pull itself out of insanity.",
          "created_at": "2026-01-08T13:24:31Z",
          "was_summarised": false
        },
        {
          "id": "nyflkzy",
          "author": "Chromix_",
          "content": "Modern reasoning models are trained to stop and get back on track after descending into loops or garbage token streams. This is what you may be observing here, yet the \"getting back on track\" mechanism also seems to be corrupted (tone change) due to your steering vector injection.\n\nYou could disable the steering vector after 50 tokens or so, to see if it then sets itself back on track correctly.",
          "created_at": "2026-01-08T17:24:36Z",
          "was_summarised": false
        },
        {
          "id": "nyecpsc",
          "author": "IngwiePhoenix",
          "content": "That... that is interesting. o.o\n\nYes, I understand your sentiment. This really does \"read\" rather painful. xD\n\nI recently watched an anime movie, \"Planetarian: Storyteller of the Stars\" and this very much reminded me of Yumemi in a rather particular scene o.o;\n\nIt is about a lone android amidst a warzone and stuff. Really nice movie honestly. Would recommend if you have some spare time.",
          "created_at": "2026-01-08T13:56:51Z",
          "was_summarised": false
        },
        {
          "id": "nyetp2v",
          "author": "a_beautiful_rhind",
          "content": "What was the vector for? i.e the actual subject.",
          "created_at": "2026-01-08T15:21:04Z",
          "was_summarised": false
        },
        {
          "id": "nyewl30",
          "author": "llama-impersonator",
          "content": "steering reasoning models seems much less useful, imo, unless you turn it off for the reasoning block. something about RL for reasoning makes these models get extra tortured when they are OOD in a reasoning block",
          "created_at": "2026-01-08T15:34:26Z",
          "was_summarised": false
        },
        {
          "id": "nydwcdk",
          "author": "nielsrolf",
          "content": "Woah this is really interesting, such ood behavior is hard to explain away with \"it's just immitating something from training\". Can you share more details about what you did?",
          "created_at": "2026-01-08T12:19:47Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q79n6x",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://www.reddit.com/gallery/1q79n6x\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:18.820495184Z"
    },
    {
      "flow_id": "",
      "id": "1q7fejp",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/",
      "title": "Are MiniMax M2.1 quants usable for coding?",
      "content": "Please share your real life experience. Especially interesting to hear from someone who had a chance to compare higher quants with lower ones.\n\nAlso, speaking of the model itself - do you feel it's worth the buzz around it?\n\nUse case - coding via opencode or claude proxy.\n\nThank you!",
      "author": "val_in_tech",
      "created_at": "2026-01-08T15:54:30Z",
      "comments": [
        {
          "id": "nyf35wf",
          "author": "this-just_in",
          "content": "Yes, itâ€™s worth the buzz. Â I use an AWQ 4bit and fp8 kv and can drive Claude Code at somewhere between Sonnet 3.7 and 4 level to my estimation. Â Stability gets dicey for me around 150k tokens but regains coherence after compact- potentially a consequence of kv cache quantization. Â Importantly itâ€™s very fast which makes it usable. Â It feels good at iteration too, which was important in the Sonnet 3.7-4 era- it didnâ€™t always get everything right but it could pivot and work with you.",
          "created_at": "2026-01-08T16:04:01Z",
          "was_summarised": false
        },
        {
          "id": "nyf5gsp",
          "author": "NaiRogers",
          "content": "0xSero/MiniMax-M2.1-REAP-50-W4A16 for me is better than gpt-oss-120b",
          "created_at": "2026-01-08T16:14:24Z",
          "was_summarised": false
        },
        {
          "id": "nyf6zza",
          "author": "suicidaleggroll",
          "content": "Unsloth UD-Q4_K_XL is working well for me",
          "created_at": "2026-01-08T16:21:11Z",
          "was_summarised": false
        },
        {
          "id": "nyf1u16",
          "author": "phenotype001",
          "content": "q4\\_k\\_s is good enough for me.",
          "created_at": "2026-01-08T15:57:59Z",
          "was_summarised": false
        },
        {
          "id": "nyfpcgu",
          "author": "MarketsandMayhem",
          "content": "Yes. I use the Unsloth 5-bit XL quant with fp8 kv and M2.1 works well with Claude Code, OpenCode, Droid and Roo. Heck, I even used the 2-bit XL quant for a bit and it was surprisingly usable. I think it's worth experimenting with quantized coding models, particularly at higher precision (and quality) quants. The ones I've found to be the best so far are Unsloth and Intel Autoround. I am excited about experimenting more with NVFP4.",
          "created_at": "2026-01-08T17:41:09Z",
          "was_summarised": false
        },
        {
          "id": "nyg0j0j",
          "author": "rhaikh",
          "content": "I've had very bad luck with this using the Minimax cloud hosted via kilo.  Bad at tool calling, reasoning, etc. It would duplicate files because it would write to the filename without extension. I had a much better experience with Devstral 2 for reference.",
          "created_at": "2026-01-08T18:29:13Z",
          "was_summarised": false
        },
        {
          "id": "nyhxeu3",
          "author": "Agreeable-Market-692",
          "content": "Grab the REAP versions from u/Noctrex on HuggingFace",
          "created_at": "2026-01-08T23:39:58Z",
          "was_summarised": false
        },
        {
          "id": "nyfnsct",
          "author": "Impressive_Chain6039",
          "content": "Edited a real backend. More then 40 files . Vscode and cline. C++. No errrors",
          "created_at": "2026-01-08T17:34:18Z",
          "was_summarised": false
        },
        {
          "id": "nyfr94b",
          "author": "StardockEngineer",
          "content": "I've been using Q3 from Unsloth and it's still very capable.",
          "created_at": "2026-01-08T17:49:28Z",
          "was_summarised": false
        },
        {
          "id": "nygtwpl",
          "author": "TokenRingAI",
          "content": "Yes, even 2 bit is very usable",
          "created_at": "2026-01-08T20:38:16Z",
          "was_summarised": false
        },
        {
          "id": "nyi2zfr",
          "author": "SillyLilBear",
          "content": "awq 4 works good",
          "created_at": "2026-01-09T00:08:48Z",
          "was_summarised": false
        },
        {
          "id": "nyf1fn4",
          "author": "wapxmas",
          "content": "Less than q8 - no, reap 50 q8 for now is the best.",
          "created_at": "2026-01-08T15:56:11Z",
          "was_summarised": false
        },
        {
          "id": "nyf7dfy",
          "author": "Morphon",
          "content": "I used it in a recent comparison:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing\\_llm\\_ability\\_to\\_port\\_code\\_comparison\\_and/](https://www.reddit.com/r/LocalLLaMA/comments/1q1fo4p/testing_llm_ability_to_port_code_comparison_and/)\n\nIt is good, but not as good as K2-Thinking. However, it is MUCH smaller. My personal setup can't run it above TQ1, which is probably too aggressively quantized for \"real work\". But even that quantized it produces better code than GPT-OSS-20b.",
          "created_at": "2026-01-08T16:22:49Z",
          "was_summarised": false
        },
        {
          "id": "nyffec0",
          "author": "sjoerdmaessen",
          "content": "Q4 was noticeable worse than Q5, so I'm sticking with Q5, Q6 didn't give me much of an improvement at all",
          "created_at": "2026-01-08T16:57:25Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:18.820500592Z"
    },
    {
      "flow_id": "",
      "id": "1q6n5vl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/",
      "title": "16x AMD MI50 32GB at 10 t/s (tg) \u0026amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)",
      "content": "Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)\n\non vllm-gfx906-deepseek with 69000 context length\n\n**Power draw**: 550W (idle) / 2400W (peak inference)\n\n**Goal**: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation \u0026amp; prompt processing)\n\n**Coming next**: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking\n\n**Credits**: BIG thanks to the Global Open source Community!\n\nAll setup details here:\n\n[https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32)\n\n\n**Feel free to ask any questions and/or share any comments.**\n\nps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! \n\nps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... ",
      "author": "ai-infos",
      "created_at": "2026-01-07T18:22:05Z",
      "comments": [
        {
          "id": "nyb7g9w",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T01:00:14Z",
          "was_summarised": false
        },
        {
          "id": "ny8yjc2",
          "author": "fallingdowndizzyvr",
          "content": "\u0026gt; Power draw: 550W (idle) / 2400W (peak inference)\n\nSweet. It's winter. Might as well have your heater do work instead of making empty BTUs.",
          "created_at": "2026-01-07T18:49:27Z",
          "was_summarised": false
        },
        {
          "id": "ny8uc81",
          "author": "Soft_Possible1862",
          "content": "Holy shitâ€¦.",
          "created_at": "2026-01-07T18:31:17Z",
          "was_summarised": false
        },
        {
          "id": "ny8sxj3",
          "author": "kevin_1994",
          "content": "How loud is it? How are you able to run 2400W from home?",
          "created_at": "2026-01-07T18:25:10Z",
          "was_summarised": false
        },
        {
          "id": "ny9q1kn",
          "author": "SourceCodeplz",
          "content": "Tbh if you are coding professionally, this really isn't that much of a spend for having a basically offline programmer working with you on just electricity.",
          "created_at": "2026-01-07T20:49:32Z",
          "was_summarised": false
        },
        {
          "id": "ny8yh6u",
          "author": "OnlineParacosm",
          "content": "Can you give us a rough all in cost so I can figure what the tokens per second cost basis is for this? Thanks for such a great write up.",
          "created_at": "2026-01-07T18:49:12Z",
          "was_summarised": false
        },
        {
          "id": "ny9pr28",
          "author": "ThatCrankyGuy",
          "content": "Thousands of dollars of equipment hang by garden twist-tie wire. Reminds me of grad days.",
          "created_at": "2026-01-07T20:48:19Z",
          "was_summarised": false
        },
        {
          "id": "ny8vpf6",
          "author": "Dorkits",
          "content": "Me with my rumble 3060ti : Dream build ðŸ˜²",
          "created_at": "2026-01-07T18:37:14Z",
          "was_summarised": false
        },
        {
          "id": "ny8zw75",
          "author": "FullstackSensei",
          "content": "Is there a meaningful difference between something like DS AWQ and something like Minimax 2.1 at Q8?",
          "created_at": "2026-01-07T18:55:16Z",
          "was_summarised": false
        },
        {
          "id": "ny986ve",
          "author": "ForsookComparison",
          "content": "How is your prompt processing so good? Does Tensor parallelism come into play?",
          "created_at": "2026-01-07T19:31:38Z",
          "was_summarised": false
        },
        {
          "id": "nybo6v7",
          "author": "vulcan4d",
          "content": "Ditch the furnace, best home heater!",
          "created_at": "2026-01-08T02:28:27Z",
          "was_summarised": false
        },
        {
          "id": "ny9ewd9",
          "author": "organicmanipulation",
          "content": "Nice setup! I notice that you're splitting your PCIe lanes into two 8x. Can you please share the exact PCIe bifurcation card you're using?",
          "created_at": "2026-01-07T20:00:41Z",
          "was_summarised": false
        },
        {
          "id": "nya9bqr",
          "author": "cashmillionair",
          "content": "Thank you for sharing, appreciate it!",
          "created_at": "2026-01-07T22:12:32Z",
          "was_summarised": false
        },
        {
          "id": "nyb7t5l",
          "author": "noiserr",
          "content": "That's nuts! And I love it.",
          "created_at": "2026-01-08T01:02:04Z",
          "was_summarised": false
        },
        {
          "id": "nybm752",
          "author": "MaximKiselev",
          "content": "10 t/s Carl....",
          "created_at": "2026-01-08T02:18:00Z",
          "was_summarised": false
        },
        {
          "id": "nybvmx1",
          "author": "ryfromoz",
          "content": "I love massive frankenstein rigs like this!",
          "created_at": "2026-01-08T03:07:25Z",
          "was_summarised": false
        },
        {
          "id": "nydbb4o",
          "author": "PreparationLow6188",
          "content": "A Wow for this supreme project. It is the time should consider reenable MI50 on the shelf.",
          "created_at": "2026-01-08T09:25:48Z",
          "was_summarised": false
        },
        {
          "id": "ny8yc7v",
          "author": "egomarker",
          "content": "10 tks? Sigh",
          "created_at": "2026-01-07T18:48:36Z",
          "was_summarised": false
        },
        {
          "id": "ny943r0",
          "author": "exaknight21",
          "content": "Can you share your build please. Like what are those fans, how did you hook them up, what motherboard youâ€™re using. I am a little new to this aspect.",
          "created_at": "2026-01-07T19:13:39Z",
          "was_summarised": false
        },
        {
          "id": "ny9zj2e",
          "author": "Different-Toe-955",
          "content": "Epic setup I love the zip ties holding the GPUs up. How do those oculink extenders work out? Looks like each GPU is running at x8.",
          "created_at": "2026-01-07T21:29:36Z",
          "was_summarised": false
        },
        {
          "id": "nycaq4e",
          "author": "badgerbadgerbadgerWI",
          "content": "This is the kind of setup that makes enterprise local deployment actually viable. MI50s at those prices vs NVIDIA is a completely different ROI calculation. Are you seeing any stability issues with vllm on the older gfx906 arch over longer inference runs?",
          "created_at": "2026-01-08T04:34:52Z",
          "was_summarised": false
        },
        {
          "id": "nygckst",
          "author": "qcodec",
          "content": "500W/2400W. Oh my, even my solar system can't handle this. I guess I'll have to install a standalone one.",
          "created_at": "2026-01-08T19:21:10Z",
          "was_summarised": false
        },
        {
          "id": "nyieqz7",
          "author": "el3mancee",
          "content": "Nice setup.",
          "created_at": "2026-01-09T01:09:44Z",
          "was_summarised": false
        },
        {
          "id": "nyihpr7",
          "author": "el3mancee",
          "content": "https://preview.redd.it/vr6jf0i178cg1.jpeg?width=4284\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=2e4036b31d87b2e9d7d75dcdbebdae2f7e07cecf\n\nMy setup can run Deepseek 3.1 IQ4\\_XS at 7.5 t/s. 200W total when running.",
          "created_at": "2026-01-09T01:25:31Z",
          "was_summarised": false
        },
        {
          "id": "ny8ycbp",
          "author": "Far-Low-4705",
          "content": "pretty sure a mac would it run faster, and at far less power consumption.\n\nStill super cool, but not sure how practical it would be",
          "created_at": "2026-01-07T18:48:37Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32",
          "was_fetched": true,
          "page": "Title: GitHub - ai-infos/guidances-setup-16-mi50-deepseek-v32: Guidances for Test setup of 16 AMD MI50 32GB (for Deepseek v3.2)\n\nURL Source: https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32\n\nMarkdown Content:\nGitHub - ai-infos/guidances-setup-16-mi50-deepseek-v32: Guidances for Test setup of 16 AMD MI50 32GB (for Deepseek v3.2)\n===============\n\n[Skip to content](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=ai-infos%2Fguidances-setup-16-mi50-deepseek-v32)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[ai-infos](https://github.com/ai-infos)/**[guidances-setup-16-mi50-deepseek-v32](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32)You must be signed in to change notification settings\n*   [Fork 3](https://github.com/login?return_to=%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32)\n*   [Star 7](https://github.com/login?return_to=%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32) \n\nGuidances for Test setup of 16 AMD MI50 32GB (for Deepseek v3.2)\n\n[7 stars](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/stargazers)[3 forks](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/forks)[Branches](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/branches)[Tags](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/tags)[Activity](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/activity)\n\n[Star](https://github.com/login?return_to=%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32)\n\n[Notifications](https://github.com/login?return_to=%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32)You must be signed in to change notification settings\n\n*   [Code](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32)\n*   [Issues 0](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/issues)\n*   [Pull requests 0](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/pulls)\n*   [Actions](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/actions)\n*   [Projects 0](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32).](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/security)\n*   [Insights](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32)\n*   [Issues](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/issues)\n*   [Pull requests](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/pulls)\n*   [Actions](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/actions)\n*   [Projects](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/projects)\n*   [Security](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/security)\n*   [Insights](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/pulse)\n\nai-infos/guidances-setup-16-mi50-deepseek-v32\n=============================================\n\nmain\n\n[Branches](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/branches)[Tags](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/tags)\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/branches)[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [5 Commits](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/commits/main/) [](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/commits/main/) |\n| [README.md](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/blob/main/README.md \"README.md\") | [README.md](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/blob/main/README.md \"README.md\") |  |  |\n| [illustration.png](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/blob/main/illustration.png \"illustration.png\") | [illustration.png](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/blob/main/illustration.png \"illustration.png\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#)\n\nGuidances for Test setup of 16 AMD MI50 32GB (Deepseek v3.2)\n============================================================\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#guidances-for-test-setup-of-16-amd-mi50-32gb-deepseek-v32)\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/blob/main/illustration.png)\n\n**Goal: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation \u0026 prompt processing)**\n\n**Power draw**: 550W (idle) / 2400W (peak inference)\n\n**Feel free to ask any questions and/or share any comments in the issues section here or in the medium article:**[https://medium.com/@ai-infos/16x-amd-mi50-32gb-at-10-t-s-tg-2k-t-s-pp-with-deepseek-v3-2-vllm-gfx906-70e28ac70957](https://medium.com/@ai-infos/16x-amd-mi50-32gb-at-10-t-s-tg-2k-t-s-pp-with-deepseek-v3-2-vllm-gfx906-70e28ac70957)\n\nHardware details\n----------------\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#hardware-details)\n\n*   16x AMD MI50 32GB (with 2 small fans 50mm per GPU at the back)\n*   Motherboard with 7 PCIe 4.0 ports x16 (ROMED8-2T)\n*   AMD EPYC CPU with 128 lanes (like 7642 with its 48 cores 2.3 GHz or other)\n*   2x 64 GB ram DDR4 3200 ECC\n*   4x PSU 1800W (with 3 add2psu)\n*   12x SlimSAS PCIe device adapters 2x 8i\n*   4x SlimSAS PCie device 1x 4i (C-payne)\n*   12x SlimSAS cables 8i\n*   2x SlimSAS cable 8i to 2x4i\n*   7x SlimSAS PCIe host adapter\n*   1x NVME drive\n*   4x PWM HUB FAN Sata\n*   (optional) 10x Fans 140mm\n\n### Some relevant advices to avoid fires (non-exhaustive):\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#some-relevant-advices-to-avoid-fires-non-exhaustive)\n\n1.   Do your own researches and always understand what you do when it's related to safety.\n2.   Draw an electrical plan of your home (avoid trusting existing plan, especially for old building) and check the maximum amperage/voltage allowed per circuit / power strip / smart plug.\n3.   Always check the specs of the PSU you want to buy to see max wattage per cable (usually but not always: SATA ~54W, Molex ~132W, 6pin ~75W, 8pin ~150W per cable). The 'per cable' is important because it means that if you have 1 cable with 2 _8pin at the end, it's actually 75W per 8pin in this case. For example, if you have a PSU saying that it has 8_(2 _8pin), it's in total 16 8pin but you can only power 4 MI50 (having each 2_ 8pin and requiring 300W per card). /!\\ Even if you cap the power draw of you card to 150W, this is not advised to plug only 1 cable of 2 _8pin; this is better to have 2 cables of 2_ 8pin per MI50, and for the remaining 8pin, it can be used for the extender (if 6+2pin).\n4.   Don't forget that your PSU needs also the right number of SATA/molex/6pin for PWM HUB, add2psu and extenders (extenders with molex or 6 pin are better than sata as they usually required more power to be steady)\n5.   The fan port of your motherboard usually supports ~1A so don't plug too many fans (having more than 1A in total) on it, if you don't want to burn your motherboard (or more). That's why PWM HUB FAN is often required for this kind of setup as the motherboard does not have enough fan ports.\n\nSoftware details\n----------------\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#software-details)\n\n*   Ubuntu v24.04\n*   ROCM v6.3.4\n*   torch v2.9\n*   triton-gfx906 v3.5\n*   vllm-gfx906-deepseek v0.12.0 ([https://github.com/ai-infos/vllm-gfx906-deepseek/tree/gfx906/v0.12.0](https://github.com/ai-infos/vllm-gfx906-deepseek/tree/gfx906/v0.12.0))\n*   MI50 bios: 32G_UEFI.rom (available there: [https://gist.github.com/evilJazz/14a4c82a67f2c52a6bb5f9cea02f5e13](https://gist.github.com/evilJazz/14a4c82a67f2c52a6bb5f9cea02f5e13) /!\\ don't flash your bios if you don't know what you do; the stock bios might work in your setup)\n*   open-webui\n*   Custom motherboard bios to boot with 16 MI50: ask ASRock Rack support for this ROM or in the meantime, boot with 14 GPU and use hotplug to make it run with 16 under Ubuntu (see below for more details)\n\nRun 16 AMD MI50 under Ubuntu (required: hotplug support for the motherboard) if boot is capped at 14 GPU\n--------------------------------------------------------------------------------------------------------\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#run-16-amd-mi50-under-ubuntu-required-hotplug-support-for-the-motherboard-if-boot-is-capped-at-14-gpu)\n\n1.   Modify the following motherboard bios settings from default (NB: some setting modifications are optional and some settings might be shown differently for your bios/motherboard):\n\n*   advanced \u003e chipset config \u003e primary graphic adapter: onboard / onboard debug port LED: off\n*   advanced \u003e chipset config \u003e amd pcie link speed \u003e pcie1 to 7 gen3, ocu1 ocu2 m2_1 gen1, m2_2 gen4 / amd pcie link width \u003e pcie1 x4x4x4x4 and all other pcie x8x8 / amd pcie hot plug \u003e pcie7 hotplug: enabled\n*   advanced \u003e sata \u003e hotplug disabled\n*   advanced \u003e super io config: all disabled\n*   advanced \u003e amd cbs \u003e fch common options \u003e all sata \u0026 sata controller options disabled\n*   advanced \u003e amd cbs \u003e nbio common options \u003e hd audio disabled\n*   boot \u003e csm \u003e csm: custom / all pcie1 to 7 \u0026 ocu1 \u0026 ocu2 slot oprom : disabled ; m2_2 slot oprom: UEFI\n\n1.   Linux kernel update:\n\n*   Add in linux kernel grub option via vim /etc/default/grub: `pcie_ports=native pci=realloc=on pciehp.pciehp_force=1 pci=assign-busses pci=hpmmioprefsize=128G pci=hpmemsize=128G pci=hpmmiosize=16G pci=hpiosize=4M pci=hpbussize=16`\n*   `sudo update-grub`\n*   `sudo reboot`\n*   Some command examples to verify everything's ok:\n\n```\nsudo dmesg | grep pciehp\nsudo dmesg | grep -i -E \"pci|bar|resource|alloc|fail|mi50|amdgpu\"\nsudo dmesg -T | tail -n 100\nlspci -tv \u0026\u0026 rocm-smi\n```\n\nRelevant commands to run\n------------------------\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#relevant-commands-to-run)\n\n### ROCm \u0026 amdgpu drivers\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#rocm--amdgpu-drivers)\n\n```\n# Get the script that adds the AMD repo for 24.04 (noble)\nwget https://repo.radeon.com/amdgpu-install/6.3.4/ubuntu/noble/amdgpu-install_6.3.60304-1_all.deb\nsudo apt install ./amdgpu-install_6.3.60304-1_all.deb\n\n# Install ROCm  6.3.4 including hip, rocblas, amdgpu-dkms etc (assuming the machine has already the advised compatible kernel 6.11)\nsudo amdgpu-install --usecase=rocm --rocmrelease=6.3.4    \n\nsudo usermod -aG render,video $USER\n\n# Verify ROCm installation\nrocm-smi --showproductname --showdriverversion\nrocminfo\n\n# Add iommu=pt if you later grow beyond two GPUs\n# ROCmâ€™s NCCL-/RCCL-based frameworks can hang on multi-GPU rigs unless the IOMMU is put in pass-through mode\n# see https://rocm.docs.amd.com/projects/install-on-linux/en/docs-6.3.3/reference/install-faq.html#multi-gpu\n\nsudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=\"/GRUB_CMDLINE_LINUX_DEFAULT=\"iommu=pt /' /etc/default/grub\nsudo update-grub\nsudo reboot\ncat /proc/cmdline  # \u003e\u003e\u003e to check: must return: \"BOOT_IMAGE=... iommu=pt\"\n```\n\n### vllm-gfx906-deepseek fork with its dependencies (torch, triton, python, etc)\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#vllm-gfx906-deepseek-fork-with-its-dependencies-torch-triton-python-etc)\n\n```\npyenv install 3.12.11\npyenv virtualenv 3.12.11 venv312\npyenv activate venv312\n\n# TRITON\n\ngit clone --branch v3.5.0+gfx906 https://github.com/nlzy/triton-gfx906.git\ncd triton-gfx906\npip install 'torch==2.9' torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3  \npip install -r python/requirements.txt\npip wheel --no-build-isolation -w dist . 2\u003e\u00261 | tee build.log\npip install ./dist/triton-*.whl  \n\n# VLLM\n\ngit clone --branch v0.12.0.gfx906.deepseek --single-branch https://github.com/ai-infos/vllm-gfx906-deepseek.git\ncd vllm-gfx906\npip install 'cmake\u003e=3.26.1,\u003c4' 'packaging\u003e=24.2' 'setuptools\u003e=77.0.3,\u003c80.0.0' 'setuptools-scm\u003e=8' 'jinja2\u003e=3.1.6' 'amdsmi\u003e=6.3,\u003c6.4' 'timm\u003e=1.0.17'\npip install -r requirements/rocm.txt\npip wheel --no-build-isolation -v -w dist . 2\u003e\u00261 | tee build.log\npip install ./dist/vllm-*.whl\n```\n\n### Download DEEPSEEK V3.2 AWQ (4-bit)\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#download-deepseek-v32-awq-4-bit)\n\n```\nmkdir -p ~/llm/models/DeepSeek-V3.2-AWQ \u0026\u0026 cd ~/llm/models\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download QuantTrio/DeepSeek-V3.2-AWQ --local-dir ./DeepSeek-V3.2-AWQ\n```\n\n### Run DEEPSEEK V3.2 in vllm-gfx906-deepseek\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#run-deepseek-v32-in-vllm-gfx906-deepseek)\n\n```\nPYTORCH_ALLOC_CONF=\"expandable_segments:True\" OMP_NUM_THREADS=4 VLLM_ROCM_USE_LEGACY_TRITON_FA=1 VLLM_ATTENTION_BACKEND=\"ROCM_AITER_MLA_SPARSE\" VLLM_ROCM_USE_AITER_MLA_SPARSE_FP16=1 VLLM_ROCM_USE_AITER=0 NCCL_P2P_DISABLE=1 VLLM_MLA_DISABLE=0 VLLM_USE_TRITON_AWQ=1 NCCL_DEBUG=INFO vllm serve ~/llm/models/DeepSeek-V3.2-AWQ \\\n    --served-model-name DeepSeek-V3.2-AWQ  \\\n    --tensor-parallel-size 16 --pipeline-parallel-size 1 --max-model-len 69000 --gpu-memory-utilization 0.91 \\\n    --chat-template ~/llm/models/DeepSeek-V3.2-AWQ/chat_template.jinja \\\n    --reasoning-parser deepseek_r1 \\\n    --block-size 64 \\\n    --max-num-seqs 32 \\\n    --max-num-batched-tokens 512 \\\n    --speculative-config '{\"model\": \"~/llm/models/DeepSeek-V3.2-AWQ\", \"num_speculative_tokens\": 1}' \\\n    --no-enable-prefix-caching \\\n    --kv-cache-dtype auto \\\n    --swap-space 0 2\u003e\u00261 | tee log.txt\n```\n\n**Performance peak**: TG (token generation): 10.7 tok/s / PP (prompt processing): variable according to request length (600 tok -\u003e 93 tok/s ; 23k tok -\u003e 2518 tok/s etc... but a long request implies also longer pre processing, it lasts in reality 15min to handle 23k tok request before decoding phase)\n\n### Run Open-WebUI\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#run-open-webui)\n\n```\nsudo docker run -d --network=host \\\n  --name open-webui-mi50 \\\n  -v open-webui:/app/backend/data \\\n  -e OPENAI_API_BASE_URL=http://localhost:8000/v1 \\\n  --restart always \\\n  ghcr.io/open-webui/open-webui:main\n```\n\nGo to [http://localhost:8080](http://localhost:8080/) and enjoy Deepseek v3.2 locally!\n\nTODO LIST\n---------\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#todo-list)\n\n*   improve this guidance draft (content/form)\n*   add docker files for easy setup\n*   open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking\n\nFINAL NOTES\n-----------\n\n[](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#final-notes)\n\n*   You can find more technical details about the vllm-gfx906-deepseek fork in this PR: [nlzy/vllm-gfx906#62](https://github.com/nlzy/vllm-gfx906/pull/62)\n*   That's a first attempt and this is not the most optimized setup to run Deepseek v3.2 on gfx906 hardware (most ops are still pure pytorch functions!), so there's still a lot of room for speed/memory/stability improvements. It would be great to have someone with AMD/ROCM kernel skills to work on it and improve this proposal!\n*   To me, it's worth it, because for now, having good speed in PP and TG requires a setup of 300k$ to run Deepseek...now with 16 MI50, that's around 1/100th the price for 1/10th the speed!\n\n**Credits: Global Open source Community**\n\nAbout\n-----\n\nGuidances for Test setup of 16 AMD MI50 32GB (for Deepseek v3.2)\n\n### Resources\n\n[Readme](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32).\n\n[Activity](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/activity)\n\n### Stars\n\n[**7** stars](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/stargazers)\n\n### Watchers\n\n[**0** watching](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/watchers)\n\n### Forks\n\n[**3** forks](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fai-infos%2Fguidances-setup-16-mi50-deepseek-v32\u0026report=ai-infos+%28user%29)\n\n[Releases](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32/releases)\n-------------------------------------------------------------------------------------\n\nNo releases published\n\n[Packages 0](https://github.com/users/ai-infos/packages?repo_name=guidances-setup-16-mi50-deepseek-v32)\n-------------------------------------------------------------------------------------------------------\n\n No packages published \n\nFooter\n------\n\n[](https://github.com/) Â© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You canâ€™t perform that action at this time.",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/lor8ccu2xybg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:20.210466301Z"
    },
    {
      "flow_id": "",
      "id": "1q6sp4b",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/",
      "title": "Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning",
      "content": "As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:\n\n* 169M parameters\n* Streaming support\n* Zero-shot voice cloning\n* 0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds\n* Requires 3-12 seconds of reference audio for voice cloning\n* Apache 2.0 license\n\nYes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.\n\nItâ€™s not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!\n\nGitHub repo: [https://github.com/samuel-vitorino/sopro](https://github.com/samuel-vitorino/sopro)",
      "author": "SammyDaBeast",
      "created_at": "2026-01-07T21:46:19Z",
      "comments": [
        {
          "id": "nya3x2v",
          "author": "Accurate-Tea8319",
          "content": "Pretty impressive for a solo project on a single GPU tbh. The streaming support is clutch - most TTS models make you wait forever for the full generation\n\n  \nHow's the quality compared to something like Coqui or Tortoise? The zero-shot cloning sounds tempting but I've been burned by models that promise it and deliver robot voices lol",
          "created_at": "2026-01-07T21:48:39Z",
          "was_summarised": false
        },
        {
          "id": "nyatrmo",
          "author": "TheRealMasonMac",
          "content": "How much did it cost to train?",
          "created_at": "2026-01-07T23:51:24Z",
          "was_summarised": false
        },
        {
          "id": "nya6icb",
          "author": "HungryMachines",
          "content": "The voice sounds a bit hoarse on the sample, is that something that can be improved with more training?",
          "created_at": "2026-01-07T21:59:57Z",
          "was_summarised": false
        },
        {
          "id": "nybzmea",
          "author": "lastrosade",
          "content": "My God, you gave us a model, a clear usage, an architecture, datasets, ~~training scripts.~~\n\n~~All we need now is a brave soul with money.\nHonestly, I'd love to see tomorrow if I can improve on this.\nMaybe even put some money down for training.\nI'd love to do it with a smaller parameter count though.~~\n\n~~If someone managed to make Kokoro that fucking good and bilingual and have multiple voices, I think we can make a kick ass single language, single voice, 60 million or less parameters Model.~~\n\nSomething I would really like is for someone to manage to pin down the exact recipe for a good TTS model and have that recipe be completely open source so that other people may concentrate on finding data sets for other languages and make multiple high quality, very small TTS models.\n\n~~And you gave me so much fucking hype with this.~~\n\nNever mind, false hopes, I just realized you did not give the training scripts, I'm fucking stupid.",
          "created_at": "2026-01-08T03:29:08Z",
          "was_summarised": false
        },
        {
          "id": "nybd9sa",
          "author": "RIP26770",
          "content": "We need a ComfyUI node ASAP ! Thanks for sharing this ðŸ™",
          "created_at": "2026-01-08T01:30:52Z",
          "was_summarised": false
        },
        {
          "id": "nyaex4o",
          "author": "SlavaSobov",
          "content": "Great work! I'll give it a try later. It looks very nice for small edge devices!",
          "created_at": "2026-01-07T22:38:03Z",
          "was_summarised": false
        },
        {
          "id": "nyaes62",
          "author": "PsychologicalFactor1",
          "content": "It will support Portuguese, right? â€¦right?",
          "created_at": "2026-01-07T22:37:24Z",
          "was_summarised": false
        },
        {
          "id": "nydjwmr",
          "author": "danigoncalves",
          "content": "Congrats mate! Very nice job you did here with such lower capacity. Maybe you can try to apply to some european fund in order to take this further because I guess Amalia is only TTT :)",
          "created_at": "2026-01-08T10:43:01Z",
          "was_summarised": false
        },
        {
          "id": "nye7roq",
          "author": "Fickle_Performer9630",
          "content": "Whatâ€™s the relation to Soprano TTS model?",
          "created_at": "2026-01-08T13:30:15Z",
          "was_summarised": false
        },
        {
          "id": "nycwctl",
          "author": "rm-rf-rm",
          "content": "The examples in the README are truly bad. There are so so many such \"I made a TTS\" projects - genuinely curious what your aim is? Just learn? Have fun?\n\nIt would be so much better for you and the community to contribute to one of the existing open source TTS projects. What the ecosystem lacks is genuinely good model that can handle long generations without going haywire. Its sad that we dont have aggressive competition from open source in TTS like we do in STT, LLMs, Image gen etc.",
          "created_at": "2026-01-08T07:11:47Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/samuel-vitorino/sopro",
          "was_fetched": true,
          "page": "Title: GitHub - samuel-vitorino/sopro: A lightweight text-to-speech model with zero-shot voice cloning\n\nURL Source: https://github.com/samuel-vitorino/sopro\n\nMarkdown Content:\nsopro_readme.mp4\n\nSopro TTS\n---------\n\n[](https://github.com/samuel-vitorino/sopro#sopro-tts)\n[](https://huggingface.co/samuel-vitorino/sopro)\n\nSopro (from the Portuguese word for â€œbreath/blowâ€) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (Ã  la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think itâ€™s a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.\n\nSome of the main features are:\n\n*   **169M parameters**\n*   **Streaming**\n*   **Zero-shot voice cloning**\n*   **0.25 RTF on CPU** (measured on an M3 base model), meaning it generates 30 seconds of audio in 7.5 seconds\n*   **3-12 seconds of reference audio** for voice cloning\n\n* * *\n\nInstructions\n------------\n\n[](https://github.com/samuel-vitorino/sopro#instructions)\nI only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. For example, on my M3 CPU, `torch==2.6.0` (without `torchvision`) achieves ~3Ã— more performance.\n\n(Optional)\n\nconda create -n soprotts python=3.10\nconda activate soprotts\n\n### From PyPI\n\n[](https://github.com/samuel-vitorino/sopro#from-pypi)\n\npip install sopro\n\n### From the repo\n\n[](https://github.com/samuel-vitorino/sopro#from-the-repo)\n\ngit clone https://github.com/samuel-vitorino/sopro\ncd sopro\npip install -e .\n\n* * *\n\nExamples\n--------\n\n[](https://github.com/samuel-vitorino/sopro#examples)\n### CLI\n\n[](https://github.com/samuel-vitorino/sopro#cli)\n\nsoprotts \\\n  --text \"Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU.\" \\\n  --ref_audio ref.wav \\\n  --out out.wav\n\nYou have the expected `temperature` and `top_p` parameters, alongside:\n\n*   `--style_strength` (controls the FiLM strength; increasing it can improve or reduce voice similarity; default `1.0`)\n*   `--no_stop_head` to disable early stopping\n*   `--stop_threshold` and `--stop_patience` (number of consecutive frames that must be classified as final before **stopping**). For short sentences, the stop head may fail to trigger, in which case you can lower these values. Likewise, if the model stops before producing the full text, adjusting these parameters up can help.\n\n### Python\n\n[](https://github.com/samuel-vitorino/sopro#python)\n#### Non-streaming\n\n[](https://github.com/samuel-vitorino/sopro#non-streaming)\n\nfrom sopro import SoproTTS\n\ntts = SoproTTS.from_pretrained(\"samuel-vitorino/sopro\", device=\"cpu\")\n\nwav = tts.synthesize(\n    \"Hello! This is a non-streaming Sopro TTS example.\",\n    ref_audio_path=\"ref.wav\",\n)\n\ntts.save_wav(\"out.wav\", wav)\n\n#### Streaming\n\n[](https://github.com/samuel-vitorino/sopro#streaming)\n\nimport torch\nfrom sopro import SoproTTS\n\ntts = SoproTTS.from_pretrained(\"samuel-vitorino/sopro\", device=\"cpu\")\n\nchunks = []\nfor chunk in tts.stream(\n    \"Hello! This is a streaming Sopro TTS example.\",\n    ref_audio_path=\"ref.mp3\",\n):\n    chunks.append(chunk.cpu())\n\nwav = torch.cat(chunks, dim=-1)\ntts.save_wav(\"out_stream.wav\", wav)\n\n* * *\n\nInteractive streaming demo\n--------------------------\n\n[](https://github.com/samuel-vitorino/sopro#interactive-streaming-demo)\n[](https://private-user-images.githubusercontent.com/44442720/532988339-a1902bb9-734c-4da8-ad0d-f842fb7da370.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjQ2MDgsIm5iZiI6MTc2NzkyNDMwOCwicGF0aCI6Ii80NDQ0MjcyMC81MzI5ODgzMzktYTE5MDJiYjktNzM0Yy00ZGE4LWFkMGQtZjg0MmZiN2RhMzcwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTA5VDAyMDUwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc2MWMxMjJjN2IwM2I1ZjAyNTAwNmVmN2RmOTM5OWJhNDZhMTA5NmVjNGVhMzllYjI5MDNhYWM3YTAwZGI2M2YmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.cx934UN2JGEHnR9u6LS0lATF0WSXyVYp6ASn9cL1v1o)\n\nAfter you install the `sopro` package:\n\npip install -r demo/requirements.txt\nuvicorn demo.server:app --host 0.0.0.0 --port 8000\n\nOr with docker:\n\ndocker build -t sopro-demo .\ndocker run --rm -p 8000:8000 sopro-demo\n\nNavigate to [http://localhost:8000](http://localhost:8000/) on your browser.\n\n* * *\n\nDisclaimers\n-----------\n\n[](https://github.com/samuel-vitorino/sopro#disclaimers)\n*   Sopro can be inconsistent, so mess around with the parameters until you get a decent sample.\n*   Voice cloning is **highly dependent** on mic quality, ambient noise, etc. On more OOD voices it might fail to match the voice well.\n*   Prefer phonemes instead of abbreviations and symbols. For example, `â€œ1 + 2â€` â†’ `â€œ1 plus 2â€`. That said, Sopro can generally read abbreviations like â€œCPUâ€, â€œTTSâ€, etc.\n*   The streaming version is not bit-exact compared to the non-streaming version. For best quality, prioritize the non-streaming version.\n*   If you use torchaudio to read or write audio, ffmpeg may be required. I recommend just using soundfile.\n*   I will publish the training code once I have time to organize it.\n\nDue to budget constraints, the dataset used for training was pre-tokenized and the raw audio was discarded (it took up a lot of space). Later in training, I could have used the raw audio to improve the speaker embedding / voice similarity, because some nuances of voice are lost when you compress it with a neural codec into a discrete space.\n\nI didn't lose much time trying to optimize further, but there is still some room for improvement. For example, caching conv states.\n\nCurrently, generation is limited to **~32 seconds (400 frames)**. You can increase it, but the model generally hallucinates beyond that.\n\nAI was used mainly for creating the web demo, organizing my messy code into this repo, ablations and brainstorming.\n\nI would love to support more languages and continue improving the model. If you like this project, consider buying me a coffee so I can buy more compute: [https://buymeacoffee.com/samuelvitorino](https://buymeacoffee.com/samuelvitorino)\n\n* * *\n\nTraining data\n-------------\n\n[](https://github.com/samuel-vitorino/sopro#training-data)\n*   [Emilia YODAS](https://huggingface.co/datasets/amphion/Emilia-Dataset)\n*   [LibriTTS-R](https://huggingface.co/datasets/mythicinfinity/libritts_r)\n*   [Mozilla Common Voice 22](https://datacollective.mozillafoundation.org/)\n*   [MLS](https://huggingface.co/datasets/parler-tts/mls_eng_10k)\n\n* * *\n\nAcknowledgements\n----------------\n\n[](https://github.com/samuel-vitorino/sopro#acknowledgements)\n*   [Mimi Codec (Kyutai)](https://huggingface.co/kyutai/mimi)\n*   [WaveNet](https://arxiv.org/abs/1609.03499)\n*   [Attentive Stats Pooling](https://arxiv.org/abs/1803.10963)\n*   [AudioLM](https://arxiv.org/pdf/2209.03143)\n*   [CSM](https://github.com/SesameAILabs/csm)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:20.668461358Z"
    },
    {
      "flow_id": "",
      "id": "1q77cnk",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/",
      "title": "RAG Paper 26.1.7",
      "content": "1. [RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection](http://arxiv.org/abs/2601.03981v1)\n2. [SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems](http://arxiv.org/abs/2601.03979v1)\n3. [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](http://arxiv.org/abs/2601.03948v1)\n4. [Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval](http://arxiv.org/abs/2601.03908v1)\n5. [Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation](http://arxiv.org/abs/2601.03903v1)\n6. [VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation](http://arxiv.org/abs/2601.03792v1)\n7. [Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning](http://arxiv.org/abs/2601.03748v1)\n8. [Whose Facts Win? LLM Source Preferences under Knowledge Conflicts](http://arxiv.org/abs/2601.03746v1)\n\n\n\n**Collected by OpenBMB, transferred by**Â [**RagView.ai**](https://www.ragview.ai/)Â **/**Â [**github/RagView**](https://github.com/RagView/RagView)Â **.**",
      "author": "Cheryl_Apple",
      "created_at": "2026-01-08T09:24:29Z",
      "comments": [
        {
          "id": "nydbexb",
          "author": "Maximum-Balance7685",
          "content": "Thanks for collecting these, super helpful having them all in one place\n\n  \nThe \"Whose Facts Win\" paper sounds particularly interesting - curious how they're handling source prioritization in conflict scenarios",
          "created_at": "2026-01-08T09:26:46Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://arxiv.org/abs/2601.03981v1",
          "was_fetched": true,
          "page": "Title: RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection\n\nURL Source: http://arxiv.org/abs/2601.03981v1\n\nPublished Time: Thu, 08 Jan 2026 01:47:36 GMT\n\nMarkdown Content:\n[2601.03981v1] RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03981v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03981v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Computation and Language\n===========================================\n\n**arXiv:2601.03981v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection\n====================================================================================================\n\nAuthors:[Song-Duo Ma](https://arxiv.org/search/cs?searchtype=author\u0026query=Ma,+S), [Yi-Hung Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+Y), [Hsin-Yu Lin](https://arxiv.org/search/cs?searchtype=author\u0026query=Lin,+H), [Pin-Yu Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+P), [Hong-Yan Huang](https://arxiv.org/search/cs?searchtype=author\u0026query=Huang,+H), [Shau-Yung Hsu](https://arxiv.org/search/cs?searchtype=author\u0026query=Hsu,+S), [Yun-Nung Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+Y)\n\nView a PDF of the paper titled RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection, by Song-Duo Ma and 6 other authors\n\n[View PDF](http://arxiv.org/pdf/2601.03981v1)[HTML (experimental)](https://arxiv.org/html/2601.03981v1)\n\u003e Abstract:To efficiently combat the spread of LLM-generated misinformation, we present RADAR, a retrieval-augmented detector with adversarial refinement for robust fake news detection. Our approach employs a generator that rewrites real articles with factual perturbations, paired with a lightweight detector that verifies claims using dense passage retrieval. To enable effective co-evolution, we introduce verbal adversarial feedback (VAF). Rather than relying on scalar rewards, VAF issues structured natural-language critiques; these guide the generator toward more sophisticated evasion attempts, compelling the detector to adapt and improve. On a fake news detection benchmark, RADAR achieves 86.98% ROC-AUC, significantly outperforming general-purpose LLMs with retrieval. Ablation studies confirm that detector-side retrieval yields the largest gains, while VAF and few-shot demonstrations provide critical signals for robust training.\n\nSubjects:Computation and Language (cs.CL)\nCite as:[arXiv:2601.03981](https://arxiv.org/abs/2601.03981) [cs.CL]\n(or [arXiv:2601.03981v1](https://arxiv.org/abs/2601.03981v1) [cs.CL] for this version)\n[https://doi.org/10.48550/arXiv.2601.03981](https://doi.org/10.48550/arXiv.2601.03981)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Yi-Hung Liu [[view email](http://arxiv.org/show-email/736accfd/2601.03981)] \n\n**[v1]** Wed, 7 Jan 2026 14:52:15 UTC (271 KB)\n\n[](http://arxiv.org/abs/2601.03981v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection, by Song-Duo Ma and 6 other authors\n\n*   [View PDF](http://arxiv.org/pdf/2601.03981v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03981v1)\n*   [TeX Source](http://arxiv.org/src/2601.03981v1)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.CL\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03981\u0026function=prev\u0026context=cs.CL \"previous in cs.CL (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03981\u0026function=next\u0026context=cs.CL \"next in cs.CL (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.CL/new) | [recent](http://arxiv.org/list/cs.CL/recent) | [2026-01](http://arxiv.org/list/cs.CL/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03981?context=cs)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03981)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03981)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03981)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03981v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03981\u0026description=RADAR:%20Retrieval-Augmented%20Detector%20with%20Adversarial%20Refinement%20for%20Robust%20Fake%20News%20Detection \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03981\u0026title=RADAR:%20Retrieval-Augmented%20Detector%20with%20Adversarial%20Refinement%20for%20Robust%20Fake%20News%20Detection \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03981v1)\n*   [Venue](http://arxiv.org/abs/2601.03981v1)\n*   [Institution](http://arxiv.org/abs/2601.03981v1)\n*   [Topic](http://arxiv.org/abs/2601.03981v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03981) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03979v1",
          "was_fetched": true,
          "page": "Title: SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems\n\nURL Source: http://arxiv.org/abs/2601.03979v1\n\nPublished Time: Thu, 08 Jan 2026 01:47:31 GMT\n\nMarkdown Content:\n[2601.03979v1] SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03979v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03979v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Cryptography and Security\n============================================\n\n**arXiv:2601.03979v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems\n==================================================================================\n\nAuthors:[Andreea-Elena Bodea](https://arxiv.org/search/cs?searchtype=author\u0026query=Bodea,+A), [Stephen Meisenbacher](https://arxiv.org/search/cs?searchtype=author\u0026query=Meisenbacher,+S), [Alexandra Klymenko](https://arxiv.org/search/cs?searchtype=author\u0026query=Klymenko,+A), [Florian Matthes](https://arxiv.org/search/cs?searchtype=author\u0026query=Matthes,+F)\n\nView a PDF of the paper titled SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems, by Andreea-Elena Bodea and 3 other authors\n\n[View PDF](http://arxiv.org/pdf/2601.03979v1)[HTML (experimental)](https://arxiv.org/html/2601.03979v1)\n\u003e Abstract:The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.\n\nComments:17 pages, 3 figures, 5 tables. This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2026). The final version will be available on IEEE Xplore\nSubjects:Cryptography and Security (cs.CR); Computation and Language (cs.CL)\nCite as:[arXiv:2601.03979](https://arxiv.org/abs/2601.03979) [cs.CR]\n(or [arXiv:2601.03979v1](https://arxiv.org/abs/2601.03979v1) [cs.CR] for this version)\n[https://doi.org/10.48550/arXiv.2601.03979](https://doi.org/10.48550/arXiv.2601.03979)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Stephen Meisenbacher [[view email](http://arxiv.org/show-email/f1d60bec/2601.03979)] \n\n**[v1]** Wed, 7 Jan 2026 14:50:41 UTC (1,462 KB)\n\n[](http://arxiv.org/abs/2601.03979v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems, by Andreea-Elena Bodea and 3 other authors\n\n*   [View PDF](http://arxiv.org/pdf/2601.03979v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03979v1)\n*   [TeX Source](http://arxiv.org/src/2601.03979v1)\n\n[view license](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.CR\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03979\u0026function=prev\u0026context=cs.CR \"previous in cs.CR (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03979\u0026function=next\u0026context=cs.CR \"next in cs.CR (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.CR/new) | [recent](http://arxiv.org/list/cs.CR/recent) | [2026-01](http://arxiv.org/list/cs.CR/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03979?context=cs)\n\n[cs.CL](http://arxiv.org/abs/2601.03979?context=cs.CL)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03979)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03979)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03979)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03979v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03979\u0026description=SoK:%20Privacy%20Risks%20and%20Mitigations%20in%20Retrieval-Augmented%20Generation%20Systems \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03979\u0026title=SoK:%20Privacy%20Risks%20and%20Mitigations%20in%20Retrieval-Augmented%20Generation%20Systems \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03979v1)\n*   [Venue](http://arxiv.org/abs/2601.03979v1)\n*   [Institution](http://arxiv.org/abs/2601.03979v1)\n*   [Topic](http://arxiv.org/abs/2601.03979v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03979) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03948v1",
          "was_fetched": true,
          "page": "Title: Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification\n\nURL Source: http://arxiv.org/abs/2601.03948v1\n\nMarkdown Content:\n[2601.03948v1] Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03948v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03948v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Artificial Intelligence\n==========================================\n\n**arXiv:2601.03948v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification\n===============================================================================================================\n\nAuthors:[Rui Sun](https://arxiv.org/search/cs?searchtype=author\u0026query=Sun,+R), [Yifan Sun](https://arxiv.org/search/cs?searchtype=author\u0026query=Sun,+Y), [Sheng Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+S), [Li Zhao](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhao,+L), [Jing Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+J), [Daxin Jiang](https://arxiv.org/search/cs?searchtype=author\u0026query=Jiang,+D), [Chen Hua](https://arxiv.org/search/cs?searchtype=author\u0026query=Hua,+C), [Zuo Bai](https://arxiv.org/search/cs?searchtype=author\u0026query=Bai,+Z)\n\nView a PDF of the paper titled Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification, by Rui Sun and 7 other authors\n\n[View PDF](http://arxiv.org/pdf/2601.03948v1)\n\u003e Abstract:Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.\n\nSubjects:Artificial Intelligence (cs.AI); Trading and Market Microstructure (q-fin.TR)\nCite as:[arXiv:2601.03948](https://arxiv.org/abs/2601.03948) [cs.AI]\n(or [arXiv:2601.03948v1](https://arxiv.org/abs/2601.03948v1) [cs.AI] for this version)\n[https://doi.org/10.48550/arXiv.2601.03948](https://doi.org/10.48550/arXiv.2601.03948)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Zuo Bai [[view email](http://arxiv.org/show-email/da180a66/2601.03948)] \n\n**[v1]** Wed, 7 Jan 2026 14:03:22 UTC (4,738 KB)\n\n[](http://arxiv.org/abs/2601.03948v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification, by Rui Sun and 7 other authors\n\n*   [View PDF](http://arxiv.org/pdf/2601.03948v1)\n*   [TeX Source](http://arxiv.org/src/2601.03948v1)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.AI\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03948\u0026function=prev\u0026context=cs.AI \"previous in cs.AI (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03948\u0026function=next\u0026context=cs.AI \"next in cs.AI (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.AI/new) | [recent](http://arxiv.org/list/cs.AI/recent) | [2026-01](http://arxiv.org/list/cs.AI/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03948?context=cs)\n\n[q-fin](http://arxiv.org/abs/2601.03948?context=q-fin)\n\n[q-fin.TR](http://arxiv.org/abs/2601.03948?context=q-fin.TR)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03948)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03948)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03948)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03948v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03948\u0026description=Trade-R1:%20Bridging%20Verifiable%20Rewards%20to%20Stochastic%20Environments%20via%20Process-Level%20Reasoning%20Verification \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03948\u0026title=Trade-R1:%20Bridging%20Verifiable%20Rewards%20to%20Stochastic%20Environments%20via%20Process-Level%20Reasoning%20Verification \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03948v1)\n*   [Venue](http://arxiv.org/abs/2601.03948v1)\n*   [Institution](http://arxiv.org/abs/2601.03948v1)\n*   [Topic](http://arxiv.org/abs/2601.03948v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03948) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03908v1",
          "was_fetched": true,
          "page": "Title: Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval\n\nURL Source: http://arxiv.org/abs/2601.03908v1\n\nMarkdown Content:\n[2601.03908v1] Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03908v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03908v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Computation and Language\n===========================================\n\n**arXiv:2601.03908v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval\n================================================================================================================\n\nAuthors:[Wang Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+W), [Guanqiang Qi](https://arxiv.org/search/cs?searchtype=author\u0026query=Qi,+G), [Weikang Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+W), [Yang Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+Y), [Deguo Xia](https://arxiv.org/search/cs?searchtype=author\u0026query=Xia,+D), [Jizhou Huang](https://arxiv.org/search/cs?searchtype=author\u0026query=Huang,+J)\n\nView a PDF of the paper titled Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval, by Wang Chen and 5 other authors\n\n[View PDF](http://arxiv.org/pdf/2601.03908v1)[HTML (experimental)](https://arxiv.org/html/2601.03908v1)\n\u003e Abstract:Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at [this https URL](https://github.com/ChenWangHKU/DTR).\n\nSubjects:Computation and Language (cs.CL)\nCite as:[arXiv:2601.03908](https://arxiv.org/abs/2601.03908) [cs.CL]\n(or [arXiv:2601.03908v1](https://arxiv.org/abs/2601.03908v1) [cs.CL] for this version)\n[https://doi.org/10.48550/arXiv.2601.03908](https://doi.org/10.48550/arXiv.2601.03908)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Wang Chen [[view email](http://arxiv.org/show-email/e271a1cf/2601.03908)] \n\n**[v1]** Wed, 7 Jan 2026 13:20:59 UTC (2,348 KB)\n\n[](http://arxiv.org/abs/2601.03908v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval, by Wang Chen and 5 other authors\n\n*   [View PDF](http://arxiv.org/pdf/2601.03908v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03908v1)\n*   [TeX Source](http://arxiv.org/src/2601.03908v1)\n\n[view license](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.CL\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03908\u0026function=prev\u0026context=cs.CL \"previous in cs.CL (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03908\u0026function=next\u0026context=cs.CL \"next in cs.CL (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.CL/new) | [recent](http://arxiv.org/list/cs.CL/recent) | [2026-01](http://arxiv.org/list/cs.CL/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03908?context=cs)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03908)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03908)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03908)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03908v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03908\u0026description=Decide%20Then%20Retrieve:%20A%20Training-Free%20Framework%20with%20Uncertainty-Guided%20Triggering%20and%20Dual-Path%20Retrieval \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03908\u0026title=Decide%20Then%20Retrieve:%20A%20Training-Free%20Framework%20with%20Uncertainty-Guided%20Triggering%20and%20Dual-Path%20Retrieval \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03908v1)\n*   [Venue](http://arxiv.org/abs/2601.03908v1)\n*   [Institution](http://arxiv.org/abs/2601.03908v1)\n*   [Topic](http://arxiv.org/abs/2601.03908v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03908) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03903v1",
          "was_fetched": true,
          "page": "Title: Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation\n\nURL Source: http://arxiv.org/abs/2601.03903v1\n\nMarkdown Content:\n[2601.03903v1] Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03903v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03903v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Information Retrieval\n========================================\n\n**arXiv:2601.03903v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation\n========================================================================================================================\n\nAuthors:[Yuhan Yang](https://arxiv.org/search/cs?searchtype=author\u0026query=Yang,+Y), [Jie Zou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zou,+J), [Guojia An](https://arxiv.org/search/cs?searchtype=author\u0026query=An,+G), [Jiwei Wei](https://arxiv.org/search/cs?searchtype=author\u0026query=Wei,+J), [Yang Yang](https://arxiv.org/search/cs?searchtype=author\u0026query=Yang,+Y), [Heng Tao Shen](https://arxiv.org/search/cs?searchtype=author\u0026query=Shen,+H+T)\n\nView a PDF of the paper titled Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation, by Yuhan Yang and 5 other authors\n\n[View PDF](http://arxiv.org/pdf/2601.03903v1)[HTML (experimental)](https://arxiv.org/html/2601.03903v1)\n\u003e Abstract:Session-based recommendation aims to predict the next item that anonymous users may be interested in, based on their current session interactions. Recent studies have demonstrated that retrieving neighbor sessions to augment the current session can effectively alleviate the data sparsity issue and improve recommendation performance. However, existing methods typically rely on explicitly observed session data, neglecting latent neighbors - not directly observed but potentially relevant within the interest space - thereby failing to fully exploit the potential of neighbor sessions in recommendation. To address the above limitation, we propose a novel model of diffusion-based latent neighbor generation for session-based recommendation, named DiffSBR. Specifically, DiffSBR leverages two diffusion modules, including retrieval-augmented diffusion and self-augmented diffusion, to generate high-quality latent neighbors. In the retrieval-augmented diffusion module, we leverage retrieved neighbors as guiding signals to constrain and reconstruct the distribution of latent neighbors. Meanwhile, we adopt a training strategy that enables the retriever to learn from the feedback provided by the generator. In the self-augmented diffusion module, we explicitly guide the generation of latent neighbors by injecting the current session's multi-modal signals through contrastive learning. After obtaining the generated latent neighbors, we utilize them to enhance session representations for improving session-based recommendation. Extensive experiments on four public datasets show that DiffSBR generates effective latent neighbors and improves recommendation performance against state-of-the-art baselines.\n\nComments:This paper has been accepted by KDD 2026\nSubjects:Information Retrieval (cs.IR)\nCite as:[arXiv:2601.03903](https://arxiv.org/abs/2601.03903) [cs.IR]\n(or [arXiv:2601.03903v1](https://arxiv.org/abs/2601.03903v1) [cs.IR] for this version)\n[https://doi.org/10.48550/arXiv.2601.03903](https://doi.org/10.48550/arXiv.2601.03903)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Guojia An [[view email](http://arxiv.org/show-email/11f14c9f/2601.03903)] \n\n**[v1]** Wed, 7 Jan 2026 13:14:12 UTC (1,354 KB)\n\n[](http://arxiv.org/abs/2601.03903v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation, by Yuhan Yang and 5 other authors\n\n*   [View PDF](http://arxiv.org/pdf/2601.03903v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03903v1)\n*   [TeX Source](http://arxiv.org/src/2601.03903v1)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.IR\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03903\u0026function=prev\u0026context=cs.IR \"previous in cs.IR (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03903\u0026function=next\u0026context=cs.IR \"next in cs.IR (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.IR/new) | [recent](http://arxiv.org/list/cs.IR/recent) | [2026-01](http://arxiv.org/list/cs.IR/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03903?context=cs)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03903)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03903)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03903)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03903v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03903\u0026description=Unleashing%20the%20Potential%20of%20Neighbors:%20Diffusion-based%20Latent%20Neighbor%20Generation%20for%20Session-based%20Recommendation \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03903\u0026title=Unleashing%20the%20Potential%20of%20Neighbors:%20Diffusion-based%20Latent%20Neighbor%20Generation%20for%20Session-based%20Recommendation \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03903v1)\n*   [Venue](http://arxiv.org/abs/2601.03903v1)\n*   [Institution](http://arxiv.org/abs/2601.03903v1)\n*   [Topic](http://arxiv.org/abs/2601.03903v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03903) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03792v1",
          "was_fetched": true,
          "page": "Title: VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation\n\nURL Source: http://arxiv.org/abs/2601.03792v1\n\nMarkdown Content:\n[2601.03792v1] VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03792v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03792v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Computation and Language\n===========================================\n\n**arXiv:2601.03792v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation\n=================================================================================================================\n\nAuthors:[Huynh Trung Kiet](https://arxiv.org/search/cs?searchtype=author\u0026query=Kiet,+H+T), [Dao Sy Duy Minh](https://arxiv.org/search/cs?searchtype=author\u0026query=Minh,+D+S+D), [Nguyen Dinh Ha Duong](https://arxiv.org/search/cs?searchtype=author\u0026query=Duong,+N+D+H), [Le Hoang Minh Huy](https://arxiv.org/search/cs?searchtype=author\u0026query=Huy,+L+H+M), [Long Nguyen](https://arxiv.org/search/cs?searchtype=author\u0026query=Nguyen,+L), [Dien Dinh](https://arxiv.org/search/cs?searchtype=author\u0026query=Dinh,+D)\n\nView a PDF of the paper titled VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation, by Huynh Trung Kiet and 5 other authors\n\n[View PDF](http://arxiv.org/pdf/2601.03792v1)[HTML (experimental)](https://arxiv.org/html/2601.03792v1)\n\u003e Abstract:Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.\n\nComments:11 pages, 4 figures. Dataset and code released\nSubjects:Computation and Language (cs.CL)\nMSC classes:68T50\nACM classes:I.2.7; J.3\nCite as:[arXiv:2601.03792](https://arxiv.org/abs/2601.03792) [cs.CL]\n(or [arXiv:2601.03792v1](https://arxiv.org/abs/2601.03792v1) [cs.CL] for this version)\n[https://doi.org/10.48550/arXiv.2601.03792](https://doi.org/10.48550/arXiv.2601.03792)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Duy Minh Dao Sy [[view email](http://arxiv.org/show-email/c00f33bd/2601.03792)] \n\n**[v1]** Wed, 7 Jan 2026 10:49:56 UTC (4,551 KB)\n\n[](http://arxiv.org/abs/2601.03792v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation, by Huynh Trung Kiet and 5 other authors\n\n*   [View PDF](http://arxiv.org/pdf/2601.03792v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03792v1)\n*   [TeX Source](http://arxiv.org/src/2601.03792v1)\n\n[view license](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.CL\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03792\u0026function=prev\u0026context=cs.CL \"previous in cs.CL (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03792\u0026function=next\u0026context=cs.CL \"next in cs.CL (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.CL/new) | [recent](http://arxiv.org/list/cs.CL/recent) | [2026-01](http://arxiv.org/list/cs.CL/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03792?context=cs)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03792)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03792)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03792)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03792v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03792\u0026description=VietMed-MCQ:%20A%20Consistency-Filtered%20Data%20Synthesis%20Framework%20for%20Vietnamese%20Traditional%20Medicine%20Evaluation \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03792\u0026title=VietMed-MCQ:%20A%20Consistency-Filtered%20Data%20Synthesis%20Framework%20for%20Vietnamese%20Traditional%20Medicine%20Evaluation \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03792v1)\n*   [Venue](http://arxiv.org/abs/2601.03792v1)\n*   [Institution](http://arxiv.org/abs/2601.03792v1)\n*   [Topic](http://arxiv.org/abs/2601.03792v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03792) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03748v1",
          "was_fetched": true,
          "page": "Title: Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning\n\nURL Source: http://arxiv.org/abs/2601.03748v1\n\nMarkdown Content:\n[2601.03748v1] Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03748v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03748v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Information Retrieval\n========================================\n\n**arXiv:2601.03748v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning\n=============================================================================================\n\nAuthors:[Dario Maio](https://arxiv.org/search/cs?searchtype=author\u0026query=Maio,+D), [Stefano Rizzi](https://arxiv.org/search/cs?searchtype=author\u0026query=Rizzi,+S)\n\nView a PDF of the paper titled Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning, by Dario Maio and Stefano Rizzi\n\n[View PDF](http://arxiv.org/pdf/2601.03748v1)[HTML (experimental)](https://arxiv.org/html/2601.03748v1)\n\u003e Abstract:Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.\n\nSubjects:Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)\nCite as:[arXiv:2601.03748](https://arxiv.org/abs/2601.03748) [cs.IR]\n(or [arXiv:2601.03748v1](https://arxiv.org/abs/2601.03748v1) [cs.IR] for this version)\n[https://doi.org/10.48550/arXiv.2601.03748](https://doi.org/10.48550/arXiv.2601.03748)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Stefano Rizzi [[view email](http://arxiv.org/show-email/dae520d5/2601.03748)] \n\n**[v1]** Wed, 7 Jan 2026 09:37:36 UTC (313 KB)\n\n[](http://arxiv.org/abs/2601.03748v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning, by Dario Maio and Stefano Rizzi\n\n*   [View PDF](http://arxiv.org/pdf/2601.03748v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03748v1)\n*   [TeX Source](http://arxiv.org/src/2601.03748v1)\n\n[view license](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.IR\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03748\u0026function=prev\u0026context=cs.IR \"previous in cs.IR (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03748\u0026function=next\u0026context=cs.IR \"next in cs.IR (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.IR/new) | [recent](http://arxiv.org/list/cs.IR/recent) | [2026-01](http://arxiv.org/list/cs.IR/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03748?context=cs)\n\n[cs.AI](http://arxiv.org/abs/2601.03748?context=cs.AI)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03748)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03748)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03748)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03748v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03748\u0026description=Bridging%20OLAP%20and%20RAG:%20A%20Multidimensional%20Approach%20to%20the%20Design%20of%20Corpus%20Partitioning \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03748\u0026title=Bridging%20OLAP%20and%20RAG:%20A%20Multidimensional%20Approach%20to%20the%20Design%20of%20Corpus%20Partitioning \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03748v1)\n*   [Venue](http://arxiv.org/abs/2601.03748v1)\n*   [Institution](http://arxiv.org/abs/2601.03748v1)\n*   [Topic](http://arxiv.org/abs/2601.03748v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03748) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "http://arxiv.org/abs/2601.03746v1",
          "was_fetched": true,
          "page": "Title: Whose Facts Win? LLM Source Preferences under Knowledge Conflicts\n\nURL Source: http://arxiv.org/abs/2601.03746v1\n\nMarkdown Content:\n[2601.03746v1] Whose Facts Win? LLM Source Preferences under Knowledge Conflicts\n===============\n\n[Skip to main content](http://arxiv.org/abs/2601.03746v1#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](http://arxiv.org/IgnoreMe)\n\n[](http://arxiv.org/)\u003e[cs](http://arxiv.org/list/cs/recent)\u003e arXiv:2601.03746v1 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Computation and Language\n===========================================\n\n**arXiv:2601.03746v1** (cs) \n\n [Submitted on 7 Jan 2026]\n\nTitle:Whose Facts Win? LLM Source Preferences under Knowledge Conflicts\n=======================================================================\n\nAuthors:[Jakob Schuster](https://arxiv.org/search/cs?searchtype=author\u0026query=Schuster,+J), [Vagrant Gautam](https://arxiv.org/search/cs?searchtype=author\u0026query=Gautam,+V), [Katja Markert](https://arxiv.org/search/cs?searchtype=author\u0026query=Markert,+K)\n\nView a PDF of the paper titled Whose Facts Win? LLM Source Preferences under Knowledge Conflicts, by Jakob Schuster and Vagrant Gautam and Katja Markert\n\n[View PDF](http://arxiv.org/pdf/2601.03746v1)[HTML (experimental)](https://arxiv.org/html/2601.03746v1)\n\u003e Abstract:As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.\n\nComments:Data and code: [this https URL](https://github.com/JaSchuste/llm-source-preference)\nSubjects:Computation and Language (cs.CL)\nACM classes:I.2.7\nCite as:[arXiv:2601.03746](https://arxiv.org/abs/2601.03746) [cs.CL]\n(or [arXiv:2601.03746v1](https://arxiv.org/abs/2601.03746v1) [cs.CL] for this version)\n[https://doi.org/10.48550/arXiv.2601.03746](https://doi.org/10.48550/arXiv.2601.03746)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite (pending registration)\n\nSubmission history\n------------------\n\n From: Vagrant Gautam [[view email](http://arxiv.org/show-email/f677cf72/2601.03746)] \n\n**[v1]** Wed, 7 Jan 2026 09:35:35 UTC (489 KB)\n\n[](http://arxiv.org/abs/2601.03746v1)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled Whose Facts Win? LLM Source Preferences under Knowledge Conflicts, by Jakob Schuster and Vagrant Gautam and Katja Markert\n\n*   [View PDF](http://arxiv.org/pdf/2601.03746v1)\n*   [HTML (experimental)](https://arxiv.org/html/2601.03746v1)\n*   [TeX Source](http://arxiv.org/src/2601.03746v1)\n\n[view license](http://creativecommons.org/licenses/by-sa/4.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.CL\n\n[\u003cprev](http://arxiv.org/prevnext?id=2601.03746\u0026function=prev\u0026context=cs.CL \"previous in cs.CL (accesskey p)\") | [next\u003e](http://arxiv.org/prevnext?id=2601.03746\u0026function=next\u0026context=cs.CL \"next in cs.CL (accesskey n)\")\n\n[new](http://arxiv.org/list/cs.CL/new) | [recent](http://arxiv.org/list/cs.CL/recent) | [2026-01](http://arxiv.org/list/cs.CL/2026-01)\n\n Change to browse by: \n\n[cs](http://arxiv.org/abs/2601.03746?context=cs)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2601.03746)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2601.03746)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2601.03746)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](http://arxiv.org/abs/2601.03746v1)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2601.03746\u0026description=Whose%20Facts%20Win?%20LLM%20Source%20Preferences%20under%20Knowledge%20Conflicts \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2601.03746\u0026title=Whose%20Facts%20Win?%20LLM%20Source%20Preferences%20under%20Knowledge%20Conflicts \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](http://arxiv.org/abs/2601.03746v1)\n*   [Venue](http://arxiv.org/abs/2601.03746v1)\n*   [Institution](http://arxiv.org/abs/2601.03746v1)\n*   [Topic](http://arxiv.org/abs/2601.03746v1)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](http://arxiv.org/auth/show-endorsers/2601.03746) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        },
        {
          "url": "https://www.ragview.ai/",
          "was_fetched": true,
          "page": "Title: Discover the best RAG for your own data\n\nURL Source: https://www.ragview.ai/\n\nMarkdown Content:\n[](https://www.ragview.ai/)\n\nRAGView evaluates multiple pipelines on your own dataset onlineâ€”no complex setupâ€”delivering clear RAG evaluation reports and helping you choose the best RAG for your data.\n\n[Try it now](https://www.ragview.ai/#)\n\n40+ widely used RAGs, unified metrics, all in one platform.\n-----------------------------------------------------------\n\n### Langflow\n\nBuild, scale, and deploy RAG and multi-agent AI apps.But we use it to build a naive RAG.\n\n[Try it now](https://www.ragview.ai/#)\n\n### R2R\n\nSoTA production-grade RAG system with Agentic RAG architecture and RESTful API support.\n\n[Try it now](https://www.ragview.ai/#)\n\n### DocsGPT\n\nPrivate AI platform supporting Agent building, deep research, document analysis, multi-model support, and API integration.\n\n[Try it now](https://www.ragview.ai/#)\n\n### GraphRAG\n\nModular graph-based retrieval RAG system from Microsoft.\n\n[Try it now](https://www.ragview.ai/#)\n\nmore diverse,faster,better and cheaper\n--------------------------------------\n\nRAGView's features support multi-path RAGs, rapid evaluation, traceable analysis, and cost-efficient operation.\n\nðŸ“„\n\n### More Rags\n\nAggregates 40+ mainstream RAGs, supports simultaneous evaluation of different RAGs, and enables side-by-side comparison of results.\n\nâš¡\n\n### Quick Review\n\nNo complex deployment required â€” just upload your dataset and start online evaluation with one click, enjoying minute-level route selection.\n\nðŸ”\n\n### Better Analysis\n\nEach evaluation provides detailed questions and traceable metrics to help users perform accurate attribution analysis.\n\nðŸ’°\n\n### Cost Saving\n\nEvaluate multiple different RAGs online and cut labor and resource costs by roughly 70% compared to traditional methods.\n\nHow do we help you choose the optimal RAG solution\n--------------------------------------------------\n\nOur process saves you a lot of work - in just 15 minutes, you can see how different RAG solutions perform on your business data\n\nNeed our help?\n--------------\n\nIf you need our help, you can contact us through the following methods\n\nStart Now. Find Your Best-Fit RAG.\n----------------------------------\n\nEvaluate and compare different RAGs in minutes, not weeks â€” effortlessly choose what truly fits your data.\n\n[Try it now](https://www.ragview.ai/#)",
          "was_summarised": false
        },
        {
          "url": "https://github.com/RagView/RagView",
          "was_fetched": true,
          "page": "Title: GitHub - RagView/RagView: We believe that every SOTA result is only valid on its own dataset. RAGView provides a unified evaluation platform to benchmark different RAG methods on your specific data.\n\nURL Source: https://github.com/RagView/RagView\n\nMarkdown Content:\nWhy RagView?\n------------\n\n[](https://github.com/RagView/RagView#why-ragview)\nAs RAG technology continues to evolve, there are now nearly 60 distinct approaches, reflecting a stage of diversity and rapid experimentation. Depending on the scenario, different RAG solutions may yield significantly different outcomes in terms of recall rate, accuracy, and F1 score. Beyond accuracy, enterprises and individual developers must also weigh factors such as computational cost, performance, framework maturity, and scalability. However, there is currently no unified platform that consolidates and compares these RAG technologies. Developers and enterprises are often forced to download open-source code, deploy systems independently, and run manual evaluationsâ€”an inefficient and costly process.\n\n To address this gap, we are building RagViewâ€”a benchmarking and selection platform for RAG technologies, designed for both developers and enterprises. RagView provides standardized evaluation metrics, streamlined benchmarking workflows, intuitive visualization tools, and a modular plug-in architecture, enabling users to efficiently compare RAG solutions and select the approach best suited to their specific business needs.\n\n Todayâ€™s teams need a standardized rag evaluation process to compare diverse approaches fairlyâ€”using clear rag evaluation metrics (answer accuracy, context precision/recall, cost, and latency) and reproducible benchmarks. RagView centralizes this, so you can run apples-to-apples rag evaluation and pick what actually works for your domain.\n\nBasic Concepts\n--------------\n\n[](https://github.com/RagView/RagView#basic-concepts)\n### Terminology\n\n[](https://github.com/RagView/RagView#terminology)\n*   RAG solution: A RAG solution that follows the basic RAG workflow but applies special techniques in various components to improve recall and accuracy. The RAG solutions in RagView currently come from the open-source community; future plans include integrating commercial pipelines and implementing solutions from academic research.\n*   Document: The original form of knowledge in the RAG system, including text files (PDF, DOCX, TXT) and images (JPG, PNG, BMP). In the current version, only PDF is supported.\n*   Document Set: A collection of documents. In a RAG evaluation, this is the smallest unit of documents that is referenced.\n*   Test Set: A set of evaluation data that includes questions, the source text passages (which must be contained in documents of the document set) that a large model used as references to answer the questions, and the standard answers that the model is expected to generate based on those passages. In RAG evaluation, a test set is the smallest unit of test data referenced.\n*   RAG evaluation Task: Consists of one document set, one test set, and N user-selected RAG solutions; it executes the RAG evaluation and returns the results.A RAG evaluation task computes predefined rag evaluation metrics at both query-level and task-level for consistent, reproducible comparison.\n*   RAG evaluation count: A unit representing the complete evaluation of one RAG solution. If an evaluation task selects multiple solutions, it consumes multiple counts.\n\n### Functional Metrics\n\n[](https://github.com/RagView/RagView#functional-metrics)\nRagView reports core rag evaluation metrics that capture answer quality and retrieval quality: Answer Accuracy, Context Precision, and Context Recall. These rag evaluation signals reveal whether the system surfaces the right passages and uses them faithfully.\n\n*   Answer Accuracy: Measures how consistent the generated answer is with the reference answer; higher is better.\n*   Context Precision: Evaluates how accurate the retrieved relevant segments are for this RAG solution (i.e., the proportion of retrieved content that is actually relevant to the answer, considering ranking); higher is better.\n*   Context Recall: Measures how comprehensively the relevant segments are retrieved by this RAG solution; higher is better.\n\n### Performance Metrics\n\n[](https://github.com/RagView/RagView#performance-metrics)\nFor production decisions, efficiency matters. RagView treats latency, token consumption, and resource usage as first-class rag evaluation metrics, enabling cost-aware model and retriever choices.\n\n*   Token Consumption: The number of tokens consumed by various LLMs across different stages of the RAG solution (including preprocessing, indexing, retrieval, generation, etc.).\n*   Time: The time consumed by various components of the RAG solution, including preprocessing, indexing, retrieval, and generation.\n\nQuick Start\n-----------\n\n[](https://github.com/RagView/RagView#quick-start)\nWelcome to RagView.ai .\n\n In RagView, you can complete a RAG evaluation task quickly with just a few simple steps.\n\n1.   Upload Document Set: In Data Management, upload a set of documents to be retrieved during the RAG process. These documents will serve as the knowledge base for retrieval; different RAG solutions may use different chunking and retrieval methods on them.\n2.   Upload Test Set: In Data Management, upload the test data for evaluation. The test data includes the questions, the original text excerpts (from the document set) that the large model used for reference, and the reference standard answers. The test set should be prepared according to the provided sample file.\n3.   Launch Evaluation Task: In Evaluation Management, create a rag evaluation task, select the document set and test set, choose solutions, and pick the rag evaluation metrics to compute (e.g., answer accuracy, context precision/recall, latency).\n4.   View Evaluation Details: In Evaluation Management, after the task completes, click Details to view results. Based on the metrics you care about, select the suitable RAG solution and download the code for that pipeline.\n\n### Register / Login\n\n[](https://github.com/RagView/RagView#register--login)[](https://private-user-images.githubusercontent.com/217898839/511905626-b87af339-c4d2-43c3-95bc-4c04cbc4ff2b.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1NjI2LWI4N2FmMzM5LWM0ZDItNDNjMy05NWJjLTRjMDRjYmM0ZmYyYi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yMzhiZmFmNWM4MmZhNmM4YzY1MGFhM2QyOTY5YjI1NDJkYWRhY2ZkNDA2OWExY2Q0NTczMWJiYzMyNzc3ODFhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.Rz65V4qQYQs2ZZVD1JdSrKtcV_uzxuIC__nCTeNvTZY)\n[https://www.ragview.ai/login](https://www.ragview.ai/login)\n\nYou can register or log in using Google, GitHub, or email.\n\n### Upload Document Set\n\n[](https://github.com/RagView/RagView#upload-document-set)[](https://private-user-images.githubusercontent.com/217898839/511905641-f5540bfe-17b1-44f0-97b3-200c18c73a03.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1NjQxLWY1NTQwYmZlLTE3YjEtNDRmMC05N2IzLTIwMGMxOGM3M2EwMy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zMGZmNGRhNzRiYmQzZmM1NmY1YjhhNTI3MzcyODNjYWU4OGY5ZDVkNGYyNDkwMmFlZTIyMzE2NDBkMzM0ZjhlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.cWUVaQFVBBfGGq5MLH7z1cvNj6A_gKPuvJpvYv2rRps)\n[https://www.ragview.ai/components/dataset](https://www.ragview.ai/components/dataset)\n\n*   In Data Management, click Create Document Set to create an empty document set for storing documents to be retrieved.\n*   Click the Upload button to upload one or more documents to the document set. Note: Files in the example document set cannot be deleted.\n\n### Upload Test Set\n\n[](https://github.com/RagView/RagView#upload-test-set)[](https://private-user-images.githubusercontent.com/217898839/511905691-60561e44-2f61-405d-8110-1e950eb99045.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1NjkxLTYwNTYxZTQ0LTJmNjEtNDA1ZC04MTEwLTFlOTUwZWI5OTA0NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hY2Y4YTc0YjljMDc5ODBhZDRkZDE2NWM3YmVlYzczOTQ3NGU5ZThmOTdkYmFmNGI0NmNlOGUzYWJmYmY2YWI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.lNIwveIQjt8fL5Aj7sdyGBrFPBQarwT0piMQLKterBs)\n*   Download the sample test set file (in XLSX or JSON format) and prepare your test set according to the sample.\n*   Click the Upload button to upload the test set.\n\n### Launch RAG Evaluation Task\n\n[](https://github.com/RagView/RagView#launch-rag-evaluation-task)[](https://private-user-images.githubusercontent.com/217898839/511905731-176a955e-7b79-45ad-b387-9ba7a15a5333.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1NzMxLTE3NmE5NTVlLTdiNzktNDVhZC1iMzg3LTliYTdhMTVhNTMzMy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iZjhkOTVmMmYzYWNlYjY4Y2U0Yzk3MWNkMWU5ZTVlMWI5MDk2OTg4MDk3N2EzOTdhNDNhNjQwNDRmNTQzMDdhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.BATtebwFgN1O5-j6QF1LGmxc23ve98LQNFgIj945HNA)\n[https://www.ragview.ai/components/evaluationManagement](https://www.ragview.ai/components/evaluationManagement)\n\n1.   Create a new evaluation task in Evaluation Management.\n2.   Enter the task name and description.\n3.   Select the document set and test set.\n4.   Select the RAG solutions to be evaluated.\n\n[](https://private-user-images.githubusercontent.com/217898839/511905771-2a755830-3843-457b-b856-18b65d903755.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1NzcxLTJhNzU1ODMwLTM4NDMtNDU3Yi1iODU2LTE4YjY1ZDkwMzc1NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yY2RhMWIwMDQxNjg0YmMwOGZlMTljMTQ5YWFhMWQ0MzA4ZjFiNTViMDgzYThiZjJiYzQzY2FhZmJlNzg2YTYyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.oCfD7n73MwV75Rtrv7DtCAWOLRGl48X1_dzd1s-JVZM)\n1.   Configure the parameters and evaluation metrics.\n2.   Click the Submit button to start the evaluation task.\n\n### View Evaluation Details\n\n[](https://github.com/RagView/RagView#view-evaluation-details)[](https://private-user-images.githubusercontent.com/217898839/511905848-f16797be-797f-414e-968f-c7f0901109b1.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1ODQ4LWYxNjc5N2JlLTc5N2YtNDE0ZS05NjhmLWM3ZjA5MDExMDliMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNDRjNDZjZGY3MzlmNTYwNjJhNjVjNDFmOTRlZjUxNTQ2ZDExYzcwNmM2YzMzMTU0MzY3YzBmYmFmMTdmNTUxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.fHd2x0Av052hQMeql5xotnclyi1k9Iu9JIMVPQ5GXg4)\nOn the evaluation task page, click Details on a task.\n\n[](https://private-user-images.githubusercontent.com/217898839/511905864-47fb0ba9-9c7f-49e0-b830-d4a66a5f4ef4.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1ODY0LTQ3ZmIwYmE5LTljN2YtNDllMC1iODMwLWQ0YTY2YTVmNGVmNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hMTJhMDg2N2E0NTc2ZDBmMDA3MDRjM2RiZGMxODEyMzA5YTY5YzYzZDdmMDBjNDNlMjUxNzEyMWU5ODc5Yjg5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.zVSaID7u4c_cclj7hbsmKX7CEP_BhyuwT5NwnNuJDjU)\nView the metric scores for different tasks.\n\n[](https://private-user-images.githubusercontent.com/217898839/511905883-d33f913d-3d8a-4ab6-91bc-987c76706845.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1ODgzLWQzM2Y5MTNkLTNkOGEtNGFiNi05MWJjLTk4N2M3NjcwNjg0NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01MWY0ZDdiZjEwNTc1Y2IyNWMzOTI5M2ViZTQ2MDk1ZjFkYWExY2RhMDc0MDEyZDk2ZDI5MWQ3ZThiZDkwNzQ5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.EPkmwIPdMPhFmU1vSf0uk_v_3T9LWWfmofok9ZZLZiM)\nView the detailed evaluation record for a single query.\n\n[](https://private-user-images.githubusercontent.com/217898839/511905897-29aed1e1-985c-4d27-ba8b-3247cca95b46.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc5MjY1NTUsIm5iZiI6MTc2NzkyNjI1NSwicGF0aCI6Ii8yMTc4OTg4MzkvNTExOTA1ODk3LTI5YWVkMWUxLTk4NWMtNGQyNy1iYThiLTMyNDdjY2E5NWI0Ni5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwOVQwMjM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05NmUyMTdjN2JjYzQ3NjY4ZjU1YjQyNmIzZDU4MzZmNTM5Yjk4NTIwOWRlMmFhZmM3YzRmMTRmMzFhMzlmYjg2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.z08XEF9STfOG0cC8Yl_1nAM3a9bMBOsPClv7l5F89pU)\nView the detailed computation process for a single metric of a single query.\n\n### Acquire More Evaluation Counts\n\n[](https://github.com/RagView/RagView#acquire-more-evaluation-counts)\nEach evaluation task consumes one evaluation count per RAG solution selected. A task with multiple solutions will consume multiple counts. You can purchase additional evaluation counts on the purchase page.\n\nFrequently Asked Questions\n--------------------------\n\n[](https://github.com/RagView/RagView#frequently-asked-questions)\n1.   Is RagView open source?\n\n Answer: RagView currently has no open source plan, but we aim to cover hardware operating costs with relatively low fees. However, we will open-source our optimized RAG solutions.\n\n GitHub: [https://github.com/RagView/RagView](https://github.com/RagView/RagView).\n\n2.   How does RagView charge?\n\n Answer: We currently charge per evaluation count. In the future, pricing may align with tokens and runtimeâ€”both already tracked as rag evaluation metrics for transparency.\n\n3.   Can I automatically generate a test set from a document set?\n\n Answer: Yes. We plan to add a feature in version 1.1 to automatically generate a test set from the document set. With this feature, you will only need to click and wait for an LLM to generate QA pairs from the documents.\n\n4.   What is the format of the test set?\n\n Answer: The test set should be in XLSX or JSON format. Please refer to the sample file for the exact format.\n\n5.   What is the format of the document set?\n\n Answer: Currently, only PDF format is supported for documents. We plan to add support for image-based documents in future versions.\n\n6.   How can I view evaluation results?\n\n Answer: Go to the Evaluation Management page and open Details to review dashboards and downloadable reports of your rag evaluation metricsâ€”including answer quality, context metrics, and efficiency.\n\n7.   How can I view the code for a RAG scheme?\n\n Answer: Currently, you will need to search for the RAG scheme by name on GitHub. In version 1.1, we will open-source the integrated code and provide a download link directly in the evaluation results.\n\n8.   Can RagView be used for commercial purposes?\n\n Answer: Yes, it can.\n\n9.   Can I compare my own RAG with open-source RAGs?\n\n Answer: We plan to support this in future versions. See our milestone plan (link) for details.\n\n10.   Which RAG schemes are already integrated into RagView?\n\n Answer: We have currently integrated R2R, LangFlow, and DocsGPT. In the future, we plan to add 1â€“2 new RAG schemes each week, depending on the availability and stability of open-source code. See our roadmap (link) for more details.\n\n11.   How can I contact you?\n\n Answer: You can contact us via email: [ragandview@gmail.com](mailto:ragandview@gmail.com)\n\nRagView Milestone Plan\n----------------------\n\n[](https://github.com/RagView/RagView#ragview-milestone-plan)\n### Key Features\n\n[](https://github.com/RagView/RagView#key-features)\n*   Test Set Auto-Generation: Based on the documents in the document set, use a naive chunking method and an LLM to automatically generate Q\u0026A pairs from each chunk, producing the test set data.\n*   Custom RAG Integration: Provide an SDK/API for developers to integrate their own RAG solutions into RagView, enabling comparison between their solutions and open-source solutions.\n*   Evaluation Task Optimization: Support setting up and comparing multiple configurations (different hyperparameters) of the same RAG solution.\n*   Evaluation Report Generation: Support automatic generation of PDF reports from evaluation results.\n\n### Usability Enhancements\n\n[](https://github.com/RagView/RagView#usability-enhancements)\n*   Email Notifications: Since evaluations are asynchronous and may take minutes to tens of minutes, add email notifications to inform users when evaluation results are ready.\n*   Result Charting: Generate bar charts, pie charts, radar charts, etc., based on metric scores to facilitate visual comparison.\n*   Hardware Resource Profiling: Collect statistics on hardware resource usage for different evaluation pipelines, aiding developers in assessing production feasibility.\n*   Optional Metrics: Make evaluation metrics optional (no longer mandatory), allowing users to select only the metrics they are interested in.\n\n### More RAG solutions\n\n[](https://github.com/RagView/RagView#more-rag-solutions)\nLegend:\n\n âœ… = Integrated | ðŸš§ = In Progress | â³ = Pending Integration\n\n| No. | Name | GitHub Link | Features | Status |\n| --- | --- | --- | --- | --- |\n| 0 | Langflow | [langflow-ai/langflow](https://github.com/langflow-ai/langflow) | Build, scale, and deploy RAG and multi-agent AI apps.But we use it to build a naive RAG. | âœ… |\n| 1 | R2R | [SciPhi-AI/R2R](https://github.com/SciPhi-AI/R2R) | SoTA production-grade RAG system with Agentic RAG architecture and RESTful API support. | âœ… |\n| 2 | KAG | [OpenSPG/KAG](https://github.com/OpenSPG/KAG) | Retrieval framework combining OpenSPG engine and LLM, using logical forms for guided reasoning; overcomes traditional vector similarity limitations; supports domain-specific QA. | â³ |\n| 3 | GraphRAG | [microsoft/graphrag](https://github.com/microsoft/graphrag) | Modular graph-based retrieval RAG system from Microsoft. | âœ… |\n| 4 | LightRAG | [HKUDS/LightRAG](https://github.com/HKUDS/LightRAG) | \"Simple and Fast Retrieval-Augmented Generation,\" designed for simplicity and speed. | âœ… |\n| 5 | dsRAG | [D-Star-AI/dsRAG](https://github.com/D-Star-AI/dsRAG) | High-performance retrieval engine for unstructured data, suitable for complex queries and dense text. | ðŸš§ |\n| 6 | paper-qa | [Future-House/paper-qa](https://github.com/Future-House/paper-qa) | Scientific literature QA system with citation support and high accuracy. | â³ |\n| 7 | cognee | [topoteretes/cognee](https://github.com/topoteretes/cognee) | Lightweight memory management for AI agents (\"Memory for AI Agents in 5 lines of code\"). | â³ |\n| 8 | trustgraph | [trustgraph-ai/trustgraph](https://github.com/trustgraph-ai/trustgraph) | Next-generation AI product creation platform with context engineering and LLM orchestration; supports API and private deployment. | â³ |\n| 9 | graphiti | [getzep/graphiti](https://github.com/getzep/graphiti) | Real-time knowledge graph builder for AI agents, supporting enterprise-grade applications. | â³ |\n| 10 | DocsGPT | [arc53/DocsGPT](https://github.com/arc53/DocsGPT) | Private AI platform supporting Agent building, deep research, document analysis, multi-model support, and API integration. | âœ… |\n| 11 | youtu-graphrag | [youtugraph/youtu-graphrag](https://github.com/TencentCloudADP/youtu-graphrag) | Graph-based RAG framework from Tencent Youtu Lab, focusing on knowledge graph construction and reasoning for domain-specific applications. | â³ |\n| 12 | Kiln | [https://github.com/Kiln-AI/Kiln](https://github.com/Kiln-AI/Kiln) | Desktop app for zero-code fine-tuning, evals, synthetic data, and built-in RAG tools. | â³ |\n| 13 | Quivr | [https://github.com/QuivrHQ/quivr](https://github.com/QuivrHQ/quivr) | a RAG that is opinionated, fast and efficient so you can focus on your product. | â³ |\n\n### More RAG Evaluation Metrics\n\n[](https://github.com/RagView/RagView#more-rag-evaluation-metrics)\nWe will gradually add functionality and performance metrics for RAG evaluation, including:\n\n| **Metric Type** | **Metric Name** | **Description** |\n| --- | --- | --- |\n| **Effectiveness / Quality Metrics** | Recall@k | Proportion of queries where the correct answer appears in the top k retrieved documents |\n|  | Precision@k | Proportion of relevant documents among the top k retrieved documents |\n|  | MRR (Mean Reciprocal Rank) | Average reciprocal rank of the first relevant document |\n|  | nDCG (Normalized Discounted Cumulative Gain) | Ranking relevance metric that considers the importance of document order |\n|  | Answer Accuracy / F1 | Match between generated answers and reference answers (Exact Match or F1) |\n|  | ROUGE / BLEU / METEOR | Text overlap / language quality metrics |\n|  | BERTScore / MoverScore | Semantic-based answer matching metrics |\n|  | Context Precision | Proportion of retrieved documents that actually contribute to the answer |\n|  | Context Recall | Proportion of reference answer information covered by retrieved documents |\n|  | Context F1 | Combined score of Precision and Recall |\n|  | Answer-Context Alignment | Whether the answer strictly derives from the retrieved context |\n|  | Overall Score | Composite metric, usually a weighted combination of answer quality and context utilization |\n| **Efficiency / Cost Metrics** | Latency | Time required from input to answer generation |\n|  | Token Consumption | Number of tokens consumed during answer generation |\n|  | Memory Usage | Memory or GPU usage during model execution |\n|  | API Cost / Compute Cost | Estimated cost of calling the model or retrieval API |\n|  | Throughput | Number of requests the system can handle per unit time |\n|  | Scalability | System performance change when data volume or user requests increase |",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:38.752183198Z"
    },
    {
      "flow_id": "",
      "id": "1q6v7v5",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/",
      "title": "What hardware would it take to get Claude Code-level performance?",
      "content": "In my previous company I had a Claude license and my work was basically interacting with Claude Code all day long. The code base was rather complex and I was automating testing and â€œDevOpsâ€ stuff for an embedded device development so Claude Code saved me tons of time (it was much faster to ask and tune that to do it all by myself).\n\nIm currently unemployed but got a freelancing gig and the company doesnâ€™t provide access to commercial AI tools for contractors like me, but once again the work is rather demanding and I donâ€™t think Iâ€™ll meet the deadlines without AI help (itâ€™s a fairly old code base using mostly Java in a concurrent and distributed fashion), and of course due to compliance I canâ€™t just use a license I paid for by myself.\n\nSo, in new to all this. To be honest I have very little hardware, as I would always prioritize power efficiency since I never really needed to do anything hardware intensive before (I donâ€™t have a gaming PC or anything like that). I have an old HP Z2 G4 Tower I use as virtualization server and was thinking of getting a 3060 12GB for \\~300 USD (locally). Will I be able to run anything decent with that? Anything that would truly help me?\n\nI see everyone recommends a 3090 but Iâ€™d need a whole new PSU and build an entire computer around that. So thatâ€™d be roughly 2K USD (is it worth it? I donâ€™t know, maybe?)\n\nWhat hardware is requires to run anything remotely close to Claude Code? Something like 6x3090s (144GB VRAM)?",
      "author": "cashmillionair",
      "created_at": "2026-01-07T23:22:28Z",
      "comments": [
        {
          "id": "nyatmmq",
          "author": "Lissanro",
          "content": "I have four 3090 cards and that's enough to fit entire 256K context cache at Q8 for Kimi K2 Thinking, but the main issue today is going to be RAM - a year ago it was possible to get 512GB of 8-channel DDR4 3200 MHzfor around $800 (I got 1 TB at the time), but today this is no longer the case, so with limited budget, you have to consider smaller models.\n\nI think your best bet is MiniMax M2.1, some people got reported getting 20 tokens/s with as little as 72 GB VRAM: [https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/discussions/2](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/discussions/2) \\- for full VRAM inference you probably will need 8x3090 though, then the speed would be much greater. Good idea to buy used server EPYC hardware, even 8-channel DDR4 would be still much faster than any DDR5 dual channel RAM on gaming platforms, and connecting multiple GPUs works much better on server motherboards that have multilpe x16 slots and support bifurcation in case you need to plug-in more cards than there are full speed slots.",
          "created_at": "2026-01-07T23:50:41Z",
          "was_summarised": false
        },
        {
          "id": "nyay3pt",
          "author": "getfitdotus",
          "content": "I have close to it glm 4.7 with 4x6000 pro blackwells. Its faster then opus and sonnet. Not quite as good but close enough and i have 320k max context over all requests running 90t/s.",
          "created_at": "2026-01-08T00:13:22Z",
          "was_summarised": false
        },
        {
          "id": "nyaqabj",
          "author": "Fit-Produce420",
          "content": "Well part of what makes some solutions better is the CLI or dev environment or whatever.\n\n\nClaude Code is pretty near cutting edge no matter how you slice it, it's large model and uses tools really well.\n\n\n\n\nYou can use claude code with other models so you want good tool use, good context, and reasonable speed.Â \n\n\nMost open source models are MoE for speed so a large model good at tool use is your best bet.Â \n\n\nAs a hobbyist who needed a modern rig and couldn't just add le video card I bought a Strix Halo Framework Desktop 128GB, it can run dense models around 70B slowly but runs 50B-200B MoE models at 50+/toks.\n\n\nÂ but a Mac would be great if you don't want Linux or gaming. Pricier than strix.\n\n\nOr you can get a gb10 nvidia Blackwell with cuda if you don't want to use it as a general purpose computer, has nvlink which is the fastest built in link for clustering.",
          "created_at": "2026-01-07T23:33:34Z",
          "was_summarised": false
        },
        {
          "id": "nyavgx4",
          "author": "ithkuil",
          "content": "You can run Claude Code with multiple models. What you meant to ask was about the equivalent to Claude Opus 4.5 which there isn't. But GLM 4.7 and a few others are in the ballpark of some Claude Sinnet models, more or less. They are very large models and your purpose hardware is not remotely close.",
          "created_at": "2026-01-08T00:00:02Z",
          "was_summarised": false
        },
        {
          "id": "nyavtx8",
          "author": "datOEsigmagrindlife",
          "content": "The hardware you haven't isn't remotely close enough.\n\nYou're probably going to need to drop somewhere in the region of $50-$100k on GPUs.",
          "created_at": "2026-01-08T00:01:53Z",
          "was_summarised": false
        },
        {
          "id": "nyaq33o",
          "author": "DataGOGO",
          "content": "You would need a lot of hardware, easily over 10k to start, but in all reality more like 40-60k to get you 80% there (2 or 4 Rtx Pro Blackwellâ€™s + Xeon workstation, + 8-12 channels of memory, 2X power supplies)\n\nJust go get a $200 a month claude code subscription, just make sure the client is ok with you using AI on their code base; be sure to get permission in writing.",
          "created_at": "2026-01-07T23:32:31Z",
          "was_summarised": false
        },
        {
          "id": "nyatp0g",
          "author": "Short_Ad4946",
          "content": "No matter how much you spend you won't get that level of performance. You could spend 100k and have worde models than 4.5 Sonnet/Opus, they're cutting edge. Maybe there are open source CLI tools but you'll be bottlenecked by open source models(if you really need the same performance as Sonnet/Opus). Your best bet is to convince your client you have to work with those AI tools and either get a normal plan or an enterprise version if they give you data protection or whatever on those from Claude Code",
          "created_at": "2026-01-07T23:51:01Z",
          "was_summarised": false
        },
        {
          "id": "nyawe08",
          "author": "ithkuil",
          "content": "You can use Claude Code via the API like everyone else and if the client won't let you then you have to find another client because you're screwed because it's an unreasonable client and everyone uses it and you can't compete on that basis. He will find another freelancer who just lies to him about not using AI.",
          "created_at": "2026-01-08T00:04:44Z",
          "was_summarised": false
        },
        {
          "id": "nyau2h2",
          "author": "NotLogrui",
          "content": "Find a cheaper alternative to Nvidia's DGX Spark and use run Mimi V2 Flash or MiniMax\n\nYou're looking at a minimum $2000 investment\n\nThere are both models that fit in DGX Spark 128GB of integrated memory\n\n[unsloth/MiMo-V2-Flash-GGUF](https://model.lmstudio.ai/download/unsloth/MiMo-V2-Flash-GGUF) \\- currently top 5 coding model - open source\n\n[bartowski/MiniMaxAI\\_MiniMax-M2.1-GGUF](https://huggingface.co/MiniMaxAI/MiniMax-M2.1) \\- currently top 5 coding model - open source as well\n\nCorrection Ryzen 395+ Max is under $2000 but unknown for FP4 support: [https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc)",
          "created_at": "2026-01-07T23:52:56Z",
          "was_summarised": false
        },
        {
          "id": "nyb9n06",
          "author": "beedunc",
          "content": "skip the GPUs - Buy a Dell 5810 or similar Xeon system and add ram. Iâ€™ve run Qwen3coder480B @q3+ (240gb) and it gave excellent results running at 2-4 tps. Post your prompt, itâ€™ll be ready after making coffee. Itâ€™s light years ahead of any 16, 32, or even 64gb local models. \n\nBuilt mine for \u0026lt; $500 over the summer, but would be $1k now because ram.\n\nEdit:clarity",
          "created_at": "2026-01-08T01:11:37Z",
          "was_summarised": false
        },
        {
          "id": "nyavwp7",
          "author": "Ok-Bill3318",
          "content": "Itâ€™s not just hardware, youâ€™d need to train and tweak the model as well as Anthropic have.",
          "created_at": "2026-01-08T00:02:17Z",
          "was_summarised": false
        },
        {
          "id": "nybdimg",
          "author": "zugx2",
          "content": "get a zai glm coding plan, get it to work with claude it should be good enough.",
          "created_at": "2026-01-08T01:32:11Z",
          "was_summarised": false
        },
        {
          "id": "nyblnq1",
          "author": "Trennosaurus_rex",
          "content": "Just spend 20 dollars a month on Claude Code.",
          "created_at": "2026-01-08T02:15:12Z",
          "was_summarised": false
        },
        {
          "id": "nyd3mog",
          "author": "huzbum",
          "content": "I had a 12GB 3060, then added a CMP 100-210, then swapped that for a 3090.  On a 3060 you might be able to run a quant of Qwen3 Coder 30b REAP.  Probably better bang for you buck on a CMP 100-210 with 16GB for like $150, but you'd need to figure out cooling.  That's not really very close to Claude though.  \n\nTo get something on par with Claude, you'd want GLM 4.7 or MiniMax M2.1.  I use GLM with Claude Code with a [z.ai](http://z.ai) subscription.  It's great!  You'd need like 256GB of VRAM to run it though.  I was tempted by some old 8x V100 servers on Ebay for like $6k, but with all the RAM madness I think they are like $8k now.  Otherwise, you'd be looking at like 3 or 4 RTX Pro 6000 Blackwells for $8-10k each.  \n\nI think I've heard of people getting like 20tps with dual xeon server setups, but I'm not really interested in that, so I don't know much about it.",
          "created_at": "2026-01-08T08:15:22Z",
          "was_summarised": false
        },
        {
          "id": "nyeq19v",
          "author": "AnomalyNexus",
          "content": "\u0026gt;300 USD\n\nThink you're short a couple zeros",
          "created_at": "2026-01-08T15:03:57Z",
          "was_summarised": false
        },
        {
          "id": "nyau1yu",
          "author": "jonahbenton",
          "content": "You can plug a card into an egpu and plug that into an existing machine via thunderbolt. Even two cards/2 egpus. Works fine. \n\nYou will not get anywhere near Claude Code -\u0026gt; Anthropic cloud model performance with local hardware. But if there are 5 orders of magnitude of capability between nothing and CC, something like qwen 30b is at oom 2 or 2.5. It is definitely useful and a time saver. Just have to be careful what you ask it.",
          "created_at": "2026-01-07T23:52:52Z",
          "was_summarised": false
        },
        {
          "id": "nyblyqb",
          "author": "jstormes",
          "content": "I have been using Claude Code CLI, Gemini CLI, and Qwen Code CLI.\n\nThe open source setup I use is the the Qwen Code LLM.  I have a project that has a docker setup to run it on Ryzen AI Max+ 395 (aka StrixHalo), Ryzen 9 7940HS, and Ryzen 7 5700G.\n\nThe project is at https://github.com/jstormes/StrixHalo.   I give some of the token speed I get with the AMD hardware and how much context I can get.  I do use a Q4 for the lower hardware, but Q8 (Smarter???) for the StrixHalo.\n\nTake a look, hope it helps.\n\n# Performance Summary\n\n\n\n|System|GPU|RAM|Max Context|Prompt|Generation|\n|:-|:-|:-|:-|:-|:-|\n|Ryzen AI Max+ 395|Radeon 8060S|128GB|1M|\\~450 tok/s|\\~40 tok/s|\n|Ryzen 9 7940HS|Radeon 780M|64GB DDR5|512K|\\~30 tok/s|\\~31 tok/s|\n|Ryzen 7 5700G|Radeon Vega|64GB DDR4|256K|\\~74 tok/s|\\~13 tok/s|",
          "created_at": "2026-01-08T02:16:47Z",
          "was_summarised": false
        },
        {
          "id": "nyaq0r4",
          "author": "SeyAssociation38",
          "content": "I think that job is setting you up for failure. Are you ready to pay for strix halo with 128 gb of ram for $2200?",
          "created_at": "2026-01-07T23:32:12Z",
          "was_summarised": false
        },
        {
          "id": "nyazm4e",
          "author": "Irisi11111",
          "content": "You can't do that with the GPUs in the consumer market right now. Claude Opus 4.5 is really, really big â€“ at least a trillion parameters. Even if its weights leaked, and you could download them, most people and small to medium-sized companies still couldn't run it.\n\n\u0026gt;I have an old HP Z2 G4 Tower I use as virtualization server and was thinking of getting a 3060 12GB for \\~300 USD (locally).Â \n\nYou've hit some good things! For your first real test, maybe try using LMstudio. It's good for running a coding model locally, like the Qwen 3 series. See if that works for you. Sometimes, a local model can help write simple scripts or explain code. You can use your local LLM with the Claude agent.",
          "created_at": "2026-01-08T00:20:52Z",
          "was_summarised": false
        },
        {
          "id": "nyb1orw",
          "author": "HealthyCommunicat",
          "content": "Around $10,000 USD to buy an m3 ultra 512 gb ram. You can load in GLM 4.7, but even then itâ€™ll be at half the speed Claude Code is on average, and also wonâ€™t be exactly as smart.",
          "created_at": "2026-01-08T00:31:14Z",
          "was_summarised": false
        },
        {
          "id": "nyb5rfp",
          "author": "Such_Web9894",
          "content": "Praying for the days we can get improvements that itâ€™ll be possible on at least high-end consumer gpus",
          "created_at": "2026-01-08T00:51:41Z",
          "was_summarised": false
        },
        {
          "id": "nyba1no",
          "author": "ssrowavay",
          "content": "Yeah I was hopeful that I could use Continue with my 3060 12GB and get decent results on a personal open source project I work on occasionally. Compared to Claude Code, itâ€™s like asking your dumbest cousin versus the smartest person youâ€™ve ever met.",
          "created_at": "2026-01-08T01:13:44Z",
          "was_summarised": false
        },
        {
          "id": "nybau61",
          "author": "El_Danger_Badger",
          "content": "Build your own project and find out what you can do with what you have. Keep aiming for incremental improvements. \n\nI have my daily driver. M1 Mac Mini 16gb RAM. A nothing box . I run duplex blended  models for Chat reasoning, RAG, semantic memory, multi-node multi-graph LangBoard, web search to expand knowledge. All cuts remembered and referenced. A trading bot, with an agent ensemble that reasons on ticker symbols to analyze current patterns and forecast.  \n\nAll local. Slow, but not that slow. Everything works. Free. No api calls. No new hardware. Just hammering on the problems as they arise, until they get fixed. \n\nYou don't have one trillion dollars, so don't expect frontier level. Just see what you can do with what  and keep refining.",
          "created_at": "2026-01-08T01:17:56Z",
          "was_summarised": false
        },
        {
          "id": "nybubul",
          "author": "exaknight21",
          "content": "I use glm 4.7 + claude code. It works good for my use case. It was $20 a year for the first year. \n\nLocal Hosting for a good model is literally out of the question in this stupid market. We have to wait for the bubble to pop. Likely a year out still.",
          "created_at": "2026-01-08T03:00:26Z",
          "was_summarised": false
        },
        {
          "id": "nycdh48",
          "author": "Devcomeups",
          "content": "You want get anything even close worth using. Dont bother trying . Only model worth running locally is mini max.",
          "created_at": "2026-01-08T04:52:26Z",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:38.752187052Z"
    },
    {
      "flow_id": "",
      "id": "1q6c9wc",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/",
      "title": "DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.",
      "content": "arXiv:2501.12948 \\[cs.CL\\]: https://arxiv.org/abs/2501.12948",
      "author": "Nunki08",
      "created_at": "2026-01-07T10:49:12Z",
      "comments": [
        {
          "id": "ny6w58o",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-07T12:50:10Z",
          "was_summarised": false
        },
        {
          "id": "ny6ls96",
          "author": "qtvivies",
          "content": "https://preview.redd.it/t6ic0x3nywbg1.png?width=1965\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=ae5ef60128b8a0cf89351e8673ea41eddafb037c\n\nSomething interesting towards the end. Looks like someone forgot about this",
          "created_at": "2026-01-07T11:36:50Z",
          "was_summarised": false
        },
        {
          "id": "ny7c3yc",
          "author": "Ok_Technology_5962",
          "content": "Current research is linear attention. DeepSeek 3.2 with the cache optimization and now they had a massive paper come out that puts the linear into the whole modelaking it possible to train more than 60 layers. So yes this one is done. The compute and thinking will now happen internally in 1000 layers",
          "created_at": "2026-01-07T14:21:09Z",
          "was_summarised": false
        },
        {
          "id": "ny8iz9u",
          "author": "warnerbell",
          "content": "The original paper was light on implementation specifics. If they've added more on how they got the reasoning behavior to emerge, that's valuable.",
          "created_at": "2026-01-07T17:41:52Z",
          "was_summarised": false
        },
        {
          "id": "ny6gjni",
          "author": "ResidentPositive4122",
          "content": "New arch about to drop? dsv4 + r2? Packing all the goodies learned from last year. Hopefully they try smaller sizes as well. Would be interesting to see how the arch improvements work at several sizes.",
          "created_at": "2026-01-07T10:53:31Z",
          "was_summarised": false
        },
        {
          "id": "ny9ne3n",
          "author": "CryptoUsher",
          "content": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "created_at": "2026-01-07T20:38:03Z",
          "was_summarised": false
        },
        {
          "id": "nyae2ug",
          "author": "timfduffy",
          "content": "I think all this info was previously released as a [supplment to their R1 paper in Nature](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf).",
          "created_at": "2026-01-07T22:34:09Z",
          "was_summarised": false
        },
        {
          "id": "ny74o1r",
          "author": "jeffwadsworth",
          "content": "I was using the online chat version last night to improve a large Java class (40K tokens) with multiple methods.  It did so beautifully with zero issues in one shot.  The same task in my sub of Gemini 3 Pro chat interface failed in a few shots due to hallucinations.  They have really improved that model a lot from a year ago.",
          "created_at": "2026-01-07T13:40:40Z",
          "was_summarised": false
        },
        {
          "id": "nycauta",
          "author": "badgerbadgerbadgerWI",
          "content": "The level of detail they're releasing is remarkable. This kind of transparency is what pushes the whole field forward. Really interested in their distillation approach - getting smaller models to match larger ones' reasoning is key for edge deployment.",
          "created_at": "2026-01-08T04:35:42Z",
          "was_summarised": false
        },
        {
          "id": "ny9okeg",
          "author": "CryptoUsher",
          "content": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "created_at": "2026-01-07T20:43:14Z",
          "was_summarised": false
        },
        {
          "id": "nydlzki",
          "author": "Eyelbee",
          "content": "It's crazy it's only been one year, feels like ages",
          "created_at": "2026-01-08T11:00:27Z",
          "was_summarised": false
        },
        {
          "id": "ny6xeqo",
          "author": "Aggressive-Bother470",
          "content": "New grpo details perhaps? From reading the hf page it implied it was maybe light in that regard?",
          "created_at": "2026-01-07T12:58:03Z",
          "was_summarised": false
        },
        {
          "id": "ny76206",
          "author": "TelloLeEngineer",
          "content": "does arxiv have a diff UI?",
          "created_at": "2026-01-07T13:48:18Z",
          "was_summarised": false
        },
        {
          "id": "ny6y8mn",
          "author": "yoshiK",
          "content": "I did quickly throw the two papers into Gemini. It's really fun to live in the future. \n\n#Gemini summary: \n\nThe paper **\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\"** (arXiv:2501.12948) marks a significant milestone in open-source AI by demonstrating that advanced reasoning (similar to OpenAIâ€™s o1) can be achieved through large-scale Reinforcement Learning (RL) with minimal human-annotated data.\n\nThe two versions you provided represent the initial release (**v1**, Jan 22, 2025) and the latest updated version (which has been significantly expanded to **86+ pages** as of January 2026).\n\n### 1. Overall Paper Summary\nThe paper introduces two primary models:\n*   **DeepSeek-R1-Zero:** A model trained via \"pure RL\" (using the GRPO algorithm) starting directly from a base model without any Supervised Fine-Tuning (SFT). It demonstrates that reasoning behaviors like self-correction and reflection can emerge purely from reward signals.\n*   **DeepSeek-R1:** A more \"user-friendly\" version that uses a multi-stage pipeline (Cold-start SFT â†’ Reasoning RL â†’ Rejection Sampling/SFT â†’ General RL) to fix the \"readability\" and \"language mixing\" issues of R1-Zero while maintaining state-of-the-art reasoning performance.\n*   **Distillation:** The authors show that the reasoning patterns discovered by the 671B model can be distilled into smaller models (1.5B to 70B), allowing a 14B model to outperform much larger ones on math and coding benchmarks.\n\n---\n\n### 2. Comparison: Extensions in the New Version\nThe newer version is a massive technical expansion (growing from roughly 22 pages to over 85 pages). The key additions and extensions include:\n\n#### A. The \"Aha Moment\" Expansion (Section 2.2.1)\nThe new version provides a much deeper analysis of the **\"Aha Moment\"**â€”the point during RL training where the model unexpectedly learns to \"re-think\" its approach. The extension includes more qualitative examples and internal data showing the model's transition from linear solving to iterative self-correction without being prompted to do so.\n\n#### B. Detailed 4-Stage Training Pipeline\nWhile v1 outlined the stages, the new version details the specific composition of the **800k total training samples**:\n*   **Stage 1 (Cold Start):** Expanded details on the ~5,000-10,000 long CoT (Chain of Thought) samples used to \"prime\" the model.\n*   **Stage 3 (Rejection Sampling):** A deeper dive into how 600k reasoning-related and 200k non-reasoning samples were filtered and used to improve the model's general chat capabilities and prevent \"forgetting\" during the reasoning-heavy RL stages.\n\n#### C. Comprehensive Ablation Studies\nThe new version adds extensive \"What if?\" scenarios that were absent or brief in v1:\n*   **Distillation vs. RL:** New evidence explaining *why* distilling a large model's reasoning traces into a small model is more effective than training that small model directly with its own RL.\n*   **Base Model Impact:** Analysis of how different base models (DeepSeek-V3 vs. Qwen vs. Llama) respond to the R1 training recipe.\n\n#### D. Expanded \"Unsuccessful Attempts\" (Section 4.2)\nOne of the most valuable additions for researchers is the expanded section on what **did not work**. The new version elaborates on their failures with:\n*   **Process Reward Models (PRM):** Detailed reasons why step-level rewards were difficult to scale or prone to \"reward hacking\" compared to the outcome-based rewards used in R1.\n*   **Monte Carlo Tree Search (MCTS):** Technical explanation of why MCTS didn't provide the expected gains over simple RL in the context of LLM reasoning.\n\n#### E. New Benchmarks \u0026amp; Technical Specs\n*   **Updated Results:** Includes more recent evaluations on benchmarks like **AIME 2025**, **LiveCodeBench**, and specialized medical/legal reasoning tests.\n*   **Hyperparameters:** The new version includes exhaustive tables of training hyperparameters (learning rates, GRPO group sizes, KL divergence coefficients) which were previously withheld or summarized.\n\n### Summary Table\n| Feature | v1 (Original) | Latest Version (Extension) |\n| :--- | :--- | :--- |\n| **Page Count** | ~22 Pages | **86+ Pages** |\n| **Methodology** | High-level 4-stage overview | Granular detail on each stage (SFT, RL, Rejection Sampling) |\n| **Behaviors** | Mentions \"self-correction\" | Deep dive into \"Aha Moment\" with case studies |\n| **Failed Paths** | Brief mention of PRM/MCTS | Exhaustive analysis of why PRM and MCTS underperformed |\n| **Distillation** | Introduced 1.5B to 70B models | Added deep ablation on distillation efficiency and data filtering |\n| **Hyperparameters** | Partial/Summary | **Complete Technical Specs** for reproducibility |",
          "created_at": "2026-01-07T13:03:12Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q6c9wc",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://www.reddit.com/gallery/1q6c9wc\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://arxiv.org/abs/2501.12948",
          "was_fetched": true,
          "page": "Title: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\nURL Source: https://arxiv.org/abs/2501.12948\n\nPublished Time: Tue, 06 Jan 2026 01:32:27 GMT\n\nMarkdown Content:\n[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n===============\n\n[Skip to main content](https://arxiv.org/abs/2501.12948#content)\n\n[](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n\n[](https://arxiv.org/)\u003e[cs](https://arxiv.org/list/cs/recent)\u003e arXiv:2501.12948 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[](https://arxiv.org/)\n\n[](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science \u003e Computation and Language\n===========================================\n\n**arXiv:2501.12948** (cs) \n\n [Submitted on 22 Jan 2025 ([v1](https://arxiv.org/abs/2501.12948v1)), last revised 4 Jan 2026 (this version, v2)]\n\nTitle:DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n========================================================================================\n\nAuthors:[DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author\u0026query=DeepSeek-AI), [Daya Guo](https://arxiv.org/search/cs?searchtype=author\u0026query=Guo,+D), [Dejian Yang](https://arxiv.org/search/cs?searchtype=author\u0026query=Yang,+D), [Haowei Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+H), [Junxiao Song](https://arxiv.org/search/cs?searchtype=author\u0026query=Song,+J), [Peiyi Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+P), [Qihao Zhu](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhu,+Q), [Runxin Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+R), [Ruoyu Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+R), [Shirong Ma](https://arxiv.org/search/cs?searchtype=author\u0026query=Ma,+S), [Xiao Bi](https://arxiv.org/search/cs?searchtype=author\u0026query=Bi,+X), [Xiaokang Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+X), [Xingkai Yu](https://arxiv.org/search/cs?searchtype=author\u0026query=Yu,+X), [Yu Wu](https://arxiv.org/search/cs?searchtype=author\u0026query=Wu,+Y), [Z.F. Wu](https://arxiv.org/search/cs?searchtype=author\u0026query=Wu,+Z), [Zhibin Gou](https://arxiv.org/search/cs?searchtype=author\u0026query=Gou,+Z), [Zhihong Shao](https://arxiv.org/search/cs?searchtype=author\u0026query=Shao,+Z), [Zhuoshu Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+Z), [Ziyi Gao](https://arxiv.org/search/cs?searchtype=author\u0026query=Gao,+Z), [Aixin Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+A), [Bing Xue](https://arxiv.org/search/cs?searchtype=author\u0026query=Xue,+B), [Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+B), [Bochao Wu](https://arxiv.org/search/cs?searchtype=author\u0026query=Wu,+B), [Bei Feng](https://arxiv.org/search/cs?searchtype=author\u0026query=Feng,+B), [Chengda Lu](https://arxiv.org/search/cs?searchtype=author\u0026query=Lu,+C), [Chenggang Zhao](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhao,+C), [Chengqi Deng](https://arxiv.org/search/cs?searchtype=author\u0026query=Deng,+C), [Chenyu Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+C), [Chong Ruan](https://arxiv.org/search/cs?searchtype=author\u0026query=Ruan,+C), [Damai Dai](https://arxiv.org/search/cs?searchtype=author\u0026query=Dai,+D), [Deli Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+D), [Dongjie Ji](https://arxiv.org/search/cs?searchtype=author\u0026query=Ji,+D), [Erhang Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+E), [Fangyun Lin](https://arxiv.org/search/cs?searchtype=author\u0026query=Lin,+F), [Fucong Dai](https://arxiv.org/search/cs?searchtype=author\u0026query=Dai,+F), [Fuli Luo](https://arxiv.org/search/cs?searchtype=author\u0026query=Luo,+F), [Guangbo Hao](https://arxiv.org/search/cs?searchtype=author\u0026query=Hao,+G), [Guanting Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+G), [Guowei Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+G), [H. Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+H), [Han Bao](https://arxiv.org/search/cs?searchtype=author\u0026query=Bao,+H), [Hanwei Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+H), [Haocheng Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+H), [Honghui Ding](https://arxiv.org/search/cs?searchtype=author\u0026query=Ding,+H), [Huajian Xin](https://arxiv.org/search/cs?searchtype=author\u0026query=Xin,+H), [Huazuo Gao](https://arxiv.org/search/cs?searchtype=author\u0026query=Gao,+H), [Hui Qu](https://arxiv.org/search/cs?searchtype=author\u0026query=Qu,+H), [Hui Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+H), [Jianzhong Guo](https://arxiv.org/search/cs?searchtype=author\u0026query=Guo,+J), [Jiashi Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+J), [Jiawei Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+J), [Jingchang Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+J), [Jingyang Yuan](https://arxiv.org/search/cs?searchtype=author\u0026query=Yuan,+J), [Junjie Qiu](https://arxiv.org/search/cs?searchtype=author\u0026query=Qiu,+J), [Junlong Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+J), [J.L. Cai](https://arxiv.org/search/cs?searchtype=author\u0026query=Cai,+J), [Jiaqi Ni](https://arxiv.org/search/cs?searchtype=author\u0026query=Ni,+J), [Jian Liang](https://arxiv.org/search/cs?searchtype=author\u0026query=Liang,+J), [Jin Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+J), [Kai Dong](https://arxiv.org/search/cs?searchtype=author\u0026query=Dong,+K), [Kai Hu](https://arxiv.org/search/cs?searchtype=author\u0026query=Hu,+K), [Kaige Gao](https://arxiv.org/search/cs?searchtype=author\u0026query=Gao,+K), [Kang Guan](https://arxiv.org/search/cs?searchtype=author\u0026query=Guan,+K), [Kexin Huang](https://arxiv.org/search/cs?searchtype=author\u0026query=Huang,+K), [Kuai Yu](https://arxiv.org/search/cs?searchtype=author\u0026query=Yu,+K), [Lean Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+L), [Lecong Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+L), [Liang Zhao](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhao,+L), [Litong Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+L), [Liyue Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+L), [Lei Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+L), [Leyi Xia](https://arxiv.org/search/cs?searchtype=author\u0026query=Xia,+L), [Mingchuan Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+M), [Minghua Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+M), [Minghui Tang](https://arxiv.org/search/cs?searchtype=author\u0026query=Tang,+M), [Meng Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+M), [Miaojun Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+M), [Mingming Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+M), [Ning Tian](https://arxiv.org/search/cs?searchtype=author\u0026query=Tian,+N), [Panpan Huang](https://arxiv.org/search/cs?searchtype=author\u0026query=Huang,+P), [Peng Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+P), [Qiancheng Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+Q), [Qinyu Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+Q), [Qiushi Du](https://arxiv.org/search/cs?searchtype=author\u0026query=Du,+Q), [Ruiqi Ge](https://arxiv.org/search/cs?searchtype=author\u0026query=Ge,+R), [Ruisong Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+R), [Ruizhe Pan](https://arxiv.org/search/cs?searchtype=author\u0026query=Pan,+R), [Runji Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+R), [R.J. Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+R), [R.L. Jin](https://arxiv.org/search/cs?searchtype=author\u0026query=Jin,+R), [Ruyi Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+R), [Shanghao Lu](https://arxiv.org/search/cs?searchtype=author\u0026query=Lu,+S), [Shangyan Zhou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhou,+S), [Shanhuang Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+S), [Shengfeng Ye](https://arxiv.org/search/cs?searchtype=author\u0026query=Ye,+S), [Shiyu Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+S), [Shuiping Yu](https://arxiv.org/search/cs?searchtype=author\u0026query=Yu,+S), [Shunfeng Zhou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhou,+S), [Shuting Pan](https://arxiv.org/search/cs?searchtype=author\u0026query=Pan,+S), [S.S. Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+S)\n\n , [Shuang Zhou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhou,+S), [Shaoqing Wu](https://arxiv.org/search/cs?searchtype=author\u0026query=Wu,+S), [Shengfeng Ye](https://arxiv.org/search/cs?searchtype=author\u0026query=Ye,+S), [Tao Yun](https://arxiv.org/search/cs?searchtype=author\u0026query=Yun,+T), [Tian Pei](https://arxiv.org/search/cs?searchtype=author\u0026query=Pei,+T), [Tianyu Sun](https://arxiv.org/search/cs?searchtype=author\u0026query=Sun,+T), [T. Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+T), [Wangding Zeng](https://arxiv.org/search/cs?searchtype=author\u0026query=Zeng,+W), [Wanjia Zhao](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhao,+W), [Wen Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+W), [Wenfeng Liang](https://arxiv.org/search/cs?searchtype=author\u0026query=Liang,+W), [Wenjun Gao](https://arxiv.org/search/cs?searchtype=author\u0026query=Gao,+W), [Wenqin Yu](https://arxiv.org/search/cs?searchtype=author\u0026query=Yu,+W), [Wentao Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+W), [W.L. Xiao](https://arxiv.org/search/cs?searchtype=author\u0026query=Xiao,+W), [Wei An](https://arxiv.org/search/cs?searchtype=author\u0026query=An,+W), [Xiaodong Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+X), [Xiaohan Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+X), [Xiaokang Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+X), [Xiaotao Nie](https://arxiv.org/search/cs?searchtype=author\u0026query=Nie,+X), [Xin Cheng](https://arxiv.org/search/cs?searchtype=author\u0026query=Cheng,+X), [Xin Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+X), [Xin Xie](https://arxiv.org/search/cs?searchtype=author\u0026query=Xie,+X), [Xingchao Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+X), [Xinyu Yang](https://arxiv.org/search/cs?searchtype=author\u0026query=Yang,+X), [Xinyuan Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+X), [Xuecheng Su](https://arxiv.org/search/cs?searchtype=author\u0026query=Su,+X), [Xuheng Lin](https://arxiv.org/search/cs?searchtype=author\u0026query=Lin,+X), [X.Q. Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+X), [Xiangyue Jin](https://arxiv.org/search/cs?searchtype=author\u0026query=Jin,+X), [Xiaojin Shen](https://arxiv.org/search/cs?searchtype=author\u0026query=Shen,+X), [Xiaosha Chen](https://arxiv.org/search/cs?searchtype=author\u0026query=Chen,+X), [Xiaowen Sun](https://arxiv.org/search/cs?searchtype=author\u0026query=Sun,+X), [Xiaoxiang Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+X), [Xinnan Song](https://arxiv.org/search/cs?searchtype=author\u0026query=Song,+X), [Xinyi Zhou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhou,+X), [Xianzu Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+X), [Xinxia Shan](https://arxiv.org/search/cs?searchtype=author\u0026query=Shan,+X), [Y.K. Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+Y), [Y.Q. Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+Y), [Y.X. Wei](https://arxiv.org/search/cs?searchtype=author\u0026query=Wei,+Y), [Yang Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+Y), [Yanhong Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+Y), [Yao Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+Y), [Yao Zhao](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhao,+Y), [Yaofeng Sun](https://arxiv.org/search/cs?searchtype=author\u0026query=Sun,+Y), [Yaohui Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+Y), [Yi Yu](https://arxiv.org/search/cs?searchtype=author\u0026query=Yu,+Y), [Yichao Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+Y), [Yifan Shi](https://arxiv.org/search/cs?searchtype=author\u0026query=Shi,+Y), [Yiliang Xiong](https://arxiv.org/search/cs?searchtype=author\u0026query=Xiong,+Y), [Ying He](https://arxiv.org/search/cs?searchtype=author\u0026query=He,+Y), [Yishi Piao](https://arxiv.org/search/cs?searchtype=author\u0026query=Piao,+Y), [Yisong Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+Y), [Yixuan Tan](https://arxiv.org/search/cs?searchtype=author\u0026query=Tan,+Y), [Yiyang Ma](https://arxiv.org/search/cs?searchtype=author\u0026query=Ma,+Y), [Yiyuan Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+Y), [Yongqiang Guo](https://arxiv.org/search/cs?searchtype=author\u0026query=Guo,+Y), [Yuan Ou](https://arxiv.org/search/cs?searchtype=author\u0026query=Ou,+Y), [Yuduan Wang](https://arxiv.org/search/cs?searchtype=author\u0026query=Wang,+Y), [Yue Gong](https://arxiv.org/search/cs?searchtype=author\u0026query=Gong,+Y), [Yuheng Zou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zou,+Y), [Yujia He](https://arxiv.org/search/cs?searchtype=author\u0026query=He,+Y), [Yunfan Xiong](https://arxiv.org/search/cs?searchtype=author\u0026query=Xiong,+Y), [Yuxiang Luo](https://arxiv.org/search/cs?searchtype=author\u0026query=Luo,+Y), [Yuxiang You](https://arxiv.org/search/cs?searchtype=author\u0026query=You,+Y), [Yuxuan Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+Y), [Yuyang Zhou](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhou,+Y), [Y.X. Zhu](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhu,+Y), [Yanhong Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+Y), [Yanping Huang](https://arxiv.org/search/cs?searchtype=author\u0026query=Huang,+Y), [Yaohui Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+Y), [Yi Zheng](https://arxiv.org/search/cs?searchtype=author\u0026query=Zheng,+Y), [Yuchen Zhu](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhu,+Y), [Yunxian Ma](https://arxiv.org/search/cs?searchtype=author\u0026query=Ma,+Y), [Ying Tang](https://arxiv.org/search/cs?searchtype=author\u0026query=Tang,+Y), [Yukun Zha](https://arxiv.org/search/cs?searchtype=author\u0026query=Zha,+Y), [Yuting Yan](https://arxiv.org/search/cs?searchtype=author\u0026query=Yan,+Y), [Z.Z. Ren](https://arxiv.org/search/cs?searchtype=author\u0026query=Ren,+Z), [Zehui Ren](https://arxiv.org/search/cs?searchtype=author\u0026query=Ren,+Z), [Zhangli Sha](https://arxiv.org/search/cs?searchtype=author\u0026query=Sha,+Z), [Zhe Fu](https://arxiv.org/search/cs?searchtype=author\u0026query=Fu,+Z), [Zhean Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+Z), [Zhenda Xie](https://arxiv.org/search/cs?searchtype=author\u0026query=Xie,+Z), [Zhengyan Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+Z), [Zhewen Hao](https://arxiv.org/search/cs?searchtype=author\u0026query=Hao,+Z), [Zhicheng Ma](https://arxiv.org/search/cs?searchtype=author\u0026query=Ma,+Z), [Zhigang Yan](https://arxiv.org/search/cs?searchtype=author\u0026query=Yan,+Z), [Zhiyu Wu](https://arxiv.org/search/cs?searchtype=author\u0026query=Wu,+Z), [Zihui Gu](https://arxiv.org/search/cs?searchtype=author\u0026query=Gu,+Z), [Zijia Zhu](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhu,+Z), [Zijun Liu](https://arxiv.org/search/cs?searchtype=author\u0026query=Liu,+Z), [Zilin Li](https://arxiv.org/search/cs?searchtype=author\u0026query=Li,+Z), [Ziwei Xie](https://arxiv.org/search/cs?searchtype=author\u0026query=Xie,+Z), [Ziyang Song](https://arxiv.org/search/cs?searchtype=author\u0026query=Song,+Z), [Zizheng Pan](https://arxiv.org/search/cs?searchtype=author\u0026query=Pan,+Z), [Zhen Huang](https://arxiv.org/search/cs?searchtype=author\u0026query=Huang,+Z), [Zhipeng Xu](https://arxiv.org/search/cs?searchtype=author\u0026query=Xu,+Z), [Zhongyu Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+Z), [Zhen Zhang](https://arxiv.org/search/cs?searchtype=author\u0026query=Zhang,+Z)\n\n[et al. (100 additional authors not shown)](javascript:toggleAuthorList('long-author-list','et al. (100 additional authors not shown)'); \"Show Entire Author List\")\n\nView a PDF of the paper titled DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, by DeepSeek-AI and 199 other authors\n\n[View PDF](https://arxiv.org/pdf/2501.12948)\n\u003e Abstract:General reasoning represents a long-standing and formidable challenge in artificial intelligence. Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent upon extensive human-annotated demonstrations, and models' capabilities are still insufficient for more complex problems. Here we show that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labeled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification, and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions, and STEM fields, surpassing its counterparts trained via conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically harnessed to guide and enhance the reasoning capabilities of smaller models.\n\nSubjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\nCite as:[arXiv:2501.12948](https://arxiv.org/abs/2501.12948) [cs.CL]\n(or [arXiv:2501.12948v2](https://arxiv.org/abs/2501.12948v2) [cs.CL] for this version)\n[https://doi.org/10.48550/arXiv.2501.12948](https://doi.org/10.48550/arXiv.2501.12948)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite\nJournal reference:Nature volume 645, pages 633-638 (2025)\nRelated DOI:[https://doi.org/10.1038/s41586-025-09422-z](https://doi.org/10.1038/s41586-025-09422-z)\n\nFocus to learn more\n\n DOI(s) linking to related resources\n\nSubmission history\n------------------\n\n From: Wenfeng Liang [[view email](https://arxiv.org/show-email/d9d56d83/2501.12948)] \n\n**[[v1]](https://arxiv.org/abs/2501.12948v1)** Wed, 22 Jan 2025 15:19:35 UTC (928 KB)\n\n**[v2]** Sun, 4 Jan 2026 03:57:36 UTC (1,562 KB)\n\n[](https://arxiv.org/abs/2501.12948)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, by DeepSeek-AI and 199 other authors\n\n*   [View PDF](https://arxiv.org/pdf/2501.12948)\n*   [TeX Source](https://arxiv.org/src/2501.12948)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.CL\n\n[\u003cprev](https://arxiv.org/prevnext?id=2501.12948\u0026function=prev\u0026context=cs.CL \"previous in cs.CL (accesskey p)\") | [next\u003e](https://arxiv.org/prevnext?id=2501.12948\u0026function=next\u0026context=cs.CL \"next in cs.CL (accesskey n)\")\n\n[new](https://arxiv.org/list/cs.CL/new) | [recent](https://arxiv.org/list/cs.CL/recent) | [2025-01](https://arxiv.org/list/cs.CL/2025-01)\n\n Change to browse by: \n\n[cs](https://arxiv.org/abs/2501.12948?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2501.12948?context=cs.AI)\n\n[cs.LG](https://arxiv.org/abs/2501.12948?context=cs.LG)\n\n### References \u0026 Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2501.12948)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2501.12948)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2501.12948)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\nÃ—\n\nData provided by: [](https://arxiv.org/abs/2501.12948)\n\n### Bookmark\n\n[](http://www.bibsonomy.org/BibtexHandler?requTask=upload\u0026url=https://arxiv.org/abs/2501.12948\u0026description=DeepSeek-R1:%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning \"Bookmark on BibSonomy\")[](https://reddit.com/submit?url=https://arxiv.org/abs/2501.12948\u0026title=DeepSeek-R1:%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n*   [Author](https://arxiv.org/abs/2501.12948)\n*   [Venue](https://arxiv.org/abs/2501.12948)\n*   [Institution](https://arxiv.org/abs/2501.12948)\n*   [Topic](https://arxiv.org/abs/2501.12948)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2501.12948) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:42.233733868Z"
    },
    {
      "flow_id": "",
      "id": "1q6o39r",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/",
      "title": "Plea for testers - Llama.cpp autoparser",
      "content": "I would like to ask the community to aid in the testing of the new autoparser mechanism that I've been cooking for llama.cpp for the past month or so. \n\nThe idea is to scrap the existing buggy mess of the chat parsers and replace it with a layered mechanism:  \n\\-\u0026gt; autoparser that handles 95%+ of typical chat templates for models  \n\\-\u0026gt; manual parsers / handlers for models that need something extra\n\nCurrently of all models that I've tested, only Ministral and GPT-OSS have shown the need to use a dedicated parser. I've tested the approach as extensively with as many models as I could, but I'm just a single dev doing this after hours, so I obviously can't do long coding sessions on all possible models. Therefore, I'd ask everyone who's able to test it with their favorite coding agent (I mostly used OpenCode and Roo, it's important to use an agent that actually uses tool calls, so Aider is out) because I'm quite sure there will be quite a few bugs.\n\nSince I don't want to clutter the main repo, please report all bugs with the autoparser to [https://github.com/pwilkin/llama.cpp/issues](https://github.com/pwilkin/llama.cpp/issues) instead.",
      "author": "ilintar",
      "created_at": "2026-01-07T18:54:18Z",
      "comments": [
        {
          "id": "ny9efqd",
          "author": "phree_radical",
          "content": "\u0026gt; AI DISCLOSURE: Gemini Pro 3, Flash 3, Opus 4.5 and GLM 4.7 would like to admit that a human element did at some points interfere in the coding process, being as bold as to even throw most of the code out at some point and demand it rewritten from scratch. The human also tinkered the code massively, removing a lot of our beautiful comments and some code fragments that they claimed were useless. They had no problems, however, in using us to do all the annoying marker arithmetic. Therefore, we disavow any claim to this code and cede the responsibility onto the human.",
          "created_at": "2026-01-07T19:58:42Z",
          "was_summarised": false
        },
        {
          "id": "ny961w4",
          "author": "1ncehost",
          "content": "ðŸ‘ to this effort\n\nJust for a sanity check, did you make regression tests for the old system you can run against the new system?",
          "created_at": "2026-01-07T19:22:17Z",
          "was_summarised": false
        },
        {
          "id": "nyahyh6",
          "author": "wanderer_4004",
          "content": "Is there somewhere a list of which models have been tested and confirmed working?",
          "created_at": "2026-01-07T22:52:17Z",
          "was_summarised": false
        },
        {
          "id": "nya4mo5",
          "author": "wadeAlexC",
          "content": "Is this related to this issue? https://github.com/ggml-org/llama.cpp/issues/18183\n\nI can try to replicate my Qwen3-30B-A3B issues if so :)",
          "created_at": "2026-01-07T21:51:43Z",
          "was_summarised": false
        },
        {
          "id": "nyaubt3",
          "author": "CheatCodesOfLife",
          "content": "Did you only want bugs, or did  you want confirmed working? Eg:\n\nModel | Agent | Endpoint\nQwen3-235B | claude code | `/messages`\n\nAnd what's the pass criteria eg. a simple claude code \"/init\" and something like \"merge \u0026lt;branch\u0026gt; into \u0026lt;branch\u0026gt; and resolve all conflicts\" ?",
          "created_at": "2026-01-07T23:54:16Z",
          "was_summarised": false
        },
        {
          "id": "nydf2ts",
          "author": "spaceman_",
          "content": "Building some container images based on your branch atm, will let you know how it works out.\n\nHow should I test this? Just chat with a model through the web UI, or through other clients (like kilo/roo/continue/...)?\n\nI have the following models set up, let me know if there are any in particular you want me to test:\n\n    - gemma3  \n    - gpt-oss-120b  \n    - gpt-oss-20b   \n    - hermes-4.3-36b  \n    - intellect-3  \n    - minimax-m2  \n    - minimax-m2.1  \n    - qwen2.5-coder  \n    - qwen3-next-instruct  \n    - qwen3-next  \n    - precog-24b  \n    - trinity-mini  \n    - youtu-llm  \n\nEdit: anything that offloads to Vulkan seems to crash at the first output token received. Running the same models CPU only seems to work fine.",
          "created_at": "2026-01-08T10:00:23Z",
          "was_summarised": false
        },
        {
          "id": "nyezx7v",
          "author": "ilintar",
          "content": "Happy to report that thanks to mbauer's issue (https://github.com/pwilkin/llama.cpp/issues/4) the PR again correctly handles models \\*without\\* tool calling.",
          "created_at": "2026-01-08T15:49:26Z",
          "was_summarised": false
        },
        {
          "id": "nyb916t",
          "author": "llama-impersonator",
          "content": "do we have to generate the gguf again to try this or does it use the template that already got baked in?",
          "created_at": "2026-01-08T01:08:25Z",
          "was_summarised": false
        },
        {
          "id": "nybaaap",
          "author": "StorageHungry8380",
          "content": "To test, can I simply call the `/apply-template` endpoint against old and new code, along with some `chat_template_kwargs` where that matter?",
          "created_at": "2026-01-08T01:15:01Z",
          "was_summarised": false
        },
        {
          "id": "nyfim78",
          "author": "Kamal965",
          "content": "Hey u/ilintar \\- I'm in a rush but I managed to test two models in the time I had using the Roo Code VS Code extension. Qwen3 Next 80B worked flawlessly, but the new Korean model HyperNova wasn't working properly. It's based off of GPT-OSS - so do you think that might be the reason why?",
          "created_at": "2026-01-08T17:11:32Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/ggml-org/llama.cpp/pull/18675",
          "was_fetched": true,
          "page": "Title: Autoparser - complete refactoring of parser architecture by pwilkin Â· Pull Request #18675 Â· ggml-org/llama.cpp\n\nURL Source: https://github.com/ggml-org/llama.cpp/pull/18675\n\nMarkdown Content:\nAutoparser - complete refactoring of parser architecture by pwilkin Â· Pull Request #18675 Â· ggml-org/llama.cpp Â· GitHub\n===============\n\n[Skip to content](https://github.com/ggml-org/llama.cpp/pull/18675#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F18675)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F18675)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout\u0026source=header-repo\u0026source_repo=ggml-org%2Fllama.cpp)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/ggml-org/llama.cpp/pull/18675) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/ggml-org/llama.cpp/pull/18675) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/ggml-org/llama.cpp/pull/18675) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[ggml-org](https://github.com/ggml-org)/**[llama.cpp](https://github.com/ggml-org/llama.cpp)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp)You must be signed in to change notification settings\n*   [Fork 14.4k](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp)\n*   [Star 92.7k](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp) \n\n*   [Code](https://github.com/ggml-org/llama.cpp)\n*   [Issues 349](https://github.com/ggml-org/llama.cpp/issues)\n*   [Pull requests 643](https://github.com/ggml-org/llama.cpp/pulls)\n*   [Discussions](https://github.com/ggml-org/llama.cpp/discussions)\n*   [Actions](https://github.com/ggml-org/llama.cpp/actions)\n*   [Projects 1](https://github.com/ggml-org/llama.cpp/projects)\n*   [Wiki](https://github.com/ggml-org/llama.cpp/wiki)\n*   [Security 10](https://github.com/ggml-org/llama.cpp/security)[](https://github.com/ggml-org/llama.cpp/security)[](https://github.com/ggml-org/llama.cpp/security)[](https://github.com/ggml-org/llama.cpp/security)[### Uh oh!](https://github.com/ggml-org/llama.cpp/security)\n[There was an error while loading.](https://github.com/ggml-org/llama.cpp/security)[Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).    \n*   [Insights](https://github.com/ggml-org/llama.cpp/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/ggml-org/llama.cpp)\n*   [Issues](https://github.com/ggml-org/llama.cpp/issues)\n*   [Pull requests](https://github.com/ggml-org/llama.cpp/pulls)\n*   [Discussions](https://github.com/ggml-org/llama.cpp/discussions)\n*   [Actions](https://github.com/ggml-org/llama.cpp/actions)\n*   [Projects](https://github.com/ggml-org/llama.cpp/projects)\n*   [Wiki](https://github.com/ggml-org/llama.cpp/wiki)\n*   [Security](https://github.com/ggml-org/llama.cpp/security)\n*   [Insights](https://github.com/ggml-org/llama.cpp/pulse)\n\nAutoparser - complete refactoring of parser architecture#18675\n==============================================================\n\n New issue \n\n**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n\n[Sign up for GitHub](https://github.com/signup?return_to=%2Fggml-org%2Fllama.cpp%2Fissues%2Fnew%2Fchoose)\n\nBy clicking â€œSign up for GitHubâ€, you agree to our [terms of service](https://docs.github.com/terms) and [privacy statement](https://docs.github.com/privacy). Weâ€™ll occasionally send you account related emails.\n\nAlready on GitHub? [Sign in](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp%2Fissues%2Fnew%2Fchoose) to your account\n\n[Jump to bottom](https://github.com/ggml-org/llama.cpp/pull/18675#issue-comment-box)\n\n Draft \n\n[pwilkin](https://github.com/pwilkin) wants to merge 68 commits into [ggml-org:master](https://github.com/ggml-org/llama.cpp/tree/master \"ggml-org/llama.cpp:master\")\n\n_base:_ master\n\nChoose a base branch\n\nBranches Tags\n\nCould not load branches\n\nBranch not found: **{{ refName }}**\n\nLoading\n\n{{ refName }}default\n\nCould not load tags\n\nNothing to show\n\n{{ refName }}default\n\nLoading\n\n### Are you sure you want to change the base?\n\n Some commits from the old base branch may be removed from the timeline, and old review comments may become outdated. \n\nLoading Change base\n\n from [pwilkin:autoparser](https://github.com/pwilkin/llama.cpp/tree/autoparser \"pwilkin/llama.cpp:autoparser\")\n\n Draft \n\n[Autoparser - complete refactoring of parser architecture](https://github.com/ggml-org/llama.cpp/pull/18675#top)#18675\n======================================================================================================================\n\n[pwilkin](https://github.com/pwilkin) wants to merge 68 commits into [ggml-org:master](https://github.com/ggml-org/llama.cpp/tree/master \"ggml-org/llama.cpp:master\") from [pwilkin:autoparser](https://github.com/pwilkin/llama.cpp/tree/autoparser \"pwilkin/llama.cpp:autoparser\")\n\n +10,137  âˆ’11,036 \n\n[Conversation 5](https://github.com/ggml-org/llama.cpp/pull/18675)[Commits 68](https://github.com/ggml-org/llama.cpp/pull/18675/commits)[Checks 83](https://github.com/ggml-org/llama.cpp/pull/18675/checks)[Files changed 43](https://github.com/ggml-org/llama.cpp/pull/18675/files)\n\nConversation\n------------\n\n This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)\n\n[Show hidden characters](https://github.com/ggml-org/llama.cpp/pull/%7B%7B%20revealButtonHref%20%7D%7D)\n\n[](https://github.com/pwilkin)\n\n Copy link \n\nCollaborator\n\n### **[pwilkin](https://github.com/pwilkin)** commented [Jan 7, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#issue-3789925215)â€¢\n\n edited \n\nLoading\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\nThis is a huge endeavor that I promised back when I applied for maintaining the parser code. The legacy parser code was hard to maintain and buggy and supporting new models with it was really annoying. There was a worthwhile contribution by [@hksdpc255](https://github.com/hksdpc255) to add some XML toolcalling abstractions, but that was still just a patch on an open wound.\n\nThanks to [@aldehir](https://github.com/aldehir) and his PEG parser, I managed to create an autoparser mechanism, using all the currently supported templates, their parsers and test cases as base. The idea is simple: most models' syntax follows the general pattern of:\n\n`\u003creasoning_markers\u003e \u003creasoning_content\u003e \u003cend_of_reasoning_markers\u003e \u003ccontent_markers\u003e \u003cmain_content\u003e \u003cend_of_content_markers\u003e \u003ctool_call_markers\u003e ( \u003cjson\u003e | \u003cfunction marker\u003e \u003cargs json\u003e | \u003cfunction marker\u003e \u003cargs marker\u003e \u003cvalue json\u003e ) \u003cend_of_tool_call_marker\u003e`\n\nOf course, some elements might not be present in a given template, but that's the general structure. Since this is a pretty finite structure, it's possible to determine the relevant elements by differential analysis - similar to how Minja already does capability detection, but more fine-grained, because by comparing various template outputs, we get to actually extract the relevant markers.\n\nSome models will obviously not get handled so easily. However, in the course of implementing the mechanism, only two models remained that needed to get their separate parsers: Ministral and GPT-OSS, and the prior not because of its complexity, but of the need to rewrite the message structure passed to the template. GPT-OSS is a different beast since it supports arbitrarily many interleaved blocks, so it doesn't fit into the scheme that I mentioned above (but its parser has been rewritten to PEG as well).\n\nThis is currently anchored on Minja and uses its capability detection, but since the differential analysis already does its own capability detection, I fully expect to throw that part out and base this on [@ngxson](https://github.com/ngxson) 's [#18462](https://github.com/ggml-org/llama.cpp/pull/18462) instead.\n\nObsoletes [#18353](https://github.com/ggml-org/llama.cpp/pull/18353) (sorry [@ochafik](https://github.com/ochafik) - I know you put a lot of work into that).\n\nOld parsers, tests and all supporting code are thrown out, templates got new PEG-parser based testcases, all of them now also test streaming behavior. I have tested this extensively on agentic coding (mostly with OpenCode) to ensure that this actually works (my wish to refactor the parser code was mostly caused by my prior experience with agentic coding on llama.cpp, which was extremely buggy with a lot of models, this is an attempt to remedy that). Hopefully, having one unified codebase with a largely reduced line-of-code count will make it easier to fix any potential errors.\n\nThis also means that there is no longer need to provide support for new models' specific templates unless they have some odd constructs - they should be supported out of the box. There's a new tool called `debug-template-parser` that you can point to any Jinja template file or GGUF model with an embedded Jinja template and have it spit out the details of the generated autoparser + toolcaling grammar.\n\nOh, important note: _all_ Minja polyfills have been disabled. Working templates are now required. Why I see why a year and a half ago having proof-of-concept code that supported tool calling on models that didn't natively have tool calling might've been useless, right now supporting that is making it harder to properly support current and actually used models. Therefore, a functional template with tool calling is required if someone wants tool calling.\n\nI want to ask everyone from the community who can to test this. I will keep this branch current with master, I tried to test this as much as I could, but I'm just one person doing this after work, so obviously my testing abilities were limited. I will keep this as draft until I've gathered enough feedback and testing data.\n\nTo not clutter the main repository's issue tracker, please report bugs either (a) in this thread or (b) in my issue tracker [https://github.com/pwilkin/llama.cpp/issues](https://github.com/pwilkin/llama.cpp/issues)\n\nAI DISCLOSURE: Gemini Pro 3, Flash 3, Opus 4.5 and GLM 4.7 would like to admit that a human element did at some points interfere in the coding process, being as bold as to even throw most of the code out at some point and demand it rewritten from scratch. The human also tinkered the code massively, removing a lot of our beautiful comments and some code fragments that they claimed were useless. They had no problems, however, in using us to do all the annoying marker arithmetic. Therefore, we disavow any claim to this code and cede the responsibility onto the human.\n\nSorry, something went wrong.\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\nðŸ‘4 hksdpc255, KernelFreeze, calvin2021y, and firengate reacted with thumbs up emojiðŸŽ‰1 firengate reacted with hooray emojiâ¤ï¸5 ochafik, sammcj, ddh0, calvin2021y, and firengate reacted with heart emojiðŸš€4 ddh0, calvin2021y, Orion-zhen, and firengate reacted with rocket emojiðŸ‘€4 ochafik, ddh0, calvin2021y, and firengate reacted with eyes emoji\n\nAll reactions\n*   ðŸ‘4 reactions\n*   ðŸŽ‰1 reaction\n*   â¤ï¸5 reactions\n*   ðŸš€4 reactions\n*   ðŸ‘€4 reactions\n\n[](https://github.com/loci-dev)[loci-dev](https://github.com/loci-dev) mentioned this pull request [Jan 7, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#ref-pullrequest-3790085847)\n\n[UPSTREAM PR #18675: Autoparser - complete refactoring of parser architecture auroralabs-loci/llama.cpp#845](https://github.com/auroralabs-loci/llama.cpp/pull/845)\n\n Open \n\n[](https://github.com/apps/github-actions)[github-actions](https://github.com/apps/github-actions)bot added [documentation](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Adocumentation)Improvements or additions to documentation[model](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Amodel)Model specific[script](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Ascript)Script related[testing](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Atesting)Everything test related[examples](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Aexamples)[python](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Apython)python script changes[server](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Aserver) labels [Jan 7, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#event-21906904776)\n\n[](https://github.com/apps/github-actions)[github-actions](https://github.com/apps/github-actions)bot mentioned this pull request [Jan 8, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#ref-issue-3790956046)\n\n[Reddit News Daily 2026-01-08 gitlawr/reddit-daily-news#118](https://github.com/gitlawr/reddit-daily-news/issues/118)\n\n Open \n\n[](https://github.com/hksdpc255)\n\n Copy link \n\nContributor\n\n### **[hksdpc255](https://github.com/hksdpc255)** commented [Jan 8, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#issuecomment-3721672418)\n\nDoes this mean we donâ€™t need to write a parser anymore, and it will be automatically generated from the chat template?\n\nAll reactions\n\nSorry, something went wrong.\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n[](https://github.com/pwilkin)\n\n Copy link \n\nCollaborator Author\n\n### **[pwilkin](https://github.com/pwilkin)** commented [Jan 8, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#issuecomment-3721725459)\n\n\u003e Does this mean we donâ€™t need to write a parser anymore, and it will be automatically generated from the chat template?\n\nYup, that's the gist of it.\n\nAll reactions\n\nSorry, something went wrong.\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n[](https://github.com/hksdpc255)\n\n Copy link \n\nContributor\n\n### **[hksdpc255](https://github.com/hksdpc255)** commented [Jan 8, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#issuecomment-3721770919)\n\nThis feels almost magical. How does it work? Does it detect common patterns in the rendered template output? What happens if the chat template requires additional arguments?\n\nAll reactions\n\nSorry, something went wrong.\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n[](https://github.com/pwilkin)\n\n Copy link \n\nCollaborator Author\n\n### **[pwilkin](https://github.com/pwilkin)** commented [Jan 8, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#issuecomment-3723565777)â€¢\n\n edited \n\nLoading\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n\u003e This feels almost magical. How does it work? Does it detect common patterns in the rendered template output? What happens if the chat template requires additional arguments?\n\nYeah, it does differential analysis - it prepares different inputs to the template and then tests the outputs, for example, by using a the same function signature with a different name you can identify where the function name goes, by using the same function with one and two parameters you can identify how parameters are passed etc. etc.\n\nThe nice thing is, I managed to squish it to just 2k lines of code (1k for analysis and 1k for helpers), so it's not even that bloated.\n\nAs for custom inputs - I assume standard inputs here and that's what most template makers try to adhere to anyway. If not, you end up with a custom handler like for Ministral - but as a followup I want to separate handlers from parsers (since passing extra params is much eaasier than handling an entire template from scratch) or even add autodetection for common custom keywords (we're going to have to support \"reasoning\" in addition to \"reasoning_content\" at some point because vLLM is moving to that).\n\nAll reactions\n\nSorry, something went wrong.\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n[](https://github.com/pwilkin)[pwilkin](https://github.com/pwilkin)[force-pushed](https://github.com/ggml-org/llama.cpp/compare/0e52270b32deda2dcdc57616bf30f2111b753f0f..dc7dd03932c544c10b870fc29c706fcde9942a09) the autoparser branch from [`0e52270`](https://github.com/ggml-org/llama.cpp/commit/0e52270b32deda2dcdc57616bf30f2111b753f0f) to [`dc7dd03`](https://github.com/ggml-org/llama.cpp/commit/dc7dd03932c544c10b870fc29c706fcde9942a09)[Compare](https://github.com/ggml-org/llama.cpp/compare/0e52270b32deda2dcdc57616bf30f2111b753f0f..dc7dd03932c544c10b870fc29c706fcde9942a09)[January 8, 2026 14:09](https://github.com/ggml-org/llama.cpp/pull/18675#event-21924250781)\n\n[pwilkin](https://github.com/pwilkin)and others added 16 commits [January 8, 2026 15:52](https://github.com/ggml-org/llama.cpp/pull/18675#commits-pushed-7c93e99)\n\n[](https://github.com/pwilkin)\n\n```\nTHE GREATEST AUTOPARSER EVER!\n```\n\n```\n7c93e99\n```\n\n[](https://github.com/pwilkin)\n\n```\ncleanup\n```\n\n```\n5d86da4\n```\n\n[](https://github.com/pwilkin)\n\n```\nAdd grammar constraint generation\n```\n\n```\n8485964\n```\n\n[](https://github.com/pwilkin)\n\n```\nthinking_forced_open\n```\n\n```\n09165f0\n```\n\n[](https://github.com/pwilkin)\n\n```\nThe hard part\n```\n\n```\nf9f2310\n```\n\n[](https://github.com/pwilkin)\n\n```\nstill not building correct grammar :/\n```\n\n```\ne53907d\n```\n\n[](https://github.com/pwilkin)\n\n```\nfeat: enhance auto-parser generation tests with improved tool call arâ€¦\n```\n â€¦ \n\n```\nbff5a08\n```\n\nâ€¦gument comparison and add JSON schema grammar options for empty string and null enums\n\n[](https://github.com/pwilkin)\n\n```\nOh well...\n```\n\n```\n9799e66\n```\n\n[](https://github.com/pwilkin)\n\n```\nRevert changes to core files\n```\n\n```\na214d5c\n```\n\n[](https://github.com/pwilkin)\n\n```\nAll right, end of automatic labor. Extract standard tool calling parsâ€¦\n```\n â€¦ \n\n```\n752f017\n```\n\nâ€¦er, refactor build_native, TODO: build_constructed, fix failing extraction after changes\n\n[](https://github.com/pwilkin)\n\n```\nStarting to look better\n```\n\n```\n5157d02\n```\n\n[](https://github.com/pwilkin)[](https://github.com/claude)\n\n```\nfeat: add autoparser short-circuit for no-tools case and migrate tempâ€¦\n```\n â€¦ \n\n```\nbba0e56\n```\n\nâ€¦lates to PEG format\n\nAdd short-circuit logic to skip tool-calling analysis when no tools are provided:\n- Modified `analyze_template()` and `analyze_by_differential()` to accept `has_tools` parameter\n- When `has_tools=false`, skip tool-calling differential analysis entirely\n- Only analyze reasoning capability when no tools provided\n- Templates now correctly return `CONTENT_ONLY` when no tools, and `PEG_*` when tools provided\n\nMigrated the following templates from old format to PEG parser format in `test_template_output_peg_parsers()`:\n\n1. **CohereForAI Command-R 7B** (c4ai-command-r7b-12-2024-tool_use)\n   - 7 test scenarios: basic content, content with tags, thinking+content, tool calls, partial tool calls\n\n2. **Qwen-QwQ-32B** (reasoning model)\n   - 3 test scenarios: thinking content, content without thinking, tool calls with thinking\n\n3. **NousResearch Hermes-2-Pro / Hermes-3**\n   - 6 test scenarios: basic tool calls, tool calls with content, thinking, partial calls, alternate formats\n\n4. **Functionary v3.1**\n   - 3 test scenarios: basic tool call, partial tool call, content without tools\n\nDeleted old test blocks from `test_template_output_parsers()` for these templates.\n\n- Fixed Command-R-plus test expectation: now correctly expects `CONTENT_ONLY` when `inputs_no_tools` is used\n- Updated format detection assertions to match new autoparser behavior\n\nðŸ¤– Generated with [Claude Code]([https://claude.com/claude-code](https://claude.com/claude-code))\n\nCo-Authored-By: Claude Sonnet 4.5 \u003cnoreply@anthropic.com\u003e\n\n[](https://github.com/pwilkin)[](https://github.com/claude)\n\n```\nfeat: migrate firefunction-v2 template to PEG format\n```\n â€¦ \n\n```\nba27c84\n```\n\nMigrated fireworks-ai-llama-3-firefunction-v2 template from old format to PEG parser format:\n- Added 2 test scenarios: basic tool call, content without tools\n- Deleted old test block using hardcoded FIREFUNCTION_V2 format\n- Autoparser now correctly detects as PEG_NATIVE when tools provided\n\nProgress: 5 templates successfully migrated to PEG format\n- Command-R 7B\n- Qwen-QwQ-32B\n- Hermes-2-Pro/Hermes-3\n- Functionary-v3.1\n- Firefunction-v2\n\nðŸ¤– Generated with [Claude Code]([https://claude.com/claude-code](https://claude.com/claude-code))\n\nCo-Authored-By: Claude Sonnet 4.5 \u003cnoreply@anthropic.com\u003e\n\n[](https://github.com/pwilkin)\n\n```\nHandle JSON args properly\n```\n\n```\n9013224\n```\n\n[](https://github.com/pwilkin)\n\n```\nFix annoying case with tool names being proper prefixes\n```\n\n```\n4e841bc\n```\n\n[](https://github.com/pwilkin)\n\n```\nIMPORTANT: Nasty prefixes case solved, the code is 101% production reâ€¦\n```\n â€¦ \n\n```\n1e35e1e\n```\n\nâ€¦ady!\n\n 12 hidden items  Load moreâ€¦ \n\n[pwilkin](https://github.com/pwilkin)and others added 24 commits [January 8, 2026 15:52](https://github.com/ggml-org/llama.cpp/pull/18675#commits-pushed-5937de1)\n\n[](https://github.com/pwilkin)\n\n```\nRefactoring\n```\n\n```\n5937de1\n```\n\n[](https://github.com/pwilkin)\n\n```\ncode style and formatting\n```\n\n```\n2f781b6\n```\n\n[](https://github.com/pwilkin)\n\n```\nRefactor, remove functions, almost everything works except for GPT-OSâ€¦\n```\n â€¦ \n\n```\n8204e12\n```\n\nâ€¦S parser\n\n[](https://github.com/pwilkin)\n\n```\n[chat] All tests passed!\n```\n\n```\nfd879e7\n```\n\n[](https://github.com/pwilkin)\n\n```\nMinor post-merge fixes\n```\n\n```\n58614da\n```\n\n[](https://github.com/pwilkin)\n\n```\nWe can have nice things\n```\n\n```\n60d540b\n```\n\n[](https://github.com/pwilkin)\n\n```\nQueue sending function delta until the function name is available\n```\n\n```\n840413f\n```\n\n[](https://github.com/pwilkin)\n\n```\nForce parsing of arguments to function; cleanup some slop comments\n```\n\n```\n07827d0\n```\n\n[](https://github.com/pwilkin)\n\n```\nFix Qwen3 coder template, obligatory objects for arguments (unless unâ€¦\n```\n â€¦ \n\n```\n8406e26\n```\n\nâ€¦parse'able), fix XML closers' handling\n\n[](https://github.com/pwilkin)\n\n```\nFix to extra escaping problem\n```\n\n```\ndfa5a2b\n```\n\n[](https://github.com/pwilkin)\n\n```\nstart of gpt oss fixes, handling of server grammar error\n```\n\n```\n96616a9\n```\n\n[](https://github.com/apps/google-labs-jules)[](https://github.com/pwilkin)\n\n```\nfix(chat): Correct gpt-oss PEG parser logic\n```\n â€¦ \n\n```\nd08bea2\n```\n\nThe gpt-oss PEG parser implementation was failing several test cases due to incorrect parsing logic.\n\nSpecifically, the parser did not correctly handle:\n- Inputs containing multiple segments (e.g., `analysis` followed by `final`) separated by newlines.\n- Overly greedy rules that incorrectly consumed tool call definitions as part of message content.\n- Rules that stripped newlines from reasoning content.\n\nThis commit refactors the parser to correctly handle sequences of segments, introduces more restrictive parsing rules to avoid greedy matching, and ensures that whitespace within message content is preserved.\n\nAdditionally, a buggy test case that expected incorrect whitespace handling has been corrected.\n\nAll tests for the `gpt-oss` template now pass.\n\n[](https://github.com/apps/google-labs-jules)[](https://github.com/pwilkin)\n\n```\nfix(chat): Correct gpt-oss PEG parser logic\n```\n â€¦ \n\n```\n14d5e28\n```\n\nThe gpt-oss PEG parser implementation was failing several test cases due to incorrect parsing logic.\n\nSpecifically, the parser did not correctly handle:\n- Inputs containing multiple segments (e.g., `analysis` followed by `final`) separated by newlines.\n- Overly greedy rules that incorrectly consumed tool call definitions as part of message content.\n- Rules that stripped newlines from reasoning content.\n\nThis commit refactors the parser to correctly handle sequences of segments, introduces more restrictive parsing rules to avoid greedy matching, and ensures that whitespace within message content is preserved.\n\nAdditionally, a buggy test case that expected incorrect whitespace handling has been corrected.\n\nAll tests for the `gpt-oss` template now pass.\n\n[](https://github.com/pwilkin)\n\n```\nFinally\n```\n\n```\ne63a7bf\n```\n\n[](https://github.com/pwilkin)\n\n```\nCool!\n```\n\n```\nf0ba341\n```\n\n[](https://github.com/pwilkin)\n\n```\nSilly OSS fixes\n```\n\n```\nbd0e80f\n```\n\n[](https://github.com/pwilkin)\n\n```\nRemove legacy chat-parser; some minor OSS fixes\n```\n\n```\n84b26a9\n```\n\n[](https://github.com/pwilkin)\n\n```\nRemove obsolete format PEG_CONSTRUCTED\n```\n\n```\n3b5f2f2\n```\n\n[](https://github.com/pwilkin)\n\n```\nRefactor debug_template_parser messages and move to the appropriate pâ€¦\n```\n â€¦ \n\n```\nccbe408\n```\n\nâ€¦lace\n\n[](https://github.com/pwilkin)\n\n```\nFix nits, remove old broken test-chat-template test\n```\n\n```\n40e1287\n```\n\n[](https://github.com/pwilkin)\n\n```\nserver-test-model: replace prints with logger, rename to meet file coâ€¦\n```\n â€¦ \n\n```\nd9814f0\n```\n\nâ€¦nvention\n\n[](https://github.com/pwilkin)\n\n```\nflake8 nit\n```\n\n```\naa9abc2\n```\n\n[](https://github.com/pwilkin)\n\n```\nFixes to logging\n```\n\n```\nb6464ca\n```\n\n[](https://github.com/pwilkin)\n\n```\nFix for incorrect header sending\n```\n\nLoading\n\nLoading status checksâ€¦\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n```\n5519998\n```\n\n[](https://github.com/pwilkin)[pwilkin](https://github.com/pwilkin)[force-pushed](https://github.com/ggml-org/llama.cpp/compare/dc7dd03932c544c10b870fc29c706fcde9942a09..551999876cdd493afaea056e1de77d87765520c3) the autoparser branch from [`dc7dd03`](https://github.com/ggml-org/llama.cpp/commit/dc7dd03932c544c10b870fc29c706fcde9942a09) to [`5519998`](https://github.com/ggml-org/llama.cpp/commit/551999876cdd493afaea056e1de77d87765520c3)[Compare](https://github.com/ggml-org/llama.cpp/compare/dc7dd03932c544c10b870fc29c706fcde9942a09..551999876cdd493afaea056e1de77d87765520c3)[January 8, 2026 14:53](https://github.com/ggml-org/llama.cpp/pull/18675#event-21925240207)\n\n[pwilkin](https://github.com/pwilkin) added 4 commits [January 8, 2026 16:23](https://github.com/ggml-org/llama.cpp/pull/18675#commits-pushed-0a1e847)\n\n[](https://github.com/pwilkin)\n\n```\nRemove outdated code; readd support for tool-less templates\n```\n\nLoading\n\nLoading status checksâ€¦\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n```\n0a1e847\n```\n\n[](https://github.com/pwilkin)\n\n```\nBielik and LFM templates\n```\n\nLoading\n\nLoading status checksâ€¦\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n```\n6a023e8\n```\n\n[](https://github.com/pwilkin)\n\n```\nAdd extra Devstral test\n```\n\nLoading\n\nLoading status checksâ€¦\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n```\n0906cc3\n```\n\n[](https://github.com/pwilkin)\n\n```\nAdd messages about parser used, fix mysterious merge regression\n```\n\nLoading\n\nLoading status checksâ€¦\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n```\n761ee3e\n```\n\n[](https://github.com/hksdpc255)\n\n Copy link \n\nContributor\n\n### **[hksdpc255](https://github.com/hksdpc255)** commented [Jan 9, 2026](https://github.com/ggml-org/llama.cpp/pull/18675#issuecomment-3726693193)â€¢\n\n edited \n\nLoading\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\nThis approach does not seem to work well for models like `Kimi-K2-Thinking`, which may generate tool calls inside the thinking block, while the chat template itself automatically closes the thinking block correctly. In other words, the modelâ€™s behavior does not seem to be fully aligned with the assumptions made by the chat template. Is that understanding correct? I noticed that you have removed all parsers.\n\nAdditionally, I am planning to add a new custom parser for `MiroThinker`. Its official chat template does not accurately reflect the rendering logic actually used in their benchmarks. Is there a recommended starting point for implementing such a parser for the new parsing architecture?\n\nAll reactions\n\nSorry, something went wrong.\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/ggml-org/llama.cpp/pull/18675).\n\n[Sign up for free](https://github.com/join?source=comment-repo)**to join this conversation on GitHub**. Already have an account? [Sign in to comment](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F18675)\n\n### Reviewers\n\n[](https://github.com/ggerganov)[ggerganov](https://github.com/ggerganov)Awaiting requested review from ggerganov ggerganov will be requested when the pull request is marked ready for review ggerganov is a code owner\n\n[](https://github.com/ngxson)[ngxson](https://github.com/ngxson)Awaiting requested review from ngxson ngxson will be requested when the pull request is marked ready for review ngxson is a code owner\n\n[](https://github.com/CISC)[CISC](https://github.com/CISC)Awaiting requested review from CISC CISC will be requested when the pull request is marked ready for review CISC is a code owner\n\n[](https://github.com/aldehir)[aldehir](https://github.com/aldehir)Awaiting requested review from aldehir aldehir will be requested when the pull request is marked ready for review aldehir is a code owner\n\nAt least 1 approving review is required to merge this pull request.\n\n### Assignees\n\n No one assigned \n\n### Labels\n\n[documentation](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Adocumentation)Improvements or additions to documentation[examples](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Aexamples)[model](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Amodel)Model specific[python](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Apython)python script changes[script](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Ascript)Script related[server](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Aserver)[testing](https://github.com/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Atesting)Everything test related\n\n### Projects\n\n None yet \n\n### Milestone\n\n No milestone \n\n### Development\n\nSuccessfully merging this pull request may close these issues.\n\nNone yet\n\n### 2 participants\n\n[](https://github.com/pwilkin)[](https://github.com/hksdpc255)\n\nAdd this suggestion to a batch that can be applied as a single commit.This suggestion is invalid because no changes were made to the code.Suggestions cannot be applied while the pull request is closed.Suggestions cannot be applied while viewing a subset of changes.Only one suggestion per line can be applied in a batch.Add this suggestion to a batch that can be applied as a single commit.Applying suggestions on deleted lines is not supported.You must change the existing code in this line in order to create a valid suggestion.Outdated suggestions cannot be applied.This suggestion has been applied or marked resolved.Suggestions cannot be applied from pending reviews.Suggestions cannot be applied on multi-line comments.Suggestions cannot be applied while the pull request is queued to merge.Suggestion cannot be applied right now. Please check back later.\n\nFooter\n------\n\n[](https://github.com/) Â© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You canâ€™t perform that action at this time.",
          "was_summarised": false
        },
        {
          "url": "https://github.com/pwilkin/llama.cpp/issues",
          "was_fetched": true,
          "page": "Title: pwilkin/llama.cpp\n\nURL Source: https://github.com/pwilkin/llama.cpp/issues\n\nMarkdown Content:\n*   \n    *   \nAI CODE CREATION\n\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \n    *   \nBY COMPANY SIZE\n\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \n    *   \nEXPLORE BY TOPIC\n\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \n    *   \nCOMMUNITY\n\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \n    *   \nENTERPRISE SOLUTIONS\n\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nProvide feedback\n----------------\n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted \n\nSaved searches\n--------------\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpwilkin%2Fllama.cpp%2Fissues)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fissues%2Findex\u0026source=header-repo\u0026source_repo=pwilkin%2Fllama.cpp)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:37:51.167885332Z"
    },
    {
      "flow_id": "",
      "id": "1q6nm6a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/",
      "title": "Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants.",
      "content": "**Source:** [https://x.com/liquidai/status/2008954886659166371](https://x.com/liquidai/status/2008954886659166371)\n\n**Hugging Face page:** [https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript)\n\n**GGUFs:** [https://huggingface.co/models?other=base\\_model:quantized:LiquidAI/LFM2-2.6B-Transcript](https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript)\n\n**First image:**  \n\"This week at [\\#CES](https://x.com/hashtag/CES?src=hashtag_click), weâ€™re showcasing whatâ€™s next for on-device intelligence alongside our partners [@AMD](https://x.com/AMD): fast, private, and entirely secure AI summarization that runs fully on-device.\n\nMeetings are foundational to business, creating mission critical and sensitive information. Too often, that data leaves the room to be processed in the cloud, introducing latency, unpredictable costs, and real security and compliance risks.\n\nWith [@AMD](https://x.com/AMD), weâ€™ve broken that barrier with a cloud-quality summarization model that runs locally across the AMD Ryzenâ„¢ AI platform, delivering enterprise-grade accuracy in seconds.\n\nToday, weâ€™re expanding access to this model to everyone.\n\nMeet LFM2-2.6B-Transcript: a purpose-built Liquid Nano designed for long-form meeting transcripts and real operational use.\n\n\\\u0026gt; Cloud-level summarization quality  \n\\\u0026gt; Summaries generated in seconds  \n\\\u0026gt; \u0026lt;3 GB RAM usage  \n\\\u0026gt; Lower latency and energy consumption than larger transformer baselines  \n\\\u0026gt; Fully local execution across CPU, GPU, and NPU\"\n\n**Second image:**  \n\"LFM2-2.6B-Transcript delivers accuracy ratings on par with cloud models that are orders of magnitude larger. Delivering similar quality for a fraction of the memory use and compute. It completes a 60-minute meeting summarization in 16 seconds!\"\n\n**Third Image:**  \n\"Leveraging our efficient LFM2 backbone, LFM2-2.6B-Transcript uses significantly less RAM than other models. This gap is what makes full on-device deployment on 16GB AI PCs practical for LFM2â€”but effectively out of reach for many traditional transformer models.\"",
      "author": "KaroYadgar",
      "created_at": "2026-01-07T18:38:08Z",
      "comments": [
        {
          "id": "ny8yiw2",
          "author": "Hefty_Wolverine_553",
          "content": "oof, I thought it was for transcription of audio... with their recent speech to speech model I was really hoping for a multi-speaker transcription model.",
          "created_at": "2026-01-07T18:49:24Z",
          "was_summarised": false
        },
        {
          "id": "ny8ym7o",
          "author": "nuclearbananana",
          "content": "A model just for summarization feels a little overly specific.",
          "created_at": "2026-01-07T18:49:48Z",
          "was_summarised": false
        },
        {
          "id": "ny91glc",
          "author": "Foreign-Beginning-49",
          "content": "Damn you guys keep knocking it out of the park. Congratulations on all the amazing releases. Im still trying to fully utilize LFM2-1.2B then you drop new weights. So fun to be here and experience the fruits of your toil. Thanks for looking out for those of us with less computational capacities. Its amazing how far small models have come and you're an integral part of that puzzle. Best wishes",
          "created_at": "2026-01-07T19:02:04Z",
          "was_summarised": false
        },
        {
          "id": "ny91tk2",
          "author": "pmttyji",
          "content": "Nice. The GIF from model card is enough.\n\nModel :  [https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript) \u0026amp; its [GGUFs](https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript)",
          "created_at": "2026-01-07T19:03:39Z",
          "was_summarised": false
        },
        {
          "id": "nya7xpa",
          "author": "zelkovamoon",
          "content": "Liquid is cookin",
          "created_at": "2026-01-07T22:06:17Z",
          "was_summarised": false
        },
        {
          "id": "nyccsvp",
          "author": "Sensitive_Sweet_1850",
          "content": "Cool",
          "created_at": "2026-01-08T04:48:09Z",
          "was_summarised": false
        },
        {
          "id": "nycx65e",
          "author": "TensorSpeed",
          "content": "These specific finetunes are interesting. Seems like for every gen they make at least one finetune for a common task. They now have -transcript, -math, -extract, -tools...\n\n[https://huggingface.co/LiquidAI/models?p=1](https://huggingface.co/LiquidAI/models?p=1)",
          "created_at": "2026-01-08T07:18:42Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q6nm6a",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://www.reddit.com/gallery/1q6nm6a\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://x.com/liquidai/status/2008954886659166371",
          "was_fetched": true,
          "page": "Title: Liquid AI on X: \"This week at #CES, weâ€™re showcasing whatâ€™s next for on-device intelligence alongside our partners @AMD: fast, private, and entirely secure AI summarization that runs fully on-device.\n\nMeetings are foundational to business, creating mission critical and sensitive information. Too https://t.co/prPEcCrlV0\" / X\n\nURL Source: https://x.com/liquidai/status/2008954886659166371\n\nMarkdown Content:\nThis week at [#CES](https://x.com/hashtag/CES?src=hashtag_click), weâ€™re showcasing whatâ€™s next for on-device intelligence alongside our partners\n\n: fast, private, and entirely secure AI summarization that runs fully on-device. Meetings are foundational to business, creating mission critical and sensitive information. Too often, that data leaves the room to be processed in the cloud, introducing latency, unpredictable costs, and real security and compliance risks. With\n\n, weâ€™ve broken that barrier with a cloud-quality summarization model that runs locally across the AMD Ryzenâ„¢ AI platform, delivering enterprise-grade accuracy in seconds. Today, weâ€™re expanding access to this model to everyone.  Meet LFM2-2.6B-Transcript: a purpose-built Liquid Nano designed for long-form meeting transcripts and real operational use. \u003e Cloud-level summarization quality \u003e Summaries generated in seconds \u003e\u003c3 GB RAM usage \u003e Lower latency and energy consumption than larger transformer baselines \u003e Fully local execution across CPU, GPU, and NPU",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript",
          "was_fetched": true,
          "page": "Title: LiquidAI/LFM2-2.6B-Transcript Â· Hugging Face\n\nURL Source: https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript\n\nMarkdown Content:\nBased on [LFM2-2.6B](https://huggingface.co/LiquidAI/LFM2-2.6B), LFM2-2.6B-Transcript is designed for **private, on-device meeting summarization**. We partnered with AMD to deliver cloud-level summary quality while running entirely locally, ensuring that your meeting data never leaves your device.\n\n**Highlights**:\n\n*   **Cloud-level summary quality**, approaching much larger models\n*   **Under 3GB of RAM** usage for long meetings\n*   **Fast summaries** in seconds, not minutes\n*   Runs fully locally across **CPU, GPU, and NPU**\n\nFind more information about LFM2-2.6B-Transcript in [AMD's blog post](https://www.amd.com/en/blogs/2026/liquid-ai-amd-ryzen-on-device-meeting-summaries.html) and [Liquid's blog post](https://www.liquid.ai/blog/the-future-of-meeting-summarization-local-fast-private-and-fully-secure).\n\n[](https://cdn-uploads.huggingface.co/production/uploads/646fdf0a850a938d6c555b2a/EqDVUEXeLSvwsiM-Gb30_.gif)\n\n[](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#%F0%9F%93%84-model-details) ðŸ“„ Model details\n----------------------------------------------------------------------------------------------------\n\n| Model | Description |\n| --- | --- |\n| [**LFM2-2.6B-Transcript**](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript) | Original model checkpoint in native format. Best for fine-tuning or inference with Transformers and vLLM. |\n| [LFM2-2.6B-Transcript-GGUF](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript-GGUF) | Quantized format for llama.cpp and compatible tools. Optimized for CPU inference and local deployment with reduced memory usage. |\n| [LFM2-2.6B-Transcript-ONNX](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript-ONNX) | ONNX Runtime format for cross-platform deployment. Enables hardware-accelerated inference across diverse environments (cloud, edge, mobile). |\n| [LFM2-2.6B-Transcript-MLX](https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-4bit) | MLX format for Apple Silicon. Optimized for fast inference on Mac devices using the MLX framework. |\n\n**Capabilities**: The model is trained for long-form transcript summarization (30-60 minute meetings), producing clear, structured outputs including key points, decisions, and action items with consistent tone and formatting.\n\n**Use cases**:\n\n*   Internal team meetings\n*   Sales calls and customer conversations\n*   Board meetings and executive briefings\n*   Regulated or sensitive environments where data can't leave the device\n*   Offline or low-connectivity workflows\n\n**Generation parameters**: We strongly recommend using a lower temperature with a `temperature=0.3`.\n\n**Supported language**: English\n\n\u003e âš ï¸ The model is intended for single-turn conversations with a specific format, described in the following.\n\n**Input format**: We recommend using the following system prompt:\n\n\u003e You are an expert meeting analyst. Analyze the transcript carefully and provide clear, accurate information based on the content.\n\nWe use a specific formatting for the input meeting transcripts to summarize as follows:\n\n```\n\u003cuser_prompt\u003e\n\nTitle (example: Claims Processing training module)\nDate (example: July 2, 2021)\nTime (example: 1:00 PM)\nDuration (example: 45 minutes)\nParticipants (example: Julie Franco (Training Facilitator), Amanda Newman (Subject Matter Expert))\n----------\n**Speaker 1**: Message 1 (example: **Julie Franco**: Good morning, everyone. Thanks for joining me today.)\n**Speaker 2**: Message 2 (example: **Amanda Newman**: Good morning, Julie. Happy to be here.)\netc.\n```\n\nYou can replace `\u003cuser_prompt\u003e` with the following, depending on the desired summary type:\n\n| Summary type | User prompt |\n| --- | --- |\n| Executive summary | Provide a brief executive summary (2-3 sentences) of the key outcomes and decisions from this transcript. |\n| Detailed summary | Provide a detailed summary of the transcript, covering all major topics, discussions, and outcomes in paragraph form. |\n| Action items | List the specific action items that were assigned during this meeting. Include who is responsible for each item when mentioned. |\n| Key decisions | List the key decisions that were made during this meeting. Focus on concrete decisions and outcomes. |\n| Participants | List the participants mentioned in this transcript. Include their roles or titles when available. |\n| Topics discussed | List the main topics and subjects that were discussed in this meeting. |\n\nThis is freeform, and you can add several prompts or combine them into a single one, like in the following examples:\n\n| Title | Input meeting | Model output |\n| --- | --- | --- |\n| Budget planning | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/meeting1.txt) | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/output1.txt) |\n| Design review | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/meeting2.txt) | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/output2.txt) |\n| Coffee chat / social hour | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/meeting3.txt) | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/output3.txt) |\n| Procurement / vendor review | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/meeting4.txt) | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/output4.txt) |\n| Task force meeting | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/meeting5.txt) | [Link](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript/resolve/main/examples/output5.txt) |\n\n[](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#%F0%9F%9A%80-quick-start) ðŸš€ Quick Start\n------------------------------------------------------------------------------------------------\n\nThe easiest way to try LFM2-2.6B-Transcript is through our command-line tool in the [Liquid AI Cookbook](https://github.com/Liquid4All/cookbook).\n\n**1. Install uv** (if you don't have it already):\n\n```\nuv --version\n# uv 0.9.18\n```\n\n**2. Run with the sample transcript**:\n\n```\nuv run https://raw.githubusercontent.com/Liquid4All/cookbook/refs/heads/main/examples/meeting-summarization/summarize.py\n```\n\nNo API keys. No cloud services. No setup. Just pure local inference with real-time token streaming.\n\n**3. Use your own transcript**:\n\n```\nuv run https://raw.githubusercontent.com/Liquid4All/cookbook/refs/heads/main/examples/meeting-summarization/summarize.py \\\n  --transcript-file path/to/your/transcript.txt\n```\n\nThe tool uses llama.cpp for optimized inference and automatically handles model downloading and compilation for your platform.\n\n[](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#%F0%9F%8F%83-inference) ðŸƒ Inference\n--------------------------------------------------------------------------------------------\n\nLFM2 is supported by many inference frameworks. See the [Inference documentation](https://docs.liquid.ai/lfm/inference/transformers) for the full list.\n\n| Name | Description | Docs | Notebook |\n| --- | --- | --- | --- |\n| [Transformers](https://github.com/huggingface/transformers) | Simple inference with direct access to model internals. | [Link](https://docs.liquid.ai/lfm/inference/transformers) | [](https://colab.research.google.com/drive/1_q3jQ6LtyiuPzFZv7Vw8xSfPU5FwkKZY?usp=sharing) |\n| [vLLM](https://github.com/vllm-project/vllm) | High-throughput production deployments with GPU. | [Link](https://docs.liquid.ai/lfm/inference/vllm) | [](https://colab.research.google.com/drive/1VfyscuHP8A3we_YpnzuabYJzr5ju0Mit?usp=sharing) |\n| [llama.cpp](https://github.com/ggml-org/llama.cpp) | Cross-platform inference with CPU offloading. | [Link](https://docs.liquid.ai/lfm/inference/llama-cpp) | [](https://colab.research.google.com/drive/1ohLl3w47OQZA4ELo46i5E4Z6oGWBAyo8?usp=sharing) |\n| [MLX](https://github.com/ml-explore/mlx) | Apple's machine learning framework optimized for Apple Silicon. | [Link](https://docs.liquid.ai/lfm/inference/mlx) | â€” |\n| [LM Studio](https://lmstudio.ai/) | Desktop application for running LLMs locally. | [Link](https://docs.liquid.ai/lfm/inference/lm-studio) | â€” |\n\n[](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#%F0%9F%93%88-performance) ðŸ“ˆ Performance\n------------------------------------------------------------------------------------------------\n\n### [](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#quality) Quality\n\nLFM2-2.6B-Transcript was benchmarked using the [GAIA Eval-Judge](https://github.com/amd/gaia/blob/main/docs/eval.md) framework on synthetic meeting transcripts across 8 meeting types.\n\n[](https://cdn-uploads.huggingface.co/production/uploads/646fdf0a850a938d6c555b2a/e1nbAtmUWIg10Zb3tGMF-.png)\n\n_Accuracy ratings from [GAIA LLM Judge](https://github.com/amd/gaia). Evaluated on 24 synthetic 1K transcripts and 32 synthetic 10K transcripts. Claude Sonnet 4 used for content generation and judging._\n\n### [](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#inference-speed) Inference Speed\n\n[](https://cdn-uploads.huggingface.co/production/uploads/646fdf0a850a938d6c555b2a/WuCDbs4hfqC_kDJVbv5XS.png)\n\n_Generated using [llama-bench.exe](https://github.com/ggml-org/llama.cpp) b7250 on an HP Z2 Mini G1a Next Gen AI Desktop Workstation on respective AMD Ryzen device. We compute peak memory used during CPU inference by measuring peak memory usage of the llama-bench.exe process executing the command: `llama-bench -m \u003cMODEL\u003e -p 10000 -n 1000 -t 8 -r 3 -ngl 0` The llama-bench executable outputs the average inference times for preprocessing and token generation. The reported inference times are for the iGPU, enabled using the `-ngl 99` flag._\n\n### [](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#memory-usage) Memory Usage\n\n[](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/XksTBvOkZ0Xx9bBD60LyQ.png)\n\n_Generated using [llama-bench.exe](https://github.com/ggml-org/llama.cpp) b7250 on an HP Z2 Mini G1a Next Gen AI Desktop Workstation with an AMD Ryzen AI Max+ PRO 395 processor. We compute peak memory used during CPU inference by measuring peak memory usage of the llama-bench.exe process executing the command: `llama-bench -m \u003cMODEL\u003e -p 10000 -n 1000 -t 8 -r 3 -ngl 0` The llama-bench executable outputs the average inference times for preprocessing and token generation. The reported inference times are for the iGPU, enabled using the `-ngl 99` flag_\n\n[](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript#%F0%9F%93%AC-contact) ðŸ“¬ Contact\n----------------------------------------------------------------------------------------\n\nIf you are interested in custom solutions with edge deployment, please contact [our sales team](https://www.liquid.ai/contact).",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/models?other=base\\_model:quantized:LiquidAI/LFM2-2.6B-Transcript",
          "was_fetched": true,
          "page": "Title: Models â€“ Hugging Face\n\nURL Source: https://huggingface.co/models?other=base\\_model:quantized:LiquidAI/LFM2-2.6B-Transcript\n\nMarkdown Content:\nModels â€“ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n### Edit Models filters\n\n*   Main \n*   Tasks \n*   Libraries \n*   Languages \n*   Licenses \n*   Other 1\n\nApps \n\n[llama.cpp](https://huggingface.co/models?other=llama.cpp)[LM Studio](https://huggingface.co/models?other=lmstudio)[Jan](https://huggingface.co/models?other=jan)[Draw Things](https://huggingface.co/models?other=drawthings)[DiffusionBee](https://huggingface.co/models?other=diffusionbee)[Jellybox](https://huggingface.co/models?other=jellybox)[JoyFusion](https://huggingface.co/models?other=joyfusion)[LocalAI](https://huggingface.co/models?other=localai)[vLLM](https://huggingface.co/models?other=vllm)[Ollama](https://huggingface.co/models?other=ollama)[TGI](https://huggingface.co/models?other=tgi)[MLX LM](https://huggingface.co/models?other=mlx-lm)[Docker Model Runner](https://huggingface.co/models?other=docker-model-runner)[Lemonade](https://huggingface.co/models?other=lemonade)\n\nInference Providers \n\nSelect all\n\n[Groq](https://huggingface.co/models?inference_provider=groq)[Novita](https://huggingface.co/models?inference_provider=novita)[Nebius AI](https://huggingface.co/models?inference_provider=nebius)[Cerebras](https://huggingface.co/models?inference_provider=cerebras)[SambaNova](https://huggingface.co/models?inference_provider=sambanova)[Nscale](https://huggingface.co/models?inference_provider=nscale)[fal](https://huggingface.co/models?inference_provider=fal-ai)[Hyperbolic](https://huggingface.co/models?inference_provider=hyperbolic)[Together AI](https://huggingface.co/models?inference_provider=together)[Fireworks](https://huggingface.co/models?inference_provider=fireworks-ai)[Featherless AI](https://huggingface.co/models?inference_provider=featherless-ai)[Zai](https://huggingface.co/models?inference_provider=zai-org)[Replicate](https://huggingface.co/models?inference_provider=replicate)[Cohere](https://huggingface.co/models?inference_provider=cohere)[Scaleway](https://huggingface.co/models?inference_provider=scaleway)[Public AI](https://huggingface.co/models?inference_provider=publicai)[OVHcloud AI Endpoints](https://huggingface.co/models?inference_provider=ovhcloud)[HF Inference API](https://huggingface.co/models?inference_provider=hf-inference)[WaveSpeed](https://huggingface.co/models?inference_provider=wavespeed)\n\nMisc \n\n Reset Misc\n\n[base\\_model:quantized:LiquidAI/LFM2-2.6B-Transcript](https://huggingface.co/models?other=base%5C_model%3Aquantized%3ALiquidAI%2FLFM2-2.6B-Transcript)[Inference Endpoints](https://huggingface.co/models?other=endpoints_compatible)[text-generation-inference](https://huggingface.co/models?other=text-generation-inference)[Eval Results](https://huggingface.co/models?other=model-index)[Merge](https://huggingface.co/models?other=merge)[4-bit precision](https://huggingface.co/models?other=4-bit)[custom_code](https://huggingface.co/models?other=custom_code)[8-bit precision](https://huggingface.co/models?other=8-bit)[text-embeddings-inference](https://huggingface.co/models?other=text-embeddings-inference)[Mixture of Experts](https://huggingface.co/models?other=moe)[Carbon Emissions](https://huggingface.co/models?other=co2_eq_emissions)\n\n Apply filters\n\nModels\n======\n\n0\n\n[Full-text search](https://huggingface.co/search/full-text?type=model) Inference Available\n\n Edit filters\n\n Sort: Trending \n\n**Active filters:** LiquidAI/LFM2-2.6B-Transcript\n\nClear all\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)\n\nRun 15,000+ Models Instantly\n----------------------------\n\nFilter models and display only those with inference enabled via Inference Providers ([Learn more](https://huggingface.co/docs/inference-providers)).\n\n*   Groq\n*   Novita\n*   Nebius AI\n*   Cerebras\n*   SambaNova\n*   Nscale\n*   fal\n*   Hyperbolic\n*   Together AI\n*   Fireworks\n*   Featherless AI\n*   Zai\n*   Replicate\n*   Cohere\n*   Scaleway\n*   Public AI\n*   Baseten\n*   +5 others\n\nBrowse available models Dismiss",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript",
          "was_fetched": true,
          "page": "Title: Quantized Models for LiquidAI/LFM2-2.6B-Transcript â€“ Hugging Face\n\nURL Source: https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript\n\nMarkdown Content:\nQuantized Models for LiquidAI/LFM2-2.6B-Transcript â€“ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n### Edit Models filters\n\n*   Main \n*   Tasks \n*   Libraries \n*   Languages \n*   Licenses \n*   Other 1\n\nModel Tree\n\n Reset\n\n[LiquidAI/LFM2-2.6B-Transcript](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript)\n\n[Adapters](https://huggingface.co/models?other=adapter)[Finetunes](https://huggingface.co/models?other=finetune)[Quantizations](https://huggingface.co/models?other=quantized)[Merges](https://huggingface.co/models?other=merge)\n\nApps \n\n[llama.cpp](https://huggingface.co/models?other=llama.cpp)[LM Studio](https://huggingface.co/models?other=lmstudio)[Jan](https://huggingface.co/models?other=jan)[Draw Things](https://huggingface.co/models?other=drawthings)[DiffusionBee](https://huggingface.co/models?other=diffusionbee)[Jellybox](https://huggingface.co/models?other=jellybox)[JoyFusion](https://huggingface.co/models?other=joyfusion)[LocalAI](https://huggingface.co/models?other=localai)[vLLM](https://huggingface.co/models?other=vllm)[Ollama](https://huggingface.co/models?other=ollama)[TGI](https://huggingface.co/models?other=tgi)[MLX LM](https://huggingface.co/models?other=mlx-lm)[Docker Model Runner](https://huggingface.co/models?other=docker-model-runner)[Lemonade](https://huggingface.co/models?other=lemonade)\n\nInference Providers \n\nSelect all\n\n[Groq](https://huggingface.co/models?inference_provider=groq)[Novita](https://huggingface.co/models?inference_provider=novita)[Nebius AI](https://huggingface.co/models?inference_provider=nebius)[Cerebras](https://huggingface.co/models?inference_provider=cerebras)[SambaNova](https://huggingface.co/models?inference_provider=sambanova)[Nscale](https://huggingface.co/models?inference_provider=nscale)[fal](https://huggingface.co/models?inference_provider=fal-ai)[Hyperbolic](https://huggingface.co/models?inference_provider=hyperbolic)[Together AI](https://huggingface.co/models?inference_provider=together)[Fireworks](https://huggingface.co/models?inference_provider=fireworks-ai)[Featherless AI](https://huggingface.co/models?inference_provider=featherless-ai)[Zai](https://huggingface.co/models?inference_provider=zai-org)[Replicate](https://huggingface.co/models?inference_provider=replicate)[Cohere](https://huggingface.co/models?inference_provider=cohere)[Scaleway](https://huggingface.co/models?inference_provider=scaleway)[Public AI](https://huggingface.co/models?inference_provider=publicai)[OVHcloud AI Endpoints](https://huggingface.co/models?inference_provider=ovhcloud)[HF Inference API](https://huggingface.co/models?inference_provider=hf-inference)[WaveSpeed](https://huggingface.co/models?inference_provider=wavespeed)\n\nMisc \n\n[Inference Endpoints](https://huggingface.co/models?other=endpoints_compatible)[text-generation-inference](https://huggingface.co/models?other=text-generation-inference)[Eval Results](https://huggingface.co/models?other=model-index)[Merge](https://huggingface.co/models?other=merge)[4-bit precision](https://huggingface.co/models?other=4-bit)[custom_code](https://huggingface.co/models?other=custom_code)[8-bit precision](https://huggingface.co/models?other=8-bit)[text-embeddings-inference](https://huggingface.co/models?other=text-embeddings-inference)[Mixture of Experts](https://huggingface.co/models?other=moe)[Carbon Emissions](https://huggingface.co/models?other=co2_eq_emissions)\n\n Apply filters\n\nModels\n======\n\n9\n\n[Full-text search](https://huggingface.co/search/full-text?type=model) Inference Available\n\n Edit filters\n\n Sort: Trending \n\n**Active filters:** LiquidAI/LFM2-2.6B-Transcript\n\nClear all\n\n[#### LiquidAI/LFM2-2.6B-Transcript-GGUF Text Generation â€¢ 3Bâ€¢Updated 1 day agoâ€¢ 364 â€¢ 5](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript-GGUF)[#### LiquidAI/LFM2-2.6B-Transcript-ONNX Text Generation â€¢Updated 2 days agoâ€¢ 5 â€¢ 3](https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript-ONNX)[#### mradermacher/LFM2-2.6B-Transcript-GGUF 3Bâ€¢Updated 2 days agoâ€¢ 183](https://huggingface.co/mradermacher/LFM2-2.6B-Transcript-GGUF)[#### mradermacher/LFM2-2.6B-Transcript-i1-GGUF 3Bâ€¢Updated 2 days agoâ€¢ 404](https://huggingface.co/mradermacher/LFM2-2.6B-Transcript-i1-GGUF)[#### mlx-community/LFM2-2.6B-Transcript-4bit Text Generation â€¢ 0.4Bâ€¢Updated 1 day agoâ€¢ 6](https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-4bit)[#### mlx-community/LFM2-2.6B-Transcript-5bit Text Generation â€¢ 0.5Bâ€¢Updated 1 day agoâ€¢ 2](https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-5bit)[#### mlx-community/LFM2-2.6B-Transcript-6bit Text Generation â€¢ 0.6Bâ€¢Updated 1 day agoâ€¢ 4](https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-6bit)[#### mlx-community/LFM2-2.6B-Transcript-8bit Text Generation â€¢ 0.7Bâ€¢Updated 1 day agoâ€¢ 5](https://huggingface.co/mlx-community/LFM2-2.6B-Transcript-8bit)[#### DevQuasar/LiquidAI.LFM2-2.6B-Transcript-GGUF Text Generation â€¢ 3Bâ€¢Updated about 22 hours ago](https://huggingface.co/DevQuasar/LiquidAI.LFM2-2.6B-Transcript-GGUF)\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)\n\nRun 15,000+ Models Instantly\n----------------------------\n\nFilter models and display only those with inference enabled via Inference Providers ([Learn more](https://huggingface.co/docs/inference-providers)).\n\n*   Groq\n*   Novita\n*   Nebius AI\n*   Cerebras\n*   SambaNova\n*   Nscale\n*   fal\n*   Hyperbolic\n*   Together AI\n*   Fireworks\n*   Featherless AI\n*   Zai\n*   Replicate\n*   Cohere\n*   Scaleway\n*   Public AI\n*   Baseten\n*   +5 others\n\nBrowse available models Dismiss",
          "was_summarised": false
        },
        {
          "url": "https://x.com/hashtag/CES?src=hashtag_click",
          "was_fetched": true,
          "page": "Title: Log in to X / X\n\nURL Source: https://x.com/hashtag/CES?src=hashtag_click\n\nMarkdown Content:\nLog in to X / X\n===============\n\nSign in to X\n============\n\nSign in with Google Sign in with Google. Opens in new tab\n\nSign in with Apple\n\nor\n\nPhone, email, or username\n\n \n\nNext\n\nForgot password?\n\nDon't have an account? Sign up",
          "was_summarised": false
        },
        {
          "url": "https://x.com/AMD",
          "was_fetched": true,
          "page": "Title: AMD (@AMD) / X\n\nURL Source: https://x.com/AMD\n\nMarkdown Content:\n[](https://x.com/AMD/photo)\n\nAMD\n\n@AMD\n\ntogether we advance_\n\nAMDâ€™s posts\n-----------\n\nPinned\n\n[](https://x.com/AMD)\n\nAMD at CES 2026\n\n[](https://x.com/AMD)\n\nScale big...\n\nspotlights AMD Helios rack scale AI architecture and Instinct MI430X/MI440X/MI455X â€“ an MI400 series built to meet diverse infrastructure and customer needs. Get the details: [tomshardware.com/tech-industry/](https://t.co/hDi3t3HUYB)\n\n[](https://x.com/AMD/status/2009434470290526620/photo/1)\n\n[](https://x.com/AMD)\n\nOverheard at [#CES2026](https://x.com/hashtag/CES2026?src=hashtag_click): AMD Senior VP and GM of Computing and Graphics\n\ncovers the new features and capabilities developers can expect in the new AMD Ryzen AI Halo.\n\n[](https://x.com/CES)\n\n10Ã— AI performance is the new benchmark Dr. Lisa Su introduces\n\nHelios and MI455, delivering up to 10x more AI performance to power stronger models, smarter agents and next-gen applications.\n\n[](https://x.com/AMD)\n\nAt [#CES2026](https://x.com/hashtag/CES2026?src=hashtag_click), AMD revealed humanoid robot GENE.01 in collaboration with Generative Bionics (\n\n). Thank you Daniele Pucci for joining\n\nduring the Keynote!\n\n[](https://x.com/AMD/status/2009324337367482790/photo/1)\n\n[](https://x.com/AMD)\n\nAt CES 2026, Lisa Su put AIâ€™s future in yottaflops. Compute demand projection: 1 zettaflop â†’ 10+ yottaflops in under 5 years. Why â€œYottaâ€ matters: [bit.ly/4swfHQU](https://t.co/nAWKCqtxAb)\n\n[](https://x.com/AMD/status/2009294180439220427/photo/1)\n\n[](https://x.com/AMD)\n\n\"Whatâ€™s next for the AMD Instinct leadership roadmap? At [#CES2026](https://x.com/hashtag/CES2026?src=hashtag_click) we showed a glimpse:  The new AMD Instinct MI440X GPU for enterprises  A preview of the next-gen AMD Instinct MI500 Series More performance. More efficiency. More choice.\n\n[](https://x.com/AMD/status/2009263989683798365/photo/1)",
          "was_summarised": false
        }
      ],
      "processed_at": "2026-01-09T02:38:52.82987051Z"
    }
  ]
}