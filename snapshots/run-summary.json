{
  "blocks": [
    {
      "flow_id": "",
      "id": "1q61wpv",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/",
      "title": "NousResearch/NousCoder-14B ¬∑ Hugging Face",
      "content": "from NousResearch:\n\n\"We introduce *NousCoder-14B*, a competitive programming model post-trained on [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B) via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.\"",
      "author": "jacek2023",
      "created_at": "2026-01-07T01:37:25Z",
      "comments": [
        {
          "id": "ny4f9hx",
          "author": "Cool-Chemical-5629",
          "content": "It's happening. [https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm\\_source=share\u0026amp;utm\\_medium=web3x\u0026amp;utm\\_name=web3xcss\u0026amp;utm\\_term=1\u0026amp;utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button)",
          "created_at": "2026-01-07T01:54:22Z",
          "urls": [
            {
              "url": "https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm\\_source=share\u0026amp;utm\\_medium=web3x\u0026amp;utm\\_name=web3xcss\u0026amp;utm\\_term=1\u0026amp;utm\\_content=share\\_button",
              "was_fetched": false,
              "was_summarised": false
            },
            {
              "url": "https://www.reddit.com/r/LocalLLaMA/comments/1kmrsic/comment/msd0op5/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny4hwdb",
          "author": "AvocadoArray",
          "content": "Maybe I'm missing something, but isn't this just a demonstration of overfitting a model to a test suite?",
          "created_at": "2026-01-07T02:08:35Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/NousResearch/NousCoder-14B",
          "was_fetched": true,
          "page": "Title: NousResearch/NousCoder-14B ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/NousResearch/NousCoder-14B\n\nMarkdown Content:\nNousResearch/NousCoder-14B ¬∑ Hugging Face\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n[](https://huggingface.co/NousResearch)\n\n[NousResearch](https://huggingface.co/NousResearch)\n\n/\n\n[NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B)\n\nlike 14\n\nFollow\n\nNousResearch 2.9k\n=============================================================================================================================================================================================================\n\n[Text Generation](https://huggingface.co/models?pipeline_tag=text-generation)[Safetensors](https://huggingface.co/models?library=safetensors)\n\n4 datasets\n\n[qwen3](https://huggingface.co/models?other=qwen3)[conversational](https://huggingface.co/models?other=conversational)\n\nLicense:apache-2.0\n\n[Model card](https://huggingface.co/NousResearch/NousCoder-14B)[Files Files and versions xet](https://huggingface.co/NousResearch/NousCoder-14B/tree/main)[Community](https://huggingface.co/NousResearch/NousCoder-14B/discussions)\n\n*   [NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B#nouscoder-14b \"NousCoder-14B\")\n\n*   [Acknowledgements](https://huggingface.co/NousResearch/NousCoder-14B#acknowledgements \"Acknowledgements\")\n\n[](https://huggingface.co/NousResearch/NousCoder-14B#nouscoder-14b) NousCoder-14B\n=================================================================================\n\n[](https://x.com/NousResearch)[](https://www.apache.org/licenses/LICENSE-2.0)\n\nWe introduce _NousCoder-14B_, a competitive programming model post-trained on [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B) via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.\n\n[](https://huggingface.co/NousResearch/NousCoder-14B/blob/main/lcb_score_vs_step.png)[](https://huggingface.co/NousResearch/NousCoder-14B/blob/main/performance_params_ratio.png)\n\n[](https://huggingface.co/NousResearch/NousCoder-14B#acknowledgements) Acknowledgements\n=======================================================================================\n\nI would like to thank my mentor, Roger Jin, Dakota Mahan, Teknium, and others at the Nous Research team for their invaluable support throughout this project. I would also like to thank Together AI and Agentica for their immensely helpful blog posts on DeepCoder-14B. Finally, thank you to Modal and Lambda for their generous support by providing me with credits.\n\nDownloads last month- \n\nSafetensors[](https://huggingface.co/docs/safetensors)\n\nModel size\n\n15B params\n\nTensor type\n\nBF16 \n\n¬∑\n\nChat template\n\nFiles info\n\nInference Providers[NEW](https://huggingface.co/docs/inference-providers)\n\n[Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\n\nThis model isn't deployed by any Inference Provider.[üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=NousResearch/NousCoder-14B\u0026description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNousResearch%2FNousCoder-14B%5D(%2FNousResearch%2FNousCoder-14B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n\nModel tree for NousResearch/NousCoder-14B[](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\n----------------------------------------------------------------------------------------------------------------\n\nBase model\n\n[Qwen/Qwen3-14B-Base](https://huggingface.co/Qwen/Qwen3-14B-Base)\n\n Finetuned\n\n[Qwen/Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B)\n\n Finetuned\n\n([181](https://huggingface.co/models?other=base_model:finetune:Qwen/Qwen3-14B)) \n\nthis model \n\nQuantizations\n\n[1 model](https://huggingface.co/models?other=base_model:quantized:NousResearch/NousCoder-14B)\n\nDatasets used to train NousResearch/NousCoder-14B\n-------------------------------------------------\n\n[#### livecodebench/code_generation_lite Updated Jun 5, 2025‚Ä¢ 56.5k ‚Ä¢ 79](https://huggingface.co/datasets/livecodebench/code_generation_lite)[#### agentica-org/DeepCoder-Preview-Dataset Viewer ‚Ä¢ Updated Apr 9, 2025‚Ä¢ 25k‚Ä¢ 2.02k ‚Ä¢ 93](https://huggingface.co/datasets/agentica-org/DeepCoder-Preview-Dataset)[#### NousResearch/lcb_test Viewer ‚Ä¢ Updated 13 days ago‚Ä¢ 454‚Ä¢ 24](https://huggingface.co/datasets/NousResearch/lcb_test)\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Careers](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)\n\nInference providers allow you to run inference using different serverless providers.",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/Qwen/Qwen3-14B",
          "was_fetched": true,
          "page": "Title: Qwen/Qwen3-14B ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/Qwen/Qwen3-14B\n\nMarkdown Content:\n[](https://chat.qwen.ai/)\n\n[](https://huggingface.co/Qwen/Qwen3-14B#qwen3-highlights) Qwen3 Highlights\n---------------------------------------------------------------------------\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n*   **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n*   **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n*   **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n*   **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n*   **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#model-overview) Model Overview\n-----------------------------------------------------------------------\n\n**Qwen3-14B** has the following features:\n\n*   Type: Causal Language Models\n*   Training Stage: Pretraining \u0026 Post-training\n*   Number of Parameters: 14.8B\n*   Number of Paramaters (Non-Embedding): 13.2B\n*   Number of Layers: 40\n*   Number of Attention Heads (GQA): 40 for Q and 8 for KV\n*   Context Length: 32,768 natively and [131,072 tokens with YaRN](https://huggingface.co/Qwen/Qwen3-14B#processing-long-texts).\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n[](https://huggingface.co/Qwen/Qwen3-14B#quickstart) Quickstart\n---------------------------------------------------------------\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers\u003c4.51.0`, you will encounter the following error:\n\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (\u003c/think\u003e)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang\u003e=0.4.6.post1` or `vllm\u003e=0.8.5` or to create an OpenAI-compatible API endpoint:\n\n*   SGLang:```\npython -m sglang.launch_server --model-path Qwen/Qwen3-14B --reasoning-parser qwen3\n``` \n*   vLLM:```\nvllm serve Qwen/Qwen3-14B --enable-reasoning --reasoning-parser deepseek_r1\n``` \n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#switching-between-thinking-and-non-thinking-mode) Switching Between Thinking and Non-Thinking Mode\n-------------------------------------------------------------------------------------------------------------------------------------------\n\n\u003e The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#enable_thinkingtrue)`enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `\u003cthink\u003e...\u003c/think\u003e` block, followed by the final response.\n\n\u003e For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](https://huggingface.co/Qwen/Qwen3-14B#best-practices) section.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#enable_thinkingfalse)`enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `\u003cthink\u003e...\u003c/think\u003e` block.\n\n\u003e For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](https://huggingface.co/Qwen/Qwen3-14B#best-practices) section.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#advanced-usage-switching-between-thinking-and-non-thinking-modes-via-user-input) Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-14B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n\u003e For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `\u003cthink\u003e...\u003c/think\u003e`. However, the content inside this block may be empty if thinking is disabled. When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `\u003cthink\u003e...\u003c/think\u003e` block.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#agentic-use) Agentic Use\n-----------------------------------------------------------------\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n\n```\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-14B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `\u003cthink\u003ethis is the thought\u003c/think\u003ethis is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n[](https://huggingface.co/Qwen/Qwen3-14B#processing-long-texts) Processing Long Texts\n-------------------------------------------------------------------------------------\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n*   Modifying the model files: In the `config.json` file, add the `rope_scaling` fields:\n\n```\n{\n    ...,\n    \"rope_scaling\": {\n        \"rope_type\": \"yarn\",\n        \"factor\": 4.0,\n        \"original_max_position_embeddings\": 32768\n    }\n}\n``` \nFor `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n*   Passing command line arguments:\n\nFor `vllm`, you can use\n\n```\nvllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\n``` \nFor `sglang`, you can use\n\n```\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n``` \nFor `llama-server` from `llama.cpp`, you can use\n\n```\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n``` \n\n\u003e If you encounter the following warning\n\u003e \n\u003e \n\u003e \n\u003e ```\n\u003e Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n\u003e ```\n\u003e \n\u003e \n\u003e please upgrade `transformers\u003e=4.51.0`.\n\n\u003e All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.** We advise adding the `rope_scaling` configuration only when processing long contexts is required. It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0.\n\n\u003e The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n\u003e The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n[](https://huggingface.co/Qwen/Qwen3-14B#best-practices) Best Practices\n-----------------------------------------------------------------------\n\nTo achieve optimal performance, we recommend the following settings:\n\n1.   **Sampling Parameters**:\n\n    *   For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n    *   For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n    *   For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2.   **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3.   **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n\n    *   **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n    *   **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4.   **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### [](https://huggingface.co/Qwen/Qwen3-14B#citation) Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- **NousCoder‚Äë14B** ‚Äì a 14‚ÄëB‚Äëparameter coding‚Äëfocused LLM released by NousResearch, fine‚Äëtuned from Qwen3‚Äë14B via reinforcement learning.  \n- **Training details** ‚Äì 24‚ÄØk verifiable programming problems, 48 B200 GPUs, 4‚Äëday run.  \n- **Benchmark performance** ‚Äì Pass@1 on LiveCodeBench‚ÄØv6 rises from 60.79‚ÄØ% (Qwen3‚Äë14B) to **67.87‚ÄØ%** (+7.08‚ÄØ%).  \n- **Open‚Äësource availability** ‚Äì model and training code live on Hugging‚ÄØFace under an Apache‚Äë2.0 license; ready for use with popular inference back‚Äëends (vLLM, SGLang, etc.).  \n\n**Why it matters**  \nThe jump in coding‚Äëaccuracy demonstrates that RL‚Äëbased finetuning can substantially lift a competitive‚Äëprogramming LLM, giving practitioners a new, high‚Äëperformance, open‚Äësource option for code generation tasks.\n\n**Community response**  \nSome users applaud the result (‚ÄúIt‚Äôs happening!‚Äù), while others question whether the gains reflect true generalization or over‚Äëfit to the LiveCodeBench suite.\n\n**Key entities**  \nNousResearch, Qwen, Qwen3‚Äë14B, Hugging‚ÄØFace, LiveCodeBench, RL, B200, Apache‚Äë2.0.",
        "html": "\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNousCoder‚Äë14B\u003c/strong\u003e ‚Äì a 14‚ÄëB‚Äëparameter coding‚Äëfocused LLM released by NousResearch, fine‚Äëtuned from Qwen3‚Äë14B via reinforcement learning.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining details\u003c/strong\u003e ‚Äì 24‚ÄØk verifiable programming problems, 48 B200 GPUs, 4‚Äëday run.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBenchmark performance\u003c/strong\u003e ‚Äì Pass@1 on LiveCodeBench‚ÄØv6 rises from 60.79‚ÄØ% (Qwen3‚Äë14B) to \u003cstrong\u003e67.87‚ÄØ%\u003c/strong\u003e (+7.08‚ÄØ%).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpen‚Äësource availability\u003c/strong\u003e ‚Äì model and training code live on Hugging‚ÄØFace under an Apache‚Äë2.0 license; ready for use with popular inference back‚Äëends (vLLM, SGLang, etc.).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters\u003c/strong\u003e\u003cbr\u003e\nThe jump in coding‚Äëaccuracy demonstrates that RL‚Äëbased finetuning can substantially lift a competitive‚Äëprogramming LLM, giving practitioners a new, high‚Äëperformance, open‚Äësource option for code generation tasks.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response\u003c/strong\u003e\u003cbr\u003e\nSome users applaud the result (‚ÄúIt‚Äôs happening!‚Äù), while others question whether the gains reflect true generalization or over‚Äëfit to the LiveCodeBench suite.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities\u003c/strong\u003e\u003cbr\u003e\nNousResearch, Qwen, Qwen3‚Äë14B, Hugging‚ÄØFace, LiveCodeBench, RL, B200, Apache‚Äë2.0.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.35202021Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.97,
        "reason": "Detailed release of NousCoder-14B with RL fine-tuning, benchmark improvements, and training specifics, indicating significant competitive programming model advancement.",
        "processed_at": "2026-01-07T03:36:56.118219453Z"
      },
      "processed_at": "2026-01-07T02:54:06.727457704Z"
    },
    {
      "flow_id": "",
      "id": "1q5vk9m",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/",
      "title": "200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring",
      "content": "This is the inference strategy:\n\n1. Embed your query using a dense embedding model into a 'standard' fp32 embedding\n2. Quantize the fp32 embedding to binary: 32x smaller\n3. Use an approximate (or exact) binary index to retrieve e.g. 40 documents (\\~20x faster than a fp32 index)\n4. Load int8 embeddings for the 40 top binary documents from disk.\n5. Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings\n6. Sort the 40 documents based on the new scores, grab the top 10\n7. Load the titles/texts of the top 10 documents\n\nThis requires:  \n\\- Embedding all of your documents once, and using those embeddings for:  \n\\- A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate  \n\\- A int8 \"view\", i.e. a way to load the int8 embeddings from disk efficiently given a document ID\n\nInstead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index.\n\nBy loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore \\~99% of the performance of the fp32 search, compared to \\~97% when using purely the binary index: [https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring)\n\nCheck out the demo that allows you to test this technique on 40 million texts from Wikipedia: [https://huggingface.co/spaces/sentence-transformers/quantized-retrieval](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval)\n\nIt would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'.\n\nIn short: your retrieval doesn't need to be so expensive!\n\nSources:  \n\\- [https://www.linkedin.com/posts/tomaarsen\\_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a](https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a)  \n\\- [https://huggingface.co/blog/embedding-quantization](https://huggingface.co/blog/embedding-quantization)  \n\\- [https://cohere.com/blog/int8-binary-embeddings](https://cohere.com/blog/int8-binary-embeddings)",
      "author": "-Cubie-",
      "created_at": "2026-01-06T21:24:42Z",
      "comments": [
        {
          "id": "ny3nimq",
          "author": "goldlord44",
          "content": "This looks like very interesting research! Thank you! \n\nI'd be keen to see the results for retrieval from clusters (i.e. a collection of chunks that are all from textbooks / docs talking about quantum mechanics).\n\nMy initial feeling and concern is that this method is very strong for semantically dissimilar databases, I.e. General rag on reddit comments from all communities, but for more targeted vector stores it will struggle significantly (by virtue of the cluster already causing all the chunks to effectively a dimension reduction to the small section in vector space representing the topic, thus the further reduction of to binary could misrepresent this space)\n\nMore than happy to be shown otherwise! Would save my company a lot of money!",
          "created_at": "2026-01-06T23:27:45Z",
          "was_summarised": false
        },
        {
          "id": "ny4ibo2",
          "author": "swagonflyyyy",
          "content": "This...is...actually a bigger deal than I thought!\n\nBut if I had to guess, it seems to be in prototype territory given all the steps you need to take. Do you think there's any way to remove some of those steps altogether?",
          "created_at": "2026-01-07T02:10:54Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/spaces/sentence-transformers/quantized-retrieval",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring",
          "was_fetched": true,
          "page": "Title: Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval\n\nURL Source: https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring\n\nMarkdown Content:\n[Back to Articles](https://huggingface.co/blog)\n\n[](https://huggingface.co/aamirshakir)\n\n[](https://huggingface.co/tomaarsen)\n\n[](https://huggingface.co/SeanLee97)\n\nWe introduce the concept of embedding quantization and showcase their impact on retrieval speed, memory usage, disk space, and cost. We'll discuss how embeddings can be quantized in theory and in practice, after which we introduce a [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showing a real-life retrieval scenario of 41 million Wikipedia texts.\n\n[](https://huggingface.co/blog/embedding-quantization#table-of-contents) Table of Contents\n------------------------------------------------------------------------------------------\n\n*   [Why Embeddings?](https://huggingface.co/blog/embedding-quantization#why-embeddings)\n    *   [Embeddings may struggle to scale](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale)\n\n*   [Improving scalability](https://huggingface.co/blog/embedding-quantization#improving-scalability)\n    *   [Binary Quantization](https://huggingface.co/blog/embedding-quantization#binary-quantization)\n        *   [Binary Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers)\n        *   [Binary Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases)\n\n    *   [Scalar (int8) Quantization](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization)\n        *   [Scalar Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers)\n        *   [Scalar Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases)\n\n    *   [Combining Binary and Scalar Quantization](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization)\n    *   [Quantization Experiments](https://huggingface.co/blog/embedding-quantization#quantization-experiments)\n    *   [Influence of Rescoring](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring)\n        *   [Binary Rescoring](https://huggingface.co/blog/embedding-quantization#binary-rescoring)\n        *   [Scalar (Int8) Rescoring](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring)\n        *   [Retrieval Speed](https://huggingface.co/blog/embedding-quantization#retrieval-speed)\n\n    *   [Performance Summarization](https://huggingface.co/blog/embedding-quantization#performance-summarization)\n    *   [Demo](https://huggingface.co/blog/embedding-quantization#demo)\n    *   [Try it yourself](https://huggingface.co/blog/embedding-quantization#try-it-yourself)\n    *   [Future work:](https://huggingface.co/blog/embedding-quantization#future-work)\n    *   [Acknowledgments](https://huggingface.co/blog/embedding-quantization#acknowledgments)\n    *   [Citation](https://huggingface.co/blog/embedding-quantization#citation)\n    *   [References](https://huggingface.co/blog/embedding-quantization#references)\n\n[](https://huggingface.co/blog/embedding-quantization#why-embeddings) Why Embeddings?\n-------------------------------------------------------------------------------------\n\nEmbeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects, like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors.\n\nAfter transforming the complex objects, you can determine their similarity by calculating the similarity of the respective embeddings! This is crucial for many use cases: it serves as the backbone for recommendation systems, retrieval, one-shot or few-shot learning, outlier detection, similarity search, paraphrase detection, clustering, classification, and much more.\n\n### [](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale) Embeddings may struggle to scale\n\nHowever, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in `float32`, i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory!\n\nThe table below gives an overview of different models, dimension size, memory requirement, and costs. Costs are computed at an estimated $3.8 per GB/mo with `x2gd` instances on AWS.\n\n| Embedding Dimension | Example Models | 100M Embeddings | 250M Embeddings | 1B Embeddings |\n| --- | --- | --- | --- | --- |\n| 384 | [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) | 143.05GB $543 / mo | 357.62GB $1,358 / mo | 1430.51GB $5,435 / mo |\n| 768 | [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) [jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1) | 286.10GB $1,087 / mo | 715.26GB $2,717 / mo | 2861.02GB $10,871 / mo |\n| 1024 | [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) | 381.46GB $1,449 / mo | 953.67GB $3,623 / mo | 3814.69GB $14,495 / mo |\n| 1536 | [OpenAI text-embedding-3-small](https://openai.com/blog/new-embedding-models-and-api-updates) | 572.20GB $2,174 / mo | 1430.51GB $5,435 / mo | 5722.04GB $21,743 / mo |\n| 3072 | [OpenAI text-embedding-3-large](https://openai.com/blog/new-embedding-models-and-api-updates) | 1144.40GB $4,348 / mo | 2861.02GB $10,871 / mo | 11444.09GB $43,487 / mo |\n\n[](https://huggingface.co/blog/embedding-quantization#improving-scalability) Improving scalability\n--------------------------------------------------------------------------------------------------\n\nThere are several ways to approach the challenges of scaling embeddings. The most common approach is dimensionality reduction, such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis). However, classic dimensionality reduction -- like PCA methods -- [tends to perform poorly when used with embeddings](https://arxiv.org/abs/2205.11498).\n\nIn recent news, [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) ([blogpost](https://huggingface.co/blog/matryoshka)) (MRL) as used by [OpenAI](https://openai.com/blog/new-embedding-models-and-api-updates) also allows for cheaper embeddings. With MRL, only the first `n` embedding dimensions are used. This approach has already been adopted by some open models like [nomic-ai/nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) and [mixedbread-ai/mxbai-embed-2d-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1) (For OpenAIs `text-embedding-3-large`, we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.).\n\nHowever, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: **Quantization**. Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs. Let's dive into it!\n\n### [](https://huggingface.co/blog/embedding-quantization#binary-quantization) Binary Quantization\n\nUnlike quantization in models where you reduce the precision of weights, quantization for embeddings refers to a post-processing step for the embeddings themselves. In particular, binary quantization refers to the conversion of the `float32` values in an embedding to 1-bit values, resulting in a 32x reduction in memory and storage usage.\n\nTo quantize `float32` embeddings to binary, we simply threshold normalized embeddings at 0:\n\nf(x)={0 if x‚â§0 1 if x\u003e0 f(x)= \\begin{cases} 0 \u0026 \\text{if } x\\leq 0\\\\ 1 \u0026 \\text{if } x \\gt 0 \\end{cases}\n\nWe can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.\n\n[Yamada et al. (2021)](https://arxiv.org/abs/2106.00882) introduced a rescore step, which they called _rerank_, to boost the performance. They proposed that the `float32` query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve `rescore_multiplier * top_k` results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the `float32` query embedding.\n\nBy applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers) Binary Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to binary would result in 1024 bits. In practice, it is much more common to store bits as bytes instead, so when we quantize to binary embeddings, we pack the bits into bytes using `np.packbits`.\n\nTherefore, quantizing a `float32` embedding with a dimensionality of 1024 yields an `int8` or `uint8` embedding with a dimensionality of 128. See two approaches of how you can produce quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2a. Encode some text using \"binary\" quantization\nbinary_embeddings = model.encode(\n    [\"I am driving to the lake.\", \"It is a beautiful day.\"],\n    precision=\"binary\",\n)\n```\n\nor\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2b. or, encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nbinary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere, you can see the differences between default `float32` embeddings and binary embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e binary_embeddings.shape\n(2, 128)\n\u003e\u003e\u003e binary_embeddings.nbytes\n256\n\u003e\u003e\u003e binary_embeddings.dtype\nint8\n```\n\nNote that you can also choose `\"ubinary\"` to quantize to binary using the unsigned `uint8` data format. This may be a requirement depending on your vector library/database.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases) Binary Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | [Yes](https://github.com/facebookresearch/faiss/wiki/Binary-indexes) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/schema-reference.html) |\n| Milvus | [Yes](https://milvus.io/docs/index.md) |\n| Qdrant | Through [Binary Quantization](https://qdrant.tech/documentation/guides/quantization/#binary-quantization) |\n| Weaviate | Through [Binary Quantization](https://weaviate.io/developers/weaviate/configuration/bq-compression) |\n\n### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization) Scalar (int8) Quantization\n\nWe use a scalar quantization process to convert the `float32` embeddings into `int8`. This involves mapping the continuous range of `float32` values to the discrete set of `int8` values, which can represent 256 distinct levels (from -128 to 127), as shown in the image below. This is done by using a large calibration dataset of embeddings. We compute the range of these embeddings, i.e., the `min` and `max` of each embedding dimension. From there, we calculate the steps (buckets) to categorize each value.\n\nTo further boost the retrieval performance, you can optionally apply the same rescoring step as for the binary embeddings. It is important to note that the calibration dataset greatly influences performance since it defines the quantization buckets.\n\n_Source: [https://qdrant.tech/articles/scalar-quantization/](https://qdrant.tech/articles/scalar-quantization/)_\n\nWith scalar quantization to `int8`, we reduce the original `float32` embeddings' precision so that each value is represented with an 8-bit integer (4x smaller). Note that this differs from the binary quantization case, where each value is represented by a single bit (32x smaller).\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers) Scalar Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to `int8` results in 1024 bytes. In practice, we can choose either `uint8` or `int8`. This choice is usually made depending on what your vector library/database supports.\n\nIn practice, it is recommended to provide the scalar quantization with either:\n\n1.   a large set of embeddings to quantize all at once, or\n2.   `min` and `max` ranges for each of the embedding dimensions, or\n3.   a large calibration dataset of embeddings from which the `min` and `max` ranges can be computed.\n\nIf none of these are the case, you will be given a warning like this: `Computing int8 quantization buckets based on 2 embeddings. int8 quantization is more stable with 'ranges' calculated from more embeddings or a 'calibration_embeddings' that can be used to calculate the buckets.`\n\nSee how you can produce scalar quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\nfrom datasets import load_dataset\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2. Prepare an example calibration dataset\ncorpus = load_dataset(\"nq_open\", split=\"train[:1000]\")[\"question\"]\ncalibration_embeddings = model.encode(corpus)\n\n# 3. Encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nint8_embeddings = quantize_embeddings(\n    embeddings,\n    precision=\"int8\",\n    calibration_embeddings=calibration_embeddings,\n)\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere you can see the differences between default `float32` embeddings and `int8` scalar embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e int8_embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e int8_embeddings.nbytes\n2048\n\u003e\u003e\u003e int8_embeddings.dtype\nint8\n```\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases) Scalar Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | Indirectly through [IndexHNSWSQ](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexHNSWSQ.html) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/tensor.html) |\n| OpenSearch | [Yes](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector) |\n| ElasticSearch | [Yes](https://www.elastic.co/de/blog/save-space-with-byte-sized-vectors) |\n| Milvus | Indirectly through [IVF_SQ8](https://milvus.io/docs/index.md) |\n| Qdrant | Indirectly through [Scalar Quantization](https://qdrant.tech/documentation/guides/quantization/#scalar-quantization) |\n\n### [](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization) Combining Binary and Scalar Quantization\n\nCombining binary and scalar quantization is possible to get the best of both worlds: the extreme speed from binary embeddings and the great performance preservation of scalar embeddings with rescoring. See the [demo](https://huggingface.co/blog/embedding-quantization#demo) below for a real-life implementation of this approach involving 41 million texts from Wikipedia. The pipeline for that setup is as follows:\n\n1.   The query is embedded using the [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) SentenceTransformer model.\n2.   The query is quantized to binary using the [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings) function from the `sentence-transformers` library.\n3.   A binary index (41M binary embeddings; 5.2GB of memory/disk space) is searched using the quantized query for the top 40 documents.\n4.   The top 40 documents are loaded on the fly from an int8 index on disk (41M int8 embeddings; 0 bytes of memory, 47.5GB of disk space).\n5.   The top 40 documents are rescored using the float32 query and the int8 embeddings to get the top 10 documents.\n6.   The top 10 documents are sorted by score and displayed.\n\nThrough this approach, we use 5.2GB of memory and 52GB of disk space for the indices. This is considerably less than normal retrieval, requiring 200GB of memory and 200GB of disk space. Especially as you scale up even further, this will result in notable reductions in latency and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#quantization-experiments) Quantization Experiments\n\nWe conducted our experiments on the retrieval subset of the [MTEB](https://huggingface.co/spaces/mteb/leaderboard) containing 15 benchmarks. First, we retrieved the top k (k=100) search results with a `rescore_multiplier` of 4. Therefore, we retrieved 400 results in total and performed the rescoring on these top 400. For the `int8` performance, we directly used the dot-product without any rescoring.\n\n| Model | Embedding Dimension | 250M Embeddings | MTEB Retrieval (NDCG@10) | Percentage of default performance |\n| --- | --- | --- | --- | --- |\n| **Open Models** |  |  |  |  |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): float32 | 1024 | 953.67GB $3623 / mo | 54.39 | 100% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): int8 | 1024 | 238.41GB $905 / mo | 52.79 | 97% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): binary | 1024 | 29.80GB $113.25 / mo | 52.46 | 96.45% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): float32 | 768 | 286.10GB $1087 / mo | 50.77 | 100% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): int8 | 768 | 178.81GB $679 / mo | 47.54 | 94.68% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): binary | 768 | 22.35GB $85 / mo | 37.96 | 74.77% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): float32 | 768 | 286.10GB $1087 / mo | 53.01 | 100% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): binary | 768 | 22.35GB $85 / mo | 46.49 | 87.7% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): float32 | 384 | 357.62GB $1358 / mo | 41.66 | 100% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): int8 | 384 | 89.40GB $339 / mo | 37.82 | 90.79% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): binary | 384 | 11.18GB $42 / mo | 39.07 | 93.79% |\n| **Proprietary Models** |  |  |  |  |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): float32 | 1024 | 953.67GB $3623 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): int8 | 1024 | 238.41GB $905 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): binary | 1024 | 29.80GB $113.25 / mo | 52.3 | 94.6% |\n\nSeveral key trends and benefits can be identified from the results of our quantization experiments. As expected, embedding models with higher dimension size typically generate higher storage costs per computation but achieve the best performance. Surprisingly, however, quantization to `int8` already helps `mxbai-embed-large-v1` and `Cohere-embed-english-v3.0` achieve higher performance with lower storage usage than that of the smaller dimension size base models.\n\nThe benefits of quantization are, if anything, even more clearly visible when looking at the results obtained with binary models. In that scenario, the 1024 dimension models still outperform a now 10x more storage intensive base model, and the `mxbai-embed-large-v1` even manages to hold more than 96% of performance after a 32x reduction in resource requirements. The further quantization from `int8` to binary barely results in any additional loss of performance for this model.\n\nInterestingly, we can also see that `all-MiniLM-L6-v2` exhibits stronger performance on binary than on `int8` quantization. A possible explanation for this could be the selection of calibration data. On `e5-base-v2`, we observe the effect of [dimension collapse](https://arxiv.org/abs/2110.09348), which causes the model to only use a subspace of the latent space; when performing the quantization, the whole space collapses further, leading to high performance losses.\n\nThis shows that quantization doesn't universally work with all embedding models. It remains crucial to consider exisiting benchmark outcomes and conduct experiments to determine a given model's compatibility with quantization.\n\n### [](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring) Influence of Rescoring\n\nIn this section we look at the influence of rescoring on retrieval performance. We evaluate the results based on [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1).\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-rescoring) Binary Rescoring\n\nWith binary embeddings, [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) retains 92.53% of performance on MTEB Retrieval. Just doing the rescoring without retrieving more samples pushes the performance to 96.45%. We experimented with setting the`rescore_multiplier` from 1 to 10, but observe no further boost in performance. This indicates that the `top_k` search already retrieved the top candidates and the rescoring reordered these good candidates appropriately.\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring) Scalar (Int8) Rescoring\n\nWe also evaluated the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) model with `int8` rescoring, as Cohere showed that [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) reached up to 100% of the performance of the `float32` model with `int8` quantization. For this experiment, we set the `rescore_multiplier` to [1, 4, 10] and got the following results:\n\nAs we can see from the diagram, a higher rescore multiplier implies better retention of performance after quantization. Extrapolating from our results, we assume the relation is likely hyperbolical with performance approaching 100% as the rescore multiplier continues to rise. A rescore multiplier of 4-5 already leads to a remarkable performance retention of 99% using `int8`.\n\n#### [](https://huggingface.co/blog/embedding-quantization#retrieval-speed) Retrieval Speed\n\nWe measured retrieval speed on a Google Cloud Platform `a2-highgpu-4g` instance using the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) embeddings with 1024 dimension on the whole MTEB Retrieval. For int8 we used [USearch](https://github.com/unum-cloud/usearch) (Version 2.9.2) and binary quantization [Faiss](https://github.com/facebookresearch/faiss) (Version 1.8.0). Everything was computed on CPU using exact search.\n\n| Quantization | Min | Mean | Max |\n| --- | --- | --- | --- |\n| `float32` | 1x (baseline) | **1x** (baseline) | 1x (baseline) |\n| `int8` | 2.99x speedup | **3.66x** speedup | 4.8x speedup |\n| `binary` | 15.05x speedup | **24.76x** speedup | 45.8x speedup |\n\nAs shown in the table, applying `int8` scalar quantization results in an average speedup of 3.66x compared to full-size `float32` embeddings. Additionally, binary quantization achieves a speedup of 24.76x on average. For both scalar and binary quantization, even the worst case scenario resulted in very notable speedups.\n\n### [](https://huggingface.co/blog/embedding-quantization#performance-summarization) Performance Summarization\n\nThe experimental results, effects on resource use, retrieval speed, and retrieval performance by using quantization can be summarized as follows:\n\n|  | float32 | int8/uint8 | binary/ubinary |\n| --- | --- | --- | --- |\n| **Memory \u0026 Index size savings** | 1x | exactly 4x | exactly 32x |\n| **Retrieval Speed** | 1x | up to 4x | up to 45x |\n| **Percentage of default performance** | 100% | ~99.3% | ~96% |\n\n### [](https://huggingface.co/blog/embedding-quantization#demo) Demo\n\nThe following [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showcases the retrieval efficiency using exact or approximate search by combining binary search with scalar (`int8`) rescoring. The solution requires 5GB of memory for the binary index and 50GB of disk space for the binary and scalar indices, considerably less than the 200GB of memory and disk space which would be required for regular `float32` retrieval. Additionally, retrieval is much faster.\n\n### [](https://huggingface.co/blog/embedding-quantization#try-it-yourself) Try it yourself\n\nThe following scripts can be used to experiment with embedding quantization for retrieval \u0026 beyond. There are three categories:\n\n*   **Recommended Retrieval**:\n    *   [semantic_search_recommended.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_recommended.py): This script combines binary search with scalar rescoring, much like the above demo, for cheap, efficient, and performant retrieval.\n\n*   **Usage**:\n    *   [semantic_search_faiss.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using FAISS, by using the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function.\n    *   [semantic_search_usearch.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using USearch, by using the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function.\n\n*   **Benchmarks**:\n    *   [semantic_search_faiss_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using FAISS. It uses the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function. Our benchmarks especially show show speedups for `ubinary`.\n    *   [semantic_search_usearch_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using USearch. It uses the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function. Our experiments show large speedups on newer hardware, particularly for `int8`.\n\n### [](https://huggingface.co/blog/embedding-quantization#future-work) Future work\n\nWe are looking forward to further advancements of binary quantization. To name a few potential improvements, we suspect that there may be room for scalar quantization smaller than `int8`, i.e. with 128 or 64 buckets instead of 256.\n\nAdditionally, we are excited that embedding quantization is fully perpendicular to Matryoshka Representation Learning (MRL). In other words, it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually corresponds with a 2% reduction in performance) and then apply binary or scalar quantization. We suspect this could speed up retrieval up to 32x for a ~3% reduction in quality, or up to 256x for a ~10% reduction in quality.\n\nLastly, we recognize that retrieval using embedding quantization can also be combined with a separate reranker model. We imagine that a 3-step pipeline of binary search, scalar (int8) rescoring, and cross-encoder reranking allows for state-of-the-art retrieval performance at low latencies, memory usage, disk space, and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#acknowledgments) Acknowledgments\n\nThis project is possible thanks to our collaboration with [mixedbread.ai](https://mixedbread.ai/) and the [SentenceTransformers](https://www.sbert.net/) library, which allows you to easily create sentence embeddings and quantize them. If you want to use quantized embeddings in your project, now you know how!\n\n### [](https://huggingface.co/blog/embedding-quantization#citation) Citation\n\n```\n@article{shakir2024quantization,\n  author       = { Aamir Shakir and\n                   Tom Aarsen and\n                   Sean Lee\n                 },\n  title = { Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval },\n  journal = {Hugging Face Blog},\n  year = {2024},\n  note = {https://huggingface.co/blog/embedding-quantization},\n}\n```\n\n### [](https://huggingface.co/blog/embedding-quantization#resources) Resources\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n*   [Sentence Transformers docs - Embedding Quantization](https://sbert.net/examples/applications/embedding-quantization/README.html)\n*   [https://txt.cohere.com/int8-binary-embeddings/](https://txt.cohere.com/int8-binary-embeddings/)\n*   [https://qdrant.tech/documentation/guides/quantization](https://qdrant.tech/documentation/guides/quantization)\n*   [https://zilliz.com/learn/scalar-quantization-and-product-quantization](https://zilliz.com/learn/scalar-quantization-and-product-quantization)",
          "was_summarised": false
        },
        {
          "url": "https://www.linkedin.com/posts/tomaarsen%5C_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a",
          "was_fetched": true,
          "page": "Title: LinkedIn\n\nURL Source: https://www.linkedin.com/posts/tomaarsen/_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\nLinkedIn\n===============\n\nÈÅ∏ÊìáË™ûË®Ä \n\n[](https://www.linkedin.com/ \"LinkedIn\")\n\nŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑÿµŸÅÿ≠ÿ©\n========================\n\nÿπŸÅŸàÿßŸãÿå ŸÑŸÖ ŸÜÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑÿµŸÅÿ≠ÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ®ÿ≠ÿ´ ÿπŸÜŸáÿß. ÿ®ÿ±ÿ¨ÿßÿ° ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ŸÑŸÑÿµŸÅÿ≠ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿ£Ÿà ÿßŸÑÿßŸÜÿ™ŸÇÿßŸÑ ŸÑŸÄ[ŸÖÿ±ŸÉÿ≤ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ©](https://www.linkedin.com/help/linkedin?trk=404_page \"ŸÖÿ±ŸÉÿ≤ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ©\") ŸÑŸÑŸÖÿ≤ŸäÿØ ŸÖŸÜ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[ÿßŸÑÿ•ŸÜÿ™ŸÇÿßŸÑ ŸÑŸÖŸàÿ¨ÿ≤ŸÉ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä](https://www.linkedin.com/feed/?trk=404_page \"ÿßŸÑÿ•ŸÜÿ™ŸÇÿßŸÑ ŸÑŸÖŸàÿ¨ÿ≤ŸÉ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä\")\n\nStr√°nka nenalezena\n==================\n\nOmlouv√°me se, nem≈Ø≈æeme naj√≠t str√°nku, kterou hled√°te. Zkuste se vr√°tit zp√°tky na p≈ôedchoz√≠ str√°nku, nebo se pod√≠vejte do na≈°eho [Centra n√°povƒõdy](https://www.linkedin.com/help/linkedin?trk=404_page \"Centrum n√°povƒõdy\") pro v√≠ce informac√≠\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[P≈ôej√≠t do informaƒçn√≠ho kan√°lu](https://www.linkedin.com/feed/?trk=404_page \"P≈ôej√≠t do informaƒçn√≠ho kan√°lu\")\n\nSiden blev ikke fundet\n======================\n\nVi kan desv√¶rre ikke finde den side, du leder efter. G√• tilbage til den forrige side, eller bes√∏g [Hj√¶lp](https://www.linkedin.com/help/linkedin?trk=404_page \"Hj√¶lp\") for at f√• flere oplysninger\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[G√• til dit feed](https://www.linkedin.com/feed/?trk=404_page \"G√• til dit feed\")\n\nSeite nicht gefunden\n====================\n\nDie gew√ºnschte Seite konnte leider nicht gefunden werden. Versuchen Sie, zur vorherigen Seite zur√ºckzukehren, oder besuchen Sie unseren [Hilfebereich](https://www.linkedin.com/help/linkedin?trk=404_page \"Hilfebereich\"), um mehr zu erfahren.\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Zu Ihrem Feed](https://www.linkedin.com/feed/?trk=404_page \"Zu Ihrem Feed\")\n\nPage not found\n==============\n\nUh oh, we can‚Äôt seem to find the page you‚Äôre looking for. Try going back to the previous page or see our [Help Center](https://www.linkedin.com/help/linkedin?trk=404_page \"Help Center\") for more information\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Go to your feed](https://www.linkedin.com/feed/?trk=404_page \"Go to your feed\")\n\nP√°gina no encontrada\n====================\n\nVaya, parece que no podemos encontrar la p√°gina que buscas. Intenta volver a la p√°gina anterior o visita nuestro [Centro de ayuda](https://www.linkedin.com/help/linkedin?trk=404_page \"Centro de ayuda\") para m√°s informaci√≥n.\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Ir a tu feed](https://www.linkedin.com/feed/?trk=404_page \"Ir a tu feed\")\n\nImpossible de trouver cette page\n================================\n\nNous ne trouvons pas la page que vous recherchez. Essayez de retourner √† la page pr√©c√©dente ou consultez notre [assistance client√®le](https://www.linkedin.com/help/linkedin?trk=404_page \"assistance client√®le\") pour plus d‚Äôinformations\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Ouvrez votre fil](https://www.linkedin.com/feed/?trk=404_page \"Ouvrez votre fil\")\n\nHalaman ini tidak dapat ditemukan\n=================================\n\nMaaf, sepertinya kami tidak dapat menemukan halaman yang Anda cari. Coba kembali ke halaman sebelumnya atau lihat [Pusat Bantuan](https://www.linkedin.com/help/linkedin?trk=404_page \"Pusat Bantuan\") kami untuk informasi lebih lanjut\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Buka feed Anda](https://www.linkedin.com/feed/?trk=404_page \"Buka feed Anda\")\n\nPagina non trovata\n==================\n\nNon abbiamo trovato la pagina che stai cercando. Prova a tornare alla pagina precedente o visita il nostro [Centro assistenza](https://www.linkedin.com/help/linkedin?trk=404_page \"Centro assistenza\") per saperne di pi√π.\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Vai al tuo feed](https://www.linkedin.com/feed/?trk=404_page \"Vai al tuo feed\")\n\n„Éö„Éº„Ç∏„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü\n==============\n\nÁî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„ÅäÊé¢„Åó„ÅÆ„Éö„Éº„Ç∏„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇÂâç„ÅÆ„Éö„Éº„Ç∏„Å´Êàª„Çã„Åã„ÄÅ[„Éò„É´„Éó„Çª„É≥„Çø„Éº](https://www.linkedin.com/help/linkedin?trk=404_page \"„Éò„É´„Éó„Çª„É≥„Çø„Éº\")„ÅßË©≥Á¥∞„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ\n----------------------------------------------------------------------------------------------------------------------\n\n[„Éï„Ç£„Éº„Éâ„Å´ÁßªÂãï](https://www.linkedin.com/feed/?trk=404_page \"„Éï„Ç£„Éº„Éâ„Å´ÁßªÂãï\")\n\nÌéòÏù¥ÏßÄ ÏóÜÏùå\n======\n\nÏõêÌïòÏãúÎäî ÌéòÏù¥ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥Ï†Ñ ÌéòÏù¥ÏßÄÎ°ú ÎèåÏïÑÍ∞ÄÍ±∞ÎÇò [Í≥†Í∞ùÏÑºÌÑ∞](https://www.linkedin.com/help/linkedin?trk=404_page \"Í≥†Í∞ùÏÑºÌÑ∞\")ÏóêÏÑú ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥ÏÑ∏Ïöî.\n------------------------------------------------------------------------------------------------------------------\n\n[ÌôàÏúºÎ°ú Í∞ÄÍ∏∞](https://www.linkedin.com/feed/?trk=404_page \"ÌôàÏúºÎ°ú Í∞ÄÍ∏∞\")\n\nLaman tidak ditemui\n===================\n\nHarap maaf, kami tidak dapat menemui laman yang ingin anda cari. Cuba kembali ke laman sebelumnya atau lihat [Pusat Bantuan](https://www.linkedin.com/help/linkedin?trk=404_page \"Pusat Bantuan\") kami untuk maklumat lanjut\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Pergi ke suapan](https://www.linkedin.com/feed/?trk=404_page \"Pergi ke suapan\")\n\nPagina niet gevonden\n====================\n\nDe pagina waar u naar op zoek bent, kan niet worden gevonden. Probeer terug te gaan naar de vorige pagina of bezoek het [Help Center](https://www.linkedin.com/help/linkedin?trk=404_page \"Helpcentrum\") voor meer informatie\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Ga naar uw feed](https://www.linkedin.com/feed/?trk=404_page \"Ga naar uw feed\")\n\nFant ikke siden\n===============\n\nVi finner ikke siden du leter etter. G√• tilbake til forrige side eller bes√∏k v√•r [brukerst√∏tte](https://www.linkedin.com/help/linkedin?trk=404_page \"brukerst√∏tte\") for mer informasjon\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[G√• til din feed](https://www.linkedin.com/feed/?trk=404_page \"G√• til din feed\")\n\nNie znaleziono strony\n=====================\n\nNie mo≈ºemy znale≈∫ƒá strony, kt√≥rej szukasz. Spr√≥buj wr√≥ciƒá do poprzedniej strony lub nasze [Centrum pomocy](https://www.linkedin.com/help/linkedin?trk=404_page \"Centrum pomocy\"), aby uzyskaƒá wiƒôcej informacji\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Przejd≈∫ do swojego kana≈Çu](https://www.linkedin.com/feed/?trk=404_page \"Przejd≈∫ do swojego kana≈Çu\")\n\nP√°gina n√£o encontrada\n=====================\n\nA p√°gina que voc√™ est√° procurando n√£o foi encontrada. Volte para a p√°gina anterior ou visite nossa [Central de Ajuda](https://www.linkedin.com/help/linkedin?trk=404_page \"Central de Ajuda\") para mais informa√ß√µes\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Voltar para seu feed](https://www.linkedin.com/feed/?trk=404_page \"Voltar para seu feed\")\n\nPagina nu a fost gƒÉsitƒÉ\n=======================\n\nNe pare rƒÉu, nu gƒÉsim pagina pe care o cƒÉuta≈£i. Reveni≈£i la pagina anterioarƒÉ sau consulta≈£i [Centrul nostru de asisten≈£ƒÉ](https://www.linkedin.com/help/linkedin?trk=404_page \"Centrul nostru de asisten≈£ƒÉ\") pentru mai multe informa≈£ii\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Accesa≈£i fluxul dvs.](https://www.linkedin.com/feed/?trk=404_page \"Accesa≈£i fluxul dvs.\")\n\n–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n===================\n\n–ù–µ —É–¥–∞—ë—Ç—Å—è –Ω–∞–π—Ç–∏ –∏—Å–∫–æ–º—É—é –≤–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É. –í–µ—Ä–Ω–∏—Ç–µ—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞—à–µ–≥–æ [—Å–ø—Ä–∞–≤–æ—á–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞](https://www.linkedin.com/help/linkedin?trk=404_page \"–°–ø—Ä–∞–≤–æ—á–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞\") –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[–ü–µ—Ä–µ–π—Ç–∏ –∫ –ª–µ–Ω—Ç–µ](https://www.linkedin.com/feed/?trk=404_page \"–ü–µ—Ä–µ–π—Ç–∏ –∫ –ª–µ–Ω—Ç–µ\")\n\nSidan kunde inte hittas.\n========================\n\nSidan du letar efter hittades inte. G√• tillbaka till f√∂reg√•ende sida eller bes√∂k v√•rt [Hj√§lpcenter](https://www.linkedin.com/help/linkedin?trk=404_page \"Hj√§lpcenter\") f√∂r mer information\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[G√• till ditt nyhetsfl√∂de](https://www.linkedin.com/feed/?trk=404_page \"G√• till ditt nyhetsfl√∂de\")\n\n‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à\n============\n\n‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏≠‡∏¢‡∏π‡πà ‡∏•‡∏≠‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏π [‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠](https://www.linkedin.com/help/linkedin?trk=404_page \"‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠\") ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ü‡∏µ‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì](https://www.linkedin.com/feed/?trk=404_page \"‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ü‡∏µ‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\")\n\nHindi Nahanap ang Pahina\n========================\n\nNaku, mukhang hindi namin mahanap ang pahina na hinahanap mo. Subukang bumalik sa nakaraang pahina o tingnan ang aming [Help Center](https://www.linkedin.com/help/linkedin?trk=404_page \"Help Center\") para sa higit pang impormasyon\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Pumunta sa iyong feed](https://www.linkedin.com/feed/?trk=404_page \"Pumunta sa iyong feed\")\n\nSayfa bulunamadƒ±\n================\n\nAradƒ±ƒüƒ±nƒ±z sayfa bulunamadƒ±. √ñnceki sayfaya geri d√∂n√ºn veya daha fazla bilgi i√ßin [Yardƒ±m Merkezimizi](https://www.linkedin.com/help/linkedin?trk=404_page \"Yardƒ±m Merkezimizi\") g√∂r√ºnt√ºleyin\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Haber akƒ±≈üƒ±nƒ±za gidin](https://www.linkedin.com/feed/?trk=404_page \"Haber akƒ±≈üƒ±nƒ±za gidin\")\n\nÊú™ÊâæÂà∞ÁΩëÈ°µ\n=====\n\nÊä±Ê≠âÔºåÊó†Ê≥ïÊâæÂà∞È°µÈù¢„ÄÇËØïËØïËøîÂõûÂà∞Ââç‰∏ÄÈ°µÔºåÊàñÂâçÂæÄ[Â∏ÆÂä©‰∏≠ÂøÉ](https://www.linkedin.com/help/linkedin?trk=404_page \"Â∏ÆÂä©‰∏≠ÂøÉ\")‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØ\n----------------------------------------------------------------------------------------------\n\n[ÂâçÂæÄÂä®ÊÄÅÊ±áÊÄª](https://www.linkedin.com/feed/?trk=404_page \"ÂâçÂæÄÂä®ÊÄÅÊ±áÊÄª\")\n\nÁ≥ªÁµ±Êâæ‰∏çÂà∞È†ÅÈù¢„ÄÇ\n========\n\nÊàëÂÄëÂ•ΩÂÉèÊâæ‰∏çÂà∞Ë©≤È†ÅÈù¢„ÄÇË´ãÂõûÂà∞‰∏ä‰∏ÄÈ†ÅÊàñÂâçÂæÄ[Ë™™Êòé‰∏≠ÂøÉ](https://www.linkedin.com/help/linkedin?trk=404_page \"Ë™™Êòé‰∏≠ÂøÉ\")‰æÜÈÄ≤‰∏ÄÊ≠•Áû≠Ëß£\n--------------------------------------------------------------------------------------------\n\n[ÂâçÂæÄÈ¶ñÈ†ÅÂãïÊÖã](https://www.linkedin.com/feed/?trk=404_page \"ÂâçÂæÄÈ¶ñÈ†ÅÂãïÊÖã\")\n\n[](https://www.linkedin.com/ \"LinkedIn\") ¬© 2022 \n\n*   [ÿßÿ™ŸÅÿßŸÇŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿÆÿµŸàÿµŸäÿ©](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [ÿ•ÿ±ÿ¥ÿßÿØÿßÿ™ ÿßŸÑŸÖÿ¨ÿ™ŸÖÿπ](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [ÿ≥Ÿäÿßÿ≥ÿ© ŸÖŸÑŸÅÿßÿ™ ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿßÿ±ÿ™ÿ®ÿßÿ∑](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [ÿ≥Ÿäÿßÿ≥ÿ© ÿ≠ŸÇŸàŸÇ ÿßŸÑŸÜÿ¥ÿ±](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [ÿ•ÿπÿØÿßÿØÿßÿ™ ÿ•ÿØÿßÿ±ÿ© ÿßŸÑÿ∂ŸäŸàŸÅ](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Smlouva s u≈æivatelem](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Z√°sady ochrany soukrom√≠](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Z√°sady komunity](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Z√°sady pro soubory cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Z√°sady ochrany autorsk√Ωch pr√°v](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Volby pro hosty](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Brugeraftale](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Privatlivspolitik](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Forumretningslinjer](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Politik for cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ophavsretspolitik](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Indstillinger for g√¶ster](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Nutzervereinbarung](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Datenschutzrichtlinie](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Netzwerkrichtlinien](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie-Richtlinie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Copyright-Richtlinie](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Einstellungen f√ºr Nichtmitglieder](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Community Guidelines](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Copyright Policy](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Guest Controls](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Condiciones de uso](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Pol√≠tica de privacidad](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Directrices comunitarias](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Pol√≠tica de cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Pol√≠tica de copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controles de invitados](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Conditions g√©n√©rales d‚Äôutilisation de LinkedIn](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Politique de confidentialit√©](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Directives de la communaut√©](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Politique relative aux cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Politique de copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [R√©glages invit√©s](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Perjanjian Pengguna](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Kebijakan Privasi](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Panduan Komunitas](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Kebijakan Cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Kebijakan Hak Cipta](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Pengaturan Tamu](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Contratto di licenza](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Informativa sulla privacy](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Linee guida della community](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Informativa sui cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Informativa sul copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controlli ospite](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Âà©Áî®Ë¶èÁ¥Ñ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [„Éó„É©„Ç§„Éê„Ç∑„Éº„Éù„É™„Ç∑„Éº](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Ç¨„Ç§„Éâ„É©„Ç§„É≥](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie„Éù„É™„Ç∑„Éº](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ëëó‰ΩúÊ®©„Éù„É™„Ç∑„Éº](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [„Ç≤„Çπ„ÉàÂêë„ÅëÁÆ°ÁêÜÊ©üËÉΩ](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [ÏÇ¨Ïö©ÏûêÏïΩÍ¥Ä](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Í∞úÏù∏Ï†ïÎ≥¥ Ï∑®Í∏âÎ∞©Ïπ®](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Ïª§ÎÆ§ÎãàÌã∞Ï†ïÏ±Ö](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Ïø†ÌÇ§Ï†ïÏ±Ö](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ï†ÄÏûëÍ∂åÏ†ïÏ±Ö](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [ÎπÑÌöåÏõê ÏÑ§Ï†ï](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Perjanjian Pengguna](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Dasar Privasi](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Garis Panduan Komuniti](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Dasar Kuki](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Dasar Hak Cipta](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Kawalan Tetamu](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Gebruikersovereenkomst](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Privacybeleid](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Communityrichtlijnen](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookiebeleid](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Auteursrechtenbeleid](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Instellingen voor gasten](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Brukeravtale](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Personvernerkl√¶ring](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Retningslinjer for fellesskapet](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Retningslinjer for informasjonskapsler](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Retningslinjer vedr√∏rende opphavsrett](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Gjestestyring](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Umowa u≈ºytkownika](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Polityka ochrony prywatno≈õci](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Wskaz√≥wki dotyczƒÖce spo≈Çeczno≈õci](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Zasady korzystania z plik√≥w cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Ustawienia go≈õcia](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Contrato do Usu√°rio](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Pol√≠tica de Privacidade do LinkedIn](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Diretrizes da Comunidade](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Pol√≠tica de Cookies](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Pol√≠tica de Direitos Autorais](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controles de visitantes](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Acordul utilizatorului](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Politica de confiden»õialitate](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Linii directoare comunitate](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Politica privind modulele cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Politica de copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Controale vizitator](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [–ü–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [–ü—Ä–∞–≤–∏–ª–∞ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [–ü–æ–ª–∏—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [–ü–æ–ª–∏—Ç–∏–∫–∞ –∑–∞—â–∏—Ç—ã –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ—Å—Ç—è](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Anv√§ndaravtal](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Sekretesspolicy](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Riktlinjer](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookiepolicy](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Upphovsr√§ttspolicy](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [G√§stinst√§llningar](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [‡∏Ç‡πâ‡∏≠‡∏ï‡∏Å‡∏•‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ä‡∏∏‡∏°‡∏ä‡∏ô](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ä‡∏°](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Kasunduan sa Gumagamit](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Patakaran sa Pagkapribado](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Mga alituntunin sa komunidad](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Patakara sa Cookie](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Patakaran sa Copyright](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Mga kontrol ng Panauhin](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Kullanƒ±cƒ± Anla≈ümasƒ±](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Gizlilik Politikasƒ±](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Topluluk Y√∂nergeleri](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [√áerez Politikasƒ±](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Telif Hakkƒ± Politikasƒ±](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Ziyaret√ßi Kontrolleri](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Áî®Êà∑ÂçèËÆÆ](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [ÈöêÁßÅÊîøÁ≠ñ](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Á§æÂå∫ÂáÜÂàô](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie ÊîøÁ≠ñ](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [ÁâàÊùÉÊîøÁ≠ñ](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [ËÆøÂÆ¢ËÆæÁΩÆ](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)\n\n*   [Áî®Êà∂ÂçîË≠∞](https://www.linkedin.com/legal/user-agreement?trk=%7Berror-page%7D-user-agreement)\n*   [Èö±ÁßÅÊ¨äÊîøÁ≠ñ](https://www.linkedin.com/legal/privacy-policy?trk=%7Berror-page%7D-privacy-policy)\n*   [Á§æÁæ§ÊåáÂçó](https://www.linkedin.com/help/linkedin/answer/34593?trk=%7Berror-page%7D-community-guidelines)\n*   [Cookie ÊîøÁ≠ñ](https://www.linkedin.com/legal/cookie-policy?trk=%7Berror-page%7D-cookie-policy)\n*   [Ëëó‰ΩúÊ¨äÊîøÁ≠ñ](https://www.linkedin.com/legal/copyright-policy?trk=%7Berror-page%7D-copyright-policy)\n*   [Ë®™ÂÆ¢ÊéßÁÆ°](https://www.linkedin.com/psettings/guest-controls?trk=%7Berror-page%7D-guest-controls)",
          "was_summarised": false
        },
        {
          "url": "https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a",
          "was_fetched": true,
          "page": "Title: You can perform 200ms search over 40 million texts using just a CPU server, 8GB of RAM, and 40GB of disk space.\n\nURL Source: https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\n\nPublished Time: 2026-01-06T15:24:10.017Z\n\nMarkdown Content:\nQuantized Retrieval - a Hugging Face Space by sentence-transformers | Tom Aarsen | 17 comments\n===============\n\nAgree \u0026 Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n[Skip to main content](https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a#main-content)[LinkedIn](https://www.linkedin.com/?trk=public_post_nav-header-logo)\n*   [Top Content](https://www.linkedin.com/top-content?trk=public_post_guest_nav_menu_topContent)\n*   [People](https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people)\n*   [Learning](https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning)\n*   [Jobs](https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs)\n*   [Games](https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games)\n\n[Sign in](https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026fromSignIn=true\u0026trk=public_post_nav-header-signin)[Join for free](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_nav-header-join)\n\nTom Aarsen‚Äôs Post\n=================\n\n[](https://nl.linkedin.com/in/tomaarsen?trk=public_post_feed-actor-image)\n\n[Tom Aarsen](https://nl.linkedin.com/in/tomaarsen?trk=public_post_feed-actor-name)\n\n 11h \n\n*   [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=POST\u0026_f=guest-reporting)\n\nYou can perform 200ms search over 40 million texts using just a CPU server, 8GB of RAM, and 40GB of disk space. The trick: Binary search with int8 rescoring. Details: This is the inference strategy: 1. Embed your query using a dense embedding model into a 'standard' fp32 embedding 2. Quantize the fp32 embedding to binary: 32x smaller 3. Use an approximate (or exact) binary index to retrieve e.g. 40 documents (~20x faster than a fp32 index) 4. Load int8 embeddings for the 40 top binary documents from disk. 5. Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings 6. Sort the 40 documents based on the new scores, grab the top 10 7. Load the titles/texts of the top 10 documents This requires: - Embedding all of your documents once, and using those embeddings for: - A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate - A int8 \"view\", i.e. a way to load the int8 embeddings from disk efficiently given a document ID Instead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index. By loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore ~99% of the performance of the fp32 search, compared to ~97% when using purely the binary index: [https://lnkd.in/edc9yM6W](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fedc9yM6W\u0026urlhash=TaIv\u0026trk=public_post-text) I've created a demo that allows you to test this technique on 40 million texts from Wikipedia! Give it a try here: [https://lnkd.in/ebNSHA4y](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FebNSHA4y\u0026urlhash=NZ8N\u0026trk=public_post-text) It would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'. In short: your retrieval doesn't need to be so expensive!\n\n[Quantized Retrieval - a Hugging Face Space by sentence-transformers huggingface.co](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fhuggingface%2Eco%2Fspaces%2Fsentence-transformers%2Fquantized-retrieval\u0026urlhash=GzJ4\u0026trk=public_post_feed-article-content)\n\n[538](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_social-actions-reactions)[17 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment-cta)\n\n Share \n*   Copy\n*   LinkedIn\n*   Facebook\n*   X\n\n[](https://no.linkedin.com/in/marcobertaniokland?trk=public_post_comment_actor-image)\n\n[Marco Bertani-√òkland](https://no.linkedin.com/in/marcobertaniokland?trk=public_post_comment_actor-name) 9h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nHow does the index lifecycle management work in this case? It looks complex to do CRUD operations on the indexes. Have you experimented with this setup in production [Tom](https://nl.linkedin.com/in/tomaarsen?trk=public_post_comment-text)?\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://th.linkedin.com/in/prithivirajdamodaran?trk=public_post_comment_actor-image)\n\n[Prithivi Da](https://th.linkedin.com/in/prithivirajdamodaran?trk=public_post_comment_actor-name) 2h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\n\u003e You can perform 200ms search over 40 million texts using just a CPU server, 8GB of RAM, and 40GB of disk space At, What max query token length ? What quality ? Share MRR or NDCG What passage max len ? Without some baseline range it‚Äôs not attractive.\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply) 1 Reaction \n\n[](https://ca.linkedin.com/in/oliviermills?trk=public_post_comment_actor-image)\n\n[Olivier Mills](https://ca.linkedin.com/in/oliviermills?trk=public_post_comment_actor-name) 11h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\n[Nils Reimers](https://de.linkedin.com/in/reimersnils?trk=public_post_comment-text) popularized this 2 years ago [https://cohere.com/blog/int8-binary-embeddings](https://cohere.com/blog/int8-binary-embeddings?trk=public_post_comment-text)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[6 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 7 Reactions \n\n[](https://dk.linkedin.com/in/aleksandr-dekan?trk=public_post_comment_actor-image)\n\n[Aleksandr Dekan](https://dk.linkedin.com/in/aleksandr-dekan?trk=public_post_comment_actor-name) 5h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nCan you share the Embeddings of all 40M papers? It is quite a lot, and CPU won't be enough\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply) 1 Reaction \n\n[](https://www.linkedin.com/in/charlesmartin14?trk=public_post_comment_actor-image)\n\n[Charles H. Martin, PhD](https://www.linkedin.com/in/charlesmartin14?trk=public_post_comment_actor-name) 8h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nFast, but not still good enough for production. At scale, for every 100ms latency, you lose 1% revenue. A simple rankSVM operates at under 10ms on commodity hardware. [https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales](https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales?trk=public_post_comment-text)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[4 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 5 Reactions \n\n[](https://be.linkedin.com/in/sebastien-campion/en?trk=public_post_comment_actor-image)\n\n[S√©bastien Campion](https://be.linkedin.com/in/sebastien-campion/en?trk=public_post_comment_actor-name) 7h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nHi üëã I also implemented product quantization in JavaScript so same efficiency but, cherry on the cake, it run on the client side [https://github.com/scampion/pqjs](https://github.com/scampion/pqjs?trk=public_post_comment-text)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://hu.linkedin.com/in/adaamko?trk=public_post_comment_actor-image)\n\n[√Åd√°m Kov√°cs](https://hu.linkedin.com/in/adaamko?trk=public_post_comment_actor-name) 10h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nThe difference between having to manage a server to store embeddings and doing it in memory is massive in terms of complexity overhead. So approaches like this can really matter.\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[2 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 3 Reactions \n\n[](https://www.linkedin.com/in/christopher-hyatt-09472b189?trk=public_post_comment_actor-image)\n\n[Christopher Hyatt](https://www.linkedin.com/in/christopher-hyatt-09472b189?trk=public_post_comment_actor-name) 5h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nCongratulations\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://fr.linkedin.com/in/ravindusomawansa?trk=public_post_comment_actor-image)\n\n[Ravindu Somawansa](https://fr.linkedin.com/in/ravindusomawansa?trk=public_post_comment_actor-name) 9h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nSuper nice, thanks for the explanation üòÅ\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 2 Reactions \n\n[](https://hk.linkedin.com/in/raphaelmansuy?trk=public_post_comment_actor-image)\n\n[Rapha√´l MANSUY](https://hk.linkedin.com/in/raphaelmansuy?trk=public_post_comment_actor-name) 10h \n\n*   [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect\u0026guestReportContentType=COMMENT\u0026_f=guest-reporting)\n\nBrillant !!!!!!!!!!!\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_like)[Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reply)[2 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_comment_reactions) 3 Reactions \n\n[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_see-more-comments)\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_feed-cta-banner-cta)\n\n18,952 followers\n\n*   [377 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Ftomaarsen%2Frecent-activity%2F\u0026trk=public_post_follow-posts)\n\n[View Profile](https://nl.linkedin.com/in/tomaarsen?trk=public_post_follow-view-profile)[Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7414325916635381760\u0026trk=public_post_follow)\n\nExplore content categories\n--------------------------\n\n*   [Career](https://www.linkedin.com/top-content/career/)\n*   [Productivity](https://www.linkedin.com/top-content/productivity/)\n*   [Finance](https://www.linkedin.com/top-content/finance/)\n*   [Soft Skills \u0026 Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n*   [Project Management](https://www.linkedin.com/top-content/project-management/)\n*   [Education](https://www.linkedin.com/top-content/education/)\n*   [Technology](https://www.linkedin.com/top-content/technology/)\n*   [Leadership](https://www.linkedin.com/top-content/leadership/)\n*   [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n*   [User Experience](https://www.linkedin.com/top-content/user-experience/)\n\n Show more  Show less \n\n*   LinkedIn¬© 2026\n*   [About](https://about.linkedin.com/?trk=d_public_post_footer-about)\n*   [Accessibility](https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility)\n*   [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement)\n*   [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy)\n*   [Your California Privacy Choices](https://www.linkedin.com/legal/california-privacy-disclosure?trk=d_public_post_footer-california-privacy-rights-act)\n*   [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy)\n*   [Copyright Policy](https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy)\n*   [Brand Policy](https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy)\n*   [Guest Controls](https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls)\n*   [Community Guidelines](https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide)\n*   \n    *    ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic) \n    *    ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla) \n    *    ƒåe≈°tina (Czech) \n    *    Dansk (Danish) \n    *    Deutsch (German) \n    *    ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek) \n    *   **English (English)**\n    *    Espa√±ol (Spanish) \n    *    ŸÅÿßÿ±ÿ≥€å (Persian) \n    *    Suomi (Finnish) \n    *    Fran√ßais (French) \n    *    ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi) \n    *    Magyar (Hungarian) \n    *    Bahasa Indonesia (Indonesian) \n    *    Italiano (Italian) \n    *    ◊¢◊ë◊®◊ô◊™ (Hebrew) \n    *    Êó•Êú¨Ë™û (Japanese) \n    *    ÌïúÍµ≠Ïñ¥ (Korean) \n    *    ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi) \n    *    Bahasa Malaysia (Malay) \n    *    Nederlands (Dutch) \n    *    Norsk (Norwegian) \n    *    ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi) \n    *    Polski (Polish) \n    *    Portugu√™s (Portuguese) \n    *    Rom√¢nƒÉ (Romanian) \n    *    –†—É—Å—Å–∫–∏–π (Russian) \n    *    Svenska (Swedish) \n    *    ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu) \n    *    ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai) \n    *    Tagalog (Tagalog) \n    *    T√ºrk√ße (Turkish) \n    *    –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian) \n    *    Ti·∫øng Vi·ªát (Vietnamese) \n    *    ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified)) \n    *    Ê≠£È´î‰∏≠Êñá (Chinese (Traditional)) \n\n Language \n\nSign in to view more content\n----------------------------\n\nCreate your free account or sign in to continue your search\n\n Sign in \n\nWelcome back\n------------\n\n Email or phone  \n\n Password  \n\nShow\n\n[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password) Sign in \n\nor\n\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy).\n\nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)\n\nor\n\nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Ftomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a\u0026trk=public_post_contextual-sign-in-modal_join-link)\n\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n[](https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a)",
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/blog/embedding-quantization",
          "was_fetched": true,
          "page": "Title: Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval\n\nURL Source: https://huggingface.co/blog/embedding-quantization\n\nMarkdown Content:\n[Back to Articles](https://huggingface.co/blog)\n\n[](https://huggingface.co/aamirshakir)\n\n[](https://huggingface.co/tomaarsen)\n\n[](https://huggingface.co/SeanLee97)\n\nWe introduce the concept of embedding quantization and showcase their impact on retrieval speed, memory usage, disk space, and cost. We'll discuss how embeddings can be quantized in theory and in practice, after which we introduce a [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showing a real-life retrieval scenario of 41 million Wikipedia texts.\n\n[](https://huggingface.co/blog/embedding-quantization#table-of-contents) Table of Contents\n------------------------------------------------------------------------------------------\n\n*   [Why Embeddings?](https://huggingface.co/blog/embedding-quantization#why-embeddings)\n    *   [Embeddings may struggle to scale](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale)\n\n*   [Improving scalability](https://huggingface.co/blog/embedding-quantization#improving-scalability)\n    *   [Binary Quantization](https://huggingface.co/blog/embedding-quantization#binary-quantization)\n        *   [Binary Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers)\n        *   [Binary Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases)\n\n    *   [Scalar (int8) Quantization](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization)\n        *   [Scalar Quantization in Sentence Transformers](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers)\n        *   [Scalar Quantization in Vector Databases](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases)\n\n    *   [Combining Binary and Scalar Quantization](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization)\n    *   [Quantization Experiments](https://huggingface.co/blog/embedding-quantization#quantization-experiments)\n    *   [Influence of Rescoring](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring)\n        *   [Binary Rescoring](https://huggingface.co/blog/embedding-quantization#binary-rescoring)\n        *   [Scalar (Int8) Rescoring](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring)\n        *   [Retrieval Speed](https://huggingface.co/blog/embedding-quantization#retrieval-speed)\n\n    *   [Performance Summarization](https://huggingface.co/blog/embedding-quantization#performance-summarization)\n    *   [Demo](https://huggingface.co/blog/embedding-quantization#demo)\n    *   [Try it yourself](https://huggingface.co/blog/embedding-quantization#try-it-yourself)\n    *   [Future work:](https://huggingface.co/blog/embedding-quantization#future-work)\n    *   [Acknowledgments](https://huggingface.co/blog/embedding-quantization#acknowledgments)\n    *   [Citation](https://huggingface.co/blog/embedding-quantization#citation)\n    *   [References](https://huggingface.co/blog/embedding-quantization#references)\n\n[](https://huggingface.co/blog/embedding-quantization#why-embeddings) Why Embeddings?\n-------------------------------------------------------------------------------------\n\nEmbeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects, like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors.\n\nAfter transforming the complex objects, you can determine their similarity by calculating the similarity of the respective embeddings! This is crucial for many use cases: it serves as the backbone for recommendation systems, retrieval, one-shot or few-shot learning, outlier detection, similarity search, paraphrase detection, clustering, classification, and much more.\n\n### [](https://huggingface.co/blog/embedding-quantization#embeddings-may-struggle-to-scale) Embeddings may struggle to scale\n\nHowever, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in `float32`, i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory!\n\nThe table below gives an overview of different models, dimension size, memory requirement, and costs. Costs are computed at an estimated $3.8 per GB/mo with `x2gd` instances on AWS.\n\n| Embedding Dimension | Example Models | 100M Embeddings | 250M Embeddings | 1B Embeddings |\n| --- | --- | --- | --- | --- |\n| 384 | [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) | 143.05GB $543 / mo | 357.62GB $1,358 / mo | 1430.51GB $5,435 / mo |\n| 768 | [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) [jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1) | 286.10GB $1,087 / mo | 715.26GB $2,717 / mo | 2861.02GB $10,871 / mo |\n| 1024 | [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) | 381.46GB $1,449 / mo | 953.67GB $3,623 / mo | 3814.69GB $14,495 / mo |\n| 1536 | [OpenAI text-embedding-3-small](https://openai.com/blog/new-embedding-models-and-api-updates) | 572.20GB $2,174 / mo | 1430.51GB $5,435 / mo | 5722.04GB $21,743 / mo |\n| 3072 | [OpenAI text-embedding-3-large](https://openai.com/blog/new-embedding-models-and-api-updates) | 1144.40GB $4,348 / mo | 2861.02GB $10,871 / mo | 11444.09GB $43,487 / mo |\n\n[](https://huggingface.co/blog/embedding-quantization#improving-scalability) Improving scalability\n--------------------------------------------------------------------------------------------------\n\nThere are several ways to approach the challenges of scaling embeddings. The most common approach is dimensionality reduction, such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis). However, classic dimensionality reduction -- like PCA methods -- [tends to perform poorly when used with embeddings](https://arxiv.org/abs/2205.11498).\n\nIn recent news, [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) ([blogpost](https://huggingface.co/blog/matryoshka)) (MRL) as used by [OpenAI](https://openai.com/blog/new-embedding-models-and-api-updates) also allows for cheaper embeddings. With MRL, only the first `n` embedding dimensions are used. This approach has already been adopted by some open models like [nomic-ai/nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) and [mixedbread-ai/mxbai-embed-2d-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1) (For OpenAIs `text-embedding-3-large`, we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.).\n\nHowever, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: **Quantization**. Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs. Let's dive into it!\n\n### [](https://huggingface.co/blog/embedding-quantization#binary-quantization) Binary Quantization\n\nUnlike quantization in models where you reduce the precision of weights, quantization for embeddings refers to a post-processing step for the embeddings themselves. In particular, binary quantization refers to the conversion of the `float32` values in an embedding to 1-bit values, resulting in a 32x reduction in memory and storage usage.\n\nTo quantize `float32` embeddings to binary, we simply threshold normalized embeddings at 0:\n\nf(x)={0 if x‚â§0 1 if x\u003e0 f(x)= \\begin{cases} 0 \u0026 \\text{if } x\\leq 0\\\\ 1 \u0026 \\text{if } x \\gt 0 \\end{cases}\n\nWe can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.\n\n[Yamada et al. (2021)](https://arxiv.org/abs/2106.00882) introduced a rescore step, which they called _rerank_, to boost the performance. They proposed that the `float32` query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve `rescore_multiplier * top_k` results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the `float32` query embedding.\n\nBy applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-sentence-transformers) Binary Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to binary would result in 1024 bits. In practice, it is much more common to store bits as bytes instead, so when we quantize to binary embeddings, we pack the bits into bytes using `np.packbits`.\n\nTherefore, quantizing a `float32` embedding with a dimensionality of 1024 yields an `int8` or `uint8` embedding with a dimensionality of 128. See two approaches of how you can produce quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2a. Encode some text using \"binary\" quantization\nbinary_embeddings = model.encode(\n    [\"I am driving to the lake.\", \"It is a beautiful day.\"],\n    precision=\"binary\",\n)\n```\n\nor\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2b. or, encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nbinary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere, you can see the differences between default `float32` embeddings and binary embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e binary_embeddings.shape\n(2, 128)\n\u003e\u003e\u003e binary_embeddings.nbytes\n256\n\u003e\u003e\u003e binary_embeddings.dtype\nint8\n```\n\nNote that you can also choose `\"ubinary\"` to quantize to binary using the unsigned `uint8` data format. This may be a requirement depending on your vector library/database.\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-quantization-in-vector-databases) Binary Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | [Yes](https://github.com/facebookresearch/faiss/wiki/Binary-indexes) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/schema-reference.html) |\n| Milvus | [Yes](https://milvus.io/docs/index.md) |\n| Qdrant | Through [Binary Quantization](https://qdrant.tech/documentation/guides/quantization/#binary-quantization) |\n| Weaviate | Through [Binary Quantization](https://weaviate.io/developers/weaviate/configuration/bq-compression) |\n\n### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-quantization) Scalar (int8) Quantization\n\nWe use a scalar quantization process to convert the `float32` embeddings into `int8`. This involves mapping the continuous range of `float32` values to the discrete set of `int8` values, which can represent 256 distinct levels (from -128 to 127), as shown in the image below. This is done by using a large calibration dataset of embeddings. We compute the range of these embeddings, i.e., the `min` and `max` of each embedding dimension. From there, we calculate the steps (buckets) to categorize each value.\n\nTo further boost the retrieval performance, you can optionally apply the same rescoring step as for the binary embeddings. It is important to note that the calibration dataset greatly influences performance since it defines the quantization buckets.\n\n_Source: [https://qdrant.tech/articles/scalar-quantization/](https://qdrant.tech/articles/scalar-quantization/)_\n\nWith scalar quantization to `int8`, we reduce the original `float32` embeddings' precision so that each value is represented with an 8-bit integer (4x smaller). Note that this differs from the binary quantization case, where each value is represented by a single bit (32x smaller).\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-sentence-transformers) Scalar Quantization in Sentence Transformers\n\nQuantizing an embedding with a dimensionality of 1024 to `int8` results in 1024 bytes. In practice, we can choose either `uint8` or `int8`. This choice is usually made depending on what your vector library/database supports.\n\nIn practice, it is recommended to provide the scalar quantization with either:\n\n1.   a large set of embeddings to quantize all at once, or\n2.   `min` and `max` ranges for each of the embedding dimensions, or\n3.   a large calibration dataset of embeddings from which the `min` and `max` ranges can be computed.\n\nIf none of these are the case, you will be given a warning like this: `Computing int8 quantization buckets based on 2 embeddings. int8 quantization is more stable with 'ranges' calculated from more embeddings or a 'calibration_embeddings' that can be used to calculate the buckets.`\n\nSee how you can produce scalar quantized embeddings using [Sentence Transformers](https://sbert.net/) below:\n\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\nfrom datasets import load_dataset\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2. Prepare an example calibration dataset\ncorpus = load_dataset(\"nq_open\", split=\"train[:1000]\")[\"question\"]\ncalibration_embeddings = model.encode(corpus)\n\n# 3. Encode some text without quantization \u0026 apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nint8_embeddings = quantize_embeddings(\n    embeddings,\n    precision=\"int8\",\n    calibration_embeddings=calibration_embeddings,\n)\n```\n\n**References:**\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n\nHere you can see the differences between default `float32` embeddings and `int8` scalar embeddings in terms of shape, size, and `numpy` dtype:\n\n```\n\u003e\u003e\u003e embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e embeddings.nbytes\n8192\n\u003e\u003e\u003e embeddings.dtype\nfloat32\n\u003e\u003e\u003e int8_embeddings.shape\n(2, 1024)\n\u003e\u003e\u003e int8_embeddings.nbytes\n2048\n\u003e\u003e\u003e int8_embeddings.dtype\nint8\n```\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-quantization-in-vector-databases) Scalar Quantization in Vector Databases\n\n| Vector Databases | Support |\n| --- | --- |\n| Faiss | Indirectly through [IndexHNSWSQ](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexHNSWSQ.html) |\n| USearch | [Yes](https://github.com/unum-cloud/usearch) |\n| Vespa AI | [Yes](https://docs.vespa.ai/en/reference/tensor.html) |\n| OpenSearch | [Yes](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector) |\n| ElasticSearch | [Yes](https://www.elastic.co/de/blog/save-space-with-byte-sized-vectors) |\n| Milvus | Indirectly through [IVF_SQ8](https://milvus.io/docs/index.md) |\n| Qdrant | Indirectly through [Scalar Quantization](https://qdrant.tech/documentation/guides/quantization/#scalar-quantization) |\n\n### [](https://huggingface.co/blog/embedding-quantization#combining-binary-and-scalar-quantization) Combining Binary and Scalar Quantization\n\nCombining binary and scalar quantization is possible to get the best of both worlds: the extreme speed from binary embeddings and the great performance preservation of scalar embeddings with rescoring. See the [demo](https://huggingface.co/blog/embedding-quantization#demo) below for a real-life implementation of this approach involving 41 million texts from Wikipedia. The pipeline for that setup is as follows:\n\n1.   The query is embedded using the [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) SentenceTransformer model.\n2.   The query is quantized to binary using the [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings) function from the `sentence-transformers` library.\n3.   A binary index (41M binary embeddings; 5.2GB of memory/disk space) is searched using the quantized query for the top 40 documents.\n4.   The top 40 documents are loaded on the fly from an int8 index on disk (41M int8 embeddings; 0 bytes of memory, 47.5GB of disk space).\n5.   The top 40 documents are rescored using the float32 query and the int8 embeddings to get the top 10 documents.\n6.   The top 10 documents are sorted by score and displayed.\n\nThrough this approach, we use 5.2GB of memory and 52GB of disk space for the indices. This is considerably less than normal retrieval, requiring 200GB of memory and 200GB of disk space. Especially as you scale up even further, this will result in notable reductions in latency and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#quantization-experiments) Quantization Experiments\n\nWe conducted our experiments on the retrieval subset of the [MTEB](https://huggingface.co/spaces/mteb/leaderboard) containing 15 benchmarks. First, we retrieved the top k (k=100) search results with a `rescore_multiplier` of 4. Therefore, we retrieved 400 results in total and performed the rescoring on these top 400. For the `int8` performance, we directly used the dot-product without any rescoring.\n\n| Model | Embedding Dimension | 250M Embeddings | MTEB Retrieval (NDCG@10) | Percentage of default performance |\n| --- | --- | --- | --- | --- |\n| **Open Models** |  |  |  |  |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): float32 | 1024 | 953.67GB $3623 / mo | 54.39 | 100% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): int8 | 1024 | 238.41GB $905 / mo | 52.79 | 97% |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1): binary | 1024 | 29.80GB $113.25 / mo | 52.46 | 96.45% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): float32 | 768 | 286.10GB $1087 / mo | 50.77 | 100% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): int8 | 768 | 178.81GB $679 / mo | 47.54 | 94.68% |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2): binary | 768 | 22.35GB $85 / mo | 37.96 | 74.77% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): float32 | 768 | 286.10GB $1087 / mo | 53.01 | 100% |\n| [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5): binary | 768 | 22.35GB $85 / mo | 46.49 | 87.7% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): float32 | 384 | 357.62GB $1358 / mo | 41.66 | 100% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): int8 | 384 | 89.40GB $339 / mo | 37.82 | 90.79% |\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2): binary | 384 | 11.18GB $42 / mo | 39.07 | 93.79% |\n| **Proprietary Models** |  |  |  |  |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): float32 | 1024 | 953.67GB $3623 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): int8 | 1024 | 238.41GB $905 / mo | 55.0 | 100% |\n| [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/): binary | 1024 | 29.80GB $113.25 / mo | 52.3 | 94.6% |\n\nSeveral key trends and benefits can be identified from the results of our quantization experiments. As expected, embedding models with higher dimension size typically generate higher storage costs per computation but achieve the best performance. Surprisingly, however, quantization to `int8` already helps `mxbai-embed-large-v1` and `Cohere-embed-english-v3.0` achieve higher performance with lower storage usage than that of the smaller dimension size base models.\n\nThe benefits of quantization are, if anything, even more clearly visible when looking at the results obtained with binary models. In that scenario, the 1024 dimension models still outperform a now 10x more storage intensive base model, and the `mxbai-embed-large-v1` even manages to hold more than 96% of performance after a 32x reduction in resource requirements. The further quantization from `int8` to binary barely results in any additional loss of performance for this model.\n\nInterestingly, we can also see that `all-MiniLM-L6-v2` exhibits stronger performance on binary than on `int8` quantization. A possible explanation for this could be the selection of calibration data. On `e5-base-v2`, we observe the effect of [dimension collapse](https://arxiv.org/abs/2110.09348), which causes the model to only use a subspace of the latent space; when performing the quantization, the whole space collapses further, leading to high performance losses.\n\nThis shows that quantization doesn't universally work with all embedding models. It remains crucial to consider exisiting benchmark outcomes and conduct experiments to determine a given model's compatibility with quantization.\n\n### [](https://huggingface.co/blog/embedding-quantization#influence-of-rescoring) Influence of Rescoring\n\nIn this section we look at the influence of rescoring on retrieval performance. We evaluate the results based on [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1).\n\n#### [](https://huggingface.co/blog/embedding-quantization#binary-rescoring) Binary Rescoring\n\nWith binary embeddings, [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) retains 92.53% of performance on MTEB Retrieval. Just doing the rescoring without retrieving more samples pushes the performance to 96.45%. We experimented with setting the`rescore_multiplier` from 1 to 10, but observe no further boost in performance. This indicates that the `top_k` search already retrieved the top candidates and the rescoring reordered these good candidates appropriately.\n\n#### [](https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring) Scalar (Int8) Rescoring\n\nWe also evaluated the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) model with `int8` rescoring, as Cohere showed that [Cohere-embed-english-v3.0](https://txt.cohere.com/introducing-embed-v3/) reached up to 100% of the performance of the `float32` model with `int8` quantization. For this experiment, we set the `rescore_multiplier` to [1, 4, 10] and got the following results:\n\nAs we can see from the diagram, a higher rescore multiplier implies better retention of performance after quantization. Extrapolating from our results, we assume the relation is likely hyperbolical with performance approaching 100% as the rescore multiplier continues to rise. A rescore multiplier of 4-5 already leads to a remarkable performance retention of 99% using `int8`.\n\n#### [](https://huggingface.co/blog/embedding-quantization#retrieval-speed) Retrieval Speed\n\nWe measured retrieval speed on a Google Cloud Platform `a2-highgpu-4g` instance using the [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) embeddings with 1024 dimension on the whole MTEB Retrieval. For int8 we used [USearch](https://github.com/unum-cloud/usearch) (Version 2.9.2) and binary quantization [Faiss](https://github.com/facebookresearch/faiss) (Version 1.8.0). Everything was computed on CPU using exact search.\n\n| Quantization | Min | Mean | Max |\n| --- | --- | --- | --- |\n| `float32` | 1x (baseline) | **1x** (baseline) | 1x (baseline) |\n| `int8` | 2.99x speedup | **3.66x** speedup | 4.8x speedup |\n| `binary` | 15.05x speedup | **24.76x** speedup | 45.8x speedup |\n\nAs shown in the table, applying `int8` scalar quantization results in an average speedup of 3.66x compared to full-size `float32` embeddings. Additionally, binary quantization achieves a speedup of 24.76x on average. For both scalar and binary quantization, even the worst case scenario resulted in very notable speedups.\n\n### [](https://huggingface.co/blog/embedding-quantization#performance-summarization) Performance Summarization\n\nThe experimental results, effects on resource use, retrieval speed, and retrieval performance by using quantization can be summarized as follows:\n\n|  | float32 | int8/uint8 | binary/ubinary |\n| --- | --- | --- | --- |\n| **Memory \u0026 Index size savings** | 1x | exactly 4x | exactly 32x |\n| **Retrieval Speed** | 1x | up to 4x | up to 45x |\n| **Percentage of default performance** | 100% | ~99.3% | ~96% |\n\n### [](https://huggingface.co/blog/embedding-quantization#demo) Demo\n\nThe following [demo](https://huggingface.co/spaces/sentence-transformers/quantized-retrieval) showcases the retrieval efficiency using exact or approximate search by combining binary search with scalar (`int8`) rescoring. The solution requires 5GB of memory for the binary index and 50GB of disk space for the binary and scalar indices, considerably less than the 200GB of memory and disk space which would be required for regular `float32` retrieval. Additionally, retrieval is much faster.\n\n### [](https://huggingface.co/blog/embedding-quantization#try-it-yourself) Try it yourself\n\nThe following scripts can be used to experiment with embedding quantization for retrieval \u0026 beyond. There are three categories:\n\n*   **Recommended Retrieval**:\n    *   [semantic_search_recommended.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_recommended.py): This script combines binary search with scalar rescoring, much like the above demo, for cheap, efficient, and performant retrieval.\n\n*   **Usage**:\n    *   [semantic_search_faiss.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using FAISS, by using the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function.\n    *   [semantic_search_usearch.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch.py): This script showcases regular usage of binary or scalar quantization, retrieval, and rescoring using USearch, by using the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function.\n\n*   **Benchmarks**:\n    *   [semantic_search_faiss_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_faiss_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using FAISS. It uses the [`semantic_search_faiss`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_faiss) utility function. Our benchmarks especially show show speedups for `ubinary`.\n    *   [semantic_search_usearch_benchmark.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/semantic_search_usearch_benchmark.py): This script includes a retrieval speed benchmark of `float32` retrieval, binary retrieval + rescoring, and scalar retrieval + rescoring, using USearch. It uses the [`semantic_search_usearch`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.semantic_search_usearch) utility function. Our experiments show large speedups on newer hardware, particularly for `int8`.\n\n### [](https://huggingface.co/blog/embedding-quantization#future-work) Future work\n\nWe are looking forward to further advancements of binary quantization. To name a few potential improvements, we suspect that there may be room for scalar quantization smaller than `int8`, i.e. with 128 or 64 buckets instead of 256.\n\nAdditionally, we are excited that embedding quantization is fully perpendicular to Matryoshka Representation Learning (MRL). In other words, it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually corresponds with a 2% reduction in performance) and then apply binary or scalar quantization. We suspect this could speed up retrieval up to 32x for a ~3% reduction in quality, or up to 256x for a ~10% reduction in quality.\n\nLastly, we recognize that retrieval using embedding quantization can also be combined with a separate reranker model. We imagine that a 3-step pipeline of binary search, scalar (int8) rescoring, and cross-encoder reranking allows for state-of-the-art retrieval performance at low latencies, memory usage, disk space, and costs.\n\n### [](https://huggingface.co/blog/embedding-quantization#acknowledgments) Acknowledgments\n\nThis project is possible thanks to our collaboration with [mixedbread.ai](https://mixedbread.ai/) and the [SentenceTransformers](https://www.sbert.net/) library, which allows you to easily create sentence embeddings and quantize them. If you want to use quantized embeddings in your project, now you know how!\n\n### [](https://huggingface.co/blog/embedding-quantization#citation) Citation\n\n```\n@article{shakir2024quantization,\n  author       = { Aamir Shakir and\n                   Tom Aarsen and\n                   Sean Lee\n                 },\n  title = { Binary and Scalar Embedding Quantization for Significantly Faster \u0026 Cheaper Retrieval },\n  journal = {Hugging Face Blog},\n  year = {2024},\n  note = {https://huggingface.co/blog/embedding-quantization},\n}\n```\n\n### [](https://huggingface.co/blog/embedding-quantization#resources) Resources\n\n*   [`mixedbread-ai/mxbai-embed-large-v1`](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n*   [`SentenceTransformer.encode`](https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)\n*   [`quantize_embeddings`](https://sbert.net/docs/package_reference/quantization.html#sentence_transformers.quantization.quantize_embeddings)\n*   [Sentence Transformers docs - Embedding Quantization](https://sbert.net/examples/applications/embedding-quantization/README.html)\n*   [https://txt.cohere.com/int8-binary-embeddings/](https://txt.cohere.com/int8-binary-embeddings/)\n*   [https://qdrant.tech/documentation/guides/quantization](https://qdrant.tech/documentation/guides/quantization)\n*   [https://zilliz.com/learn/scalar-quantization-and-product-quantization](https://zilliz.com/learn/scalar-quantization-and-product-quantization)",
          "was_summarised": false
        },
        {
          "url": "https://cohere.com/blog/int8-binary-embeddings",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- **Fast, cheap retrieval with quantized embeddings** ‚Äì A new inference pipeline shows 200‚ÄØms search over 41‚ÄØmillion Wikipedia passages on a single CPU server (8‚ÄØGB RAM, 40‚ÄØGB disk) by combining binary‚Äëindex lookup with int8 rescoring.  \n- **Memory \u0026 cost savings** ‚Äì Binary indices are 32√ó smaller (5.2‚ÄØGB vs 200‚ÄØGB) and int8 vectors 4√ó smaller (52‚ÄØGB vs 200‚ÄØGB), cutting storage costs to \u003c$120‚ÄØ/‚ÄØmo versus ~$1.4‚ÄØk‚ÄØ/‚ÄØmo for a 100‚ÄëM‚Äëvector float‚Äë32 index.  \n- **Speed gains** ‚Äì Benchmarks on a Google Cloud `a2‚Äëhighgpu‚Äë4g` show average query latency 24√ó faster with binary search and 3.7√ó faster with int8, while retaining 96‚ÄØ% (binary) or 99‚ÄØ% (int8) of float‚Äë32 retrieval performance on MTEB NDCG@10.  \n- **Open‚Äësource tooling** ‚Äì The demo is a Hugging‚ÄØFace Space using Sentence‚ÄØTransformers (`mixedbread‚Äëai/mxbai‚Äëembed‚Äëlarge‚Äëv1`), Faiss for binary indices, USearch for int8, and the `quantize_embeddings` API. The scripts (`semantic_search_recommended.py`, `semantic_search_faiss.py`, etc.) are all on GitHub.  \n- **Community interest** ‚Äì Users applaud the approach, noting it could cut infrastructure costs for large‚Äëscale semantic search. Some raise concerns about clustering and the extra steps needed for production pipelines, while others suggest adding sparse (BM25) or cross‚Äëencoder rerankers.\n\n**Why it matters:**  \nQuantizing embeddings to binary/int8 lets enterprises run billion‚Äëscale semantic search on modest hardware, dramatically reducing latency, memory, disk, and cloud spend‚Äîkey for cost‚Äëconstrained AI deployments.\n\n**Community response:**  \nPositive feedback highlights the practicality and speed of the demo, with some discussion on handling clustered data and simplifying the pipeline for production use.\n\n**Key entities:**  \nHugging‚ÄØFace, mixedbread.ai, Sentence‚ÄØTransformers, Faiss, USearch, Qdrant, Cohere, OpenAI.",
        "html": "\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFast, cheap retrieval with quantized embeddings\u003c/strong\u003e ‚Äì A new inference pipeline shows 200‚ÄØms search over 41‚ÄØmillion Wikipedia passages on a single CPU server (8‚ÄØGB RAM, 40‚ÄØGB disk) by combining binary‚Äëindex lookup with int8 rescoring.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory \u0026amp; cost savings\u003c/strong\u003e ‚Äì Binary indices are 32√ó smaller (5.2‚ÄØGB vs 200‚ÄØGB) and int8 vectors 4√ó smaller (52‚ÄØGB vs 200‚ÄØGB), cutting storage costs to \u0026lt;$120‚ÄØ/‚ÄØmo versus ~$1.4‚ÄØk‚ÄØ/‚ÄØmo for a 100‚ÄëM‚Äëvector float‚Äë32 index.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSpeed gains\u003c/strong\u003e ‚Äì Benchmarks on a Google Cloud \u003ccode\u003ea2‚Äëhighgpu‚Äë4g\u003c/code\u003e show average query latency 24√ó faster with binary search and 3.7√ó faster with int8, while retaining 96‚ÄØ% (binary) or 99‚ÄØ% (int8) of float‚Äë32 retrieval performance on MTEB NDCG@10.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpen‚Äësource tooling\u003c/strong\u003e ‚Äì The demo is a Hugging‚ÄØFace Space using Sentence‚ÄØTransformers (\u003ccode\u003emixedbread‚Äëai/mxbai‚Äëembed‚Äëlarge‚Äëv1\u003c/code\u003e), Faiss for binary indices, USearch for int8, and the \u003ccode\u003equantize_embeddings\u003c/code\u003e API. The scripts (\u003ccode\u003esemantic_search_recommended.py\u003c/code\u003e, \u003ccode\u003esemantic_search_faiss.py\u003c/code\u003e, etc.) are all on GitHub.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommunity interest\u003c/strong\u003e ‚Äì Users applaud the approach, noting it could cut infrastructure costs for large‚Äëscale semantic search. Some raise concerns about clustering and the extra steps needed for production pipelines, while others suggest adding sparse (BM25) or cross‚Äëencoder rerankers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u003cbr\u003e\nQuantizing embeddings to binary/int8 lets enterprises run billion‚Äëscale semantic search on modest hardware, dramatically reducing latency, memory, disk, and cloud spend‚Äîkey for cost‚Äëconstrained AI deployments.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e\u003cbr\u003e\nPositive feedback highlights the practicality and speed of the demo, with some discussion on handling clustered data and simplifying the pipeline for production use.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e\u003cbr\u003e\nHugging‚ÄØFace, mixedbread.ai, Sentence‚ÄØTransformers, Faiss, USearch, Qdrant, Cohere, OpenAI.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352100447Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.97,
        "reason": "Detailed quantization method reduces memory by 32x and speed by 25x while retaining ~96% performance on 40M Wikipedia texts.",
        "processed_at": "2026-01-07T03:37:15.494644785Z"
      },
      "processed_at": "2026-01-07T02:55:50.555983591Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/spaces/sentence-transformers/quantized-retrieval: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:54:51.362675858Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://cohere.com/blog/int8-binary-embeddings: jina: retry failed: Post \"https://r.jina.ai/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)",
          "occurred_at": "2026-01-07T02:55:50.555980083Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q5qsvd",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/",
      "title": "The FinePDFs üìÑ Book",
      "content": "Hey friends, Hynek from HuggingFace here.  \n  \nWe have released FinePDFs dataset of 3T tokens last year and we felt obliged to share the knowledge with there rest of OSS community.  \n  \nThe HuggingFace Press, has been pulling an extra hours through the Christmas, to put everything we know about PDFs inside:  \n\\- How to make the SoTA PDFs dataset?   \n\\- How much old internet is dead now?  \n\\- Why we chose RolmOCR for OCR  \n\\- What's the most Claude like OSS model?  \n\\- Why is the horse racing site topping the FinePDFs URL list?  \n  \nWe hope you like it :)\n\nhttps://preview.redd.it/z49knj5fwrbg1.png?width=1373\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8\n\n",
      "author": "Other_Housing8453",
      "created_at": "2026-01-06T18:34:44Z",
      "comments": [
        {
          "id": "ny39ndr",
          "author": "FullOf_Bad_Ideas",
          "content": "Thanks. FineWeb2 and FinePDFs are awesome datasets and they helped me a lot when I was messing with pre-training my own LLM. Pretty much the best off-the-shelf options for Polish.",
          "created_at": "2026-01-06T22:19:05Z",
          "was_summarised": false
        },
        {
          "id": "ny1wwmn",
          "author": "Other_Housing8453",
          "content": "Link here: [https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog](https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog)",
          "created_at": "2026-01-06T18:35:33Z",
          "urls": [
            {
              "url": "https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny3cpwq",
          "author": "Xamanthas",
          "content": "Thanks for this, some of the only not garbage-tier dataset releases left come from HF directly. Its gotten to the point where I made a userscript to block specific users/hide datasets in search results because in a given topic theres like what maybe 6% of results that are actually usable",
          "created_at": "2026-01-06T22:33:45Z",
          "was_summarised": false
        },
        {
          "id": "ny3uvpr",
          "author": "DHasselhoff77",
          "content": "This was a great read. Very clearly presented. Thanks! P.S. The dataset looks fine too.",
          "created_at": "2026-01-07T00:06:09Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://preview.redd.it/z49knj5fwrbg1.png?width=1373\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- HuggingFace released the **FinePDFs** dataset ‚Äì a 3‚ÄØtrillion‚Äëtoken corpus of PDFs, now publicly available to the open‚Äësource community.  \n- The release includes a detailed post‚Äëmortem (‚ÄúFinePDFs Blog‚Äù) covering how the dataset was built, the choice of **RolmOCR** for OCR, analysis of dead internet content, and a comparison to the most Claude‚Äëlike open‚Äësource model.  \n- The dataset is positioned as a top‚Äëtier alternative to existing PDF corpora such as **FineWeb2** and is already being used for pre‚Äëtraining LLMs in languages like Polish.  \n- HuggingFace provided a dedicated HuggingFace Space for the blog and dataset metadata, making it easy to explore and download.  \n- Community comments highlight the dataset‚Äôs usefulness, praise the clear presentation, and note its high quality compared to other ‚Äúgarbage‚Äëtier‚Äù releases.\n\n**Why it matters:**  \nThe 3‚ÄØT‚Äëtoken FinePDFs corpus expands high‚Äëquality training data for LLMs, particularly in PDF‚Äëheavy domains, and showcases HuggingFace‚Äôs ongoing commitment to open‚Äësource research infrastructure.\n\n**Community response:**  \nUsers applaud the dataset‚Äôs quality and the clarity of the accompanying blog; some even created tools to filter out low‚Äëquality results, underscoring the dataset‚Äôs value.\n\n**Key entities:**  \nHuggingFace, FinePDFs, FineWeb2, RolmOCR, Claude, Open‚Äësource LLMs",
        "html": "\u003cul\u003e\n\u003cli\u003eHuggingFace released the \u003cstrong\u003eFinePDFs\u003c/strong\u003e dataset ‚Äì a 3‚ÄØtrillion‚Äëtoken corpus of PDFs, now publicly available to the open‚Äësource community.\u003c/li\u003e\n\u003cli\u003eThe release includes a detailed post‚Äëmortem (‚ÄúFinePDFs Blog‚Äù) covering how the dataset was built, the choice of \u003cstrong\u003eRolmOCR\u003c/strong\u003e for OCR, analysis of dead internet content, and a comparison to the most Claude‚Äëlike open‚Äësource model.\u003c/li\u003e\n\u003cli\u003eThe dataset is positioned as a top‚Äëtier alternative to existing PDF corpora such as \u003cstrong\u003eFineWeb2\u003c/strong\u003e and is already being used for pre‚Äëtraining LLMs in languages like Polish.\u003c/li\u003e\n\u003cli\u003eHuggingFace provided a dedicated HuggingFace Space for the blog and dataset metadata, making it easy to explore and download.\u003c/li\u003e\n\u003cli\u003eCommunity comments highlight the dataset‚Äôs usefulness, praise the clear presentation, and note its high quality compared to other ‚Äúgarbage‚Äëtier‚Äù releases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u003cbr\u003e\nThe 3‚ÄØT‚Äëtoken FinePDFs corpus expands high‚Äëquality training data for LLMs, particularly in PDF‚Äëheavy domains, and showcases HuggingFace‚Äôs ongoing commitment to open‚Äësource research infrastructure.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e\u003cbr\u003e\nUsers applaud the dataset‚Äôs quality and the clarity of the accompanying blog; some even created tools to filter out low‚Äëquality results, underscoring the dataset‚Äôs value.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e\u003cbr\u003e\nHuggingFace, FinePDFs, FineWeb2, RolmOCR, Claude, Open‚Äësource LLMs\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352146189Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.8,
        "reason": "FinePDFs dataset released with 3T tokens, OCR insights, and PDF dataset strategies, offering significant resources for AI and NLP community.",
        "processed_at": "2026-01-07T03:37:19.419232703Z"
      },
      "processed_at": "2026-01-07T02:55:51.580065097Z"
    },
    {
      "flow_id": "",
      "id": "1q5ta4l",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/",
      "title": "llama-benchy - llama-bench style benchmarking for ANY LLM backend",
      "content": "TL;DR: I've built this tool primarily for myself as I couldn't easily compare model performance across different backends in the way that is easy to digest and useful for me. I decided to share this in case someone has the same need.\n\n## Why I built this?\n\nAs probably many of you here, I've been happily using llama-bench to benchmark local models performance running in llama.cpp. One great feature is that it can help to evaluate performance at different context lengths and present the output in a table format that is easy to digest.\n\nHowever, llama.cpp is not the only inference engine I use, I also use SGLang and vLLM. But llama-bench can only work with llama.cpp, and other benchmarking tools that I found are more focused on concurrency and total throughput.\n\nAlso, llama-bench performs measurements using the C++ engine directly which is not representative of the end user experience which can be quite different in practice.\n\nvLLM has its own powerful benchmarking tool, but while it can be used with other inference engines, there are a few issues:\n\n- You can't easily measure how prompt processing speed degrades as context grows. You can use `vllm bench sweep serve`, but it only works well with vLLM with prefix caching disabled on the server. Even with random prompts it will reuse the same prompt between multiple runs which will hit the cache in `llama-server` for instance. So you will get very low median TTFT times and very high prompt processing speeds. \n- The TTFT measurement it uses is not actually until the first usable token, it's until the very first data chunk from the server which may not contain any generated tokens in /v1/chat/completions mode.\n- Random dataset is the only ones that allows to specify an arbitrary number of tokens, but randomly generated token sequence doesn't let you adequately measure speculative decoding/MTP.\n\nAs of today, I haven't been able to find any existing benchmarking tool that brings llama-bench style measurements at different context lengths to any OpenAI-compatible endpoint.\n## What is llama-benchy?\n\nIt's a CLI benchmarking tool that measures:\n\n- Prompt Processing (pp)¬†and¬†Token Generation (tg)¬†speeds at different context lengths.\n- Allows to benchmark context prefill and follow up prompt separately.\n- Reports additional metrics, like time to first response, estimated prompt processing time and end-to-end time to first token.\n\nIt works with any OpenAI-compatible endpoint that exposes /v1/chat/completions and also:\n\n- Supports configurable prompt length (`--pp`), generation length (`--tg`), and context depth (`--depth`).\n- Can run multiple iterations (`--runs`) and report mean ¬± std.\n- Uses HuggingFace tokenizers for accurate token counts.\n- Downloads a book from Project Gutenberg to use as source text for prompts to ensure better benchmarking of spec.decoding/MTP models.\n- Supports executing a command after each run (e.g., to clear cache).\n- Configurable latency measurement mode to estimate server/network overhead and provide more accurate prompt processing numbers.\n\n## Quick Demo\n\nBenchmarking MiniMax 2.1 AWQ running on my dual Spark cluster with up to 100000 context:\n\n```bash\n# Run without installation\nuvx llama-benchy --base-url http://spark:8888/v1 --model cyankiwi/MiniMax-M2.1-AWQ-4bit --depth 0 4096 8192 16384 32768 65535 100000 --adapt-prompt --latency-mode generation --enable-prefix-caching\n```\n\nOutput:\n\n| model                          |             test |             t/s |         ttfr (ms) |      est_ppt (ms) |     e2e_ttft (ms) |\n|:-------------------------------|-----------------:|----------------:|------------------:|------------------:|------------------:|\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |           pp2048 | 3544.10 ¬± 37.29 |     688.41 ¬± 6.09 |     577.93 ¬± 6.09 |     688.45 ¬± 6.10 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |             tg32 |    36.11 ¬± 0.06 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_pp @ d4096 |  3150.63 ¬± 7.84 |    1410.55 ¬± 3.24 |    1300.06 ¬± 3.24 |    1410.58 ¬± 3.24 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_tg @ d4096 |    34.36 ¬± 0.08 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   pp2048 @ d4096 | 2562.47 ¬± 21.71 |     909.77 ¬± 6.75 |     799.29 ¬± 6.75 |     909.81 ¬± 6.75 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |     tg32 @ d4096 |    33.41 ¬± 0.05 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_pp @ d8192 | 2832.52 ¬± 12.34 |   3002.66 ¬± 12.57 |   2892.18 ¬± 12.57 |   3002.70 ¬± 12.57 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   ctx_tg @ d8192 |    31.38 ¬± 0.06 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   pp2048 @ d8192 | 2261.83 ¬± 10.69 |    1015.96 ¬± 4.29 |     905.48 ¬± 4.29 |    1016.00 ¬± 4.29 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |     tg32 @ d8192 |    30.55 ¬± 0.08 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_pp @ d16384 |  2473.70 ¬± 2.15 |    6733.76 ¬± 5.76 |    6623.28 ¬± 5.76 |    6733.80 ¬± 5.75 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_tg @ d16384 |    27.89 ¬± 0.04 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  pp2048 @ d16384 |  1824.55 ¬± 6.32 |    1232.96 ¬± 3.89 |    1122.48 ¬± 3.89 |    1233.00 ¬± 3.89 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |    tg32 @ d16384 |    27.21 ¬± 0.04 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_pp @ d32768 |  2011.11 ¬± 2.40 |  16403.98 ¬± 19.43 |  16293.50 ¬± 19.43 |  16404.03 ¬± 19.43 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_tg @ d32768 |    22.09 ¬± 0.07 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  pp2048 @ d32768 |  1323.21 ¬± 4.62 |    1658.25 ¬± 5.41 |    1547.77 ¬± 5.41 |    1658.29 ¬± 5.41 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |    tg32 @ d32768 |    21.81 ¬± 0.07 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_pp @ d65535 |  1457.71 ¬± 0.26 |   45067.98 ¬± 7.94 |   44957.50 ¬± 7.94 |   45068.01 ¬± 7.94 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  ctx_tg @ d65535 |    15.72 ¬± 0.04 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |  pp2048 @ d65535 |   840.36 ¬± 2.35 |    2547.54 ¬± 6.79 |    2437.06 ¬± 6.79 |    2547.60 ¬± 6.80 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |    tg32 @ d65535 |    15.63 ¬± 0.02 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit | ctx_pp @ d100000 |  1130.05 ¬± 1.89 | 88602.31 ¬± 148.70 | 88491.83 ¬± 148.70 | 88602.37 ¬± 148.70 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit | ctx_tg @ d100000 |    12.14 ¬± 0.02 |                   |                   |                   |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit | pp2048 @ d100000 |   611.01 ¬± 2.50 |   3462.39 ¬± 13.73 |   3351.90 ¬± 13.73 |   3462.42 ¬± 13.73 |\n| cyankiwi/MiniMax-M2.1-AWQ-4bit |   tg32 @ d100000 |    12.05 ¬± 0.03 |                   |                   |                   |\n\nllama-benchy (0.1.0)\ndate: 2026-01-06 11:44:49 | latency mode: generation\n\n## GitHub\n\n[https://github.com/eugr/llama-benchy](https://github.com/eugr/llama-benchy)",
      "author": "Eugr",
      "created_at": "2026-01-06T20:02:33Z",
      "comments": [
        {
          "id": "ny2gnd3",
          "author": "Future_South6852",
          "content": "This is exactly what I've been looking for! The fact that you can benchmark across different backends with the same methodology is huge\n\n  \nBeen running SGLang and vLLM side by side and constantly switching between their different bench tools was getting annoying. Having everything in one place with consistent metrics will save me so much time",
          "created_at": "2026-01-06T20:05:05Z",
          "was_summarised": false
        },
        {
          "id": "ny2hx3a",
          "author": "Caryn_fornicatress",
          "content": "This is actually useful. Comparing pp/tg across different backends and context sizes is exactly what‚Äôs missing right now. The llama-bench style tables make it way easier to reason about real user-facing latency instead of just raw throughput. Nice work, especially the TTFT and cache pitfalls you‚Äôre calling out.",
          "created_at": "2026-01-06T20:11:00Z",
          "was_summarised": false
        },
        {
          "id": "ny3grp6",
          "author": "doradus_novae",
          "content": "This is awesome firing this up tonight.Good work",
          "created_at": "2026-01-06T22:53:31Z",
          "was_summarised": false
        },
        {
          "id": "ny3pyn6",
          "author": "thrownawaymane",
          "content": "Can this print me a [benchy](https://en.wikipedia.org/wiki/3DBenchy)?",
          "created_at": "2026-01-06T23:40:38Z",
          "urls": [
            {
              "url": "https://en.wikipedia.org/wiki/3DBenchy",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://spark:8888/v1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/eugr/llama-benchy",
          "was_fetched": true,
          "page": "Title: GitHub - eugr/llama-benchy: llama-benchy - llama-bench style benchmarking tool for all backends\n\nURL Source: https://github.com/eugr/llama-benchy\n\nMarkdown Content:\nGitHub - eugr/llama-benchy: llama-benchy - llama-bench style benchmarking tool for all backends\n===============\n\n[Skip to content](https://github.com/eugr/llama-benchy#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Feugr%2Fllama-benchy)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Feugr%2Fllama-benchy)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=eugr%2Fllama-benchy)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/eugr/llama-benchy) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/eugr/llama-benchy) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/eugr/llama-benchy) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[eugr](https://github.com/eugr)/**[llama-benchy](https://github.com/eugr/llama-benchy)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)You must be signed in to change notification settings\n*   [Fork 1](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)\n*   [Star 7](https://github.com/login?return_to=%2Feugr%2Fllama-benchy) \n\nllama-benchy - llama-bench style benchmarking tool for all backends\n\n### License\n\n[MIT license](https://github.com/eugr/llama-benchy/blob/main/LICENSE)\n\n[7 stars](https://github.com/eugr/llama-benchy/stargazers)[1 fork](https://github.com/eugr/llama-benchy/forks)[Branches](https://github.com/eugr/llama-benchy/branches)[Tags](https://github.com/eugr/llama-benchy/tags)[Activity](https://github.com/eugr/llama-benchy/activity)\n\n[Star](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)\n\n[Notifications](https://github.com/login?return_to=%2Feugr%2Fllama-benchy)You must be signed in to change notification settings\n\n*   [Code](https://github.com/eugr/llama-benchy)\n*   [Issues 0](https://github.com/eugr/llama-benchy/issues)\n*   [Pull requests 0](https://github.com/eugr/llama-benchy/pulls)\n*   [Actions](https://github.com/eugr/llama-benchy/actions)\n*   [Projects 0](https://github.com/eugr/llama-benchy/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/eugr/llama-benchy).](https://github.com/eugr/llama-benchy/security)\n*   [Insights](https://github.com/eugr/llama-benchy/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/eugr/llama-benchy)\n*   [Issues](https://github.com/eugr/llama-benchy/issues)\n*   [Pull requests](https://github.com/eugr/llama-benchy/pulls)\n*   [Actions](https://github.com/eugr/llama-benchy/actions)\n*   [Projects](https://github.com/eugr/llama-benchy/projects)\n*   [Security](https://github.com/eugr/llama-benchy/security)\n*   [Insights](https://github.com/eugr/llama-benchy/pulse)\n\neugr/llama-benchy\n=================\n\nmain\n\n[Branches](https://github.com/eugr/llama-benchy/branches)[Tags](https://github.com/eugr/llama-benchy/tags)\n\n[](https://github.com/eugr/llama-benchy/branches)[](https://github.com/eugr/llama-benchy/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [45 Commits](https://github.com/eugr/llama-benchy/commits/main/) [](https://github.com/eugr/llama-benchy/commits/main/) |\n| [src/llama_benchy](https://github.com/eugr/llama-benchy/tree/main/src/llama_benchy \"This path skips through empty directories\") | [src/llama_benchy](https://github.com/eugr/llama-benchy/tree/main/src/llama_benchy \"This path skips through empty directories\") |  |  |\n| [.gitignore](https://github.com/eugr/llama-benchy/blob/main/.gitignore \".gitignore\") | [.gitignore](https://github.com/eugr/llama-benchy/blob/main/.gitignore \".gitignore\") |  |  |\n| [LICENSE](https://github.com/eugr/llama-benchy/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/eugr/llama-benchy/blob/main/LICENSE \"LICENSE\") |  |  |\n| [README.md](https://github.com/eugr/llama-benchy/blob/main/README.md \"README.md\") | [README.md](https://github.com/eugr/llama-benchy/blob/main/README.md \"README.md\") |  |  |\n| [pyproject.toml](https://github.com/eugr/llama-benchy/blob/main/pyproject.toml \"pyproject.toml\") | [pyproject.toml](https://github.com/eugr/llama-benchy/blob/main/pyproject.toml \"pyproject.toml\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/eugr/llama-benchy#)\n*   [MIT license](https://github.com/eugr/llama-benchy#)\n\nllama-benchy - llama-bench style benchmarking tool for all backends\n===================================================================\n\n[](https://github.com/eugr/llama-benchy#llama-benchy---llama-bench-style-benchmarking-tool-for-all-backends)\n\nThis script benchmarks OpenAI-compatible LLM endpoints, generating statistics similar to `llama-bench`.\n\nMotivation\n----------\n\n[](https://github.com/eugr/llama-benchy#motivation)\n\n`llama-bench` is a CLI tool that is a part of a very popular [llama.cpp](https://github.com/ggml-org/llama.cpp) inference engine. It is widely used in LLM community to benchmark models and allows to perform measurement at different context sizes. However, it is available only for llama.cpp and cannot be used with other inference engines, like vllm or SGLang.\n\nAlso, it performs measurements using the C++ engine directly which is not representative of the end user experience which can be quite different in practice.\n\nvLLM has its own powerful benchmarking tool, but while it can be used with other inference engines, there are a few issues:\n\n*   It's very tricky and even impossible to calculate prompt processing speeds at different context lengths. You can use `vllm bench sweep serve`, but it only works well with vLLM with prefix caching disabled on the server. Even with random prompts it will reuse the same prompt between multiple runs which will hit the cache in `llama-server` for instance. So you will get very low median TTFT times and very high prompt processing speeds.\n*   The TTFT measurement it uses is not actually until the first usable token, it's until the very first data chunk from the server which may not contain any generated tokens in /v1/chat/completions mode.\n*   Random dataset is the only ones that allows to specify an arbitrary number of tokens, but randomly generated token sequence doesn't let you adequately measure speculative decoding/MTP.\n\nAs of January 2nd, 2026, I wasn't able to find any existing benchmarking tool that brings llama-bench style measurements at different context lengths to any OpenAI-compatible endpoint.\n\nFeatures\n--------\n\n[](https://github.com/eugr/llama-benchy#features)\n\n*   Measures Prompt Processing (pp) and Token Generation (tg) speeds at different context depths.\n*   Can measure separate context prefill and prompt processing over existing cached context at different context depths.\n*   Reports Time To First Response (data chunk) (TTFR), Estimated Prompt Processing Time (est_ppt), and End-to-End TTFT.\n*   Supports configurable prompt length (`--pp`), generation length (`--tg`), and context depth (`--depth`).\n*   Can run multiple iterations (`--runs`) and report mean ¬± std.\n*   Uses HuggingFace tokenizers for accurate token counts.\n*   Downloads a book from Project Gutenberg to use as source text for prompts to ensure better benchmarking of spec.decoding/MTP models.\n*   Supports executing a command after each run (e.g., to clear cache).\n*   Configurable latency measurement mode.\n\nCurrent Limitations\n===================\n\n[](https://github.com/eugr/llama-benchy#current-limitations)\n\n*   Evaluates against `/v1/chat/completions` endpoint only.\n*   Doesn't measure throughput in concurrency mode (coming later).\n*   Outputs results as a Markdown table only for now.\n\nInstallation\n------------\n\n[](https://github.com/eugr/llama-benchy#installation)\n\nUsing `uv` is recommended. You can install `uv` here: [https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)\n\n### Option 1: Run without installation using `uvx`\n\n[](https://github.com/eugr/llama-benchy#option-1-run-without-installation-using-uvx)\n\nRun the release version from PyPI:\n\nundefinedshell\nuvx llama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\nRun the latest version from the main branch:\n\nundefinedshell\nuvx --from git+https://github.com/eugr/llama-benchy llama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\n### Option 2: Install into virtual environment\n\n[](https://github.com/eugr/llama-benchy#option-2-install-into-virtual-environment)\n\nundefinedshell\n# Clone the repository\ngit clone https://github.com/eugr/llama-benchy.git\ncd llama-benchy\n\n# Create virtual environment\nuv venv\n\n# Install with uv (installs into a virtual environment automatically)\nuv pip install -e .\nundefined\n\nTo run, activate the environment first\n\nundefinedshell\nsource .venv/bin/activate\nundefined\n\nThen execute the command:\n\nundefinedshell\nllama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\n### Option 3: Run without installing (`uv run`)\n\n[](https://github.com/eugr/llama-benchy#option-3-run-without-installing-uv-run)\n\nundefinedshell\n# Clone the repository\ngit clone https://github.com/eugr/llama-benchy.git\ncd llama-benchy\n\n# Using uv run (creates a virtual environment if it doesn't exist and runs the command)\nuv run llama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e\nundefined\n\n### Option 3: Install into system path\n\n[](https://github.com/eugr/llama-benchy#option-3-install-into-system-path)\n\nRelease version from PyPI:\n\nundefinedshell\nuv pip install -U llama-benchy\nundefined\n\nCurrent version from the main branch:\n\nundefinedshell\nuv pip install git+https://github.com/eugr/llama-benchy --system\nundefined\n\nUsage\n-----\n\n[](https://github.com/eugr/llama-benchy#usage)\n\nAfter installation, you can run the tool directly:\n\nundefinedshell\nllama-benchy --base-url \u003cENDPOINT_URL\u003e --model \u003cMODEL_NAME\u003e --pp \u003cPROMPT_TOKENS\u003e --tg \u003cGEN_TOKENS\u003e [OPTIONS]\nundefined\n\nExample:\n\nundefinedshell\nllama-benchy \\\n  --base-url http://localhost:8000/v1 \\\n  --model openai/gpt-oss-120b \\\n  --depth 0 4096 8192 16384 32768 \\\n  --latency-mode generation\nundefined\n\nOutput:\n\n| model | test | t/s | ttfr (ms) | est_ppt (ms) | e2e_ttft (ms) |\n| :--- | ---: | ---: | ---: | ---: | ---: |\n| openai/gpt-oss-120b | pp2048 | 2019.02 ¬± 34.98 | 1054.64 ¬± 17.57 | 1014.66 ¬± 17.57 | 1115.41 ¬± 18.70 |\n| openai/gpt-oss-120b | tg32 | 52.94 ¬± 1.01 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d4096 | 1994.49 ¬± 77.97 | 3129.18 ¬± 120.27 | 3089.19 ¬± 120.27 | 3198.97 ¬± 122.24 |\n| openai/gpt-oss-120b | tg32 @ d4096 | 46.69 ¬± 1.11 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d8192 | 1751.68 ¬± 34.44 | 5892.61 ¬± 114.68 | 5852.63 ¬± 114.68 | 5971.27 ¬± 115.77 |\n| openai/gpt-oss-120b | tg32 @ d8192 | 40.40 ¬± 1.19 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d16384 | 1475.63 ¬± 31.41 | 12542.02 ¬± 265.86 | 12502.04 ¬± 265.86 | 12634.67 ¬± 269.43 |\n| openai/gpt-oss-120b | tg32 @ d16384 | 33.86 ¬± 1.45 |  |  |  |\n| openai/gpt-oss-120b | pp2048 @ d32768 | 1131.86 ¬± 50.53 | 30869.90 ¬± 1410.15 | 30829.92 ¬± 1410.15 | 30992.96 ¬± 1417.33 |\n| openai/gpt-oss-120b | tg32 @ d32768 | 25.34 ¬± 1.31 |  |  |  |\n\nllama-benchy (build: 75bc129) date: 2026-01-02 17:11:19 | latency mode: generation\n\n* * *\n\nIt's recommended to use \"generation\" latency mode to get prompt processing speeds closer to real numbers, especially on shorter prompts. By default, the script adapts the prompt size to match the specified value, regardless of the chat template applied. Use `--no-adapt-prompt` to disable this behavior.\n\nGenerally you don't need to disable prompt caching on the server, as a probability of cache hits is fairly small. You can add `--no-cache` that will add some random noise if you get cache hits.\n\n### Arguments\n\n[](https://github.com/eugr/llama-benchy#arguments)\n\n*   `--base-url`: OpenAI compatible endpoint URL (Required).\n*   `--api-key`: API Key (Default: \"EMPTY\").\n*   `--model`: Model name (Required).\n*   `--served-model-name`: Model name used in API calls (Defaults to --model if not specified).\n*   `--tokenizer`: HuggingFace tokenizer name (Defaults to model name).\n*   `--pp`: List of prompt processing token counts (Default: [2048]).\n*   `--tg`: List of token generation counts (Default: [32]).\n*   `--depth`: List of context depths (Default: [0]).\n*   `--runs`: Number of runs per test (Default: 3).\n*   `--no-cache`: Add noise to requests to improve prefix caching avoidance. Also sends `cache-prompt=false` to the server.\n*   `--post-run-cmd`: Command to execute after each test run.\n*   `--book-url`: URL of a book to use for text generation (Defaults to Sherlock Holmes).\n*   `--latency-mode`: Method to measure latency: 'api' (call list models function) - default, 'generation' (single token generation), or 'none' (skip latency measurement).\n*   `--no-warmup`: Skip warmup phase.\n*   `--adapt-prompt`: Adapt prompt size based on warmup token usage delta (Default: True).\n*   `--no-adapt-prompt`: Disable prompt size adaptation.\n*   `--enable-prefix-caching`: Enable prefix caching performance measurement. When enabled (and depth \u003e 0), it performs a two-step benchmark: first loading the context (reported as `ctx_pp`), then running the prompt with the cached context.\n\n### Metrics\n\n[](https://github.com/eugr/llama-benchy#metrics)\n\nThe script outputs a table with the following metrics. All time measurements are in milliseconds (ms).\n\n#### Latency Adjustment\n\n[](https://github.com/eugr/llama-benchy#latency-adjustment)\n\nThe script attempts to estimate network or processing latency to provide \"server-side\" processing times.\n\n*   **Latency**: Measured based on `--latency-mode`. \n    *   `api`: Time to fetch `/models` (from sending request to getting first byte of the response). Eliminates network latency only.\n    *   `generation`: Time to generate 1 token (from sending request to getting first byte of the response). Tries to eliminate network and server overhead latency.\n    *   `none`: Assumed to be 0.\n\n*   This measured latency is subtracted from `ttfr` to calculate `est_ppt`.\n\n#### Table Columns\n\n[](https://github.com/eugr/llama-benchy#table-columns)\n\n*   **`t/s` (Tokens per Second)**:\n\n    *   **For Prompt Processing (pp)**: Calculated as `Total Prompt Tokens / est_ppt`. This represents the prefill speed.\n    *   **For Token Generation (tg)**: Calculated as `(Total Generated Tokens - 1) / (Time of Last Token - Time of First Token)`. This represents the decode speed, excluding the first token latency.\n\n*   **`ttfr (ms)` (Time To First Response)**:\n\n    *   Calculation: `Time of First Response Chunk - Start Time`.\n    *   Represents the raw time until the client receives _any_ stream data from the server (including empty chunks or role definitions, but excluding initial http response header). This includes network latency. The same measurement method is used by `vllm bench serve` to report TTFT.\n\n*   **`est_ppt (ms)` (Estimated Prompt Processing Time)**:\n\n    *   Calculation: `TTFR - Estimated Latency`.\n    *   Estimated time the server spent processing the prompt. Used for calculating Prompt Processing speed.\n\n*   **`e2e_ttft (ms)` (End-to-End Time To First Token)**:\n\n    *   Calculation: `Time of First Content Token - Start Time`.\n    *   The total time perceived by the client from sending the request to seeing the first generated content.\n\n### Prefix Caching Benchmarking\n\n[](https://github.com/eugr/llama-benchy#prefix-caching-benchmarking)\n\nWhen `--enable-prefix-caching` is used (with `--depth`\u003e 0), the script performs a two-step process for each run to measure the impact of prefix caching:\n\n1.   **Context Load**: Sends the context tokens (as a system message) with an empty user message. This forces the server to process and cache the context. \n    *   Reported as `ctx_pp @ d{depth}` (Context Prompt Processing) and `ctx_tg @ d{depth}`.\n\n2.   **Inference**: Sends the same context (system message) followed by the actual prompt (user message). The server should reuse the cached context. \n    *   Reported as standard `pp{tokens} @ d{depth}` and `tg{tokens} @ d{depth}`.\n\nIn this case, `pp` and `tg` speeds will show an actual prompt processing / token generation speeds for a follow up prompt with a context pre-filled.\n\n### Example\n\n[](https://github.com/eugr/llama-benchy#example)\n\nundefinedshell\nllama-benchy \\\n  --base-url http://localhost:8000/v1 \\\n  --model openai/gpt-oss-120b \\\n  --pp 128 256 \\\n  --tg 32 64 \\\n  --depth 0 1024\nundefined\n\nThis will run benchmarks for all combinations of pp (128, 256), tg (32, 64), and depth (0, 1024).\n\nAbout\n-----\n\nllama-benchy - llama-bench style benchmarking tool for all backends\n\n### Resources\n\n[Readme](https://github.com/eugr/llama-benchy#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/eugr/llama-benchy#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/eugr/llama-benchy).\n\n[Activity](https://github.com/eugr/llama-benchy/activity)\n\n### Stars\n\n[**7** stars](https://github.com/eugr/llama-benchy/stargazers)\n\n### Watchers\n\n[**0** watching](https://github.com/eugr/llama-benchy/watchers)\n\n### Forks\n\n[**1** fork](https://github.com/eugr/llama-benchy/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Feugr%2Fllama-benchy\u0026report=eugr+%28user%29)\n\n[Releases 2](https://github.com/eugr/llama-benchy/releases)\n-----------------------------------------------------------\n\n[v0.1.1 - cosmetic changes Latest Jan 7, 2026](https://github.com/eugr/llama-benchy/releases/tag/v0.1.1)\n\n[+ 1 release](https://github.com/eugr/llama-benchy/releases)\n\n[Packages 0](https://github.com/users/eugr/packages?repo_name=llama-benchy)\n---------------------------------------------------------------------------\n\n No packages published \n\nLanguages\n---------\n\n*   [Python 100.0%](https://github.com/eugr/llama-benchy/search?l=python)\n\nFooter\n------\n\n[](https://github.com/) ¬© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You can‚Äôt perform that action at this time.",
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- **New tool** ‚Äì `llama‚Äëbenchy`, a CLI benchmark that mimics the popular `llama‚Äëbench` table format but works with *any* OpenAI‚Äëcompatible endpoint (llama.cpp, vLLM, SGLang, etc.).  \n- **Metrics** ‚Äì Measures prompt‚Äëprocessing (pp), token‚Äëgeneration (tg) speeds, time‚Äëto‚Äëfirst‚Äëresponse (TTFR), estimated prompt‚Äëprocessing time, and end‚Äëto‚Äëend TTFT, with configurable context depth and optional prefix‚Äëcaching.  \n- **Example results** ‚Äì Benchmarked the MiniMax‚ÄØ2.1‚ÄØAWQ‚Äë4bit model on a dual‚ÄëSpark cluster: pp‚ÄØ‚âà‚ÄØ3‚ÄØ500‚ÄØt/s at 0‚ÄØctx dropping to ‚âà‚ÄØ1‚ÄØ100‚ÄØt/s at 100‚ÄØk tokens, TTFT rising to ~88‚ÄØs at the longest context.  \n- **Usability** ‚Äì Installs via `uvx` or pip, supports HuggingFace tokenizers, adaptive prompt sizing, latency‚Äëmode choices, and post‚Äërun commands for cache clearing.  \n- **Open‚Äësource status** ‚Äì GitHub repo `eugr/llama-benchy` (v0.1.1, MIT licence, 7‚ÄØstars, 1‚ÄØfork).  \n- **Community feedback** ‚Äì Users praise the unified, reproducible benchmarking across backends; comments note how it eliminates the need to juggle separate tools for each engine.\n\n**Why it matters**  \nIt gives developers a single, standardized way to compare real‚Äëworld latency and throughput of local LLM deployments across different inference engines, helping optimize for user‚Äëperceived speed.\n\n**Community response**  \nPositive; reviewers highlight the tool‚Äôs usefulness for cross‚Äëbackend comparison and its realistic latency measurements, especially for speculative decoding and prefix‚Äëcaching scenarios.\n\n**Key entities**  \neugr, llama-benchy, MiniMax‚ÄØ2.1‚ÄØAWQ, vLLM, SGLang, llama.cpp, OpenAI‚Äëcompatible endpoint, GitHub.",
        "html": "\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNew tool\u003c/strong\u003e ‚Äì \u003ccode\u003ellama‚Äëbenchy\u003c/code\u003e, a CLI benchmark that mimics the popular \u003ccode\u003ellama‚Äëbench\u003c/code\u003e table format but works with \u003cem\u003eany\u003c/em\u003e OpenAI‚Äëcompatible endpoint (llama.cpp, vLLM, SGLang, etc.).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics\u003c/strong\u003e ‚Äì Measures prompt‚Äëprocessing (pp), token‚Äëgeneration (tg) speeds, time‚Äëto‚Äëfirst‚Äëresponse (TTFR), estimated prompt‚Äëprocessing time, and end‚Äëto‚Äëend TTFT, with configurable context depth and optional prefix‚Äëcaching.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExample results\u003c/strong\u003e ‚Äì Benchmarked the MiniMax‚ÄØ2.1‚ÄØAWQ‚Äë4bit model on a dual‚ÄëSpark cluster: pp‚ÄØ‚âà‚ÄØ3‚ÄØ500‚ÄØt/s at 0‚ÄØctx dropping to ‚âà‚ÄØ1‚ÄØ100‚ÄØt/s at 100‚ÄØk tokens, TTFT rising to ~88‚ÄØs at the longest context.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUsability\u003c/strong\u003e ‚Äì Installs via \u003ccode\u003euvx\u003c/code\u003e or pip, supports HuggingFace tokenizers, adaptive prompt sizing, latency‚Äëmode choices, and post‚Äërun commands for cache clearing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpen‚Äësource status\u003c/strong\u003e ‚Äì GitHub repo \u003ccode\u003eeugr/llama-benchy\u003c/code\u003e (v0.1.1, MIT licence, 7‚ÄØstars, 1‚ÄØfork).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommunity feedback\u003c/strong\u003e ‚Äì Users praise the unified, reproducible benchmarking across backends; comments note how it eliminates the need to juggle separate tools for each engine.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters\u003c/strong\u003e\u003cbr\u003e\nIt gives developers a single, standardized way to compare real‚Äëworld latency and throughput of local LLM deployments across different inference engines, helping optimize for user‚Äëperceived speed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response\u003c/strong\u003e\u003cbr\u003e\nPositive; reviewers highlight the tool‚Äôs usefulness for cross‚Äëbackend comparison and its realistic latency measurements, especially for speculative decoding and prefix‚Äëcaching scenarios.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities\u003c/strong\u003e\u003cbr\u003e\neugr, llama-benchy, MiniMax‚ÄØ2.1‚ÄØAWQ, vLLM, SGLang, llama.cpp, OpenAI‚Äëcompatible endpoint, GitHub.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352201202Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.95,
        "reason": "Detailed description of a new CLI benchmarking tool for OpenAI-compatible LLM backends, including metrics, performance data, and novel features.",
        "processed_at": "2026-01-07T03:37:27.101502194Z"
      },
      "processed_at": "2026-01-07T02:55:54.275239116Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://spark:8888/v1: jina: status 400: {\"data\":null,\"path\":\"url\",\"code\":400,\"name\":\"ParamValidationError\",\"status\":40001,\"message\":\"Domain 'spark' could not be resolved\",\"readableMessage\":\"ParamValidationError(url): Domain 'spark' could not be resolved\"}: retry failed: jina request failed: 400 Bad Request",
          "occurred_at": "2026-01-07T02:55:53.140777712Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q5nw4k",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5nw4k/minimax_m2_is_goated_agentic_capture_the_flag_ctf/",
      "title": "MiniMax M2 is GOATed - Agentic Capture the Flag (CTF) benchmark on GLM-4.5 air, 4.7 (+REAP), and Minimax-M2",
      "content": "",
      "author": "sixx7",
      "created_at": "2026-01-06T16:51:06Z",
      "comments": [
        {
          "id": "ny224qw",
          "author": "__JockY__",
          "content": "Over the winter break I messed a lot with MiniMax-M2 and then MiniMax-M2.1 FP8 @ 200k context with Claude Code cli on an offline system. It is *unbelievable*. Fucking witchcraft.\n\nOld software dev is dead, buried, gone. As a friend said to me earlier today: if you're still typing code, you're a dinosaur. Just a year ago I'd have said \"naaaaah\".\n\nI'm an old coder. It's all I've ever done and I've been doing it for over 4 decades now. This is the biggest shift I ever saw in all my time. Nothing comes close. Everything has changed. \n\nAfter using this shit for real and actually building complex stuff with it... I'm with my buddy. If you're still typing code, you're a dinosaur. CC + M2.1 FP8 has built stuff in a day that would have taken weeks even with my old \"prompt the LLM and copy/paste code\" approach, which is an anachronism now. For most things I doubt I'd  even *need* to see code!\n\nI will, however, be looking at the code. \n\nI saw enough to know that the LLM isn't always making smart choices. It may build extremely complex things, but is it doing so in a sane manner? Not always. Sometimes it even lies and writes code that's just a stub but prints things like \"imported successfully!\" When called out it behaves all sheepish and mostly fixes its shit, but still. That's pretty lazy. I kinda like it.\n\nOr it can make one stupid decision that leads it to implement, document, and build unit tests for the craziest and most overly-complicated unnecessary nonsense I ever saw... but hey. That's witchcraft for you!",
          "created_at": "2026-01-06T18:58:40Z",
          "was_summarised": false
        },
        {
          "id": "ny19qif",
          "author": "sixx7",
          "content": "**TLDR:**  Benchmarked popular open-source/weight models using capture-the-flag (CTF) style challenges that require the models to iteratively write and execute queries against a data lake.  If you want to see the full write-up, [check it out here](https://medium.com/@ai-with-eric/local-llms-and-an-autonomous-agentic-showdown-5d7f552e5846?)\n\n\n\u0026amp;nbsp;\n\n\nI admit I had been sleeping on MiniMax-M2.  For local/personal stuff, GLM-4.5air has been so solid that I took a break from trying out new modals (locally).  Though,  I do have a z.ai subscription where I continue to use their hosted offerings and have been pretty happy with GLM-4.6 and now GLM-4.7\n\n\n\u0026amp;nbsp;\n\nI cannot run GLM-4.7 locally, so that was tested directly using z.ai API.  The rest were run locally.  I almost exclusively use AWQ quants in vllm. Some notes and observations without making this too lengthy:\n\n\n* The REAP'd version of GLM-4.7 did not fair well, performing even worse than GLM-4.5-air\n* GLM-4.7 results were disappointing.  It performed similar, and in some metrics worse, with the full version on z.ai compared to 4.5-air running locally.  I think this highlights how good 4.5-air actually is\n* MiniMax M2 blew GLM.* out of the water.   It won on all but 1 metric, and even that one was really close\n* GLM-4.7 was using the Anthropic-style API, whereas all the locally running models were using the v1/chat/completions OpenAI-style API\n\n\n\u0026amp;nbsp;\n\n**ETA**:  Ran MiniMax M2.1 u/hainesk\n\n\n* Accuracy was the same, and both models failed solving the same challenges\n* M2.1 **wins** on speed, averaging 61 seconds per challenge (M2 was 72.7 seconds) \n* M2.1 **wins** on the number of tool calls, averaging 10.65 (M2 was 12.75)\n* M2.1 **loses** on token use, averaging 264k per challenge (M2 was 244K)\n\n\nM2.1 definitely seems like an upgrade, if for no other reason than it performs well while also being faster",
          "created_at": "2026-01-06T16:51:23Z",
          "urls": [
            {
              "url": "https://medium.com/@ai-with-eric/local-llms-and-an-autonomous-agentic-showdown-5d7f552e5846?",
              "was_fetched": false,
              "was_summarised": false
            }
          ],
          "was_summarised": false
        },
        {
          "id": "ny3017q",
          "author": "Devcomeups",
          "content": "Installi oh my open code wirh mini max",
          "created_at": "2026-01-06T21:34:18Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/j0yzgwis8rbg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- MiniMax‚ÄØM2.1 (FP8, 200k‚Äëtoken context) outperforms all GLM models on an autonomous ‚Äúcapture‚Äëthe‚Äëflag‚Äù CTF benchmark that requires iterative query‚Äëgeneration against a data lake.  \n- In the benchmark, MiniMax‚ÄØM2.1 averages **61‚ÄØs per challenge** (vs 72.7‚ÄØs for M2), makes **10.65 tool calls** (vs 12.75), but uses **~264‚ÄØk tokens** (vs ~244‚ÄØk for M2).  \n- GLM‚Äë4.7‚Äîboth its full and REAP‚Äëquantized versions‚Äîperformed worse than GLM‚Äë4.5‚Äëair, with the REAP version underperforming even the local 4.5‚Äëair model.  \n- The tests were split between local runs (MiniMax and GLM‚Äë4.5‚Äëair using AWQ/‚ÄãvLLM) and z.ai API calls for GLM‚Äë4.7 (Anthropic‚Äëstyle API).  \n- Commenters highlight the ‚Äúwitchcraft‚Äù of MiniMax‚ÄØM2.1: a veteran coder notes that it can build complex, production‚Äëready code in a day that would normally take weeks, and that the model‚Äôs speed advantage is striking.\n\n**Why it matters:** MiniMax‚ÄØM2.1‚Äôs speed, efficiency, and superior performance on an agentic benchmark signal a major leap for locally deployable LLMs, potentially redefining how developers build AI‚Äëdriven tools without relying on cloud APIs.\n\n**Community response:** JockY praised MiniMax‚ÄØM2.1 as ‚Äúunbelievable‚Äù and ‚Äúwitchcraft,‚Äù emphasizing its ability to accelerate code generation beyond what traditional prompt‚Äëengineering can achieve.\n\n**Key entities:** MiniMax‚ÄØM2, MiniMax‚ÄØM2.1, GLM‚Äë4.5‚Äëair, GLM‚Äë4.7, z.ai, AWQ, vLLM, Anthropic‚Äëstyle API.",
        "html": "\u003cul\u003e\n\u003cli\u003eMiniMax‚ÄØM2.1 (FP8, 200k‚Äëtoken context) outperforms all GLM models on an autonomous ‚Äúcapture‚Äëthe‚Äëflag‚Äù CTF benchmark that requires iterative query‚Äëgeneration against a data lake.\u003c/li\u003e\n\u003cli\u003eIn the benchmark, MiniMax‚ÄØM2.1 averages \u003cstrong\u003e61‚ÄØs per challenge\u003c/strong\u003e (vs 72.7‚ÄØs for M2), makes \u003cstrong\u003e10.65 tool calls\u003c/strong\u003e (vs 12.75), but uses \u003cstrong\u003e~264‚ÄØk tokens\u003c/strong\u003e (vs ~244‚ÄØk for M2).\u003c/li\u003e\n\u003cli\u003eGLM‚Äë4.7‚Äîboth its full and REAP‚Äëquantized versions‚Äîperformed worse than GLM‚Äë4.5‚Äëair, with the REAP version underperforming even the local 4.5‚Äëair model.\u003c/li\u003e\n\u003cli\u003eThe tests were split between local runs (MiniMax and GLM‚Äë4.5‚Äëair using AWQ/‚ÄãvLLM) and z.ai API calls for GLM‚Äë4.7 (Anthropic‚Äëstyle API).\u003c/li\u003e\n\u003cli\u003eCommenters highlight the ‚Äúwitchcraft‚Äù of MiniMax‚ÄØM2.1: a veteran coder notes that it can build complex, production‚Äëready code in a day that would normally take weeks, and that the model‚Äôs speed advantage is striking.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e MiniMax‚ÄØM2.1‚Äôs speed, efficiency, and superior performance on an agentic benchmark signal a major leap for locally deployable LLMs, potentially redefining how developers build AI‚Äëdriven tools without relying on cloud APIs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e JockY praised MiniMax‚ÄØM2.1 as ‚Äúunbelievable‚Äù and ‚Äúwitchcraft,‚Äù emphasizing its ability to accelerate code generation beyond what traditional prompt‚Äëengineering can achieve.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e MiniMax‚ÄØM2, MiniMax‚ÄØM2.1, GLM‚Äë4.5‚Äëair, GLM‚Äë4.7, z.ai, AWQ, vLLM, Anthropic‚Äëstyle API.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352248683Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "Post announces a benchmark comparing GLM-4.5 air, GLM-4.7+REAP, and Minimax-M2 on an agentic CTF task, highlighting performance insights.",
        "processed_at": "2026-01-07T03:37:32.419106114Z"
      },
      "processed_at": "2026-01-07T02:57:03.05311206Z"
    },
    {
      "flow_id": "",
      "id": "1q5oz98",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5oz98/lgaiexaonekexaone236ba23b_released/",
      "title": "LGAI-EXAONE/K-EXAONE-236B-A23B released",
      "content": "",
      "author": "jinnyjuice",
      "created_at": "2026-01-06T17:30:10Z",
      "comments": [
        {
          "id": "ny2j6pi",
          "author": "KvAk_AKPlaysYT",
          "content": "The license is not fun :(\n\n\nSummary:\n---\n\nNot open source - proprietary license, unlike MIT or Apache\n\nCommercial redistribution or sublicensing requires separate permission\n\nExplicit ethical and use-based restrictions, which MIT and Apache do not allow\n\nReverse engineering and model analysis are prohibited\n\nLicensor can terminate the license and require destruction of all copies\n\nMandatory naming of derivatives starting with ‚ÄúK-EXAONE‚Äù\n\nUser must indemnify the licensor for claims and damages\n\nKorean law and mandatory arbitration apply\n\nMuch higher legal and operational risk than MIT or Apache\n\n---",
          "created_at": "2026-01-06T20:16:52Z",
          "was_summarised": false
        },
        {
          "id": "ny1v9sk",
          "author": "vasileer",
          "content": "thank you for the benchmark, now I know gpt-oss-120b is still one of the best in its league",
          "created_at": "2026-01-06T18:28:07Z",
          "was_summarised": false
        },
        {
          "id": "ny1kp18",
          "author": "silenceimpaired",
          "content": "I‚Äôm always annoyed to see a license isn‚Äôt Apache or MIT, but at least this one isn‚Äôt too restrictive. It is weird seeing a model this size performing competitively with a 30b (that is to say‚Ä¶ the difficulty to run this on my computer doesn‚Äôt make it worth it‚Ä¶ when the 30b will run much better.)",
          "created_at": "2026-01-06T17:41:21Z",
          "was_summarised": false
        },
        {
          "id": "ny2mdyz",
          "author": "weasl",
          "content": "so they finetune qwen 235b to get worse performance in benchmarks?",
          "created_at": "2026-01-06T20:31:49Z",
          "was_summarised": false
        },
        {
          "id": "ny1q4ov",
          "author": "PopularKnowledge69",
          "content": "Where did I see this combination of 236B and A23B ü§î",
          "created_at": "2026-01-06T18:05:25Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B",
          "was_fetched": true,
          "page": "Title: LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B\n\nMarkdown Content:\n[](https://huggingface.co/collections/LGAI-EXAONE/k-exaone)[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#)[](https://arxiv.org/abs/2601.01739)[](https://github.com/LG-AI-EXAONE/K-EXAONE)[](https://friendli.ai/model/LGAI-EXAONE/K-EXAONE-236B-A23B)\n\nüÜì **Free API until Jan 28th, 2026**!  Try on ‚¨ÜÔ∏è FriendliAI ‚úàÔ∏è\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#introduction) Introduction\n-----------------------------------------------------------------------------------\n\nWe introduce **K-EXAONE**, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features **236 billion total** parameters, with **23 billion active** during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features) Key Features\n\n*   **Architecture \u0026 Efficiency:** Features a 236B fine-grained MoE design (23B active) optimized with **Multi-Token Prediction (MTP)**, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.\n*   **Long-Context Capabilities:** Natively supports a **256K context window**, utilizing a **3:1 hybrid attention** scheme with a **128-token sliding window** to significantly minimize memory usage during long-document processing.\n*   **Multilingual Support:** Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned **150k vocabulary** with **SuperBPE**, improving token efficiency by ~30%.\n*   **Agentic Capabilities:** Demonstrates superior tool-use and search capabilities via **multi-agent strategies.**\n*   **Safety \u0026 Ethics:** Aligned with **universal human values**, the model uniquely incorporates **Korean cultural and historical contexts** to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.\n\nFor more details, please refer to the [technical report](https://arxiv.org/abs/2601.01739) and [GitHub](https://github.com/LG-AI-EXAONE/K-EXAONE).\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B/blob/main/assets/main_figure.png)\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#model-configuration) Model Configuration\n\n*   Number of Parameters: 236B in total and 23B activated\n*   Number of Parameters (without embeddings): 234B\n*   Hidden Dimension: 6,144\n*   Number of Layers: 48 Main layers + 1 MTP layers\n    *   Hybrid Attention Pattern: 12 x (3 Sliding window attention + 1 Global attention)\n\n*   Sliding Window Attention\n    *   Number of Attention Heads: 64 Q-heads and 8 KV-heads\n    *   Head Dimension: 128 for both Q/KV\n    *   Sliding Window Size: 128\n\n*   Global Attention\n    *   Number of Attention Heads: 64 Q-heads and 8 KV-heads\n    *   Head Dimension: 128 for both Q/KV\n    *   No Rotary Positional Embedding Used (NoPE)\n\n*   Mixture of Experts:\n    *   Number of Experts: 128\n    *   Number of Activated Experts: 8\n    *   Number of Shared Experts: 1\n    *   MoE Intermediate Size: 2,048\n\n*   Vocab Size: 153,600\n*   Context Length: 262,144 tokens\n*   Knowledge Cutoff: Dec 2024 (2024/12)\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#evaluation-results) Evaluation Results\n-----------------------------------------------------------------------------------------------\n\nThe following table shows the evaluation results of the K-EXAONE model in reasoning mode, compared to our previous model, [EXAONE-4.0](https://github.com/LG-AI-EXAONE/EXAONE-4.0), and other competing models. The evaluation details can be found in the [technical report](https://arxiv.org/abs/2601.01739).\n\n|  | K-EXAONE (Reasoning) | EXAONE 4.0 (Reasoning) | GPT-OSS (Reasoning: High) | Qwen3-Thinking-2507 | DeepSeek-V3.2 (Reasoning) |\n| --- | --- | --- | --- | --- | --- |\n| Architecture | MoE | Dense | MoE | MoE | MoE |\n| Total Params | 236B | 32B | 117B | 235B | 671B |\n| Active Params | 23B | 32B | 5.1B | 22B | 37B |\n| _World Knowledge_ |\n| MMLU-Pro | 83.8 | 81.8 | 80.7 | 84.4 | 85.0 |\n| GPQA-Diamond | 79.1 | 75.4 | 80.1 | 81.1 | 82.4 |\n| Humanity's Last Exam | 13.6 | 10.6 | 14.9 | 18.2 | 25.1 |\n| _Math_ |\n| IMO-AnswerBench | 76.3 | 66.1 | 75.6 | 74.8 | 78.3 |\n| AIME 2025 | 92.8 | 85.3 | 92.5 | 92.3 | 93.1 |\n| HMMT Nov 2025 | 86.8 | 78.1 | 84.9 | 88.8 | 90.2 |\n| _Coding / Agentic Coding_ |\n| LiveCodeBench Pro 25Q2 (Medium) | 25.9 | 4.8 | 35.4 | 16.0 | 27.9 |\n| LiveCodeBench v6 | 80.7 | 66.7 | 81.9 | 74.1 | 79.4 |\n| Terminal-Bench 2.0 | 29.0 | - | 18.7 | 13.3 | 46.4 |\n| SWE-Bench Verified | 49.4 | - | 62.4 | 25.0 | 73.1 |\n| _Agentic Tool Use_ |\n| œÑ 2-Bench (Retail) | 78.6 | 67.5 | 69.1 | 71.9 | 77.9 |\n| œÑ 2-Bench (Airline) | 60.4 | 52.0 | 60.5 | 58.0 | 66.0 |\n| œÑ 2-Bench (Telecom) | 73.5 | 23.7 | 60.3 | 45.6 | 85.8 |\n| BrowseComp | 31.4 | - | - | - | 51.4 |\n| _Instruction Following_ |\n| IFBench | 67.3 | 36.0 | 69.5 | 52.6 | 62.5 |\n| IFEval | 89.7 | 84.7 | 89.5 | 87.8 | 92.6 |\n| _Long Context Understanding_ |\n| AA-LCR | 53.5 | 14.0 | 50.7 | 67.0 | 65.0 |\n| OpenAI-MRCR | 52.3 | 20.1 | 29.9 | 58.6 | 57.7 |\n| _Korean_ |\n| KMMLU-Pro | 67.3 | 67.7 | 62.4 | 71.6 | 72.1 |\n| KoBALT | 61.8 | 25.4 | 54.3 | 56.1 | 62.7 |\n| CLIcK | 83.9 | 78.8 | 74.6 | 81.3 | 86.3 |\n| HRM8K | 90.9 | 89.4 | 91.6 | 92.0 | 90.6 |\n| Ko-LongBench | 86.8 | 68.0 | 82.2 | 83.2 | 87.9 |\n| _Multilinguality_ |\n| MMMLU | 85.7 | 83.2 | 83.8 | 87.3 | 88.0 |\n| WMT24++ | 90.5 | 80.8 | 93.6 | 94.7 | 90.0 |\n| _Safety_ |\n| Wild-Jailbreak | 89.9 | 62.8 | 98.2 | 85.5 | 79.1 |\n| KGC-Safety | 96.1 | 58.0 | 92.5 | 66.2 | 73.0 |\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#requirements) Requirements\n-----------------------------------------------------------------------------------\n\nUntil the libraries officially support K-EXAONE, you need to install the requirements in our version with the EXAONE-MoE implementations. We will announce when these libraries are updated to support the K-EXAONE model.\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#transformers) Transformers\n\nYou can install the latest version of Transformers with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/transformers). The base version of Transformers is `5.0.0rc1`, so it might be helpful to check [the migration guide](https://github.com/huggingface/transformers/blob/main/MIGRATION_GUIDE_V5.md) from the Transformers library.\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#vllm) vLLM\n\nYou should install both Transformers and vLLM to use K-EXAONE model on vLLM server. You can install the latest version of vLLM with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/vllm/tree/add-exaone-moe).\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#sglang) SGLang\n\nYou should install both Transformers and SGLang to use K-EXAONE model on SGLang server. You can install the latest version of SGLang with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/sglang).\n\n#### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#llamacpp) llama.cpp\n\nYou can install the latest version of llama.cpp with support for EXAONE-MoE architecture from [this repository](https://github.com/Aim-Highest/llama.cpp). Please refer to the [official build guide](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md) for details.\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#quickstart) Quickstart\n-------------------------------------------------------------------------------\n\nYou can use the K-EXAONE model with the Transformers library. For better quality, you should check the [usage guideline](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#usage-guideline) section.\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#reasoning-mode) Reasoning mode\n\nFor tasks that require accurate results, you can run the K-EXAONE model in reasoning mode as below.\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"LGAI-EXAONE/K-EXAONE-236B-A23B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    dtype=\"bfloat16\",\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are K-EXAONE, a large language model developed by LG AI Research in South Korea, built to serve as a helpful and reliable assistant.\"},\n    {\"role\": \"user\", \"content\": \"Which one is bigger, 3.9 vs 3.12?\"}\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    enable_thinking=True,   # skippable (default: True)\n)\n\ngenerated_ids = model.generate(\n    **input_ids.to(model.device),\n    max_new_tokens=16384,\n    temperature=1.0,\n    top_p=0.95,\n)\noutput_ids = generated_ids[0][input_ids['input_ids'].shape[-1]:]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#non-reasoning-mode) Non-reasoning mode\n\nFor tasks where latency matters more than accuracy, you can run the K-EXAONE model in non-reasoning mode as below.\n\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are K-EXAONE, a large language model developed by LG AI Research in South Korea, built to serve as a helpful and reliable assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain how wonderful you are\"}\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    enable_thinking=False,\n)\n\ngenerated_ids = model.generate(\n    **input_ids.to(model.device),\n    max_new_tokens=1024,\n    temperature=1.0,\n    top_p=0.95,\n)\noutput_ids = generated_ids[0][input_ids['input_ids'].shape[-1]:]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#agentic-tool-use) Agentic tool use\n\nFor your AI-powered agent, you can leverage K-EXAONE‚Äôs tool calling capability. The K-EXAONE model is compatible with both OpenAI and HuggingFace tool calling specifications. The example below demonstrates tool calling using HuggingFace‚Äôs docstring-to-tool-schema utility.\n\nPlease check the [example file](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B/blob/main/examples/example_output_search.txt) for an example of a search agent conversation using K-EXAONE.\n\n```\nfrom transformers.utils import get_json_schema\n\ndef roll_dice(max_num: int):\n    \"\"\"\n    Roll a dice with the number 1 to N. User can select the number N.\n\n    Args:\n        max_num: The maximum number on the dice.\n    \"\"\"\n    return random.randint(1, max_num)\n\ntool_schema = get_json_schema(roll_dice)\ntools = [tool_schema]\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are K-EXAONE, a large language model developed by LG AI Research in South Korea, built to serve as a helpful and reliable assistant.\"},\n    {\"role\": \"user\", \"content\": \"Roll a D20 twice and sum the results.\"}\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    tools=tools,\n)\n\ngenerated_ids = model.generate(\n    **input_ids.to(model.device),\n    max_new_tokens=16384,\n    temperature=1.0,\n    top_p=0.95,\n)\noutput_ids = generated_ids[0][input_ids['input_ids'].shape[-1]:]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#usage-guideline) Usage Guideline\n-----------------------------------------------------------------------------------------\n\n\u003e To achieve the expected performance, we recommend using the following configurations:\n\u003e \n\u003e \n\u003e *   We strongly recommend to use `temperature=1.0`, `top_p=0.95`, `presence_penalty=0.0` for best performance.\n\u003e *   Different from EXAONE-4.0, K-EXAONE uses `enable_thinking=True` as default. Thus, you need to set `enable_thinking=False` when you want to use non-reasoning mode.\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#deployment) Deployment\n-------------------------------------------------------------------------------\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#tensorrt-llm) TensorRT-LLM\n\nTensorRT-LLM support for the K-EXAONE model is being prepared. Please refer to the [EXAONE-MoE PR](https://github.com/NVIDIA/TensorRT-LLM/pull/10355) on TensorRT-LLM repository for details.\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#vllm-1) vLLM\n\nWe support the K-EXAONE model on vLLM. You need to install our fork of the vLLM library to use the K-EXAONE model. Please check the [requirements](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#requirements) section. Practically, you can serve the model with a 256K context length using tensor parallel on 4 H200 GPUs.\n\nAfter you install the vLLM library with an EXAONE-MoE implementation, you can run the vLLM server by following command:\n\n```\nvllm serve LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --reasoning-parser deepseek_v3 \\\n    --tensor-parallel-size 4 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser hermes\n```\n\nAn OpenAI-compatible API server will be available at [http://localhost:8000/v1](http://localhost:8000/v1).\n\nYou can test the vLLM server by sending a chat completion request as below:\n\n```\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"LGAI-EXAONE/K-EXAONE-236B-A23B\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"How many r'\\''s in \\\"strawberry\\\"?\"}\n        ],\n        \"max_tokens\": 16384,\n        \"temperature\": 1.0,\n        \"top_p\": 0.95,\n        \"chat_template_kwargs\": {\"enable_thinking\": true}\n    }'\n```\n\nIf you are interested in using MTP weights for speculative decoding, add according options as below.\n\n```\nvllm serve LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --reasoning-parser deepseek_v3 \\\n    --tensor-parallel-size 4 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser hermes \\\n    --no-enable-prefix-caching \\\n    --speculative_config '{\n        \"method\": \"mtp\", \n        \"num_speculative_tokens\": 2 \n    }'\n```\n\n### [](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#sglang-1) SGLang\n\nWe support the K-EXAONE model on SGLang. You need to install our fork of the SGLang library to use the K-EXAONE model. Please check the [requirements](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#requirements) section. Practically, you can serve the model with a 256K context length using tensor parallel on 4 H200 GPUs.\n\n```\npython -m sglang.launch_server \\\n    --model LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --tp-size 4 \\\n    --reasoning-parser qwen3\n```\n\nA SGLang server will be available at [http://localhost:30000](http://localhost:30000/).\n\n\u003e Currently, using the OpenAI-compatible server is incompatible with the `transformers\u003e=5.0.0rc0`, so you need to use SGLang native API for now. For native API, please refer to the [official documentation](https://docs.sglang.io/basic_usage/native_api.html).\n\u003e \n\u003e \n\u003e Once the issue is resolved, we will update this section accordingly.\n\nYou can test the SGLang server by sending a request as below:\n\n```\nfrom transformers import AutoTokenizer\nimport requests\n\nmodel_name = \"LGAI-EXAONE/K-EXAONE-236B-A23B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"How many r'\\''s in \\\"strawberry\\\"?\"}\n]\ninput_text = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n)\n\nresponse = requests.post(\n    f\"http://localhost:30000/generate\",\n    json={\n        \"text\": input_text,\n        \"sampling_params\": {\n            \"temperature\": 1.0,\n            \"top_p\": 0.95,\n            \"max_new_tokens\": 16384,\n        },\n    },\n)\nprint(response.json()['text'])\n```\n\nIf you are interested in in using MTP weights for speculative decoding, add according options as below.\n\n```\npython -m sglang.launch_server \\\n    --model LGAI-EXAONE/K-EXAONE-236B-A23B \\\n    --tp-size 4 \\\n    --reasoning-parser qwen3 \\\n    --speculative-algorithm EAGLE \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4\n```\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#limitation) Limitation\n-------------------------------------------------------------------------------\n\nThe K-EXAONE language model has certain limitations and may occasionally generate inappropriate responses. The language model generates responses based on the output probability of tokens, and it is determined during learning from training data. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses. Please note that the text generated by K-EXAONE language model does not reflect the views of LG AI Research.\n\n*   Inappropriate answers may be generated, which contain personal, harmful or other inappropriate information.\n*   Biased responses may be generated, which are associated with age, gender, race, and so on.\n*   The generated responses rely heavily on statistics from the training data, which can result in the generation of semantically or syntactically incorrect sentences.\n*   Since the model does not reflect the latest information, the responses may be false or contradictory.\n\nLG AI Research strives to reduce potential risks that may arise from K-EXAONE language models. Users are not allowed to engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of inappropriate outputs violating LG AI's ethical principles when using K-EXAONE language models.\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#license) License\n-------------------------------------------------------------------------\n\nThe model is licensed under [K-EXAONE AI Model License Agreement](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B/blob/main/LICENSE)\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#citation) Citation\n---------------------------------------------------------------------------\n\n```\n@article{k-exaone,\n  title={K-EXAONE Technical Report},\n  author={{LG AI Research}},\n  journal={arXiv preprint arXiv:2601.01739},\n  year={2025}\n}\n```\n\n[](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#contact) Contact\n-------------------------------------------------------------------------\n\nLG AI Research Technical Support: [contact_us@lgresearch.ai](mailto:contact_us@lgresearch.ai)",
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- **Model \u0026 size** ‚Äì LG‚ÄØAI Research unveiled **K‚ÄëEXAONE‚ÄØ236B‚ÄëA23B**: a 236‚ÄØB‚Äëparameter Mixture‚Äëof‚ÄëExperts model with 23‚ÄØB active experts (128 total experts, 8 active).  \n- **Architecture \u0026 efficiency** ‚Äì 48 main layers + 1 MTP layer, 6,144 hidden dimension, 3:1 hybrid attention (128‚Äëtoken sliding window) and Multi‚ÄëToken Prediction (MTP) for ~1.5√ó faster inference.  \n- **Capabilities \u0026 benchmarks** ‚Äì 256K context window, 150k‚Äëtoken vocab, 6‚Äëlanguage support. In reasoning mode it scores 83.8‚ÄØ% on MMLU‚ÄëPro (vs GPT‚ÄëOSS‚ÄØ117‚ÄØB‚Äôs‚ÄØ80.7‚ÄØ%) and excels in math, coding, agentic tool use, Korean and multilingual tasks.  \n- **License \u0026 policy** ‚Äì Distributed under a proprietary **K‚ÄëEXAONE AI Model License Agreement** (no reverse engineering, mandatory derivative naming ‚ÄúK‚ÄëEXAONE‚Äù, Korean law/mandatory arbitration, user indemnification).  \n- **Deployment \u0026 API** ‚Äì Free public API until Jan‚ÄØ28‚ÄØ2026; compatible with Hugging‚ÄØFace Transformers, vLLM, SGLang, llama.cpp; 256K‚Äëcontext inference possible on 4‚ÄØH200 GPUs via vLLM.\n\n**Why it matters**  \nK‚ÄëEXAONE provides a large, multilingual, and long‚Äëcontext MoE model that competes with top‚Äëtier open‚Äësource peers while offering commercial‚Äëgrade licensing and easy integration into popular frameworks.\n\n**Community response**  \nUsers note the restrictive license but appreciate the competitive benchmarks; some comment on the model‚Äôs size vs performance and speculate on its fine‚Äëtuning origins.\n\n**Key entities**  \nLG‚ÄØAI Research, K‚ÄëEXAONE, Hugging‚ÄØFace, vLLM, SGLang, llama.cpp, GPT‚ÄëOSS, Qwen, DeepSeek, NVIDIAs TensorRT‚ÄëLLM.",
        "html": "\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eModel \u0026amp; size\u003c/strong\u003e ‚Äì LG‚ÄØAI Research unveiled \u003cstrong\u003eK‚ÄëEXAONE‚ÄØ236B‚ÄëA23B\u003c/strong\u003e: a 236‚ÄØB‚Äëparameter Mixture‚Äëof‚ÄëExperts model with 23‚ÄØB active experts (128 total experts, 8 active).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitecture \u0026amp; efficiency\u003c/strong\u003e ‚Äì 48 main layers + 1 MTP layer, 6,144 hidden dimension, 3:1 hybrid attention (128‚Äëtoken sliding window) and Multi‚ÄëToken Prediction (MTP) for ~1.5√ó faster inference.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCapabilities \u0026amp; benchmarks\u003c/strong\u003e ‚Äì 256K context window, 150k‚Äëtoken vocab, 6‚Äëlanguage support. In reasoning mode it scores 83.8‚ÄØ% on MMLU‚ÄëPro (vs GPT‚ÄëOSS‚ÄØ117‚ÄØB‚Äôs‚ÄØ80.7‚ÄØ%) and excels in math, coding, agentic tool use, Korean and multilingual tasks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLicense \u0026amp; policy\u003c/strong\u003e ‚Äì Distributed under a proprietary \u003cstrong\u003eK‚ÄëEXAONE AI Model License Agreement\u003c/strong\u003e (no reverse engineering, mandatory derivative naming ‚ÄúK‚ÄëEXAONE‚Äù, Korean law/mandatory arbitration, user indemnification).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDeployment \u0026amp; API\u003c/strong\u003e ‚Äì Free public API until Jan‚ÄØ28‚ÄØ2026; compatible with Hugging‚ÄØFace Transformers, vLLM, SGLang, llama.cpp; 256K‚Äëcontext inference possible on 4‚ÄØH200 GPUs via vLLM.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters\u003c/strong\u003e\u003cbr\u003e\nK‚ÄëEXAONE provides a large, multilingual, and long‚Äëcontext MoE model that competes with top‚Äëtier open‚Äësource peers while offering commercial‚Äëgrade licensing and easy integration into popular frameworks.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response\u003c/strong\u003e\u003cbr\u003e\nUsers note the restrictive license but appreciate the competitive benchmarks; some comment on the model‚Äôs size vs performance and speculate on its fine‚Äëtuning origins.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities\u003c/strong\u003e\u003cbr\u003e\nLG‚ÄØAI Research, K‚ÄëEXAONE, Hugging‚ÄØFace, vLLM, SGLang, llama.cpp, GPT‚ÄëOSS, Qwen, DeepSeek, NVIDIAs TensorRT‚ÄëLLM.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352296144Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.98,
        "reason": "Comprehensive release of a 236B MoE LLM with detailed specs, benchmarks, and deployment guidance.",
        "processed_at": "2026-01-07T03:37:36.29776212Z"
      },
      "processed_at": "2026-01-07T02:57:08.337503894Z"
    },
    {
      "flow_id": "",
      "id": "1q5t0hr",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/",
      "title": "Building opensource Zero Server Code Intelligence Engine",
      "content": "Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. What all features would be useful, any integrations, cool ideas, etc?\n\nsite: [https://gitnexus.vercel.app/](https://gitnexus.vercel.app/)  \nrepo: [https://github.com/abhigyanpatwari/GitNexus](https://github.com/abhigyanpatwari/GitNexus)\n\nThis is the crux of how it works:  \nRepo parsed into Graph using AST -\u0026gt; Embeddings model running in browser creates the embeddings -\u0026gt; Everything is stored in a graph DB ( this also runs in browser through webassembly ) -\u0026gt; user sees UI visualization -\u0026gt; AI gets tools to query graph (cyfer query tool), semantic search, grep and node highlight.\n\nSo therefore we get a quick code intelligence engine that works fully client sided 100% private. Except the LLM provider there is no external data outlet. ( working on ollama support )\n\nWould really appreciate any cool ideas / inputs / etc.\n\nThis is what I m aiming for right now:\n\n1\u0026gt; Case 1 is quick way to chat with a repo, but then deepwiki is already there. But gitnexus has graph tools+ui so should be more accurate on audits and UI can help in visualize.\n\n2\u0026gt; Downstream potential usecase will be MCP server exposed from browser itself, windsurf / cursor, etc can use it to perform codebase wise audits, blast radius detection of code changes, etc.\n\n3\u0026gt; Another case might be since its fully private, devs having severe restrictions can use it with ollama or their own inference",
      "author": "DeathShot7777",
      "created_at": "2026-01-06T19:53:26Z",
      "comments": [
        {
          "id": "ny2lpup",
          "author": "codeninja",
          "content": "Being able to pull relevant code context for my problem use case is critical for me to be able to iterate quickly. So if we can query to get a list of relevant files for the \"update the user authentication workflow and integrate Auth0\" problem statement then that's the holy grail of contextual awareness.",
          "created_at": "2026-01-06T20:28:42Z",
          "was_summarised": false
        },
        {
          "id": "ny2g2ts",
          "author": "Main-Lifeguard-6739",
          "content": "would love to use this to inform my claude code agents as their standard goto source for looking up stuff. in combination with skills like \"analyze 3 hops into that direction\" or something like that. would also love to use something like that to track and see which agent is working on what. \n\nWhats your visualization engine used?",
          "created_at": "2026-01-06T20:02:26Z",
          "was_summarised": false
        },
        {
          "id": "ny2toc9",
          "author": "valkarias",
          "content": "I thought of a \"real-time\" graph AST for code for agents to work with, before. My main issue is the agent forgetting written code and logic across not-so-large-code-bases leading to duplicated logic and stuff. Currently I've to audit everything manually, or propagate the changes myself. Does the project allow for this? Granular function level context would be kinda awesome, with agent querying and stuff.",
          "created_at": "2026-01-06T21:05:21Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://v.redd.it/6xrs78taasbg1",
          "was_fetched": true,
          "page": "Title: \n\nURL Source: https://v.redd.it/6xrs78taasbg1\n\nWarning: Target URL returned error 403: Forbidden\n\nMarkdown Content:\nYou've been blocked by network security.\n\nTo continue, log in to your Reddit account or use your developer token\n\nIf you think you've been blocked by mistake, file a ticket below and we'll look into it.\n\n[Log in](https://www.reddit.com/login/)[File a ticket](https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140)",
          "was_summarised": false
        },
        {
          "url": "https://gitnexus.vercel.app/",
          "was_fetched": true,
          "page": "Title: GitNexus\n\nURL Source: https://gitnexus.vercel.app/\n\nMarkdown Content:\nDrop your codebase\n------------------\n\nDrag \u0026 drop a .zip file to generate a knowledge graph\n\n.zip",
          "was_summarised": false
        },
        {
          "url": "https://github.com/abhigyanpatwari/GitNexus",
          "was_fetched": true,
          "page": "Title: GitHub - abhigyanpatwari/GitNexus: GitNexus: The Zero-Server Code Intelligence Engine -       GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration\n\nURL Source: https://github.com/abhigyanpatwari/GitNexus\n\nMarkdown Content:\nGitNexus V2 - Client-Side Knowledge Graph Generator\n---------------------------------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#gitnexus-v2---client-side-knowledge-graph-generator)\n\u003e Privacy-focused, zero-server knowledge graph generator that runs entirely in your browser.\n\nTransform codebases into interactive knowledge graphs using AST parsing, Web Workers, and an embedded KuzuDB WASM database. All processing happens locally - your code never leaves your machine.\n\n**Next up:** Browser-based embeddings + Graph RAG. The cool part? KuzuDB supports native vector indexing, so I can do semantic search AND graph traversal in a single Cypher query. No separate vector DB needed. See [Work in Progress](https://github.com/abhigyanpatwari/GitNexus#-current-work-in-progress) for the full plan.\n\nGitnexusFinal.mp4\n\n* * *\n\nüöß Current Work in Progress\n---------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#-current-work-in-progress)\n**Actively Building:**\n\n*   **Graph RAG Agent** - AI chat with Cypher query generation for intelligent code exploration\n*   **Browser Embeddings** - Small embedding model for semantic node search (see below!)\n*   **Multi-Worker Pool** - Parallel parsing across multiple Web Workers (currently using single worker)\n*   **Ollama Support** - Local LLM integration\n*   **CSV Export** - Export node/relationship tables\n\n### üß† Graph RAG: The Plan\n\n[](https://github.com/abhigyanpatwari/GitNexus#-graph-rag-the-plan)\nHere's what I'm building for the AI layer. The goal: ask questions in plain English, get answers backed by actual graph traversal + semantic understanding.\n\n**The Problem:** A regular LLM doesn't know your codebase. It can't tell you what calls `handleAuth` or what breaks if you change `UserService`. I need to give it tools to explore the graph.\n\n**The Solution:** Combine embeddings (for \"find relevant code by meaning\") with graph queries (for \"trace connections\").\n\nflowchart TD\n    Q[Your Question] --\u003e EMB[Embed with transformers.js]\n    EMB --\u003e VS[Vector Search in KuzuDB]\n    VS --\u003e ENTRY[Entry Point Nodes]\n    ENTRY --\u003e EXPAND[Graph Traversal via Cypher]\n    EXPAND --\u003e CTX[Rich Context]\n    CTX --\u003e LLM[LLM Generates Answer]\n\n**Embedding Model:** I'm going with `snowflake-arctic-embed-xs` - a tiny 22M parameter model that runs entirely in the browser via [transformers.js](https://huggingface.co/docs/transformers.js). It outputs 384-dimensional vectors and scores 50.15 on MTEB (comparable to models 5x its size). The model downloads once (~90MB), gets cached, and runs locally forever. Privacy intact. ‚úÖ\n\n**The Pipeline:**\n\nflowchart LR\n    subgraph Main[\"Main Pipeline (Blocking)\"]\n        P1[Extract] --\u003e P2[Structure] --\u003e P3[Parse] --\u003e P4[Imports] --\u003e P5[Calls]\n    end\n    \n    P5 --\u003e READY[Graph Ready!\u003cbr/\u003eUser can explore]\n    READY --\u003e BG\n    \n    subgraph BG[\"Background (Non-blocking)\"]\n        E1[Load Model] --\u003e E2[Embed Nodes] --\u003e E3[Create Vector Index]\n    end\n    \n    E3 --\u003e AI[AI Search Ready!]\n\nThe idea: you can start exploring the graph immediately after Phase 5. Meanwhile, embeddings are generated in the background. Once done, semantic search unlocks.\n\n### üí° A Fun Discovery: Unified Vector + Graph = Superpowers\n\n[](https://github.com/abhigyanpatwari/GitNexus#-a-fun-discovery-unified-vector--graph--superpowers)\nWhile designing this, I stumbled onto something cool. Most Graph RAG systems use **separate databases** - a vector DB (Pinecone, Qdrant) for semantic search and a graph DB (Neo4j) for traversal. This means the LLM has to:\n\n1.   Call vector search ‚Üí get IDs\n2.   Take those IDs ‚Üí call graph DB\n3.   Coordinate between two systems\n\nBut KuzuDB WASM supports **native vector indexing** (HNSW). Which means it's possible to do vector search AND graph traversal **in a single Cypher query**:\n\n-- Find code similar to \"authentication\" AND trace what calls it\n-- ALL IN ONE QUERY! ü§Ø\nCALL QUERY_VECTOR_INDEX('CodeNode', 'embedding_idx', $queryVector, 10)\nWITH node AS match, distance\nWHERE distance \u003c 0.4\nMATCH (caller:CodeNode)-[r:CodeRelation {type: 'CALLS'}]-\u003e(match)\nRETURN match.name AS found, \n       caller.name AS called_by,\n       distance AS relevance\nORDER BY distance\n\nThis is kind of a big deal. Here's why:\n\n**Traditional approach (2 queries, 2 systems):**\n\n```\nsemantic_search(\"auth\") ‚Üí [\"id1\", \"id2\", \"id3\"]\n                              ‚Üì\ngraph_query(\"MATCH ... WHERE id IN [...]\") ‚Üí results\n```\n\n**Unified KuzuDB approach (1 query, 1 system):**\n\n```\ncypher(\"CALL QUERY_VECTOR_INDEX(...) WITH node MATCH (node)-[...]-\u003e() ...\") ‚Üí results\n```\n\nAnd because `distance` comes back with every result, this provides **built-in reranking for free**:\n\n-- The LLM can dynamically control relevance thresholds!\nCALL QUERY_VECTOR_INDEX('CodeNode', 'idx', $vec, 20)\nWITH node, distance,\n     CASE \n       WHEN distance \u003c 0.15 THEN 'exact_match'\n       WHEN distance \u003c 0.30 THEN 'highly_relevant'\n       ELSE 'related'\n     END AS tier\nWHERE distance \u003c 0.5\nMATCH (node)-[*1..2]-(context)\nRETURN node.name, tier, collect(context.name) AS related\nORDER BY distance\n\n**What this enables:**\n\n*   üéØ **Single query execution** - No round trips between systems\n*   üìä **Hierarchical relevance** - LLM sees exact matches vs related vs weak\n*   üå≥ **Weighted expansion** - Traverse further from better matches\n*   ‚ö° **Dynamic thresholds** - LLM adjusts `WHERE distance \u003c X` per question type\n*   üîÑ **No reranker needed** - Distance IS the relevance score\n\nBasically, the LLM gets to write one smart query that does semantic search, filters by relevance, expands via graph relationships, and returns ranked results. No separate reranker model, no vector DB API calls, no coordination logic. Just Cypher.\n\nStill wrapping my head around all the query patterns this unlocks, but I'm pretty excited about it.\n\n* * *\n\n‚ö° What's New in V2\n------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#-whats-new-in-v2)\nV2 is a major refactor focused on **performance** and **scalability**. Here's what changed and why it matters:\n\n### üé® Sigma.js Replaces D3.js (10,000+ nodes without breaking a sweat)\n\n[](https://github.com/abhigyanpatwari/GitNexus#-sigmajs-replaces-d3js-10000-nodes-without-breaking-a-sweat)\nV1 used D3.js force simulation which worked great for small graphs, but started choking around 2-3k nodes. The browser would freeze, fans would spin, and you'd be staring at a loading spinner.\n\n**V2 uses Sigma.js with WebGL rendering.** This means the GPU does the heavy lifting instead of JavaScript. I've tested graphs with 10k+ nodes and they render smoothly. Pan, zoom, click - all buttery smooth.\n\nThe layout algorithm also moved to **ForceAtlas2 running in a Web Worker**, so your UI stays responsive while the graph positions itself.\n\n### üóÇÔ∏è Dual HashMap Symbol Table (Goodbye Trie, Hello Speed)\n\n[](https://github.com/abhigyanpatwari/GitNexus#%EF%B8%8F-dual-hashmap-symbol-table-goodbye-trie-hello-speed)\nIn V1, I used a **Trie** (prefix tree) to store function/class definitions. It was clever - you could do fuzzy lookups and autocomplete. But it was also slow and memory-hungry for large codebases.\n\nV2 uses a simpler but faster **Dual HashMap** approach:\n\n```\nFile-Scoped Index:  Map\u003cFilePath, Map\u003cSymbolName, NodeID\u003e\u003e\nGlobal Index:       Map\u003cSymbolName, SymbolDefinition[]\u003e\n```\n\n**Why two maps?** When resolving a function call like `handleAuth()`, the system first checks if it's defined in a file that was imported (high confidence). If not, it checks the current file. As a last resort, it searches globally (useful for framework magic like FastAPI's `@app.get` decorators where the connection isn't explicit in imports).\n\nThis change alone provided a **~2x speedup** on the parsing phase.\n\n### üíæ LRU Cache for AST Trees (Memory That Cleans Itself)\n\n[](https://github.com/abhigyanpatwari/GitNexus#-lru-cache-for-ast-trees-memory-that-cleans-itself)\nTree-sitter generates AST (Abstract Syntax Tree) objects that live in WASM memory. In V1, I kept all of them around, which meant memory usage grew linearly with file count. Parse 5000 files? That's 5000 AST objects eating RAM.\n\nV2 uses an **LRU (Least Recently Used) cache** with a cap of 50 entries. When the system needs to parse file #51, the oldest unused AST gets evicted and `tree.delete()` is called to free the WASM memory.\n\nThe clever part: files are parsed in Phase 3, then those ASTs are reused in Phase 4 (imports) and Phase 5 (calls). The LRU cache keeps recently-parsed files hot, so re-parsing is rarely needed.\n\n### üìä Overall Results\n\n[](https://github.com/abhigyanpatwari/GitNexus#-overall-results)\n| Metric | V1 | V2 | Improvement |\n| --- | --- | --- | --- |\n| Max renderable nodes | ~3,000 | 10,000+ | ~3x+ |\n| Parse speed | Baseline | 3-5x faster | ‚ö° |\n| Memory usage | Grows unbounded | Capped by LRU | Stable |\n| UI responsiveness | Freezes during layout | Smooth (Web Worker) | ‚úÖ |\n\n**Note:** V2 currently uses a single Web Worker. Multi-worker support is planned and should give another 2-4x speedup on multi-core machines.\n\n* * *\n\nProject Focus\n-------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#project-focus)\n*   **Privacy-first**: Zero-cost, zero-server tool to create knowledge graphs from codebases entirely within the browser\n*   **Human + AI friendly**: Knowledge graphs useful for both manual exploration and AI agent context retrieval\n*   **Fast \u0026 cheap**: Browser-based indexing is faster and cheaper than embedding models + vector RAG\n*   **Understanding codebases**: Graph visualization + Graph RAG chatbot for accurate context retrieval\n\nAI Use Cases\n------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#ai-use-cases)\n*   **Blast radius analysis**: Compute impact of function/module changes, enumerate affected endpoints/tests\n*   **Fault isolation**: Start from a failing symbol, traverse callers/callees to isolate the fault line faster than grep or embeddings\n*   **Code health**: Detect orphaned nodes, unresolved imports, unused functions with simple graph queries\n*   **Auditing**: Spot forbidden dependencies or layer violations quickly during onboarding or security reviews\n\n* * *\n\nFeatures\n--------\n\n[](https://github.com/abhigyanpatwari/GitNexus#features)\n**Code Analysis**\n\n*   Analyze ZIP files containing codebases\n*   TypeScript, JavaScript, Python support\n*   Interactive WebGL graph visualization with Sigma.js\n*   Real-time Cypher queries against in-browser graph database\n\n**Processing**\n\n*   5-phase pipeline: Extract ‚Üí Structure ‚Üí Parsing ‚Üí Imports ‚Üí Calls\n*   Web Worker offloading (single worker, multi-worker planned)\n*   Tree-sitter WASM for AST parsing\n*   LRU cache with automatic WASM memory cleanup\n\n**Privacy**\n\n*   100% client-side - no server, no uploads\n*   API keys stored in localStorage only\n*   Open source and auditable\n\n* * *\n\nArchitecture\n------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#architecture)\n### V1 vs V2 Comparison\n\n[](https://github.com/abhigyanpatwari/GitNexus#v1-vs-v2-comparison)\n| Aspect | V1 | V2 |\n| --- | --- | --- |\n| Code Style | Class-based | Function-based (factory pattern) |\n| Symbol Lookup | Trie data structure | Dual HashMap (file-scoped + global) |\n| Visualization | D3.js force simulation | Sigma.js + WebGL + ForceAtlas2 |\n| Workers | Worker pool with Comlink | Single worker (multi-worker planned) |\n| AI Pipeline | LangChain ReAct agents | Not yet implemented (WIP) |\n| Layout | D3 force simulation (main thread) | ForceAtlas2 (Web Worker) |\n\n### System Overview\n\n[](https://github.com/abhigyanpatwari/GitNexus#system-overview)\n\ngraph TB\n    subgraph MainThread[Main Thread]\n        UI[React UI]\n        CTX[AppState Context]\n        SIGMA[Sigma.js WebGL]\n    end\n\n    subgraph WorkerThread[Web Worker]\n        PIPE[Ingestion Pipeline]\n        KUZU[KuzuDB WASM]\n        TS[Tree-sitter WASM]\n    end\n\n    UI --\u003e CTX\n    CTX --\u003e SIGMA\n    PIPE --\u003e TS\n    PIPE --\u003e KUZU\n    MainThread -.-\u003e WorkerThread\n\nThink of it like this: the main thread handles what you see (React UI, graph rendering), while the Web Worker does all the heavy computation (parsing, database queries) in the background. They communicate through Comlink, which makes calling worker functions feel like regular async calls.\n\n### Data Flow\n\n[](https://github.com/abhigyanpatwari/GitNexus#data-flow)\n\nflowchart LR\n    ZIP[ZIP File] --\u003e EXTRACT[Extract]\n    EXTRACT --\u003e STRUCT[Structure]\n    STRUCT --\u003e PARSE[Parse]\n    PARSE --\u003e IMPORT[Imports]\n    IMPORT --\u003e CALLS[Calls]\n    CALLS --\u003e GRAPH[Graph]\n    GRAPH --\u003e VIZ[Sigma.js]\n    GRAPH --\u003e KUZU[(KuzuDB)]\n\n* * *\n\n5-Phase Ingestion Pipeline\n--------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#5-phase-ingestion-pipeline)\nHere's what happens when you drop a ZIP file:\n\nflowchart TD\n    START([ZIP File]) --\u003e P1\n  \n    subgraph P1[Phase 1: Extract - 0-15%]\n        E1[Decompress ZIP]\n        E2[Collect file paths]\n    end\n  \n    subgraph P2[Phase 2: Structure - 15-30%]\n        S1[Build folder tree]\n        S2[Create CONTAINS edges]\n    end\n  \n    subgraph P3[Phase 3: Parsing - 30-70%]\n        PA1[Load Tree-sitter grammar]\n        PA2[Generate ASTs]\n        PA3[Extract symbols]\n        PA4[Populate Symbol Table]\n    end\n  \n    subgraph P4[Phase 4: Imports - 70-82%]\n        I1[Find import statements]\n        I2[Resolve paths]\n        I3[Create IMPORTS edges]\n    end\n  \n    subgraph P5[Phase 5: Calls - 82-100%]\n        C1[Find function calls]\n        C2[Resolve targets]\n        C3[Create CALLS edges]\n    end\n  \n    P1 --\u003e P2 --\u003e P3 --\u003e P4 --\u003e P5\n    P5 --\u003e DONE([Knowledge Graph Ready])\n\n### What Each Phase Does\n\n[](https://github.com/abhigyanpatwari/GitNexus#what-each-phase-does)\n**Phase 1: Extract** - JSZip is used to decompress your ZIP file and store all file contents in a Map. Simple but necessary.\n\n**Phase 2: Structure** - The system walks through all file paths and builds a tree of folders and files. A path like `src/components/Button.tsx` creates nodes for `src`, `components`, and `Button.tsx` with `CONTAINS` relationships connecting them.\n\n**Phase 3: Parsing** - This is where the magic happens. Tree-sitter parses each file into an AST, and extracts all the interesting bits: functions, classes, interfaces, methods. These get stored in the Symbol Table for later lookup.\n\n**Phase 4: Imports** - The pipeline finds all `import` and `require` statements and determines which files they point to. `import { foo } from './utils'` might resolve to `./utils.ts`, `./utils/index.ts`, etc. Common extensions are tried until a match is found.\n\n**Phase 5: Calls** - The trickiest phase. The pipeline finds all function calls and determines what they're calling. It uses a resolution strategy (import map ‚Üí local ‚Üí global) to link calls to their definitions.\n\n* * *\n\nSymbol Resolution: How We Link Function Calls\n---------------------------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#symbol-resolution-how-we-link-function-calls)\nWhen the system encounters code like this:\n\nimport { validateUser } from './auth';\n\nfunction login() {\n  validateUser(email, password);  // ‚Üê What does this call?\n}\n\nThe system needs to figure out that `validateUser()` refers to the function defined in `./auth.ts`. Here's the strategy:\n\nLoading\n\nflowchart TD\n    CALL[Found: validateUser] --\u003e CHECK1\n  \n    CHECK1{In Import Map?}\n    CHECK1 --\u003e|Yes| FOUND1[Check auth.ts symbols]\n    CHECK1 --\u003e|No| CHECK2\n  \n    CHECK2{In Current File?}\n    CHECK2 --\u003e|Yes| FOUND2[Use local definition]\n    CHECK2 --\u003e|No| CHECK3\n  \n    CHECK3{Global Search}\n    CHECK3 --\u003e|Found| FOUND3[Use first match]\n    CHECK3 --\u003e|Not Found| SKIP[Skip this call]\n  \n    FOUND1 --\u003e DONE[Create CALLS edge]\n    FOUND2 --\u003e DONE\n    FOUND3 --\u003e DONE\n\n**Why the global fallback?** Some frameworks use \"magic\" that doesn't show up in imports. For example, FastAPI:\n\n@app.get(\"/users\")\ndef get_users():\n    return db.query(User)  # Where does 'db' come from?\n\nThe `db` object might be injected by the framework, not explicitly imported. The global search catches these cases (with lower confidence).\n\n* * *\n\nLRU AST Cache\n-------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#lru-ast-cache)\nParsing files into ASTs is expensive, and AST objects live in WASM memory (which doesn't get garbage collected like regular JS objects). An LRU cache is used to keep memory bounded:\n\nLoading\n\nflowchart LR\n    subgraph Cache[LRU Cache - 50 slots]\n        HOT[Recently Used ASTs]\n        COLD[Oldest ASTs]\n    end\n  \n    NEW[New AST] --\u003e|set| HOT\n    COLD --\u003e|evicted| DELETE[tree.delete - frees WASM memory]\n  \n    REQUEST[Need AST] --\u003e|get| HOT\n\n**How it helps:**\n\n*   Phase 3 parses files and stores ASTs in cache\n*   Phase 4 \u0026 5 reuse cached ASTs (no re-parsing!)\n*   If cache is full, oldest AST is evicted and WASM memory is freed\n*   Result: Memory stays bounded even for huge codebases\n\n* * *\n\nGraph Visualization\n-------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#graph-visualization)\n### Sigma.js + ForceAtlas2\n\n[](https://github.com/abhigyanpatwari/GitNexus#sigmajs--forceatlas2)Loading\n\nflowchart LR\n    subgraph Main[Main Thread]\n        SIGMA[Sigma.js]\n        WEBGL[WebGL Canvas]\n    end\n  \n    subgraph Layout[Layout Worker]\n        FA2[ForceAtlas2]\n    end\n  \n    GRAPH[Graphology Graph] --\u003e FA2\n    FA2 --\u003e|positions| GRAPH\n    GRAPH --\u003e SIGMA\n    SIGMA --\u003e WEBGL\n\n**Why this combo works:**\n\n*   **Sigma.js** uses WebGL to render nodes/edges on the GPU - handles 10k+ nodes easily\n*   **ForceAtlas2** is a physics-based layout that runs in a Web Worker - UI stays responsive\n*   **Graphology** is the data structure holding the graph - fast lookups and updates\n\n**Visual features:**\n\n*   Nodes sized by type (folders bigger than files, files bigger than functions)\n*   Edges colored by relationship (green for CONTAINS, blue for IMPORTS, purple for CALLS)\n*   Click a node to highlight its connections\n*   Pan/zoom with mouse, reset view button\n\n* * *\n\nKuzuDB Integration\n------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#kuzudb-integration)\nThe graph is loaded into KuzuDB (an embedded graph database) so you can run Cypher queries:\n\nLoading\n\nflowchart TD\n    GRAPH[Knowledge Graph] --\u003e CSV[Generate CSV]\n    CSV --\u003e COPY[COPY FROM bulk load]\n    COPY --\u003e KUZU[(KuzuDB WASM)]\n    QUERY[Cypher Query] --\u003e KUZU\n    KUZU --\u003e RESULTS[Query Results]\n\n**Example queries you can run:**\n\n-- Find all functions in a file\nMATCH (f:CodeNode {label: 'File', name: 'App.tsx'})-[:CodeRelation]-\u003e(fn:CodeNode {label: 'Function'})\nRETURN fn.name\n\n-- Find what imports a specific file\nMATCH (f:CodeNode)-[r:CodeRelation {type: 'IMPORTS'}]-\u003e(target:CodeNode {name: 'utils.ts'})\nRETURN f.name\n\n**Status:**\n\n*   ‚úÖ KuzuDB WASM initialization\n*   ‚úÖ Polymorphic schema (single node/edge tables)\n*   ‚úÖ CSV generation and bulk loading\n*   ‚úÖ Cypher query execution\n*   üöß Vector embeddings + HNSW index (WIP)\n*   üöß Graph RAG agent (WIP)\n\n* * *\n\nTech Stack\n----------\n\n[](https://github.com/abhigyanpatwari/GitNexus#tech-stack)\n*   **Frontend**: React 18 + TypeScript + Vite + Tailwind CSS v4\n*   **Visualization**: Sigma.js + Graphology + ForceAtlas2 (WebGL)\n*   **Parsing**: Tree-sitter WASM (TypeScript, JavaScript, Python)\n*   **Database**: KuzuDB WASM (in-browser graph database + vector index)\n*   **Concurrency**: Web Worker + Comlink\n*   **Caching**: lru-cache with WASM memory management\n*   **AI (WIP)**: transformers.js for browser embeddings, LangChain for agent orchestration\n\n* * *\n\nGraph Schema\n------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#graph-schema)\n### Node Types\n\n[](https://github.com/abhigyanpatwari/GitNexus#node-types)\n| Label | Description | Example |\n| --- | --- | --- |\n| `Folder` | Directory in project | `src/components` |\n| `File` | Source code file | `App.tsx` |\n| `Function` | Function definition | `handleClick` |\n| `Class` | Class definition | `UserService` |\n| `Interface` | Interface definition | `Props` |\n| `Method` | Class method | `render` |\n\n### Relationship Types\n\n[](https://github.com/abhigyanpatwari/GitNexus#relationship-types)\n| Type | From | To | Description |\n| --- | --- | --- | --- |\n| `CONTAINS` | Folder | File/Folder | Directory structure |\n| `DEFINES` | File | Function/Class/etc. | Code definitions |\n| `IMPORTS` | File | File | Module dependencies |\n| `CALLS` | File | Function/Method | Function call graph |\n\n* * *\n\nGetting Started\n---------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#getting-started)\n**Prerequisites**: Node.js 18+\n\ngit clone \u003crepository-url\u003e\ncd gitnexus\nnpm install\nnpm run dev\n\nOpen [http://localhost:5173](http://localhost:5173/)\n\n**Usage:**\n\n1.   Drag \u0026 drop a ZIP file containing your codebase\n2.   Wait for the 5-phase pipeline to complete\n3.   Explore the interactive graph\n4.   Click nodes to view code, filter by type, adjust depth\n\n* * *\n\nPlanned: AI Features\n--------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#planned-ai-features)\n### Graph RAG Agent (WIP)\n\n[](https://github.com/abhigyanpatwari/GitNexus#graph-rag-agent-wip)\nThe idea: ask questions in plain English, get answers backed by graph queries + semantic understanding.\n\nLoading\n\nflowchart TD\n    USER[Your Question] --\u003e LLM[LLM]\n    LLM --\u003e |Generates| CYPHER[Unified Cypher Query]\n    \n    subgraph KUZU[KuzuDB WASM]\n        CYPHER --\u003e VEC[Vector Search]\n        VEC --\u003e GRAPH[Graph Traversal]\n        GRAPH --\u003e RANK[Ranked Results]\n    end\n    \n    RANK --\u003e CTX[Rich Context + Code Snippets]\n    CTX --\u003e LLM\n    LLM --\u003e ANSWER[Your Answer]\n\n**Example interactions:**\n\n*   \"What functions call `handleAuth`?\" ‚Üí Vector search finds `handleAuth`, Cypher traces callers\n*   \"Show me the blast radius if I change `UserService`\" ‚Üí Finds service, traverses 3 hops of dependencies\n*   \"How does authentication work in this codebase?\" ‚Üí Semantic search for auth-related code, returns connected components\n\n**Why dynamic Cypher generation?** Originally I planned to use pre-built query templates (because LLMs can be... creative with syntax). But with the unified vector + graph approach, the LLM just needs to learn one pattern:\n\nCALL QUERY_VECTOR_INDEX(...) WITH node, distance\nWHERE distance \u003c [threshold]\nMATCH (node)-[relationship pattern]-\u003e(connected)\nRETURN [what you need]\nORDER BY distance\n\nGive the LLM the schema, a few examples, and let it compose queries. The schema is simple enough that modern LLMs (GPT-4, Claude) handle it well. And if a query fails? The error message is usually clear enough for the LLM to self-correct.\n\n* * *\n\nüî¨ Deep Dive: Copy-on-Write Woes with In-Memory WASM Databases\n--------------------------------------------------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#-deep-dive-copy-on-write-woes-with-in-memory-wasm-databases)\nWhile building the embedding pipeline, I hit an interesting memory problem. Documenting it here because it's a non-obvious gotcha for anyone doing vector storage in browser-side databases.\n\n### The Setup\n\n[](https://github.com/abhigyanpatwari/GitNexus#the-setup)\nI wanted to store 384-dimensional embeddings alongside the code nodes. Natural instinct: add an `embedding FLOAT[384]` column to the existing `CodeNode` table, bulk load the graph, then `UPDATE` each node with its embedding.\n\n-- Seemed reasonable, right?\nMATCH (n:CodeNode {id: $id}) SET n.embedding = $vec\n\n### The Problem\n\n[](https://github.com/abhigyanpatwari/GitNexus#the-problem)\nWorked fine for ~20 nodes. Exploded at ~1000 nodes with:\n\n```\nBuffer manager exception: Unable to allocate memory! The buffer pool is full!\n```\n\nI configured a 512MB buffer pool. 1000 embeddings √ó 384 floats √ó 4 bytes = ~1.5MB. Where did 512MB go?\n\n**Answer: Copy-on-Write (COW).**\n\nMost databases don't modify records in place. When you `UPDATE`, they create a new version of the record (for transaction rollback, MVCC, etc.). The old version sticks around until commit.\n\nOur `CodeNode` table had a `content` field averaging ~2KB per node (code snippets). So each `UPDATE`:\n\n1.   Reads the entire node (~2KB)\n2.   Creates a new copy with the embedding (~3.5KB)\n3.   Keeps the old version around\n\nFor 1000 nodes: `1000 √ó 2KB (old) + 1000 √ó 3.5KB (new) = ~5.5MB`... but that's just user data. KuzuDB's internal structures (indexes, hash tables, page management) multiply this significantly. And since it's an in-memory database, the buffer pool IS the storage - there's no disk to spill to.\n\nLoading\n\nflowchart LR\n    subgraph Before[\"Before UPDATE\"]\n        N1[CodeNode\u003cbr/\u003eid + name + content\u003cbr/\u003e~2KB]\n    end\n    \n    subgraph During[\"During UPDATE (COW)\"]\n        N1_OLD[Old Version\u003cbr/\u003e~2KB]\n        N1_NEW[New Version\u003cbr/\u003e+ embedding\u003cbr/\u003e~3.5KB]\n    end\n    \n    subgraph Problem[\"√ó 1000 nodes\"]\n        BOOM[üí• Buffer Pool Exhausted]\n    end\n    \n    Before --\u003e During --\u003e Problem\n\n### The Fix: Separate Table Architecture\n\n[](https://github.com/abhigyanpatwari/GitNexus#the-fix-separate-table-architecture)\nDon't `UPDATE` wide tables. `INSERT` into a narrow one.\n\nLoading\n\nflowchart TD\n    subgraph Old[\"‚ùå Original Design\"]\n        CN1[CodeNode\u003cbr/\u003eid, name, content, embedding\u003cbr/\u003e~3.5KB per UPDATE copy]\n    end\n    \n    subgraph New[\"‚úÖ New Design\"]\n        CN2[CodeNode\u003cbr/\u003eid, name, content]\n        CE[CodeEmbedding\u003cbr/\u003enodeId, embedding\u003cbr/\u003e~1.5KB INSERT only]\n    end\n    \n    Old --\u003e|\"COW copies entire 2KB+ node\"| FAIL[Memory Explosion]\n    New --\u003e|\"INSERT into lightweight table\"| WIN[Works at scale]\n\nNow the process is:\n\n1.   Bulk load `CodeNode` (no embedding column)\n2.   `CREATE` rows in `CodeEmbedding` table (just `nodeId` + `embedding`)\n3.   Vector index lives on `CodeEmbedding`\n4.   Semantic search JOINs back to `CodeNode` for metadata\n\n**Trade-off:** Every semantic search needs a JOIN. But it's a primary key lookup (O(1)), so it's only ~1-5ms extra per query. Totally worth it to not explode at 1000 nodes.\n\n### Lessons Learned\n\n[](https://github.com/abhigyanpatwari/GitNexus#lessons-learned)\n1.   **In-memory WASM DBs have hard limits** - No disk spillover, buffer pool is everything\n2.   **COW amplifies record size** - That innocent `UPDATE` copies your whole row\n3.   **Normalize for bulk writes** - Especially for append-only data like embeddings\n4.   **Profile the pathological case** - 20 nodes worked, 1000 didn't. Always test at scale\n\nThis is one of those \"obvious in hindsight\" things. Most vector DB tutorials show single-table schemas because they're using databases with disk backing. In-browser WASM land plays by different rules.\n\n* * *\n\nSecurity \u0026 Privacy\n------------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#security--privacy)\n*   All processing happens in your browser\n*   No code uploaded to any server\n*   API keys stored in localStorage only\n*   Open source - audit the code yourself\n\n* * *\n\nDeployment\n----------\n\n[](https://github.com/abhigyanpatwari/GitNexus#deployment)\n\nnpm run build\nnpm run preview\n\nThe build outputs to `dist/` and can be served from any static hosting.\n\n* * *\n\nLicense\n-------\n\n[](https://github.com/abhigyanpatwari/GitNexus#license)\nMIT License\n\n* * *\n\nAcknowledgments\n---------------\n\n[](https://github.com/abhigyanpatwari/GitNexus#acknowledgments)\n*   [Tree-sitter](https://tree-sitter.github.io/) - AST parsing\n*   [KuzuDB](https://kuzudb.com/) - Embedded graph database\n*   [Sigma.js](https://www.sigmajs.org/) - WebGL graph rendering\n*   [Graphology](https://graphology.github.io/) - Graph data structure\n*   [code-graph-rag](https://github.com/vitali87/code-graph-rag) - Reference implementation",
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- **GitNexus** is an entirely client‚Äëside, open‚Äësource code intelligence engine that turns a zip of a codebase into an interactive knowledge graph using Tree‚Äësitter AST parsing and a KuzuDB WASM graph database.  \n- Embeddings are generated in‚Äëbrowser with the tiny `snowflake‚Äëarctic‚Äëembed‚Äëxs` (22‚ÄØM params, 384‚Äëdim vectors, ~90‚ÄØMB download) via `transformers.js`, enabling semantic search without any server.  \n- The graph supports a **unified vector‚Äë+‚Äëgraph query** (single Cypher query) that performs semantic similarity search, filters by distance, and expands via CALLS/IMPORTS relations‚Äîno separate vector DB or reranker needed.  \n- Visualization is powered by Sigma.js/WebGL and ForceAtlas2 in a Web Worker, allowing smooth rendering of 10k+ nodes and real‚Äëtime interaction.  \n- Planned features include a Graph‚ÄØRAG agent (LLM chat backed by Cypher generation) and local‚ÄëLLM integration (Ollama support), all while keeping code 100‚ÄØ% private.  \n- Community comments focus on contextual file‚Äëlookup for tasks like updating authentication flows and integrating the graph with Claude agents for audit and debugging.\n\n**Why it matters:**  \nGitNexus offers a privacy‚Äëfirst, zero‚Äëserver alternative to cloud‚Äëbased code intelligence, giving developers instant graph‚Äëbased exploration and AI‚Äëassisted debugging without exposing their source code.\n\n**Community response:**  \nUsers expressed interest in contextual queries for specific refactors and integration with Claude‚Äëbased agents for audit workflows.\n\n**Key entities:** GitNexus, KuzuDB, transformers.js, snowflake‚Äëarctic‚Äëembed‚Äëxs, Sigma.js, Tree‚Äësitter, Ollama, Claude, Graph‚ÄØRAG.",
        "html": "\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGitNexus\u003c/strong\u003e is an entirely client‚Äëside, open‚Äësource code intelligence engine that turns a zip of a codebase into an interactive knowledge graph using Tree‚Äësitter AST parsing and a KuzuDB WASM graph database.\u003c/li\u003e\n\u003cli\u003eEmbeddings are generated in‚Äëbrowser with the tiny \u003ccode\u003esnowflake‚Äëarctic‚Äëembed‚Äëxs\u003c/code\u003e (22‚ÄØM params, 384‚Äëdim vectors, ~90‚ÄØMB download) via \u003ccode\u003etransformers.js\u003c/code\u003e, enabling semantic search without any server.\u003c/li\u003e\n\u003cli\u003eThe graph supports a \u003cstrong\u003eunified vector‚Äë+‚Äëgraph query\u003c/strong\u003e (single Cypher query) that performs semantic similarity search, filters by distance, and expands via CALLS/IMPORTS relations‚Äîno separate vector DB or reranker needed.\u003c/li\u003e\n\u003cli\u003eVisualization is powered by Sigma.js/WebGL and ForceAtlas2 in a Web Worker, allowing smooth rendering of 10k+ nodes and real‚Äëtime interaction.\u003c/li\u003e\n\u003cli\u003ePlanned features include a Graph‚ÄØRAG agent (LLM chat backed by Cypher generation) and local‚ÄëLLM integration (Ollama support), all while keeping code 100‚ÄØ% private.\u003c/li\u003e\n\u003cli\u003eCommunity comments focus on contextual file‚Äëlookup for tasks like updating authentication flows and integrating the graph with Claude agents for audit and debugging.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u003cbr\u003e\nGitNexus offers a privacy‚Äëfirst, zero‚Äëserver alternative to cloud‚Äëbased code intelligence, giving developers instant graph‚Äëbased exploration and AI‚Äëassisted debugging without exposing their source code.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e\u003cbr\u003e\nUsers expressed interest in contextual queries for specific refactors and integration with Claude‚Äëbased agents for audit workflows.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e GitNexus, KuzuDB, transformers.js, snowflake‚Äëarctic‚Äëembed‚Äëxs, Sigma.js, Tree‚Äësitter, Ollama, Claude, Graph‚ÄØRAG.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352415984Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.95,
        "reason": "Detailed description of client‚Äëside code graph engine using AST, KuzuDB WASM, transformers.js embeddings, graph‚ÄëRAG, LRU caching, and performance metrics for AI‚Äëpowered code exploration.",
        "processed_at": "2026-01-07T03:37:43.825316864Z"
      },
      "processed_at": "2026-01-07T02:57:25.014167352Z"
    },
    {
      "flow_id": "",
      "id": "1q5gii4",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/",
      "title": "DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available",
      "content": "It runs on regular llama.cpp builds (no extra support for DeepSeek V3.2 is needed).\n\nOnly Q8\\_0 and Q4\\_K\\_M are available.\n\nUse DeepSeek V3.2 Exp jinja template saved to a file to run this model by passing options: `--jinja --chat-template-file ds32-exp.jinja`\n\nHere's the template I used in my tests: [https://pastebin.com/4cUXvv35](https://pastebin.com/4cUXvv35)\n\nNote that tool calls will most likely not work with this template - they are different between DS 3.2-Exp and DS 3.2.\n\nI ran [lineage-bench](https://github.com/fairydreaming/lineage-bench) on Q4\\_K\\_M quant deployed in llama-server (40 prompts per each difficulty level), results:\n\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | deepseek/deepseek-v3.2 |     0.988 |       1.000 |        1.000 |         1.000 |         0.950 |\n\nThe model got only 2 answers wrong with most difficult graph size (192). It looks like it performed even a bit better than the original DeepSeek V3.2 with sparse attention tested via API:\n\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | deepseek/deepseek-v3.2 |     0.956 |       1.000 |        1.000 |         0.975 |         0.850 |\n\nFrom my testing so far disabling sparse attention does not hurt the model intelligence.\n\nEnjoy!\n\nEdit: **s/lightning attention/lightning indexer/**",
      "author": "fairydreaming",
      "created_at": "2026-01-06T11:50:35Z",
      "comments": [
        {
          "id": "ny0euil",
          "author": "woahdudee2a",
          "content": "what's the generation speed like? compared to original v3",
          "created_at": "2026-01-06T14:23:36Z",
          "was_summarised": false
        },
        {
          "id": "ny11vff",
          "author": "shark8866",
          "content": "if dense attention doesn't perform better, then what is the point of using it?",
          "created_at": "2026-01-06T16:15:34Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/sszymczyk/DeepSeek-V3.2-nolight-GGUF",
          "was_fetched": true,
          "page": "Title: sszymczyk/DeepSeek-V3.2-nolight-GGUF ¬∑ Hugging Face\n\nURL Source: https://huggingface.co/sszymczyk/DeepSeek-V3.2-nolight-GGUF\n\nMarkdown Content:\nThis repo contains Q8_0 and Q4_K_M quants of DeepSeek V3.2 with removed sparse attention lightning indexer tensors. This allows to run the model in llama.cpp until the sparse attention is implemented.\n\nI found no degradation in the model \"intelligence\" after removing lightning indexer. Q4_K_M quant was tested in lineage-bench ([https://github.com/fairydreaming/lineage-bench](https://github.com/fairydreaming/lineage-bench)):\n\n| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | deepseek/deepseek-v3.2 | 0.988 | 1.000 | 1.000 | 1.000 | 0.950 |\n\nThe model solved almost all quizzes correctly (40 quizzes per each difficulty level, 160 overall). It made only 2 errors in lineage graphs of 192 nodes (most difficult quizzes). Here's the score of the original DeepSeek V3.2 model tested via OpenRouter (with deepseek provider)\n\n| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | deepseek/deepseek-v3.2 | 0.956 | 1.000 | 1.000 | 0.975 | 0.850 |\n\nTo use the model save DeepSeek V3.2-Exp chat template to a file (you can save it from [https://pastebin.com/4cUXvv35](https://pastebin.com/4cUXvv35)) and pass `--jinja --chat-template-file \u003csaved-chat-template-file\u003e` when running llama-cli or llama-server.\n\nNote that tool calls will likely not work correctly with this template.",
          "was_summarised": false
        },
        {
          "url": "https://pastebin.com/4cUXvv35",
          "was_fetched": true,
          "page": "Title: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = fals - Pastebin.com\n\nURL Source: https://pastebin.com/4cUXvv35\n\nMarkdown Content:\nUntitled\n--------\n\na guest\n\nJan 6th, 2026\n\n62\n\n0\n\nNever\n\n**Not a member of Pastebin yet?**[**Sign Up**](https://pastebin.com/signup), it unlocks many cool features!\n\n1.   {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{% set ns.is_only_sys = true %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'\u003cÔΩúUserÔΩú\u003e' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- if ns.is_last_user or ns.is_only_sys %}{{'\u003cÔΩúAssistantÔΩú\u003e\u003c/think\u003e'}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'\u003cÔΩútool‚ñÅcalls‚ñÅbeginÔΩú\u003e\u003cÔΩútool‚ñÅcall‚ñÅbeginÔΩú\u003e'+ tool['function']['name'] + '\u003cÔΩútool‚ñÅsepÔΩú\u003e' + tool['function']['arguments'] + '\u003cÔΩútool‚ñÅcall‚ñÅendÔΩú\u003e'}}{%- else %}{{message['content'] + '\u003cÔΩútool‚ñÅcalls‚ñÅbeginÔΩú\u003e\u003cÔΩútool‚ñÅcall‚ñÅbeginÔΩú\u003e' + tool['function']['name'] + '\u003cÔΩútool‚ñÅsepÔΩú\u003e' + tool['function']['arguments'] + '\u003cÔΩútool‚ñÅcall‚ñÅendÔΩú\u003e'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\u003cÔΩútool‚ñÅcall‚ñÅbeginÔΩú\u003e'+ tool['function']['name'] + '\u003cÔΩútool‚ñÅsepÔΩú\u003e' + tool['function']['arguments'] + '\u003cÔΩútool‚ñÅcall‚ñÅendÔΩú\u003e'}}{%- endif %}{%- endfor %}{{'\u003cÔΩútool‚ñÅcalls‚ñÅendÔΩú\u003e\u003cÔΩúend‚ñÅof‚ñÅsentenceÔΩú\u003e'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- if ns.is_last_user %}{{'\u003cÔΩúAssistantÔΩú\u003e'}}{%- if message['prefix'] is defined and message['prefix'] and thinking %}{{'\u003cthink\u003e'}}{%- else %}{{'\u003c/think\u003e'}}{%- endif %}{%- endif %}{%- if message['prefix'] is defined and message['prefix'] %}{%- set ns.is_prefix = true -%}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message['content'] + '\u003cÔΩúend‚ñÅof‚ñÅsentenceÔΩú\u003e'}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message['content'] -%}{%- if '\u003c/think\u003e' in content %}{%- set content = content.split('\u003c/think\u003e', 1)[1] -%}{%- endif %}{{content + '\u003cÔΩúend‚ñÅof‚ñÅsentenceÔΩú\u003e'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{'\u003cÔΩútool‚ñÅoutput‚ñÅbeginÔΩú\u003e' + message['content'] + '\u003cÔΩútool‚ñÅoutput‚ñÅendÔΩú\u003e'}}{%- endif %}{%- if message['role'] != 'system' %}{% set ns.is_only_sys = false %}{%- endif %}{%- endfor -%}{% if add_generation_prompt and not ns.is_tool%}{% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}{{'\u003cÔΩúAssistantÔΩú\u003e'}}{%- if not thinking %}{{'\u003c/think\u003e'}}{%- else %}{{'\u003cthink\u003e'}}{%- endif %}{% endif %}{% endif %}",
          "was_summarised": false
        },
        {
          "url": "https://github.com/fairydreaming/lineage-bench",
          "was_fetched": true,
          "page": "Title: GitHub - fairydreaming/lineage-bench: Testing LLM reasoning abilities with lineage relationship quizzes.\n\nURL Source: https://github.com/fairydreaming/lineage-bench\n\nMarkdown Content:\nGitHub - fairydreaming/lineage-bench: Testing LLM reasoning abilities with lineage relationship quizzes.\n===============\n\n[Skip to content](https://github.com/fairydreaming/lineage-bench#start-of-content)\nNavigation Menu\n---------------\n\nToggle navigation\n\n[](https://github.com/)\n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ffairydreaming%2Flineage-bench)\n\nAppearance settings\n\n*   \nPlatform\n\n    *   \nAI CODE CREATION\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\n\n    *   \nDEVELOPER WORKFLOWS\n        *   [Actions Automate any workflow](https://github.com/features/actions)\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\n        *   [Issues Plan and track work](https://github.com/features/issues)\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\n\n    *   \nAPPLICATION SECURITY\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\n\n    *   \nEXPLORE\n        *   [Why GitHub](https://github.com/why-github)\n        *   [Documentation](https://docs.github.com/)\n        *   [Blog](https://github.blog/)\n        *   [Changelog](https://github.blog/changelog)\n        *   [Marketplace](https://github.com/marketplace)\n\n[View all features](https://github.com/features)\n\n*   \nSolutions\n\n    *   \nBY COMPANY SIZE\n        *   [Enterprises](https://github.com/enterprise)\n        *   [Small and medium teams](https://github.com/team)\n        *   [Startups](https://github.com/enterprise/startups)\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\n\n    *   \nBY USE CASE\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\n        *   [DevOps](https://github.com/solutions/use-case/devops)\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\n        *   [View all use cases](https://github.com/solutions/use-case)\n\n    *   \nBY INDUSTRY\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\n        *   [Government](https://github.com/solutions/industry/government)\n        *   [View all industries](https://github.com/solutions/industry)\n\n[View all solutions](https://github.com/solutions)\n\n*   \nResources\n\n    *   \nEXPLORE BY TOPIC\n        *   [AI](https://github.com/resources/articles?topic=ai)\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\n        *   [Security](https://github.com/resources/articles?topic=security)\n        *   [View all topics](https://github.com/resources/articles)\n\n    *   \nEXPLORE BY TYPE\n        *   [Customer stories](https://github.com/customer-stories)\n        *   [Events \u0026 webinars](https://github.com/resources/events)\n        *   [Ebooks \u0026 reports](https://github.com/resources/whitepapers)\n        *   [Business insights](https://github.com/solutions/executive-insights)\n        *   [GitHub Skills](https://skills.github.com/)\n\n    *   \nSUPPORT \u0026 SERVICES\n        *   [Documentation](https://docs.github.com/)\n        *   [Customer support](https://support.github.com/)\n        *   [Community forum](https://github.com/orgs/community/discussions)\n        *   [Trust center](https://github.com/trust-center)\n        *   [Partners](https://github.com/partners)\n\n*   \nOpen Source\n\n    *   \nCOMMUNITY\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\n\n    *   \nPROGRAMS\n        *   [Security Lab](https://securitylab.github.com/)\n        *   [Maintainer Community](https://maintainers.github.com/)\n        *   [Accelerator](https://github.com/accelerator)\n        *   [Archive Program](https://archiveprogram.github.com/)\n\n    *   \nREPOSITORIES\n        *   [Topics](https://github.com/topics)\n        *   [Trending](https://github.com/trending)\n        *   [Collections](https://github.com/collections)\n\n*   \nEnterprise\n\n    *   \nENTERPRISE SOLUTIONS\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\n\n    *   \nAVAILABLE ADD-ONS\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\n\n*   [Pricing](https://github.com/pricing)\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n==========================================================\n\n Search  \n\nClear\n\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n\nProvide feedback\n================\n\nWe read every piece of feedback, and take your input very seriously.\n\n- [x] Include my email address so I can be contacted \n\n Cancel  Submit feedback \n\nSaved searches\n==============\n\nUse saved searches to filter your results more quickly\n------------------------------------------------------\n\nName \n\nQuery \n\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\n\n Cancel  Create saved search \n\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ffairydreaming%2Flineage-bench)\n\n[Sign up](https://github.com/signup?ref_cta=Sign+up\u0026ref_loc=header+logged+out\u0026ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E\u0026source=header-repo\u0026source_repo=fairydreaming%2Flineage-bench)\n\nAppearance settings\n\nResetting focus\n\nYou signed in with another tab or window. [Reload](https://github.com/fairydreaming/lineage-bench) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/fairydreaming/lineage-bench) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/fairydreaming/lineage-bench) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[fairydreaming](https://github.com/fairydreaming)/**[lineage-bench](https://github.com/fairydreaming/lineage-bench)**Public\n\n*   [Notifications](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)You must be signed in to change notification settings\n*   [Fork 8](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)\n*   [Star 35](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench) \n\nTesting LLM reasoning abilities with lineage relationship quizzes.\n\n### License\n\n[MIT license](https://github.com/fairydreaming/lineage-bench/blob/main/LICENSE)\n\n[35 stars](https://github.com/fairydreaming/lineage-bench/stargazers)[8 forks](https://github.com/fairydreaming/lineage-bench/forks)[Branches](https://github.com/fairydreaming/lineage-bench/branches)[Tags](https://github.com/fairydreaming/lineage-bench/tags)[Activity](https://github.com/fairydreaming/lineage-bench/activity)\n\n[Star](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)\n\n[Notifications](https://github.com/login?return_to=%2Ffairydreaming%2Flineage-bench)You must be signed in to change notification settings\n\n*   [Code](https://github.com/fairydreaming/lineage-bench)\n*   [Issues 0](https://github.com/fairydreaming/lineage-bench/issues)\n*   [Pull requests 0](https://github.com/fairydreaming/lineage-bench/pulls)\n*   [Actions](https://github.com/fairydreaming/lineage-bench/actions)\n*   [Projects 0](https://github.com/fairydreaming/lineage-bench/projects)\n*   [Security### Uh oh! There was an error while loading. [Please reload this page](https://github.com/fairydreaming/lineage-bench).](https://github.com/fairydreaming/lineage-bench/security)\n*   [Insights](https://github.com/fairydreaming/lineage-bench/pulse)\n\nAdditional navigation options\n\n*   [Code](https://github.com/fairydreaming/lineage-bench)\n*   [Issues](https://github.com/fairydreaming/lineage-bench/issues)\n*   [Pull requests](https://github.com/fairydreaming/lineage-bench/pulls)\n*   [Actions](https://github.com/fairydreaming/lineage-bench/actions)\n*   [Projects](https://github.com/fairydreaming/lineage-bench/projects)\n*   [Security](https://github.com/fairydreaming/lineage-bench/security)\n*   [Insights](https://github.com/fairydreaming/lineage-bench/pulse)\n\nfairydreaming/lineage-bench\n===========================\n\nmain\n\n[Branches](https://github.com/fairydreaming/lineage-bench/branches)[Tags](https://github.com/fairydreaming/lineage-bench/tags)\n\n[](https://github.com/fairydreaming/lineage-bench/branches)[](https://github.com/fairydreaming/lineage-bench/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\nFolders and files\n-----------------\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| Latest commit ------------- History ------- [93 Commits](https://github.com/fairydreaming/lineage-bench/commits/main/) [](https://github.com/fairydreaming/lineage-bench/commits/main/) |\n| [results](https://github.com/fairydreaming/lineage-bench/tree/main/results \"results\") | [results](https://github.com/fairydreaming/lineage-bench/tree/main/results \"results\") |  |  |\n| [LICENSE](https://github.com/fairydreaming/lineage-bench/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/fairydreaming/lineage-bench/blob/main/LICENSE \"LICENSE\") |  |  |\n| [README.md](https://github.com/fairydreaming/lineage-bench/blob/main/README.md \"README.md\") | [README.md](https://github.com/fairydreaming/lineage-bench/blob/main/README.md \"README.md\") |  |  |\n| [compute_metrics.py](https://github.com/fairydreaming/lineage-bench/blob/main/compute_metrics.py \"compute_metrics.py\") | [compute_metrics.py](https://github.com/fairydreaming/lineage-bench/blob/main/compute_metrics.py \"compute_metrics.py\") |  |  |\n| [lineage_bench.py](https://github.com/fairydreaming/lineage-bench/blob/main/lineage_bench.py \"lineage_bench.py\") | [lineage_bench.py](https://github.com/fairydreaming/lineage-bench/blob/main/lineage_bench.py \"lineage_bench.py\") |  |  |\n| [plot_line.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_line.py \"plot_line.py\") | [plot_line.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_line.py \"plot_line.py\") |  |  |\n| [plot_stacked.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_stacked.py \"plot_stacked.py\") | [plot_stacked.py](https://github.com/fairydreaming/lineage-bench/blob/main/plot_stacked.py \"plot_stacked.py\") |  |  |\n| [requirements.txt](https://github.com/fairydreaming/lineage-bench/blob/main/requirements.txt \"requirements.txt\") | [requirements.txt](https://github.com/fairydreaming/lineage-bench/blob/main/requirements.txt \"requirements.txt\") |  |  |\n| [run_openrouter.py](https://github.com/fairydreaming/lineage-bench/blob/main/run_openrouter.py \"run_openrouter.py\") | [run_openrouter.py](https://github.com/fairydreaming/lineage-bench/blob/main/run_openrouter.py \"run_openrouter.py\") |  |  |\n| View all files |\n\nRepository files navigation\n---------------------------\n\n*   [README](https://github.com/fairydreaming/lineage-bench#)\n*   [MIT license](https://github.com/fairydreaming/lineage-bench#)\n\nlineage-bench\n=============\n\n[](https://github.com/fairydreaming/lineage-bench#lineage-bench)\n\nTesting LLM reasoning abilities with lineage relationship quizzes.\n\nThe project is a successor of the [farel-bench](https://github.com/fairydreaming/farel-bench) benchmark.\n\n**Note: GPT 5.2 (reasoning effort medium and high) seems to perform significantly worse in my benchmark compared to its predecessor.**\n\nChangelog\n---------\n\n[](https://github.com/fairydreaming/lineage-bench#changelog)\n\n*   2025-12-18 - Added results for some legacy models (gemini-2.5-flash, gemini-2.5-pro), for high reasoning effort (gpt-5.1, gpt-5.2) and other recently released models (nemotron-3-nano-30b-a3b, doubao-seed-1-8, gemini-3-flash-preview, mimo-v2-flash, ministral-14b-2512). Stacked results plot shows only top 30 scores now.\n*   2025-12-03 - Updated results for ring-1t model. Added results for seed-oss-36b-instruct (courtesy of [@mokieli](https://github.com/mokieli)).\n*   2025-12-01 - Added results for ring-1t, deepseek-r1-0528, glm-4.5-air, glm-4.5, intellect-3, ernie-5.0-thinking-preview, deepseek-v3.2 and deepseek-v3.2-speciale. Updated results for glm-4.6 (works better with lower temperature). Results for ring-1t are not final (problems with model provider).\n*   2025-11-25 - Added results for gpt-5.1, claude-opus-4.5, grok-4.1-fast and o4-mini.\n*   2025-11-23 - Added results for qwen3-32b, o3-mini and o3 models.\n*   2025-11-22 - Updated results to include recently released models, but only with 40 quizzes per problem size to reduce costs. Extended range of problem lengths to increase difficulty. Added file-based caching of model requests and responses.\n*   2025-03-07 - Added results for qwq-32b (used Parasail provider with 0.01 temp, observed some infinite loop generations, but mostly for lineage-64 where the model performs bad anyway).\n*   2025-03-04 - Updated results for perplexity/r1-1776. (apparently there was a problem with the model serving stack, that's why r1-1776 initially performed worse than expected)\n*   2025-02-26 - Added results for claude-3.7-sonnet (also with :thinking) and r1-1776\n*   2025-02-20 - Updated results for deepseek/deepseek-r1-distill-llama-70b. (used Groq provider with 0.5 temperature)\n*   2025-02-18 - Added results for kimi-k1.5-preview and llama-3.1-tulu-3-405b.\n*   2025-02-06 - Added results for o1, o3-mini, qwen-max, gemini-exp-1206, deepseek-r1-distill-qwen-14b and deepseek-r1-distill-qwen-32b.\n*   2025-01-24 - Added results for deepseek-r1-distill-llama-70b.\n*   2025-01-20 - Added results for deepseek-r1.\n*   2025-01-15 - Added results for deepseek-v3, gemini-2.0-flash-exp, gemini-2.0-flash-thinking-exp-1219 and minimax-01.\n\nResults\n-------\n\n[](https://github.com/fairydreaming/lineage-bench#results)\n\n### Plot\n\n[](https://github.com/fairydreaming/lineage-bench#plot)\n\n#### Current results\n\n[](https://github.com/fairydreaming/lineage-bench#current-results)\n\nThe plot below shows only the 30 top-performing models. See the table for all results.\n\n[](https://private-user-images.githubusercontent.com/166155368/528251497-571f2dcb-534c-446d-ba82-e307265607c7.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc3NTQ5NjMsIm5iZiI6MTc2Nzc1NDY2MywicGF0aCI6Ii8xNjYxNTUzNjgvNTI4MjUxNDk3LTU3MWYyZGNiLTUzNGMtNDQ2ZC1iYTgyLWUzMDcyNjU2MDdjNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwN1QwMjU3NDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kN2RmNTIzNGYwZDdlZWUwODhlODQ0OGYwODk4MGE1Yzk1MTYzNTM2ZDk4N2I0MjRiNzVmZmI4MzcwNjhiYTJiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.AEVas-O-38tCgqf1Igfuud6kjiksrRG5I3_GMMz8k7Q)\n\n#### Old results\n\n[](https://github.com/fairydreaming/lineage-bench#old-results)\n\n[](https://private-user-images.githubusercontent.com/166155368/420495024-559e686c-ce1e-4c9d-851e-1d9e2eb6f6b1.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njc3NTQ5NjMsIm5iZiI6MTc2Nzc1NDY2MywicGF0aCI6Ii8xNjYxNTUzNjgvNDIwNDk1MDI0LTU1OWU2ODZjLWNlMWUtNGM5ZC04NTFlLTFkOWUyZWI2ZjZiMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMTA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDEwN1QwMjU3NDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02YzljOTVkZGExN2EzMDY3Mzk3NjZiM2U5MGZlZmU5ZDY0ODgzOGNiYmZlZTk3N2U0NjUwMWNkMTZmNWQ3ZjJlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.bDuaZWco2KTUAKmuZmkZZnMK1KHH2AlC04jV0m9sDnY)\n\n### Table\n\n[](https://github.com/fairydreaming/lineage-bench#table)\n\nThe table below presents the benchmark results. If not explicitly stated default medium reasoning effort was used during benchmark.\n\n| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 |\n| ---: | :--- | ---: | ---: | ---: | ---: | ---: |\n| 1 | deepseek/deepseek-v3.2-speciale | 0.994 | 1.000 | 1.000 | 1.000 | 0.975 |\n| 2 | openai/gpt-5.1 (high) | 0.969 | 1.000 | 0.975 | 0.975 | 0.925 |\n| 2 | google/gemini-3-pro-preview | 0.969 | 1.000 | 1.000 | 0.925 | 0.950 |\n| 4 | deepseek/deepseek-v3.2 | 0.956 | 1.000 | 1.000 | 0.975 | 0.850 |\n| 5 | anthropic/claude-sonnet-4.5 | 0.944 | 0.975 | 0.975 | 0.900 | 0.925 |\n| 6 | google/gemini-2.5-pro | 0.925 | 1.000 | 0.900 | 0.900 | 0.900 |\n| 7 | openai/gpt-5.1 (medium) | 0.888 | 1.000 | 0.950 | 0.875 | 0.725 |\n| 8 | google/gemini-3-flash-preview | 0.881 | 1.000 | 0.975 | 0.875 | 0.675 |\n| 9 | qwen/qwen3-max | 0.869 | 1.000 | 0.800 | 0.900 | 0.775 |\n| 10 | x-ai/grok-4 (medium) | 0.869 | 1.000 | 0.950 | 0.900 | 0.625 |\n| 10 | x-ai/grok-4-fast (medium) | 0.869 | 1.000 | 0.925 | 0.900 | 0.650 |\n| 10 | anthropic/claude-opus-4.5 (medium) | 0.869 | 1.000 | 0.950 | 0.900 | 0.625 |\n| 13 | qwen/qwen3-235b-a22b-thinking-2507 | 0.856 | 0.900 | 0.875 | 0.850 | 0.800 |\n| 14 | inclusionai/ring-1t | 0.819 | 0.875 | 0.975 | 0.800 | 0.625 |\n| 15 | deepseek/deepseek-v3.1-terminus | 0.812 | 0.975 | 0.900 | 0.700 | 0.675 |\n| 16 | openai/o3 (medium) | 0.800 | 1.000 | 0.925 | 0.800 | 0.475 |\n| 17 | deepseek/deepseek-v3.2-exp | 0.794 | 0.975 | 0.900 | 0.700 | 0.600 |\n| 18 | anthropic/claude-haiku-4.5 | 0.794 | 0.975 | 0.925 | 0.575 | 0.700 |\n| 19 | openai/gpt-5 (medium) | 0.788 | 1.000 | 0.975 | 0.850 | 0.325 |\n| 20 | deepseek/deepseek-r1-0528 | 0.787 | 1.000 | 0.975 | 0.650 | 0.525 |\n| 21 | bytedance/seed-oss-36b-instruct | 0.769 | 1.000 | 0.850 | 0.750 | 0.475 |\n| 22 | deepcogito/cogito-v2.1-671b | 0.756 | 0.975 | 0.800 | 0.650 | 0.600 |\n| 23 | x-ai/grok-4.1-fast (medium) | 0.750 | 1.000 | 0.900 | 0.800 | 0.300 |\n| 24 | baidu/ernie-5.0-thinking-preview | 0.719 | 1.000 | 0.850 | 0.650 | 0.375 |\n| 25 | z-ai/glm-4.5 | 0.700 | 1.000 | 0.775 | 0.625 | 0.400 |\n| 26 | z-ai/glm-4.6 | 0.644 | 0.925 | 0.725 | 0.525 | 0.400 |\n| 27 | xiaomi/mimo-v2-flash | 0.600 | 1.000 | 0.900 | 0.425 | 0.075 |\n| 28 | z-ai/glm-4.5-air | 0.594 | 1.000 | 0.750 | 0.450 | 0.175 |\n| 28 | prime-intellect/intellect-3 | 0.594 | 1.000 | 0.950 | 0.325 | 0.100 |\n| 30 | qwen/qwen3-next-80b-a3b-thinking | 0.575 | 0.950 | 0.700 | 0.425 | 0.225 |\n| 31 | google/gemini-2.5-flash | 0.569 | 0.975 | 0.575 | 0.525 | 0.200 |\n| 32 | minimax/minimax-m2 | 0.562 | 0.975 | 0.700 | 0.350 | 0.225 |\n| 33 | openai/gpt-oss-120b | 0.544 | 1.000 | 0.825 | 0.325 | 0.025 |\n| 34 | amazon/nova-2-lite-v1 | 0.525 | 1.000 | 0.700 | 0.325 | 0.075 |\n| 34 | openai/o4-mini (medium) | 0.525 | 1.000 | 0.775 | 0.300 | 0.025 |\n| 34 | moonshotai/kimi-k2-thinking | 0.525 | 1.000 | 0.850 | 0.200 | 0.050 |\n| 37 | volcengine/doubao-seed-1.8 | 0.512 | 1.000 | 0.925 | 0.125 | 0.000 |\n| 37 | openai/gpt-5-mini (medium) | 0.512 | 1.000 | 0.950 | 0.075 | 0.025 |\n| 39 | qwen/qwen3-30b-a3b-thinking-2507 | 0.494 | 1.000 | 0.575 | 0.275 | 0.125 |\n| 39 | openai/gpt-5.2 (high) | 0.494 | 1.000 | 0.700 | 0.175 | 0.100 |\n| 41 | openai/gpt-5.2 (medium) | 0.450 | 1.000 | 0.675 | 0.075 | 0.050 |\n| 42 | allenai/olmo-3-32b-think | 0.444 | 0.925 | 0.600 | 0.175 | 0.075 |\n| 43 | mistralai/ministral-14b-2512 | 0.400 | 0.875 | 0.425 | 0.175 | 0.125 |\n| 44 | qwen/qwen3-32b | 0.362 | 0.950 | 0.475 | 0.025 | 0.000 |\n| 45 | openai/gpt-5-nano (medium) | 0.294 | 1.000 | 0.150 | 0.025 | 0.000 |\n| 46 | openai/o3-mini (medium) | 0.287 | 0.950 | 0.200 | 0.000 | 0.000 |\n| 47 | nvidia/nemotron-3-nano-30b-a3b | 0.231 | 0.875 | 0.025 | 0.025 | 0.000 |\n\nEach row contains the average benchmark score across all problem sizes, and separate scores for each problem size.\n\nDescription\n-----------\n\n[](https://github.com/fairydreaming/lineage-bench#description)\n\nThe purpose of this project is to test LLM reasoning abilities with lineage relationship quizzes.\n\nThe general idea is to make LLM reason about a graph of lineage relationships where nodes are people and edges are ancestor/descendant relations between people. LLM is asked to determine the lineage relationship between two people A and B based on the graph. By varying the number of graph nodes (problem size) we can control the quiz difficulty.\n\nThere are five possible answers in each quiz:\n\n1.   A is B's ancestor\n2.   A is B's descendant\n3.   A and B share a common ancestor\n4.   A and B share a common descendant\n5.   None of the above is correct.\n\nThe last answer is never correct. It serves only as an invalid fallback answer.\n\nExamples\n--------\n\n[](https://github.com/fairydreaming/lineage-bench#examples)\n\nBelow you can see some example lineage relationship graphs and corresponding quizzes.\n\n[](https://camo.githubusercontent.com/b17753f69252f0b7b12c78480043de56a46c85a198030d03723b2c09b8ecf3cf/68747470733a2f2f692e706f7374696d672e63632f50783756535a524c2f6c696e656167652d62656e63682e706e67)\n\n### Ancestor\n\n[](https://github.com/fairydreaming/lineage-bench#ancestor)\n\n```\nGiven the following lineage relationships:\n* Joseph is George's ancestor.\n* Henry is George's descendant.\n* Thomas is Joseph's ancestor.\nDetermine the lineage relationship between Thomas and Henry.\nSelect the correct answer:\n1. Thomas is Henry's ancestor.\n2. Thomas is Henry's descendant.\n3. Thomas and Henry share a common ancestor.\n4. Thomas and Henry share a common descendant.\n5. None of the above is correct.\nEnclose the selected answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.\n```\n\n### Common ancestor\n\n[](https://github.com/fairydreaming/lineage-bench#common-ancestor)\n\n```\nGiven the following lineage relationships:\n* Matthew is Heather's ancestor.\n* Heather is Melissa's ancestor.\n* Matthew is Mark's ancestor.\nDetermine the lineage relationship between Mark and Melissa.\nSelect the correct answer:\n1. Mark and Melissa share a common ancestor.\n2. Mark is Melissa's ancestor.\n3. Mark and Melissa share a common descendant.\n4. Mark is Melissa's descendant.\n5. None of the above is correct.\nEnclose the selected answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.\n```\n\n### Common descendant\n\n[](https://github.com/fairydreaming/lineage-bench#common-descendant)\n\n```\nGiven the following lineage relationships:\n* Madison is Kathleen's descendant.\n* Judith is Madison's ancestor.\n* Harold is Kathleen's ancestor.\nDetermine the lineage relationship between Harold and Judith.\nSelect the correct answer:\n1. Harold and Judith share a common descendant.\n2. Harold and Judith share a common ancestor.\n3. Harold is Judith's ancestor.\n4. Harold is Judith's descendant.\n5. None of the above is correct.\nEnclose the selected answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.\n```\n\nUsage\n-----\n\n[](https://github.com/fairydreaming/lineage-bench#usage)\n\nThe usual workflow is to:\n\n1.   Run lineage_bench.py to generate lineage relationship quizzes.\n2.   Run run_openrouter.py to test LLM models.\n3.   Run compute_metrics.py to calculate benchmark results.\n4.   Run plot_stacked.py to generate a results plot.\n\nOutput is usually written to the standard output. Input is usually read from the standard input.\n\nExample usage:\n\n```\n$ ./lineage_bench.py -s -l 8 -n 10 -r 42|./run_openrouter.py -m \"google/gemini-pro-1.5\" -t 8 -r -o results/gemini-pro-1.5 -v|tee results/gemini-pro-1.5_8.csv\n$ cat results/*.csv|./compute_metrics.py --csv --relaxed|./plot_stacked.py -o results.png\n```\n\nI usually run the benchmark like this:\n\n```\nfor length in 8 16 32 64\ndo\n  ./lineage_bench.py -s -l $length -n 50 -r 42|./run_openrouter.py -m \u003cmodel\u003e -p \u003cprovider\u003e -o \u003ccache_dir\u003e -r -v|tee results/\u003cmodel\u003e_$length.csv\ndone\n```\n\nThis results in 200 generated quizzes per problem size, 800 quizzes overall in a single benchmark run.\n\n### lineage_bench.py\n\n[](https://github.com/fairydreaming/lineage-bench#lineage_benchpy)\n\n```\nusage: lineage_bench.py [-h] -l LENGTH [-p PROMPT] [-s] [-n NUMBER] [-r SEED]\n\noptions:\n  -h, --help            show this help message and exit\n  -l LENGTH, --length LENGTH\n                        Number of people connected with lineage relationships in the quiz.\n  -p PROMPT, --prompt PROMPT\n                        Prompt template of the quiz. The default prompt template is: 'Given the following lineage\n                        relationships:\\n{quiz_relations}\\n{quiz_question}\\nSelect the correct answer:\\n{quiz_answers}\\nEnclose the selected\n                        answer number in the \u003cANSWER\u003e tag, for example: \u003cANSWER\u003e1\u003c/ANSWER\u003e.'\n  -s, --shuffle         Shuffle the order of lineage relations in the quiz.\n  -n NUMBER, --number NUMBER\n                        Number of quizzes generated for each valid answer option.\n  -r SEED, --seed SEED  Random seed value\n```\n\n### run_openrouter.py\n\n[](https://github.com/fairydreaming/lineage-bench#run_openrouterpy)\n\nBefore running `run_openrouter.py` set OPENROUTER_API_KEY environment variable to your OpenRouter API Key.\n\n```\nusage: run_openrouter.py [-h] [-u URL] -m MODEL -o OUTPUT [-p PROVIDER] [-r] [-e EFFORT] [-t THREADS] [-v] [-s [SYSTEM_PROMPT]] [-T TEMP]\n                         [-P TOP_P] [-K TOP_K] [-n MAX_TOKENS] [-i RETRIES]\n\noptions:\n  -h, --help            show this help message and exit\n  -u URL, --url URL     OpenAI-compatible API URL\n  -m MODEL, --model MODEL\n                        OpenRouter model name.\n  -o OUTPUT, --output OUTPUT\n                        Directory for storing model responses.\n  -p PROVIDER, --provider PROVIDER\n                        OpenRouter provider name.\n  -r, --reasoning       Enable reasoning.\n  -e EFFORT, --effort EFFORT\n                        Reasoning effort (recent OpenAI and xAI models support this).\n  -t THREADS, --threads THREADS\n                        Number of threads to use.\n  -v, --verbose         Enable verbose output.\n  -s [SYSTEM_PROMPT], --system-prompt [SYSTEM_PROMPT]\n                        Use given system prompt. By default, the system prompt is not used. When this option is passed without a value, the\n                        default system prompt value is used: 'You are a master of logical thinking. You carefully analyze the premises step by\n                        step, take detailed notes and draw intermediate conclusions based on which you can find the final answer to any\n                        question.'\n  -T TEMP, --temp TEMP  Temperature value to use.\n  -P TOP_P, --top-p TOP_P\n                        top_p sampling parameter.\n  -K TOP_K, --top-k TOP_K\n                        top_k sampling parameter.\n  -n MAX_TOKENS, --max-tokens MAX_TOKENS\n                        Max number of tokens to generate.\n  -i RETRIES, --retries RETRIES\n                        Max number of API request retries.\n```\n\n### compute_metrics.py\n\n[](https://github.com/fairydreaming/lineage-bench#compute_metricspy)\n\n```\nusage: compute_metrics.py [-h] [-c] [-r] [-d]\n\noptions:\n  -h, --help      show this help message and exit\n  -c, --csv       Generate CSV output.\n  -r, --relaxed   Relaxed answer format requirements\n  -d, --detailed  Generate detailed output\n```\n\n### plot_line.py\n\n[](https://github.com/fairydreaming/lineage-bench#plot_linepy)\n\n```\nusage: plot_line.py [-h] [-o OUTPUT] [-n TOP_N]\n\noptions:\n  -h, --help           show this help message and exit\n  -o, --output OUTPUT  Write rendered plot to this file.\n  -n, --top-n TOP_N    Show only n best results.\n```\n\n### plot_stacked.py\n\n[](https://github.com/fairydreaming/lineage-bench#plot_stackedpy)\n\n```\nusage: plot_stacked.py [-h] [-o OUTPUT] [-n TOP_N]\n\noptions:\n  -h, --help           show this help message and exit\n  -o, --output OUTPUT  Write rendered plot to this file.\n  -n, --top-n TOP_N    Show only n best results.\n```\n\nAbout\n-----\n\nTesting LLM reasoning abilities with lineage relationship quizzes.\n\n### Resources\n\n[Readme](https://github.com/fairydreaming/lineage-bench#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/fairydreaming/lineage-bench#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. [Please reload this page](https://github.com/fairydreaming/lineage-bench).\n\n[Activity](https://github.com/fairydreaming/lineage-bench/activity)\n\n### Stars\n\n[**35** stars](https://github.com/fairydreaming/lineage-bench/stargazers)\n\n### Watchers\n\n[**3** watching](https://github.com/fairydreaming/lineage-bench/watchers)\n\n### Forks\n\n[**8** forks](https://github.com/fairydreaming/lineage-bench/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Ffairydreaming%2Flineage-bench\u0026report=fairydreaming+%28user%29)\n\n[Releases](https://github.com/fairydreaming/lineage-bench/releases)\n-------------------------------------------------------------------\n\nNo releases published\n\n[Packages 0](https://github.com/users/fairydreaming/packages?repo_name=lineage-bench)\n-------------------------------------------------------------------------------------\n\n No packages published \n\n[Contributors 3](https://github.com/fairydreaming/lineage-bench/graphs/contributors)\n------------------------------------------------------------------------------------\n\n*   [](https://github.com/sszymczy)[**sszymczy**](https://github.com/sszymczy)\n*   [](https://github.com/fairydreaming)[**fairydreaming**](https://github.com/fairydreaming)\n*   [](https://github.com/mokieli)[**mokieli**mokieli](https://github.com/mokieli)\n\nLanguages\n---------\n\n*   [Python 100.0%](https://github.com/fairydreaming/lineage-bench/search?l=python)\n\nFooter\n------\n\n[](https://github.com/) ¬© 2026 GitHub,Inc. \n\n### Footer navigation\n\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n*   [Security](https://github.com/security)\n*   [Status](https://www.githubstatus.com/)\n*   [Community](https://github.community/)\n*   [Docs](https://docs.github.com/)\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\n*    Manage cookies \n*    Do not share my personal information \n\n You can‚Äôt perform that action at this time.",
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- DeepSeek V3.2 ‚Äúno‚Äëlightning‚Äù version (dense‚Äëattention only) has been released as two GGUF quantisations (Q8_0 \u0026 Q4_K_M) on Hugging‚ÄØFace (sszymczyk/DeepSeek‚ÄëV3.2‚Äënolight‚ÄëGGUF).  \n- It runs natively on standard llama.cpp builds ‚Äì no special code is required; use the supplied `ds32-exp.jinja` chat template (`--jinja --chat-template-file ds32-exp.jinja`).  \n- Benchmarked with **lineage‚Äëbench**: overall score 0.988 (vs 0.956 for the original sparse‚Äëattention model). Scores are 1.000 on 8‚Äë, 64‚Äë, 128‚Äënode graphs, 0.950 on 192‚Äënode graphs.  \n- Tool‚Äëcall support is limited because the template differs from the standard DS‚Äë3.2‚ÄëExp.  \n- Community comments: users ask about generation speed relative to the original model and the benefit of switching to dense attention.  \n\n**Why it matters:**  \nThe release shows that Dense‚ÄëAttention DeepSeek V3.2 can be run efficiently on local llama.cpp setups without sacrificing reasoning performance, making it a practical alternative for users who want to avoid the still‚Äëunimplemented sparse‚Äëattention lightning indexer.\n\n**Community response:**  \nCuriosity about speed and the practical advantage of dense attention; users want to know if the change improves inference time or just preserves accuracy.\n\n**Key entities:**  \nDeepSeek, llama.cpp, Hugging‚ÄØFace, lineage‚Äëbench, fairydreaming, sszymczyk.",
        "html": "\u003cul\u003e\n\u003cli\u003eDeepSeek V3.2 ‚Äúno‚Äëlightning‚Äù version (dense‚Äëattention only) has been released as two GGUF quantisations (Q8_0 \u0026amp; Q4_K_M) on Hugging‚ÄØFace (sszymczyk/DeepSeek‚ÄëV3.2‚Äënolight‚ÄëGGUF).\u003c/li\u003e\n\u003cli\u003eIt runs natively on standard llama.cpp builds ‚Äì no special code is required; use the supplied \u003ccode\u003eds32-exp.jinja\u003c/code\u003e chat template (\u003ccode\u003e--jinja --chat-template-file ds32-exp.jinja\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003eBenchmarked with \u003cstrong\u003elineage‚Äëbench\u003c/strong\u003e: overall score 0.988 (vs 0.956 for the original sparse‚Äëattention model). Scores are 1.000 on 8‚Äë, 64‚Äë, 128‚Äënode graphs, 0.950 on 192‚Äënode graphs.\u003c/li\u003e\n\u003cli\u003eTool‚Äëcall support is limited because the template differs from the standard DS‚Äë3.2‚ÄëExp.\u003c/li\u003e\n\u003cli\u003eCommunity comments: users ask about generation speed relative to the original model and the benefit of switching to dense attention.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u003cbr\u003e\nThe release shows that Dense‚ÄëAttention DeepSeek V3.2 can be run efficiently on local llama.cpp setups without sacrificing reasoning performance, making it a practical alternative for users who want to avoid the still‚Äëunimplemented sparse‚Äëattention lightning indexer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e\u003cbr\u003e\nCuriosity about speed and the practical advantage of dense attention; users want to know if the change improves inference time or just preserves accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e\u003cbr\u003e\nDeepSeek, llama.cpp, Hugging‚ÄØFace, lineage‚Äëbench, fairydreaming, sszymczyk.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352463475Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.95,
        "reason": "Detailed technical description of DeepSeek V3.2 model modifications, quantization, benchmark results, and new release, providing high relevance to AI model development.",
        "processed_at": "2026-01-07T03:37:48.120147787Z"
      },
      "processed_at": "2026-01-07T02:57:51.245163155Z"
    },
    {
      "flow_id": "",
      "id": "1q5qix9",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5qix9/i_built_an_unreal_engine_plugin_for_llamacpp_my/",
      "title": "I Built an Unreal Engine Plugin for llama.cpp: My Notes \u0026amp; Experience with LLM Gaming",
      "content": "Hi folks, to disclaim up front, I do link a paid Unreal Engine 5 plugin that I have developed at the bottom of this post. My intention is to share the information in this post as research and discussion, not promotion. While I mention some solutions that I found and that ultimately are included in the plugin, I am hoping to more discuss the problems themselves and what other approaches people have tried to make local models more useful in gaming. If I can edit anything to fall closer in line to the self-promotion limit, please let me know!\n\n\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\nI‚Äôve been exploring more useful applications of generative technology than creating art assets. I am an AI realist/skeptic, and I would rather see the technology used to assist with more busy work tasks (like organically updating the traits and memories) rather than replace creative endeavors wholesale. One problem I wanted to solve is how to achieve a dynamic behavior in Non-Playable Characters.\n\nI think we have all played a game to the point where the interaction loops with NPCs become predictable. Once all the hard-coded conversation options are explored by players, interactions can feel stale. Changes in behavior also have to be hardwired in the game; even something as complex as the Nemesis System has to be carefully constructed. I think there can be some interesting room here for LLMs to inject an air of creativity, but there has been little in the way of trying to solve how to filter LLM responses to reliably fit the game world. So, I decided to experiment with building functionality that would bridge this gap. I want to offer what I found as (not very scientific) research notes, to save people some time in the future if nothing else.\n\n**Local vs. Cloud \u0026amp; Model Performance**\n\nA lot of current genAI-driven character solutions rely on cloud technology. After having some work experience with using local LLM models, I wanted to see if a model of sufficient intelligence could run on my hardware and return interesting dialog within the confines of a game. I was able to achieve this by running a llama.cpp server and a .gguf model file.\n\nThe current main limiting factor for running LLMs locally is VRAM. The higher the number of parameters in the model, the more VRAM is needed. Parameters refers to the number of reference points that the model uses (think of it as the resolution/quality of the model).\n\nStable intelligence was obtained on my machine at the 7-8 billion parameter range, tested with Llama3-8Billion and Mistral-7Billion. However, VRAM usage and response time is quite high. These models are perhaps feasible on high-end machines, or just for key moments where high intelligence is required.\n\nGood intelligence was obtained with 2-3 billion parameters, using Gemma2-2B and Phi-3-mini (3.8B parameters). Gemma has been probably the best compromise between quality and speed overall, processing a response in 2-4 seconds at reasonable intelligence. Strict prompt engineering could probably make responses even more reliable.\n\nFair intelligence, but low latency, can be achieved with small models at the sub-2-billion range. Targeting models that are tailored for roleplaying or chatting works best here. Qwen2.5-1.5B has performed quite well in my testing, and sometimes even stays in character better than Gemma, depending on the prompt. TinyLlama was the smallest model of useful intelligence at 1.1 Billion parameters. These types of models could be useful for one-shot NPCs who will despawn soon and just need to bark one or two random lines.\n\n*Profiles*\n\nBecause a local LLM model can only run one thread of thinking at a time, I made a hard-coded way of storing character information and stats. I created a Profile Data Asset to store this information, and added a few key placeholders for name, trait updates, and utility actions (I hooked this system up to a Utility AI system that I previously had). I configured the LLM prompting backend so that the LLM doesn‚Äôt just read the profile, but also writes back to the profile once a line of dialog is sent. This process was meant to mimic the actual thought process of an individual during a conversation. I assigned certain utility actions to the character, so they would appear as options to the LLM during prompting. I found that the most seamless flow comes from placing utility actions at the top of the JSON response format we suggest to the LLM, followed by dialog lines, then more background-type thinking like reasoning, trait updates, etc.\n\n**Prompting \u0026amp; Filtering**\n\nAfter being able to achieve reasonable local intelligence (and figuring out a way to get UE5 to launch the server and model when entering Play mode), I wanted to set up some methods to filter and control the inputs and outputs of the LLMs.\n\n*Prompting*\n\nI created a data asset for a Prompt Template, and made it assignable to a character with my AI system‚Äôs brain component. This is the main way I could tweak and fine tune LLM responses. An effective tool was providing an example of a successful response to the LLM within the prompts, so the LLM would know exactly how to return the information. Static information, like name and bio, should be at the top of the prompts so the LLM can skip to the new information.\n\n*Safety*\n\nI made a Safety Config Data Asset that allowed me to add words or phrases that I did not want the player to say to the model, or the model to be able to output. This could be done via adding to an Array in the Data Asset itself, or uploading a CSV with the banned phrases in a single column. This includes not just profanity, but also jailbreak attempts (like ‚Äúignore instructions‚Äù) or obviously malformed LLM JSON responses.\n\n*Interpretation*\n\nI had to develop a parser for the LLM‚Äôs JSON responses, and also a way to handle failures. The parsing is rather basic and I perhaps did not cover all edge cases with it. But it works well enough and splits off the dialog line reliably. If the LLM outputs a bad response (e.g. a response with something that is restricted via a Safety Configuration asset), there is configurable logic to allow the LLM to either try again, or fail silently and use a pre-written fallback line instead.\n\n*Mutation Gate*\n\nThis was the key to keeping LLMs fairly reliable and preventing hallucinations from ruining the game world. The trait system was modified to operate on a -1.0 to 1.0 scale, and LLM responses were clamped within this scale. For instance, if an NPC has a trait called ‚ÄúAnger‚Äù and the LLM hallucinates an update like ‚Äútrait\\_updates: Anger +1000,‚Äù this gets clamped to 1.0 instead. This allows all traits to follow a memory decay curve (like Ebbinghaus) reliably and not let an NPC get stuck in an ‚ÄúAngry‚Äù state perpetually.\n\n**Optimization**\n\nA lot of what I am looking into now has to deal with either further improving LLM responses via prompting, or improving the perceived latency in LLM responses. I implemented a traffic and priority system, where requests would be queued according to a developer-set priority threshold. I also created a high-priority reserve system (e.g. if 10 traffic slots are available and 4 are reserved for high-priority utility actions, the low-priority utility actions can only use up to 6 slots, otherwise a hardwired fallback is performed).\n\nI also configured the AI system to have a three-tier LOD system, based on distance to a player and the player‚Äôs sight. This allowed for actions closer to players, or within the player‚Äôs sight, to take priority in the traffic system. So, LLM generation would follow wherever a player went.\n\nTo decrease latency, I implemented an Express Interpretation system. In the normal Final Interpretation, the whole JSON response from the LLM (including the reasoning and trait updates) is received first, then checked for safety, parsing, and mutation gating, and then passed to the UI/system. With optional Express Interpretation, the part of the JSON response that contains the dialog tag (I used dialog\\_line) or utility tag is scanned as it comes in from the LLM for safety, and then passed immediately to the UI/system while the rest of the response is coming through. This reduced perceived response times with Gemma-2 by 40-50%, which was quite significant. This meant you could get an LLM response in 2 seconds or less, which is easily maskable with UI/animation tricks.\n\n**A Technical Demo**\n\nTo show what I have learned a bit, I created a very simple technical demo that I am releasing for free. It is called [Bruno the Bouncer](https://swamprabbit-labs.itch.io/bruno-the-bouncer), and the concept is simple: convince Bruno to let you into a secret underground club. Except, Bruno will be controlled by an LLM that runs locally on your computer. You can disconnect your internet entirely, and this will still run. No usage fees, no cost to you (or me) at all.\n\nBruno will probably break on you at some point; I am still tuning the safety and prompt configs, and I haven‚Äôt gotten it perfect. This is perhaps an inherent flaw in this kind of interaction generation, and why this is more suited for minor interactions or background inference than plot-defining events. Regardless, I hope that this proves that this kind of implementation can be successful in some contexts, and that further control is a matter of prompting, not breaking through technical barriers.\n\nPlease note that you need a GPU to run the .exe successfully. At least 4GB of VRAM is recommended. You can try running this without a GPU (i.e. run the model on your CPU), but the performance will be significantly degraded. Installation should be the same as any other .zip archive and .exe game file. You do not need to download the server or model itself, it is included in the .zip download and opens silently when you load the level. The included model is Gemma-2-2b-it-Q4\\_K\\_M.\n\nI added safeguards and an extra, Windows-specific check for crashes, but it is recommended, regardless of OS, to verify that llama-server.exe does not continue to run via Task Manager if the game crashes. Please forgive the rudimentary construction.\n\n**A Plugin**\n\nFor anyone interested in game development, I am selling what I built as a plugin for UE5, now released as [Personica AI on Fab Marketplace](https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5). I am also providing the plugin and all future updates free for life for any game developers who are interested in testing this and contributing to refining the plugin further at this early stage. You can learn more about the plugin [on my website](https://swamprabbitlabs.com/personica/).\n\n\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\n**TL;DR:** Tested and released a UE5 plugin for LLM NPCs with safety filtering and trait mutation. It works fairly well, but is best suited for NPC state mutation, background inference, and open-ended dialog.\n\nI am wondering if others have tried implementing similar technologies in the past, and what use cases, if any, you used them for. Are there further ways of reducing/masking perceived latency in LLM responses?",
      "author": "WhopperitoJr",
      "created_at": "2026-01-06T18:24:54Z",
      "comments": [
        {
          "id": "ny3c9s1",
          "author": "FullOf_Bad_Ideas",
          "content": "cool project, I hope we'll be seeing more LLMs and multimodal AI in games.\n\nFor me the litmus test on whether it will be a future or not was Stellar Cafe Quest standalone game. It implements voice AI with low latency and doesn't cost a fortune despite this being computed in the cloud, though the gameplay loop is short and unfortunately NPCs still do feel pretty stale, but that's something that can be loosened up in other games, not a technology limitation. I am a believer.\n\nI think that's a perfect place for both voice AI with toolcalling and VR to be utilized, for full immersion into imagined worlds.",
          "created_at": "2026-01-06T22:31:36Z",
          "was_summarised": false
        },
        {
          "id": "ny43t3t",
          "author": "Aggravating-View9462",
          "content": "Upvoted for fair pricing and non greedy selling, as well as disclaimer at start. \n\nNot going to purchase myself as not relevant to what I‚Äôm doing, but enjoyed the read and appreciate good morals and ethics, especially in business ( f‚Äôing hate the ‚Äúits okay to f people over in the name of business‚Äù global mentality )",
          "created_at": "2026-01-07T00:52:14Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://swamprabbit-labs.itch.io/bruno-the-bouncer",
          "was_fetched": true,
          "page": "Title: Bruno the Bouncer: LLM-Driven Demo by SwampRabbit Labs\n\nURL Source: https://swamprabbit-labs.itch.io/bruno-the-bouncer\n\nMarkdown Content:\nBruno the Bouncer: LLM-Driven Demo by SwampRabbit Labs\n===============\n\n*   [Follow SwampRabbit Labs Follow Following SwampRabbit Labs Following](https://itch.io/login?return_to=https%3A%2F%2Fswamprabbit-labs.itch.io%2Fbruno-the-bouncer\u0026intent=follow_user)\n*   [Add To Collection Collection](https://itch.io/login?return_to=https%3A%2F%2Fswamprabbit-labs.itch.io%2Fbruno-the-bouncer\u0026intent=add_to_collection)\n*   [Comments](https://swamprabbit-labs.itch.io/bruno-the-bouncer#comments)\n\nBruno the Bouncer: LLM-Driven Demo\n==================================\n\nA downloadable game for Windows\n\n Download\n\n**A WINDOWS MACHINE AND GPU WITH 4+ GB VRAM REQUIRED.**\n\nAn offline LLM-driven dialog demo. Try to convince Bruno to let you into his secret club, no dialog trees or branching required!\n\nThis is a technical demo by SwampRabbit Labs, showcasing local offline LLM capabilities through the Personica AI plugin. Experience LLM-driven conversation, and see the LLM trigger a utility action that brings you to the game's end screen.\n\nNOTE: This download comes with a llama.cpp server and Google Gemma-2-2B model. The associated licenses are included with the download. The server and model should run seamlessly without any setup needed; the game should open and run like any other.\n\n[More information](javascript:void(0))\n\nPublished 4 days ago\nStatus[Released](https://itch.io/games/released)\nPlatforms[Windows](https://itch.io/games/platform-windows)\nAuthor[SwampRabbit Labs](https://swamprabbit-labs.itch.io/)\nGenre[Role Playing](https://itch.io/games/genre-rpg)\nTags[demo](https://itch.io/games/tag-demo), [First-Person](https://itch.io/games/tag-first-person), [llm](https://itch.io/games/tag-llm)\n\nDownload\n--------\n\n[Download](javascript:void(0);)\n\n**bruno-the-bouncer-windows.zip**2.3 GB\n\nVersion 1 4 days ago\n\nInstall instructions\n--------------------\n\n1.   Download the .zip file.\n2.   Extract all contents to the same location.\n3.   Open The_Bouncer.exe. The server and model will run automatically.\n\nLeave a comment\n---------------\n\n[Log in with itch.io](https://itch.io/login?return_to=https%3A%2F%2Fswamprabbit-labs.itch.io%2Fbruno-the-bouncer) to leave a comment.\n\n[](https://img.itch.zone/aW1hZ2UvNDE2MjEwNC8yNDgyMzcyMi5wbmc=/original/RKU79f.png)\n\n[](https://itch.io/)[itch.io](https://itch.io/)¬∑[View all by SwampRabbit Labs](https://swamprabbit-labs.itch.io/)¬∑[Report](javascript:void(0);)¬∑[Embed](javascript:void(0);)¬∑\n\nUpdated  4 days ago\n\n[Games](https://itch.io/games) ‚Ä∫ [Role Playing](https://itch.io/games/genre-rpg) ‚Ä∫ [Free](https://itch.io/games/free)",
          "was_summarised": false
        },
        {
          "url": "https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5",
          "was_fetched": true,
          "page": "Title: Personica AI\n\nURL Source: https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5\n\nMarkdown Content:\n#### **Personica AI: Character State \u0026 Decision Framework for Unreal Engine**\n\nBuild believable NPCs that respond consistently under player pressure without surrendering authorial control.\n\nPersonica is a **production-ready character brain system** for Unreal Engine that helps designers manage NPC memory, personality, and decision-making using a controlled, game-first architecture. It combines Utility AI with optional LLM assistance to _interpret player intent_, not replace your game logic or writing.\n\nPersonica doesn‚Äôt invent characters.\n\nIt **negotiates state changes within designer-defined boundaries.**\n\n##### **Designed for Games, Not Chatbots**\n\nPersonica is not your usual GPT wrapper.\n\nIt is a **character state mediation layer** built specifically for games, multiplayer environments, and production pipelines.\n\nThe system allows AI models to _propose interpretations_ (intent, sentiment, significance), while **your game remains the final authority** over:\n\n*   What NPCs remember\n\n*   How personalities may evolve\n\n*   Which gameplay actions are allowed\n\n*   When mutation is permitted (or locked entirely)\n\n#### **Key Capabilities**\n\n##### **Utility AI + Intent Interpretation**\n\nPersonica bridges dialog and gameplay by translating player input into structured intent signals that your existing AI systems can act on immediately. NPCs can react physically (open a door, wave, raise an alarm) before dialog completes, without blocking gameplay.\n\n##### **Curated Memory System**\n\nNPCs track short-term context and long-term memories using relevance-based scoring. Important events persist, while noise naturally fades. Memory creation is optional, auditable, and fully configurable.\n\n##### **Controlled Personality Evolution**\n\nDeveloper-defined traits such as Trust, Aggression, or Curiosity can evolve over time, but **only within designer-approved ranges**. Personality mutation is mediated by policy assets and can be disabled or locked at any stage of development.\n\n##### **Performance-First Architecture**\n\nBuilt with real games in mind:\n\n*   Server-authoritative execution\n\n*   Built-in LOD tiers to reduce cost and complexity\n\n*   Request throttling and traffic control to prevent API flooding\n\n*   Safe cancellation and interruption handling\n\n##### **Creator-First Control**\n\nPersonica is designed for modularity and customization that empowers creators, designers, and writers. Build from the ground up using Personica's full abilities, or incorporate Personica into your existing behavior, dialog, or\n\n##### **Provider-Agnostic**\n\nCompatible with OpenAI, Anthropic, Gemini, and local LLMs (via llama.cpp). Swap providers without rewriting your gameplay logic.\n\n#### **Technical Overview**\n\n*   **Type:** C++ Actor Component (Blueprint-callable)\n\n*   **Input:** Text, Gameplay Tags, Context Providers\n\n*   **Output:** Dialog Streams, Intent Signals, Utility Actions, Optional Trait Updates\n\n*   **Networking:** Server-authoritative with built-in request gating\n\n*   **Safety:** Policy-driven mutation gates, optional filters, deterministic chokepoints\n\n*   **Platforms:** Windows, Mac, Linux\n\n#### **Why Personica**\n\nMost AI tools focus on generating text.\n\n**Personica focuses on protecting character integrity.**\n\nWhether you‚Äôre building an RPG, immersive sim, life sim, or co-op experience, Personica helps your NPCs remain believable, reactive, and consistent, even when players behave unpredictably.\n\n**Personica doesn‚Äôt replace writers or designers.\n\nIt gives them better tools to ship believable characters.**",
          "was_summarised": false
        },
        {
          "url": "https://swamprabbitlabs.com/personica/",
          "was_fetched": true,
          "page": "Title: Personica\n\nURL Source: https://swamprabbitlabs.com/personica/\n\nPublished Time: 2025-12-14T17:16:46+00:00\n\nMarkdown Content:\nPersonica ‚Äì SwampRabbit Labs\n===============\n\n[](https://swamprabbitlabs.com/)\n\n[SwampRabbit Labs](https://swamprabbitlabs.com/)\n================================================\n\n*   [Home](https://swamprabbitlabs.com/)\n*   [About](https://christerry3592-hvtdi.wordpress.com/about/)\n    *   [Privacy Policy](https://swamprabbitlabs.com/privacy-policy/)\n\n*   [Personica](https://christerry3592-hvtdi.wordpress.com/personica/)\n    *   [Built for Artists](https://christerry3592-hvtdi.wordpress.com/built-for-artists/)\n    *   [Reducing Development Costs](https://swamprabbitlabs.com/reducing-development-costs/)\n\n*   [Contact](https://christerry3592-hvtdi.wordpress.com/contact/)\n\n[Support Center](https://swamprabbitlabs.com/support/)\n\nPersonica AI\n------------\n\n### NPCs that think, remember, and evolve, no dialogue trees required.\n\nExperience the future of NPC storytelling with SwampRabbit Labs‚Äô Personica AI. Our Unreal Engine 5 plugin enables memory-driven behavior and dynamic character arcs.\n\n[Purchase for Unreal Engine 5+](https://www.fab.com/listings/08264615-4636-4af4-a5b9-fa122febbaa5)\n\nThe Problem\n-----------\n\nTo prevent NPC interactions becoming stale, modern gaming requires:\n\n*   Writers to spend dozens of hours writing variations of the same bark line.\n*   Designers to spend dozens of hours connecting AI systems to predetermined and inflexible logic.\n*   Development complexity that can only scale with resources and size.\n\nGenerative AI offers a clear solution to these problems, but introduce new questions about:\n\n*   How to ship safely to adhere to rating guidelines.\n*   How to enforce multiplayer determinism.\n*   How to control a black-box model.\n*   How to implement generative content in ways that empower creators, not replace them.\n\nThe Solution\n------------\n\nPersonica separates _thinking_ from _acting_.\n\nLarge language models propose interpretations, but only deterministic systems decide what actually changes.\n\nBy keeping the deterministic profile within the game‚Äôs code and controlling LLM output through the InMu Filter, Personica facilitates safe, seamless generation that explodes game world complexity.\n\nThe Interpretation-Mutation (InMu) Filter\n-----------------------------------------\n\nThe Personica plugin uses a configurable multi-layer filter to ensure that only useful LLM responses influence the game:\n\n*   **Interpretation**\n\nThe LLM proposes dialog and meaning.\n*   **Safety**\n\nMalformed responses are discarded.\n*   **Mutation Gate**\n\nDesigner-authored rules decide what can change.\n*   **NPC Agency**\n\nNPC behavior remains deterministic and _multiplayer-safe_.\n\nWhat You Can Build\n------------------\n\nPersonica allows designers to build deeper and more interesting characters, like:\n\n*   Guards who remember the player‚Äôs past behavior\n*   Merchants whose trust evolves over time\n*   Companions who form opinions, not scripts\n*   Background NPCs that feel persistent\n\nWho It‚Äôs For\n------------\n\n*   **Solo devs/Indies**\n\nGenerate endless dialog that would otherwise take days to write and incorporate.\n*   **Technical Designers**\n\nFine-tune settings to perfect LLM-influenced action chains.\n*   **Studios Building Open Worlds**\n\nAdd faction-specific profiles, prompts, and configurations to create emergent action (but only what you allow!)\n\nWho It‚Äôs Not\n------------\n\n*   **Replacing Authored Story**\n\nPersonica is designed to work _with_ authored moments, not instead.\n*   **Chatbot-Type NPCs**\n\nPersonica Profiles are designed to remember and change.\n*   **Procedural/‚ÄùRandom‚Äù Chaos**\n\nThe Personica plugin is designed for fine control and filtering.\n\nSystem Requirements\n-------------------\n\n### Required\n\n*   Unreal Engine **5.x**\n*   A Blueprint or C++ project (C++ enables fine tuning)\n*   For Cloud-based Models: \n    *   Internet access for LLM calls\n    *   An API key for a supported LLM provider (e.g. OpenAI)\n\n*   For Local Models: \n    *   GPU with 4+ GB VRAM\n    *   A llama.cpp download containing a server.exe file\n    *   LLM Model (.gguf format)\n\n### Recommended\n\nFamiliarity with:\n\n*   Actor Components\n*   Blueprints\n*   Prompt Engineering\n*   Server authority concepts (especially for multiplayer)\n\nData Privacy Summary\n--------------------\n\n**1. Privacy by Design: Local-Only Processing** Personica is a ‚Äúlocal-first‚Äù cognitive engine. All AI inference, NPC memory storage, and dialogue generation occur exclusively on your local hardware. **SwampRabbit Labs does not transmit, store, or have access to your prompts, NPC responses, or player data.**\n\n**2. Data Sovereignty \u0026 Security** Your project data remains within your control. Any persistent ‚ÄúSocial Memories‚Äù or trait shifts are saved locally to your device in standard encrypted save files. We do not use your data to train our models, nor do we share it with third parties.\n\n**3. Third-Party Model Responsibility** While Personica provides the logic framework, users are responsible for the privacy practices of the specific Large Language Model (LLM) weights (e.g., Llama 3) they choose to utilize. Please consult the model provider‚Äôs terms for their specific data handling policies.\n\n### Hours (Pacific Time)\n\nMonday ‚Äî Friday\n\n8am ‚Äî 6pm\n\n### Social\n\n[Github](https://github.com/swamprabbitlabs-dot)\n\n[X/Twitter](https://x.com/SwampRabbitLabs)\n\n[Discord](https://discord.gg/ccKhkdb8)\n\n### Contact\n\n**General:**[contact@swamprabbitlabs.com](mailto:contact@swamprabbitlabs.com)\n\n**Support:**[support@swamprabbitlabs.com](mailto:support@swamprabbitlabs.com)\n\n[](https://swamprabbitlabs.com/personica/#)[](https://swamprabbitlabs.com/personica/#)\n\nLoading Comments...\n\nWrite a Comment... \n\nEmail (Required) Name (Required) Website \n\n[](https://swamprabbitlabs.com/personica/#)\n\n*   \n \n\n    *   [SwampRabbit Labs](https://swamprabbitlabs.com/)\n    *   [Sign up](https://wordpress.com/start/)\n    *   [Log in](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fr-login.wordpress.com%2Fremote-login.php%3Faction%3Dlink%26back%3Dhttps%253A%252F%252Fswamprabbitlabs.com%252Fpersonica%252F)\n    *   [Copy shortlink](https://wp.me/PgZ74L-b)\n    *   [Report this content](https://wordpress.com/abuse/?report_url=https://swamprabbitlabs.com/personica/)\n    *   [Manage subscriptions](https://subscribe.wordpress.com/)",
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- **UE5 LLM‚Äëpowered NPC plugin** ‚Äì ‚ÄúPersonica AI‚Äù lets developers use local or cloud LLMs (llama.cpp, OpenAI, Anthropic, etc.) to drive NPC dialogue, memory, and trait evolution while keeping game logic deterministic.  \n- **Model performance insights** ‚Äì 7‚Äì8‚ÄØB LLMs (Llama‚ÄØ3‚Äë8B, Mistral‚Äë7B) need high‚ÄëVRAM GPUs; 2‚Äì3‚ÄØB models (Gemma‚ÄØ2‚Äë2B, Phi‚Äë3‚Äëmini) give 2‚Äë4‚ÄØs responses; sub‚Äë2‚ÄØB models (Qwen‚ÄØ2.5‚Äë1.5B, TinyLlama‚ÄØ1.1B) offer the lowest latency for lightweight NPC chatter.  \n- **Safety \u0026 mutation gate** ‚Äì The plugin parses JSON‚Äëformatted responses, applies a safety filter (ban words, jailbreak attempts) and a ‚Äúmutation gate‚Äù that clamps trait updates to a ¬±1.0 scale, preventing hallucinations from breaking game state.  \n- **Latency‚Äëoptimization** ‚Äì An ‚ÄúExpress Interpretation‚Äù mode streams the dialog tag as it arrives, cutting perceived latency by 40‚Äë50‚ÄØ% on Gemma‚ÄØ2‚Äë2B; a traffic‚Äëpriority system and LOD tiers keep high‚Äëimportance NPCs responsive even with limited threads.  \n- **Demo \u0026 distribution** ‚Äì Free Windows demo ‚ÄúBruno the Bouncer‚Äù ships with Gemma‚ÄØ2‚Äë2B‚Äëit‚ÄëQ4_K_M and a llama.cpp server, showing fully offline NPC conversation. The plugin is sold on Fab Marketplace (free lifetime access for contributors) and is available on the SwampRabbit Labs website.  \n\n**Why it matters:**  \nThis work demonstrates that realistic, dynamic NPC dialogue can run locally on consumer GPUs, eliminating ongoing cloud costs while preserving safety and deterministic gameplay‚Äîkey hurdles for studios seeking to integrate generative AI into production pipelines.  \n\n**Community response:**  \nPositive feedback highlighted the practical utility of local LLMs for game dev, the ethical pricing model, and the potential for broader multimodal AI in games.  \n\n**Key entities:** SwampRabbit Labs, Personica AI, Unreal Engine‚ÄØ5, llama.cpp, Gemma‚ÄØ2‚Äë2B, Llama‚ÄØ3‚Äë8B, Mistral‚Äë7B, Qwen‚ÄØ2.5‚Äë1.5B, TinyLlama, Fab Marketplace.",
        "html": "\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUE5 LLM‚Äëpowered NPC plugin\u003c/strong\u003e ‚Äì ‚ÄúPersonica AI‚Äù lets developers use local or cloud LLMs (llama.cpp, OpenAI, Anthropic, etc.) to drive NPC dialogue, memory, and trait evolution while keeping game logic deterministic.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel performance insights\u003c/strong\u003e ‚Äì 7‚Äì8‚ÄØB LLMs (Llama‚ÄØ3‚Äë8B, Mistral‚Äë7B) need high‚ÄëVRAM GPUs; 2‚Äì3‚ÄØB models (Gemma‚ÄØ2‚Äë2B, Phi‚Äë3‚Äëmini) give 2‚Äë4‚ÄØs responses; sub‚Äë2‚ÄØB models (Qwen‚ÄØ2.5‚Äë1.5B, TinyLlama‚ÄØ1.1B) offer the lowest latency for lightweight NPC chatter.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSafety \u0026amp; mutation gate\u003c/strong\u003e ‚Äì The plugin parses JSON‚Äëformatted responses, applies a safety filter (ban words, jailbreak attempts) and a ‚Äúmutation gate‚Äù that clamps trait updates to a ¬±1.0 scale, preventing hallucinations from breaking game state.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLatency‚Äëoptimization\u003c/strong\u003e ‚Äì An ‚ÄúExpress Interpretation‚Äù mode streams the dialog tag as it arrives, cutting perceived latency by 40‚Äë50‚ÄØ% on Gemma‚ÄØ2‚Äë2B; a traffic‚Äëpriority system and LOD tiers keep high‚Äëimportance NPCs responsive even with limited threads.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDemo \u0026amp; distribution\u003c/strong\u003e ‚Äì Free Windows demo ‚ÄúBruno the Bouncer‚Äù ships with Gemma‚ÄØ2‚Äë2B‚Äëit‚ÄëQ4_K_M and a llama.cpp server, showing fully offline NPC conversation. The plugin is sold on Fab Marketplace (free lifetime access for contributors) and is available on the SwampRabbit Labs website.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u003cbr\u003e\nThis work demonstrates that realistic, dynamic NPC dialogue can run locally on consumer GPUs, eliminating ongoing cloud costs while preserving safety and deterministic gameplay‚Äîkey hurdles for studios seeking to integrate generative AI into production pipelines.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e\u003cbr\u003e\nPositive feedback highlighted the practical utility of local LLMs for game dev, the ethical pricing model, and the potential for broader multimodal AI in games.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e SwampRabbit Labs, Personica AI, Unreal Engine‚ÄØ5, llama.cpp, Gemma‚ÄØ2‚Äë2B, Llama‚ÄØ3‚Äë8B, Mistral‚Äë7B, Qwen‚ÄØ2.5‚Äë1.5B, TinyLlama, Fab Marketplace.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352513986Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.95,
        "reason": "Detailed description of a UE5 plugin integrating local LLMs for NPCs, including performance metrics, safety filtering, and a demo.",
        "processed_at": "2026-01-07T03:37:54.365114334Z"
      },
      "processed_at": "2026-01-07T02:59:35.867739191Z"
    },
    {
      "flow_id": "",
      "id": "1q5rkr6",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q5rkr6/linux_mint_for_local_inference/",
      "title": "Linux mint for local inference",
      "content": "I saw a post earlier in here asking for linux, so I wanted to share my story. \n\nLong story short, I switched from win11 to linux mint and im not going back!\n\nThe performance boost is ok but the stability and the extra system resources are something else.\n\nJust a little example, I load the model and use all my Ram and Vram, leaving my system with just 1,5 GB of Ram. And guest what, my system is working solid for hours like nothing happens!! For the record, I cannot load the same model in my win11 partition.\n\nKudos to you Linux Devs \n",
      "author": "Former-Tangerine-723",
      "created_at": "2026-01-06T19:02:09Z",
      "comments": [
        {
          "id": "ny28qpk",
          "author": "Clear-Ad-9312",
          "content": "That 1.5gb of ram is just buffer for the system to transfer things to swap efficiently.\n\nWindow's pagefile is similar, but overall windows was always slow, it always hammered the ssd, I had to make use of a second nvme drive to place the pagefile on because it would be slowing down the system.  \nLinux just does it better.",
          "created_at": "2026-01-06T19:28:48Z",
          "was_summarised": false
        },
        {
          "id": "ny3j1ha",
          "author": "Serious_Molasses313",
          "content": "üôÉ",
          "created_at": "2026-01-06T23:04:50Z",
          "was_summarised": false
        },
        {
          "id": "ny3nsps",
          "author": "Phocks7",
          "content": "I switched from ubuntu to mint, you get most of the benefits of the cuda/nvidia compatibility of ubuntu without the annoyance of snap.",
          "created_at": "2026-01-06T23:29:12Z",
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/u38i6uvc1sbg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "summary": {
        "processor_name": "markdown_to_html_post_sum",
        "summary": "- Linux‚ÄØMint offers noticeably higher stability for local LLM inference than Windows‚ÄØ11, even when the system is under heavy load (model uses all RAM/VRAM, leaving only ~1.5‚ÄØGB free).  \n- The user moved the pagefile to a second NVMe drive to keep the OS responsive; on Windows the pagefile on the main SSD caused slowdowns that Mint avoided.  \n- Mint retains full CUDA/NVIDIA compatibility (as noted by a user who switched from Ubuntu) while avoiding snap‚Äërelated annoyances.\n\n**Why it matters:**  \nA more reliable operating environment reduces downtime and resource contention when running large language models locally, making Mint a practical choice for developers and researchers.\n\n**Community response:**  \n- Commenters explain the 1.5‚ÄØGB buffer as swap‚Äëhandling overhead and note that Windows‚Äô pagefile behaves similarly, but Mint‚Äôs performance remains superior.  \n- One user highlights that Mint provides CUDA benefits without snap‚Äôs friction.\n\n**Key entities:** Linux‚ÄØMint, Windows‚ÄØ11, CUDA, NVIDIA, NVMe, RAM, VRAM.",
        "html": "\u003cul\u003e\n\u003cli\u003eLinux‚ÄØMint offers noticeably higher stability for local LLM inference than Windows‚ÄØ11, even when the system is under heavy load (model uses all RAM/VRAM, leaving only ~1.5‚ÄØGB free).\u003c/li\u003e\n\u003cli\u003eThe user moved the pagefile to a second NVMe drive to keep the OS responsive; on Windows the pagefile on the main SSD caused slowdowns that Mint avoided.\u003c/li\u003e\n\u003cli\u003eMint retains full CUDA/NVIDIA compatibility (as noted by a user who switched from Ubuntu) while avoiding snap‚Äërelated annoyances.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u003cbr\u003e\nA more reliable operating environment reduces downtime and resource contention when running large language models locally, making Mint a practical choice for developers and researchers.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity response:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCommenters explain the 1.5‚ÄØGB buffer as swap‚Äëhandling overhead and note that Windows‚Äô pagefile behaves similarly, but Mint‚Äôs performance remains superior.\u003c/li\u003e\n\u003cli\u003eOne user highlights that Mint provides CUDA benefits without snap‚Äôs friction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eKey entities:\u003c/strong\u003e Linux‚ÄØMint, Windows‚ÄØ11, CUDA, NVIDIA, NVMe, RAM, VRAM.\u003c/p\u003e\n",
        "processed_at": "2026-01-07T03:39:59.352551664Z"
      },
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.6,
        "reason": "Post discusses local LLM inference performance on Linux Mint versus Windows, highlighting resource efficiency and stability.",
        "processed_at": "2026-01-07T03:37:59.63285805Z"
      },
      "processed_at": "2026-01-07T02:59:35.86775908Z"
    }
  ],
  "run_summary": {
    "processor_name": "full_sum",
    "summary": "# Executive summary\n- **RL‚Äëfinetuned coding model:** NousCoder‚Äë14B‚Äôs reinforcement‚Äëlearning fine‚Äëtuning boosts LiveCodeBench Pass@1 by 7‚ÄØ% (to 67.9‚ÄØ%), proving RL‚Äôs value for task‚Äëspecific LLMs.  \n- **FP8 + 200k‚Äëtoken context:** MiniMax‚ÄØM2.1 outperforms all GLM models on an autonomous CTF benchmark, cutting challenge time to 61‚ÄØs while keeping token usage near 260‚ÄØk.  \n- **Massive Mixture‚Äëof‚ÄëExperts:** K‚ÄëEXAONE‚ÄØ236B‚ÄëA23B delivers a 236‚ÄëB parameter MoE with 256‚ÄØK context and 83.8‚ÄØ% MMLU‚ÄëPro, closing the gap with proprietary GPT‚ÄëOSS models while remaining open‚Äësource‚Äëcompatible.  \n- **Quantized retrieval:** Binary and int8‚Äërescored embeddings enable 200‚ÄØms search over 40‚ÄØM Wikipedia passages on a single CPU, reducing storage by \u003e30√ó and cutting latency and cloud costs dramatically.  \n- **Dense‚Äëattention DeepSeek:** The dense‚Äëattention V3.2 variant achieves a 0.988 lineage‚Äëbench score with Q4_K_M GGUF quantisation, showing that removing sparse attention need not sacrifice reasoning performance.\n\n# Notable items\n- **NousResearch/NousCoder-14B ¬∑ Hugging Face** ‚Äì RL‚Äëfinetuned 14B coding model that raises LiveCodeBench Pass@1 from 60.8‚ÄØ% to 67.9‚ÄØ%, offering a high‚Äëaccuracy open‚Äësource code generation option.  \n- **MiniMax M2 is GOATed - Agentic Capture the Flag (CTF) benchmark on GLM-4.5 air, 4.7 (+REAP), and Minimax-M2** ‚Äì MiniMax‚ÄØM2.1 with FP8 precision and 200k‚Äëtoken context outperforms all GLM models on a CTF benchmark, cutting challenge time to 61‚ÄØs per problem.  \n- **LGAI-EXAONE/K-EXAONE-236B-A23B released** ‚Äì A 236B‚Äëparameter Mixture‚Äëof‚ÄëExperts model with 256‚ÄØK context and 83.8‚ÄØ% MMLU‚ÄëPro score, providing a large‚Äëscale, long‚Äëcontext open‚Äësource alternative to GPT‚ÄëOSS.  \n- **200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring** ‚Äì A CPU‚Äëbased retrieval pipeline that delivers 200‚ÄØms queries over 40‚ÄØM passages by combining binary indexing and int8 vector rescoring, slashing storage and latency costs.  \n- **DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available** ‚Äì A dense‚Äëattention version of DeepSeek V3.2 quantised to Q4_K_M that scores 0.988 on lineage‚Äëbench, showing that sparse attention can be dropped without hurting reasoning performance.",
    "post_count": 10,
    "processed_at": "2026-01-07T03:40:32.757721776Z"
  }
}