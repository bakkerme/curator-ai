{
  "blocks": [
    {
      "flow_id": "",
      "id": "1q7qcux",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "title": "The NO FAKES Act has a \"Fingerprinting\" Trap that kills Open Source. We need to lobby for a Safe Harbor.",
      "content": "Hey everyone,\nâ€‹Iâ€™ve been reading the text of the \"NO FAKES Act\" currently in Congress, and itâ€™s worse than I thought.\nâ€‹The Tldr: It creates a \"digital replica right\" for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who \"makes available\" a tool that is primarily used for replicas.  \nâ€‹The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation).\nâ€‹There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.\n\nWhat I did:\nI contacted my reps email to flag this as an \"innovation killer.\" If you run a repo or care about open weights, you might want to do the same. We need them to add a \"Safe Harbor\" for tool devs.\n\nS.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress https://share.google/u6dpy7ZQDvZWUrlfc",
      "author": "PostEasy7183",
      "created_at": "2026-01-08T22:33:33Z",
      "comments": [
        {
          "id": "nyhm9rk",
          "author": "Revolutionalredstone",
          "content": "Making your own devs liable is how you turn your country into a third world nation.\n\nPeople who make it easy to USE the tools are the only ones who should be liable.\n\nThere are plenty of countries which won't play these silly blame games and their devs will keep releasing all their stuff either way.\n\nDevs are the inventors of ideas and making them liable for how others missuse them just cuts you off from new ideas completely, what we need todo is make operators / sites / places the normal people go to use the less desirable filters and tech liable (Instagram etc)\n\nAlso dev software licenses say you can't misuse their tech etc, so it's a joke to pretend they are in the wrong if users abuse their license.\n\nThat's a bit like holding petrol companies liable for people who stupidly try to throw bottles of gasoline onto fires:\n\nhttps://www.youtube.com/watch?v=3l50QZiPwnY\n\nEverything can be abused / used in a destructive way / used other than - intended usage.\n\nPowerful open source technologies always win and if your country is not compatible with openness then it's gonna get left behind (think north korea starving and surviving on cracked old builds of windows xp)",
          "created_at": "2026-01-08T22:44:14Z",
          "was_summarised": false
        },
        {
          "id": "nyhobge",
          "author": "jferments",
          "content": "This has been the point of the astro-turf \"anti-AI\" movement all along. I firmly believe that big tech corporations like Google, Microsoft, and OpenAI are behind the bots spreading \"anti-AI\" propaganda that supports laws that will essentially centralize control of AI and make open-source AI illegal.",
          "created_at": "2026-01-08T22:54:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhtae5",
          "author": "fortpatches",
          "content": "I understand where you are coming from, however, you may be misreading the text.\n\nSpecifically, you seem to have overlooked the phrase \"of a specifically identified individual\". E.g., (c)(1)(B)(i) states \"is primarily designed to produce 1 or more digital replicas of a specifically identified individual or individuals without \\[authorization\\].\"  The following subsections (ii) and (iii) have similar \"specifically identified individual\" language.\n\nThis would be more like making an AI designed to make you sound like Arnold Schwarzenegger as opposed to making an AI designed to make you sound like whatever audio sample you provide to it. Or Text-to-speech that makes \"AI Arnold\" say whatever you type.\n\nMoreover, is your AI \"primarily\" designed to produce \"AI Arnold\" audio?\n\nFurther, to actual knowledge is required: (c)(3)(B) states \"with respect to an activity carried out under paragraph (2) by an individual ..., the individual ... must have actual knowledge, ... that the applicable material isâ€” (i) a digital replica that was not authorized by the applicable right holder; or (ii) a product or service described in paragraph (2)(B).\"\n\nIn other words, the liability only attaches if the dev has \"actual knowledge\" that their service \"is \\*primarily\\* designed to produce a digital replica of a \\*specifically identified individual\\*.\"",
          "created_at": "2026-01-08T23:18:46Z",
          "was_summarised": false
        },
        {
          "id": "nyhkxhr",
          "author": "Aromatic-Low-4578",
          "content": "Don't most software licenses already try to protect the developer from liability due to users?  Will be interested to see how it plays out.",
          "created_at": "2026-01-08T22:37:58Z",
          "was_summarised": false
        },
        {
          "id": "nyitkq2",
          "author": "Acceptable_Home_",
          "content": "US and tech bros are actively targeting open source models",
          "created_at": "2026-01-09T02:28:39Z",
          "was_summarised": false
        },
        {
          "id": "nyhq9v2",
          "author": "Technical_Ad_440",
          "content": "i doubt it will pass sounds like a complete dud. the top ai will fight this cause it would kill all their tools in that case. personally for me i would rather have tons of fakes and misinfo why? cause in the future those that do due diligence will be the people you want to keep around those that dont fall for the missinfo and use tools to check origins and such they are the people you want. either as fans, work colleagues etc. \n\nwe need more and more fakes and we need people to start checking stuff not banning the tools. the future people are gonna be so hardened against all the missinfo and such that it wont make much difference. it actually baffles me why people dont want to flood fakes and such to just go oh yeh that incident it was AI and move on. its like a shield against most the normal attacks people do and actually weakens the attack vectors people have. kinda like how if people know where someone is people cant dox them cause people already know. trying it just makes the person go well people already know.\n\nyou cant trust most things anymore anyways so if i see things \"exposing\" stuff i just assume its probably AI until otherwise confirmed. it also means mundane attacks in the future are just gonna fall flat completely like if it doesnt affect you you will not care whatsoever",
          "created_at": "2026-01-08T23:03:38Z",
          "was_summarised": false
        },
        {
          "id": "nyhzwpj",
          "author": "AutomaticDriver5882",
          "content": "I would just have different developers, make different pieces of the code and then you would snap in the modules so no one person is responsible but maybe thatâ€™s an oversimplification",
          "created_at": "2026-01-08T23:52:54Z",
          "was_summarised": false
        },
        {
          "id": "nyif4r5",
          "author": "davedcne",
          "content": "Honest question, do you think your rep even understood what you were trying to explain to them? I think most of our politicians are so out of touch with technology that its like trying to teach a cave man calculus.",
          "created_at": "2026-01-09T01:11:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhvrpq",
          "author": "WristbandYang",
          "content": "OP only responds using AI. This is bot behavior.",
          "created_at": "2026-01-08T23:31:25Z",
          "was_summarised": false
        },
        {
          "id": "nyile2z",
          "author": "ortegaalfredo",
          "content": "If you think about it, It's way more disturbing than you think:\n\nThey don't want to criminalize porn, they want to criminalize FAKE porn, why? because they need to be in control of the porn generation, so men and particularly young men can be controlled with it.",
          "created_at": "2026-01-09T01:45:04Z",
          "was_summarised": false
        },
        {
          "id": "nyin0jf",
          "author": "SilentLennie",
          "content": "Don't know if it matters in practice what they propose. US politics is such a mess and the business interests are so 'great' they might prevent it being passed or no enforcement will happen (regulatory capture).",
          "created_at": "2026-01-09T01:53:44Z",
          "was_summarised": false
        },
        {
          "id": "nyhroca",
          "author": "PostEasy7183",
          "content": "Comments are coming in a lot quicker than I thought tonight. Please be patient and I will try to get back with you as many of you as I can in a couple hours. Thank you and make sure to bump the thread so this gets attention. Ensure to write to your reps or contact them with any other means necessary.",
          "created_at": "2026-01-08T23:10:40Z",
          "was_summarised": false
        },
        {
          "id": "nyioqq8",
          "author": "timschwartz",
          "content": "Just frame it like guns:\n\nModels don't deepfake people, people with models deepfake people.",
          "created_at": "2026-01-09T02:03:00Z",
          "was_summarised": false
        },
        {
          "id": "nyhwkd3",
          "author": "grady_vuckovic",
          "content": "I would love to hear everyone's suggestion for what the alternative is to stop the deep fakes?\n\nThis seems to make it pretty simple. If you're a developer and you release a tool that can be used to impersonate people, you're responsible.\n\nSo let's say you're all against that, alright, fair, what's the next step? What laws, what enforcement, how do we stop this?\n\nBecause \"we just have to learn to live with it\" is not gonna fly.",
          "created_at": "2026-01-08T23:35:34Z",
          "was_summarised": false
        },
        {
          "id": "nyhubjk",
          "author": "Marksta",
          "content": "\u0026gt;a tool that is **primarily** used for replicas.\n\nIsn't the primarily part key here? Because everytime someone advertises a TTS related thing, they bold 1000 times EAZY ONE SHOT VOICE REPLICATION as if that's its only use. This thing passes, then just take that out of your description and add a warning that you're expected to own the rights to any likeness you're duplicating.\n\nThis worry is pretty same as Photoshop being able to edit images you don't own. Photoshop doesn't spam 1000 times that you can steal copyright and must have written somewhere that copyright laws are your own problem somewhere.\n\nSo all that's left is probably actual heinous sites on the chopping block who should definitely be liable if their advertising is spamming how easy their tool will allow you to rip off likenesses and generate infringing content.",
          "created_at": "2026-01-08T23:23:59Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://share.google/u6dpy7ZQDvZWUrlfc",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "The post discusses the NO FAKES Act's impact on open-source AI models, highlighting legal risks for developers and advocating for a safe harbor, which is highly relevant to AI policy and innovation.",
        "processed_at": "2026-01-12T10:56:09.183596469Z"
      },
      "processed_at": "2026-01-09T03:44:48.91822167Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://share.google/u6dpy7ZQDvZWUrlfc: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918221109Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7d8bj",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/",
      "title": "Jensen Huang saying \"AI\" 121 times during the NVIDIA CES keynote - cut with one prompt",
      "content": "Someone had to count it. Turns out Jensen said \"AI\" exactly 121 times in the CES 2025 keynote.\n\nI used [https://github.com/OpenAgentPlatform/Dive](https://github.com/OpenAgentPlatform/Dive) (open-source MCP client) + two MCPs I made:\n\n\\- [https://github.com/kevinwatt/yt-dlp-mcp](https://github.com/kevinwatt/yt-dlp-mcp) \\- YouTube download  \n\\- [https://github.com/kevinwatt/ffmpeg-mcp-lite](https://github.com/kevinwatt/ffmpeg-mcp-lite) \\- video editing\n\n**One prompt:**\n\n\u0026gt;Task: Create a compilation video of every exact moment Jensen Huang says \"AI\".  \nVideo source: [https://www.youtube.com/watch?v=0NBILspM4c4](https://www.youtube.com/watch?v=0NBILspM4c4)\n\n\u0026gt;**Instructions:**\n\n\u0026gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)\n\n\u0026gt;Parse JSON3 to find every \"AI\" instance with precise start/end times\n\n\u0026gt;Use ffmpeg to cut clips (\\~50-100ms padding for natural sound)\n\n\u0026gt;Concatenate all clips chronologically\n\n\u0026gt;Output: Jensen\\_CES\\_AI.mp4\n\nDive chained the two MCPs together - download â†’ parse timestamps â†’ cut 121 clips â†’ merge. All local, no cloud.\n\nIf you want to see how it runs: [https://www.youtube.com/watch?v=u\\_7OtyYAX74](https://www.youtube.com/watch?v=u_7OtyYAX74)\n\nThe result is... hypnotic.",
      "author": "Prior-Arm-6705",
      "created_at": "2026-01-08T14:29:47Z",
      "comments": [
        {
          "id": "nygb8aw",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T19:15:15Z",
          "was_summarised": false
        },
        {
          "id": "nyejqtm",
          "author": "YearZero",
          "content": "Honestly that's probably a great summary of the keynote. He should've just done exactly that and it wouldn't change anything.",
          "created_at": "2026-01-08T14:33:06Z",
          "was_summarised": false
        },
        {
          "id": "nyf9wy6",
          "author": "DriveSolid7073",
          "content": "\"All local, no cloud.\"   \nopen video  \n\\\u0026gt;claude opus 4.5  \nI couldn't get Dive to work with my Koboldcpp, and the functions aren't being called.",
          "created_at": "2026-01-08T16:33:54Z",
          "was_summarised": false
        },
        {
          "id": "nyekf0a",
          "author": "LambdaHominem",
          "content": "gamers nexus would be proud\n\nhttps://youtu.be/-qbylbEek-M",
          "created_at": "2026-01-08T14:36:33Z",
          "was_summarised": false
        },
        {
          "id": "nyepodd",
          "author": "LinkSea8324",
          "content": "The fuck is that latex-leather jacket",
          "created_at": "2026-01-08T15:02:13Z",
          "was_summarised": false
        },
        {
          "id": "nyekw78",
          "author": "FastDecode1",
          "content": "Reminds me of the Xbox One reveal: https://www.youtube.com/watch?v=KbWgUO-Rqcw",
          "created_at": "2026-01-08T14:38:59Z",
          "was_summarised": false
        },
        {
          "id": "nyenr5m",
          "author": "International-Try467",
          "content": "This man is the reason why everything is so expensive",
          "created_at": "2026-01-08T14:53:01Z",
          "was_summarised": false
        },
        {
          "id": "nyeqoex",
          "author": "anon235340346823",
          "content": "0:44 \"hey I have AI\"",
          "created_at": "2026-01-08T15:07:02Z",
          "was_summarised": false
        },
        {
          "id": "nyg0uk3",
          "author": "Sea_Succotash3634",
          "content": "\"Consumer\" Electronic Show",
          "created_at": "2026-01-08T18:30:35Z",
          "was_summarised": false
        },
        {
          "id": "nygtg3m",
          "author": "Freonr2",
          "content": "\"Consumer\" Electronics Show, showing billion dollar datacenter configs.",
          "created_at": "2026-01-08T20:36:11Z",
          "was_summarised": false
        },
        {
          "id": "nygf0cq",
          "author": "r0ckl0bsta",
          "content": "Old McJensen's server farm.\nA-I-A-I-O ðŸŽ¶",
          "created_at": "2026-01-08T19:31:52Z",
          "was_summarised": false
        },
        {
          "id": "nyfjql1",
          "author": "budz",
          "content": "dope, I made a script that does this , back in October      [https://imgur.com/a/h0vc2f6](https://imgur.com/a/h0vc2f6)",
          "created_at": "2026-01-08T17:16:29Z",
          "was_summarised": false
        },
        {
          "id": "nyfa965",
          "author": "MMAgeezer",
          "content": "Appreciate you sharing the details of how you did this. One small thing though, it includes multiple clips of a narrator (i.e. not Jensen, as instructed) saying \"AI\" too.",
          "created_at": "2026-01-08T16:35:22Z",
          "was_summarised": false
        },
        {
          "id": "nyfp4wj",
          "author": "positivcheg",
          "content": "Nah. AMD wins, AMD mentioned AI 299 times. Big win for AMD.\n\nSadly it was just 1 more to a nice 300.",
          "created_at": "2026-01-08T17:40:13Z",
          "was_summarised": false
        },
        {
          "id": "nyervtk",
          "author": "GoranjeWasHere",
          "content": "He's literally the only one that can say as much AI as he wants.\n\nHe literally build his whole company on AI promise before everyone outside of research circle even knew what was AI and his hardware innovated so much that AI finally became a thing.\n\nOutside of Nvidia only Tesla I think and few other small companies can shout AI without sounding like a fool. Tesla was also super early in it and it got to the point where they were building their own chips just not to pay Nvidia tax for AI before AI even became investor bait.",
          "created_at": "2026-01-08T15:12:41Z",
          "was_summarised": false
        },
        {
          "id": "nyfskqo",
          "author": "SmegPoison",
          "content": "Old MacDonald had a farm, ai-ai-oh!",
          "created_at": "2026-01-08T17:55:09Z",
          "was_summarised": false
        },
        {
          "id": "nyfuof4",
          "author": "Caladan23",
          "content": "Duh surprise... it's an AI hardware company nowadays. Nvidia as a gaming company wasn't worth even 1/10th.",
          "created_at": "2026-01-08T18:04:10Z",
          "was_summarised": false
        },
        {
          "id": "nyh9hl9",
          "author": "deltamoney",
          "content": "Did you use AI to find all occurrence of AI?",
          "created_at": "2026-01-08T21:46:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhhxe1",
          "author": "JealousAmoeba",
          "content": "I admit Iâ€™m surprised that worked. Great demo for MCP.",
          "created_at": "2026-01-08T22:24:02Z",
          "was_summarised": false
        },
        {
          "id": "nyet9oy",
          "author": "Agile_Date6729",
          "content": "someone should make a remix of it -would be a banger",
          "created_at": "2026-01-08T15:19:06Z",
          "was_summarised": false
        },
        {
          "id": "nyffw6s",
          "author": "WavierLays",
          "content": "I mean this is like making a supercut of every time KFC's CEO says \"chicken\"",
          "created_at": "2026-01-08T16:59:32Z",
          "was_summarised": false
        },
        {
          "id": "nyezpx4",
          "author": "XiRw",
          "content": "Willing to bet anything the future of gaming (based on prices going up and shortages) will be server based subscriptions like everything else out there. Youâ€™ll own nothing and be happy.",
          "created_at": "2026-01-08T15:48:33Z",
          "was_summarised": false
        },
        {
          "id": "nyfa87l",
          "author": "dasjati",
          "content": "Someone should put a nice beat under this!",
          "created_at": "2026-01-08T16:35:15Z",
          "was_summarised": false
        },
        {
          "id": "nyfu4de",
          "author": "nonaveris",
          "content": "ai ai ai Cthulhu fthangh",
          "created_at": "2026-01-08T18:01:45Z",
          "was_summarised": false
        },
        {
          "id": "nyfx33c",
          "author": "Amazing_Athlete_2265",
          "content": "AI? AI!",
          "created_at": "2026-01-08T18:14:33Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://v.redd.it/hein55gpx4cg1",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/OpenAgentPlatform/Dive",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/kevinwatt/yt-dlp-mcp",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/kevinwatt/ffmpeg-mcp-lite",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=0NBILspM4c4",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=u\\_7OtyYAX74",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.youtube.com/watch?v=u_7OtyYAX74",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.6,
        "reason": "Demonstrates an innovative use of AI tools for automated video analysis and editing, showcasing a novel workflow with open-source MCPs and local processing to extract and compile specific audio instances of 'AI' from a keynote.",
        "processed_at": "2026-01-12T10:56:09.569495301Z"
      },
      "processed_at": "2026-01-09T03:44:48.918681432Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://v.redd.it/hein55gpx4cg1: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918318214Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/OpenAgentPlatform/Dive: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918395559Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/kevinwatt/yt-dlp-mcp: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918437352Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/kevinwatt/ffmpeg-mcp-lite: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918455669Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.youtube.com/watch?v=0NBILspM4c4: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918552974Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.youtube.com/watch?v=u\\_7OtyYAX74: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918571702Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.youtube.com/watch?v=u_7OtyYAX74: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918681151Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7mvuf",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/",
      "title": "Z.ai  (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange",
      "content": "",
      "author": "Old-School8916",
      "created_at": "2026-01-08T20:23:59Z",
      "comments": [
        {
          "id": "nygtope",
          "author": "ForsookComparison",
          "content": "Hopefully they're all partying it up.\n\nAnd hopefully their new shareholders don't mind it if they spend millions in compute to give us free stuff ðŸ˜¬",
          "created_at": "2026-01-08T20:37:16Z",
          "was_summarised": false
        },
        {
          "id": "nygwihp",
          "author": "_Sneaky_Bastard_",
          "content": "they also said GLM 5 is in training. hoping it would be a open weight release.",
          "created_at": "2026-01-08T20:49:51Z",
          "was_summarised": false
        },
        {
          "id": "nygvbc0",
          "author": "TheAncientPizza711",
          "content": "They issued shares at HK$116.20 each. Opened at HK$120 and is now currently HK$131.50.\n\nStock is up 13.17% on its 1st day. Not bad.",
          "created_at": "2026-01-08T20:44:33Z",
          "was_summarised": false
        },
        {
          "id": "nyh2044",
          "author": "FullOf_Bad_Ideas",
          "content": "Minimax IPOs a day later, 9th of January.\n\nLots of info about both of them can be found here:\n\n[Zhipu offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1230/2025123000017.pdf)\n\n[Minimax offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1231/2025123100025.pdf)\n\nSource: [this website](https://www.hkex.com.hk/Services/Trading/Securities/Trading-News/Newly-Listed-Securities?sc_lang=en)\n\nOne of the very interesting thing contained there is that Zhipu has slightly negative profit margin on GLM Coding Plan. They lose money on serving alone, not even counting in marketing or R\u0026amp;D costs.",
          "created_at": "2026-01-08T21:14:09Z",
          "was_summarised": false
        },
        {
          "id": "nyh1ztw",
          "author": "jacek2023",
          "content": "Hype hype and still no Air",
          "created_at": "2026-01-08T21:14:07Z",
          "was_summarised": false
        },
        {
          "id": "nygrahs",
          "author": "rookan",
          "content": "Good for them",
          "created_at": "2026-01-08T20:26:28Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://x.com/Zai_org/status/2009290783678239032",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.8,
        "reason": "Announces the IPO of Z.ai, the AI lab behind GLM, on the Hong Kong Stock Exchange, providing significant news about a major AI company's financial milestone.",
        "processed_at": "2026-01-12T10:56:08.885552763Z"
      },
      "processed_at": "2026-01-09T03:44:48.918862264Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://x.com/Zai_org/status/2009290783678239032: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.918861723Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7jd1a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/",
      "title": "LFM2.5 1.2B Instruct is amazing",
      "content": "This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.\n\nImportant note:  \n\"\"\"  \nWe recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.\n\n\"\"\"\n\n[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)",
      "author": "Paramecium_caudatum_",
      "created_at": "2026-01-08T18:17:04Z",
      "comments": [
        {
          "id": "nyg2ddq",
          "author": "pj-frey",
          "content": "It is the perfect small \"helper\" model for Open WebUI creating tags, chat headlines, web searches and that kind of stuff. Fast AND good. I love it. Big thank you to LiquidAI (and unsloth).",
          "created_at": "2026-01-08T18:37:05Z",
          "was_summarised": false
        },
        {
          "id": "nyg9prk",
          "author": "YearZero",
          "content": "Yeah models at this size just need to work with the data you give them and adhere to the prompt as much as possible.",
          "created_at": "2026-01-08T19:08:38Z",
          "was_summarised": false
        },
        {
          "id": "nygh6w6",
          "author": "-Akos-",
          "content": "I'm amazed, especially now that it has tool use. A few days ago it didn't yet, but now I can enable MCP in LM Studio, and have blazing fast inference. On my 8th gen i7 with a 1050Ti nvidia I am getting 41 tps!!\n\nI've asked it to create a small webpage, and then it complained that it's not good at that, so indeed the programming part is correct.",
          "created_at": "2026-01-08T19:41:30Z",
          "was_summarised": false
        },
        {
          "id": "nyggecn",
          "author": "LionStrange493",
          "content": "Yeah, that caveat is important.\n\nSmaller models can be surprisingly good, but once tools/RAG get involved, edge cases show up quickly.\n\nCurious how this holds up in real agent setups.",
          "created_at": "2026-01-08T19:38:00Z",
          "was_summarised": false
        },
        {
          "id": "nyh8p9o",
          "author": "Noob_l",
          "content": "Very promising, can it also do translation? \n~~What models are recommended for local on device translation?\nI had mixed results with small qwen results. \nThank you in advance for any helpers that can point me into the right direction. (I do not know which benchmarks would be for translation)~~\n\nEdit: no it does seem like a model that cannot follow instructions on translation well. I would still love to know which models are used by the community",
          "created_at": "2026-01-08T21:43:20Z",
          "was_summarised": false
        },
        {
          "id": "nygrptl",
          "author": "crantob",
          "content": "LFM2 has a variety of interrrresting models....",
          "created_at": "2026-01-08T20:28:22Z",
          "was_summarised": false
        },
        {
          "id": "nyh2lxw",
          "author": "ElectronSpiderwort",
          "content": "It is strong for 1.2B. I argued with it a bit about a topic I know well; it was confidently incorrect for a particular formula, and even when I corrected it, it kept spitting out the incorrect formula. It just couldn't use the corrected formula because it was sure the incorrect formula was \"the standard formula\". I wonder what happens in RAG when it retrieves a piece of information it thinks is wrong? \n\nRelatedly, I thing training data should include epistemic humility",
          "created_at": "2026-01-08T21:16:51Z",
          "was_summarised": false
        },
        {
          "id": "nyh1m7k",
          "author": "DHasselhoff77",
          "content": "I tried to replace Granite-4.0-h-micro with this for tab completion but the `/completion` endpoint in llama.cpp gave 501 errors when I loaded LFM2.5-1.2B-Q8.gguf. Perhaps it's missing FIM support?",
          "created_at": "2026-01-08T21:12:26Z",
          "was_summarised": false
        },
        {
          "id": "nyhhafa",
          "author": "countAbsurdity",
          "content": "Dunno I was never impressed with previous iterations, they always misinterpreted what I wrote them or hallucinated a lot.",
          "created_at": "2026-01-08T22:21:05Z",
          "was_summarised": false
        },
        {
          "id": "nyhl8y7",
          "author": "AyraWinla",
          "content": "Favorably impressed; it actually writes pretty well and has very good understanding for its size. Under 2b, it's by far the best I've seen at it, with no real competition (small Qwen and Granite aren't too good at it in my opinion, and Gemma 2 2b is pretty outdated by now).\n\nLarger models like Gemma 3 4b or even E2B definitely give better results, but LFM2.5 1.2b runs MUCH faster on my phone; it's the first time I try a model that feels both smart and fast local on my phone. Normally it's just one or the other. I'm not going to throw away my usual Gemma models, but LFM2.5 will definitely see some use from me and I'll certainly be experimenting more with it.\n\nPretty impressive!",
          "created_at": "2026-01-08T22:39:27Z",
          "was_summarised": false
        },
        {
          "id": "nygrtn7",
          "author": "nonerequired_",
          "content": "But context size is too low. Is there any way to increase that?",
          "created_at": "2026-01-08T20:28:50Z",
          "was_summarised": false
        },
        {
          "id": "nyiglpa",
          "author": "Thin_Yoghurt_6483",
          "content": "Give practical examples of how to use a model.",
          "created_at": "2026-01-09T01:19:35Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.7,
        "reason": "The post highlights a new LLM release with specific performance claims and usage recommendations, providing technical context and a direct link to the model on Hugging Face.",
        "processed_at": "2026-01-12T10:56:08.534252016Z"
      },
      "processed_at": "2026-01-09T03:44:48.91918415Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919183639Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7a62a",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/",
      "title": "AI21 Labs releases Jamba2",
      "content": "https://preview.redd.it/zmo6dijns4cg1.png?width=1800\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb\n\n52B [https://huggingface.co/ai21labs/AI21-Jamba2-Mini](https://huggingface.co/ai21labs/AI21-Jamba2-Mini)\n\nJamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2).\n\n# Key Advantages\n\n* **Superior reliability-to-throughput ratio:** Maintains high performance at 100K+ token contexts\n* **Category-leading benchmarks:** Excels on IFBench, IFEval, Collie, and FACTS\n* **Statistically significant quality wins:** Outperforms comparable models on real-world enterprise tasks\n* **256K context window:** Processes technical manuals, research papers, and knowledge bases\n* **Apache 2.0 License:** Fully open source for commercial use\n* **Production-optimized:** Lean memory footprint for scalable deployments\n\nhttps://preview.redd.it/cqwicpwts4cg1.png?width=2400\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d\n\n3B [https://huggingface.co/ai21labs/AI21-Jamba2-3B](https://huggingface.co/ai21labs/AI21-Jamba2-3B)\n\nJamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devicesâ€”iPhones, Androids, Macs, and PCsâ€”while maintaining the grounding and instruction-following capabilities required for production use.\n\nReleased under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the [full release blog post](https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2).\n\n# \n\n# Key Advantages\n\n* **On-device deployment:** Runs efficiently on iPhones, Androids, Macs, and PCs\n* **Ultra-compact footprint:** 3B parameters enabling edge deployments with minimal resources\n* **Benchmark leadership:** Excels on IFBench, IFEval, Collie, and FACTS\n* **256K context window:** Processes long documents and knowledge bases\n* **Apache 2.0 License:** Fully open source for commercial use\n* **SSM-Transformer architecture:** Memory-efficient design for resource-constrained environments\n\nit works in llama.cpp, tested on my Windows desktop:\n\nhttps://preview.redd.it/ijzgde7bg5cg1.png?width=3802\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71\n\nfixed blog post [https://www.ai21.com/blog/introducing-jamba2/](https://www.ai21.com/blog/introducing-jamba2/)\n\nGGUFs are in progress [https://huggingface.co/mradermacher/model\\_requests/discussions/1683](https://huggingface.co/mradermacher/model_requests/discussions/1683)\n\n\n\nprevious generation of Jamba models\n\n399B [https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7)\n\n52B [https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7](https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7)\n\n3B [https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B)",
      "author": "jacek2023",
      "created_at": "2026-01-08T12:10:15Z",
      "comments": [
        {
          "id": "nye2sbf",
          "author": "ilintar",
          "content": "Previous Jamba models were terrible. They were an architectural novelty but their performance was abysmal. Curious to see if they've improved.",
          "created_at": "2026-01-08T13:01:25Z",
          "was_summarised": false
        },
        {
          "id": "nydvl7n",
          "author": "Smooth-Cow9084",
          "content": "52b named \"mini\" lol\n\n\nFor those curious, it has 12b active.Â \n\n\nThen the 3b model has no info on HF repository, for whatever reason.",
          "created_at": "2026-01-08T12:14:31Z",
          "was_summarised": false
        },
        {
          "id": "nyedmvu",
          "author": "LinkSea8324",
          "content": "Fixed blog link for the brainlets : https://ai21.com/blog/introducing-jamba2",
          "created_at": "2026-01-08T14:01:41Z",
          "was_summarised": false
        },
        {
          "id": "nyf3ail",
          "author": "FullOf_Bad_Ideas",
          "content": "It shares pre-training weights with Jamba 1.5, as per their own documentation.\n\nPre-training from scratch is becoming less and less common.\n\nI wonder where's 10T Qwen at.",
          "created_at": "2026-01-08T16:04:36Z",
          "was_summarised": false
        },
        {
          "id": "nygexex",
          "author": "YearZero",
          "content": "# Merged Benchmark Comparison Table\n\n|Benchmark Category|Jamba2 3B|Jamba2 Mini|**Qwen3 4B A3B Instruct 2507**|Qwen3 30B A3B Instruct 2507|Nemotron3 30B A3B (Non-Thinking)|Minstral3 3B Instruct 2512|Minstral3 14B Instruct 2512|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|**IFBench**|0.36|**0.38**|0.32|0.32|0.33|0.22|0.27|\n|**Collie**|0.24|0.33|**0.34**|0.29|0.30|0.20|0.26|\n|**IFEval**|0.93|**0.97**|0.90|0.91|0.88|0.67|0.73|\n|**FACTS**|0.54|**0.57**|0.54|0.54|0.48|0.44|0.49|\n|**Enterprise Reliability Score (Avg)**|0.52|**0.56**|0.53|0.51|0.50|0.38|0.44|\n\nGGUF's available here now:  \n[https://huggingface.co/bartowski/ai21labs\\_AI21-Jamba2-Mini-GGUF](https://huggingface.co/bartowski/ai21labs_AI21-Jamba2-Mini-GGUF)\n\nEdit: The 52b failed every single one of my tests (I used the Q6\\_K\\_L quant). It wasn't able to do anything right or follow instructions at all. Not sure if back-end and/or template may need adjusting, but right now the model is unusable for me.",
          "created_at": "2026-01-08T19:31:29Z",
          "was_summarised": false
        },
        {
          "id": "nyesr4v",
          "author": "Cool-Chemical-5629",
          "content": "Just a note. Jamba 1.7 alone wasn't the first generation. There were also 1.6 and 1.5.",
          "created_at": "2026-01-08T15:16:43Z",
          "was_summarised": false
        },
        {
          "id": "nyeiu0b",
          "author": "Accomplished_Ad9530",
          "content": "Apache 2.0 for the 52B, nice. Only the 3B had a permissive license in the prior gen, so itâ€™s nice to see larger models open up.",
          "created_at": "2026-01-08T14:28:29Z",
          "was_summarised": false
        },
        {
          "id": "nyho3pp",
          "author": "zoyer2",
          "content": "tested some one-shot coding tasks using ai21labs\\_AI21-Jamba2-Mini-Q4\\_K\\_M.gguf (52b) in llama.cpp vs:  \n\\- Qwen3-Next-80B-A3B-Instruct-IQ4\\_XS.gguf  \n\\- cerebras\\_GLM-4.5-Air-REAP-82B-A12B-IQ3\\_XXS.gguf  \n\\- Qwen3-Coder-30B-A3B-Instruct-UD-Q6\\_K\\_XL.gguf\n\nwasn't close to beat them, many times just started to outputting crap. I really would want a model this size to be a great coder model",
          "created_at": "2026-01-08T22:52:56Z",
          "was_summarised": false
        },
        {
          "id": "nye7ag6",
          "author": "SlowFail2433",
          "content": "Wow a 400B sub-quadratic model\n\n\nThis is by far the largest sub-quadratic model ever released as far as I know",
          "created_at": "2026-01-08T13:27:36Z",
          "was_summarised": false
        },
        {
          "id": "nydyh0r",
          "author": "abkibaarnsit",
          "content": "Blog post giving 404",
          "created_at": "2026-01-08T12:34:10Z",
          "was_summarised": false
        },
        {
          "id": "nyegfqg",
          "author": "International-Try467",
          "content": "Glad to see that AI-21 is still around. I remember them from the AI Dungeon days where they replaced GPT-3 with Jurassic instead. I wonder if their models are less slopped than OpenAI's",
          "created_at": "2026-01-08T14:16:18Z",
          "was_summarised": false
        },
        {
          "id": "nyf05ou",
          "author": "Cool-Chemical-5629",
          "content": "I guess there's no day one support for LlamaCpp. It usually leads to the models being buried under newer ones which have support on day one. What would be really cool is the REAP version 30B and support in LlamaCpp.",
          "created_at": "2026-01-08T15:50:29Z",
          "was_summarised": false
        },
        {
          "id": "nyfims9",
          "author": "FizzarolliAI",
          "content": "PSA: AI21 is an Israeli company founded by ex-IDF spies from their NSA equivalent who support the ongoing attempts at ethnic cleansing and genocide in Palestine. They are not worth supporting, and neither are their models.",
          "created_at": "2026-01-08T17:11:36Z",
          "was_summarised": false
        },
        {
          "id": "nye3dns",
          "author": "indicava",
          "content": "Blog post is 404â€™d, anyone know what kind of VRAM requirements we are looking at here for the 3B model (at native BF16)?",
          "created_at": "2026-01-08T13:05:03Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-Mini",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-3B",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://www.ai21.com/blog/introducing-jamba2/",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/mradermacher/model%5C_requests/discussions/1683",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/mradermacher/model_requests/discussions/1683",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 1,
        "reason": "AI21 Labs releases Jamba2 models, including 52B and 3B variants with 256K context, Apache 2.0 license, and SSM-Transformer architecture, highlighting enterprise reliability and on-device deployment.",
        "processed_at": "2026-01-12T10:56:09.211300037Z"
      },
      "processed_at": "2026-01-09T03:44:48.920464526Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-Mini: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919568791Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919650965Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-3B: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919762158Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919887199Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.ai21.com/blog/introducing-jamba2/: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.919994885Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/mradermacher/model%5C_requests/discussions/1683: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.92010216Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/mradermacher/model_requests/discussions/1683: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920173252Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920255948Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920375478Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920464246Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q77rxh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/",
      "title": "Z-image base model is being prepared for release",
      "content": "[https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08](https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08)",
      "author": "Ravencloud007",
      "created_at": "2026-01-08T09:51:33Z",
      "comments": [
        {
          "id": "nydp3k8",
          "author": "AmazinglyObliviouse",
          "content": "Wake me up when it actually is released, I do not care for your month long teasing.",
          "created_at": "2026-01-08T11:26:07Z",
          "was_summarised": false
        },
        {
          "id": "nye3gr5",
          "author": "FastDecode1",
          "content": "Gooners waiting with bated breath, blue balls, and shivers runnin'.",
          "created_at": "2026-01-08T13:05:33Z",
          "was_summarised": false
        },
        {
          "id": "nydfyut",
          "author": "Geritas",
          "content": "True if big.\n\nI thought they were quietly abandoning the idea.",
          "created_at": "2026-01-08T10:08:18Z",
          "was_summarised": false
        },
        {
          "id": "nyilefa",
          "author": "q5sys",
          "content": "This really only means that they are releasing it on THAT platform.  It doesn't necessarily mean they're going to release open weights to us at the same time.   \n  \nI want them to release open weights, but a Cloud service prepping for it to be able to be used doesn't mean it's getting released to run on our systems. (I hope I'm wrong)",
          "created_at": "2026-01-09T01:45:07Z",
          "was_summarised": false
        },
        {
          "id": "nyilp9m",
          "author": "sammoga123",
          "content": "I thought it would only be T2I, but it seems it will also be able to edit images. I just hope it allows more than one input image, and that it's at least on par with Qwen Edit, because Flux 2 is still useless compared to Qwen.",
          "created_at": "2026-01-09T01:46:44Z",
          "was_summarised": false
        },
        {
          "id": "nyfjv5u",
          "author": "YearZero",
          "content": "What's the purpose of an image gen base model? I understand text base models without instruction tuning just complete/continue a text. What would this one do, and what would it be used for? Would it complete an incomplete image or something? So if I gave it an image with a missing section, would it fill that in? Would it handle any instructions? I know qwen-image-edit can already do this, but it's instruct tuned for that function, and it can work on sections of an image with the right scaffolding.",
          "created_at": "2026-01-08T17:17:02Z",
          "was_summarised": false
        },
        {
          "id": "nyh874x",
          "author": "azerpsen",
          "content": "Waiting for the posts crying why the model is too censored (it refused to generate furry futa inflation incest CP images)",
          "created_at": "2026-01-08T21:41:09Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/038zb25ok3cg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.8,
        "reason": "The post indicates that the Z-Image base model is being prepared for release, supported by GitHub commit activity showing recent updates and integration efforts, suggesting imminent availability of a new AI image generation model.",
        "processed_at": "2026-01-12T10:56:06.89657295Z"
      },
      "processed_at": "2026-01-09T03:44:48.9205561Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher\u0026amp;since=2025-12-31\u0026amp;until=2026-01-08: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920554908Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7o8kl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/",
      "title": "GLM-4.7 on 4x RTX 3090 with ik_llama.cpp",
      "content": "With the help of Opus 4.5 I got unsloth/GLM-4.7-GGUF (Q4\\_K\\_M) running on my 4x RTX 3090 setup using ik\\_llama.cpp in Docker. I wanted to share my benchmark results and configuration, and ask if these numbers are what I should expect - or if there's room for improvement.\n\n# My Setup\n\n|Component|Specs|\n|:-|:-|\n|Motherboard|Supermicro H12SSL-i|\n|CPU|AMD EPYC 7282|\n|GPUs|4x NVIDIA RTX 3090 (96GB VRAM total, all at PCIe x16)|\n|RAM|256GB DDR4-2133|\n|Storage|2 TB NVMe SSD|\n\n# Benchmark Results\n\n|Config|Context|n-cpu-moe|Batch|VRAM/GPU|Prompt|**Generation**|\n|:-|:-|:-|:-|:-|:-|:-|\n|Initial (mmap)|16K|all|512|\\~5 GB|2.8 t/s|3.1 t/s|\n|split-mode layer|16K|partial|4096|\\~17 GB|2.8 t/s|âš ï¸ 0.29 t/s|\n|\\+ no-mmap|16K|all|4096|\\~10 GB|8.5 t/s|3.45 t/s|\n|\\+ n-cpu-moe 72|16K|72|4096|\\~17 GB|9.9 t/s|4.12 t/s|\n|**Best 8K**|**8K**|**65**|**4096**|**\\~21 GB**|**12.0 t/s**|**4.48 t/s** â­|\n|**Best 16K**|**16K**|**68**|**2048**|**\\~19 GB**|**10.5 t/s**|**4.28 t/s** â­|\n\n# Benchmark Methodology\n\nAll tests were performed using the same simple request via curl:\n\n    curl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"model\": \"GLM-4.7-GUFF\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Write a short Haiku.\"}],\n        \"temperature\": 0.7,\n        \"max_tokens\": 100\n      }'\n\nThe response includes timing information:\n\n    {\n      \"timings\": {\n        \"prompt_n\": 17,\n        \"prompt_ms\": 1419.902,\n        \"prompt_per_second\": 11.97,\n        \"predicted_n\": 100,\n        \"predicted_ms\": 22301.81,\n        \"predicted_per_second\": 4.48\n      }\n    }\n\n* **prompt\\_per\\_second**: How fast the input tokens are processed\n* **predicted\\_per\\_second**: How fast new tokens are generated (this is what matters most for chat)\n\nEach configuration was tested with a fresh server start (cold start) and the first request after warmup. Note that GLM-4.7 has a \"thinking/reasoning\" mode enabled by default, so the 100 generated tokens include internal reasoning tokens.\n\n# My Current Configuration\n\n# Best for 8K Context (fastest):\n\n    llama-server \\\n        --model \"/models/GLM-4-Q4_K_M-00001-of-00005.gguf\" \\\n        --host 0.0.0.0 --port 8080 \\\n        --ctx-size 8192 \\\n        --n-gpu-layers 999 \\\n        --split-mode graph \\\n        --flash-attn on \\\n        --no-mmap \\\n        -b 4096 -ub 4096 \\\n        --cache-type-k q4_0 --cache-type-v q4_0 \\\n        --k-cache-hadamard \\\n        --jinja \\\n        --n-cpu-moe 65\n\n# Best for 16K Context:\n\n    llama-server \\\n        --model \"/models/GLM-4-Q4_K_M-00001-of-00005.gguf\" \\\n        --host 0.0.0.0 --port 8080 \\\n        --ctx-size 16384 \\\n        --n-gpu-layers 999 \\\n        --split-mode graph \\\n        --flash-attn on \\\n        --no-mmap \\\n        -b 2048 -ub 2048 \\\n        --cache-type-k q4_0 --cache-type-v q4_0 \\\n        --k-cache-hadamard \\\n        --jinja \\\n        --n-cpu-moe 68\n\n# Key Findings:\n\n1. `--no-mmap` **is crucial** \\- Loading the model into RAM instead of memory-mapping from SSD **tripled** my prompt processing speed (2.8 â†’ 12 t/s)\n2. `--split-mode graph` **not** `layer` \\- Layer mode gave me only 0.29 t/s because GPUs process sequentially. Graph mode enables true tensor parallelism.\n3. `--n-cpu-moe X` \\- This flag controls how many MoE layers stay on CPU.\n4. **Batch size matters** \\- Smaller batches (2048) allowed more MoE layers on GPU for 16K context.\n\n# Docker Setup\n\nI'm running this in Docker. Here's my `docker-compose.yml`:\n\n    services:\n      glm-4:\n        build:\n          context: .\n          dockerfile: Dockerfile\n        container_name: glm-4-server\n        deploy:\n          resources:\n            reservations:\n              devices:\n                - driver: nvidia\n                  count: all\n                  capabilities: [gpu]\n        volumes:\n          - /path/to/models:/models:ro\n        ports:\n          - \"8080:8080\"\n        environment:\n          - CTX_MODE=${CTX_MODE:-8k}  # Switch between 8k/16k\n          - NO_MMAP=true\n          - KV_CACHE_K=q4_0\n          - KV_CACHE_V=q4_0\n          - K_CACHE_HADAMARD=true\n        shm_size: '32gb'\n        ipc: host\n        restart: unless-stopped\n\nAnd my `Dockerfile` builds ik\\_llama.cpp with CUDA support:\n\n    FROM nvidia/cuda:12.4.0-devel-ubuntu22.04\n    \n    # Install dependencies\n    RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\\n        git cmake build-essential curl \\\n        \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*\n    \n    # Clone and build ik_llama.cpp\n    WORKDIR /opt\n    RUN git clone https://github.com/ikawrakow/ik_llama.cpp.git\n    WORKDIR /opt/ik_llama.cpp\n    \n    RUN cmake -B build \\\n        -DGGML_CUDA=ON \\\n        -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n        -DCMAKE_CUDA_ARCHITECTURES=\"86\" \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        \u0026amp;\u0026amp; cmake --build build --config Release -j$(nproc) \\\n        \u0026amp;\u0026amp; cmake --install build\n    \n    EXPOSE 8080\n    COPY entrypoint.sh /entrypoint.sh\n    RUN chmod +x /entrypoint.sh\n    ENTRYPOINT [\"/entrypoint.sh\"]\n\n# Questions\n\n1. **Are these speeds (4.48 t/s generation) normal for this setup?** I've seen some posts mentioning 5-6 t/s with 2x RTX 5090, but they had 64GB VRAM total vs my 96GB.\n2. **Any other flags I should try?** I tested `--run-time-repack` but it didn't help much.\n3. **Is there a better MoE offloading strategy?** I'm using `--n-cpu-moe` but I know there's also the `-ot` regex approach.\n4. **Would a different quantization help?** Currently using Q4\\_K\\_M. Would IQ4\\_XS or Q5\\_K\\_M be faster/better?\n5. **Low GPU power usage during inference?** My cards are power-limited to 275W each, but during inference they only draw \\~100-120W. Could this be a bottleneck limiting my token/s?\n\nI would love to hear your thoughts and any optimization tips.",
      "author": "iamn0",
      "created_at": "2026-01-08T21:14:19Z",
      "comments": [
        {
          "id": "nyhbw4l",
          "author": "massive_rock33",
          "content": "The token gen seems too slow, im surprised it's this slow with 4 gpu",
          "created_at": "2026-01-08T21:57:11Z",
          "was_summarised": false
        },
        {
          "id": "nyhsog9",
          "author": "Lissanro",
          "content": "My rig is somewhat similar to yours, also EPYC and DDR4-based, with four 3090 cards. I have EPYC 7763 with 1TB DDR4 3200MHz RAM + 4x3090 GPUs, and with IQ4 quant of GLM-4.7 I get about 6 tokens/s generation 200 tokens/s prompt processing., with 19 full layers on GPUs along with common expert tensors and 200K context cache at Q8. Here is my command for reference:\n\n    numactl --cpunodebind=0 --interleave=all ~/pkgs/ik_llama.cpp/build/bin/llama-server \\\n    --model /mnt/neuro/models/GLM-4.7-IQ4_K/GLM-4.7-IQ4_K-00001-of-00006.gguf \\\n    --ctx-size 202752 --n-gpu-layers 62 --tensor-split 25,23,25,27 -ctk q8_0 -ctv q8_0 -b 4096 -ub 4096 -fa on \\\n    -ot \"blk\\.(3|4|5|6)\\.ffn_.*=CUDA0\" \\\n    -ot \"blk\\.(8|9|10|11|12)\\.ffn_.*=CUDA1\" \\\n    -ot \"blk\\.(13|14|15|16|17)\\.ffn_.*=CUDA2\" \\\n    -ot \"blk\\.(18|19|20|21|22)\\.ffn_.*=CUDA3\" \\\n    -ot exps=CPU \\\n    --threads 64 --host 0.0.0.0 --port 5000 \\\n    --jinja \\\n    --slot-save-path /var/cache/ik_llama.cpp/glm-4.7\n\nBy the way, with  `--split-mode graph` I get crash:\n\n    /home/lissanro/pkgs/ik_llama.cpp/src/llama.cpp:566: GGML_ASSERT(kl \u0026amp;\u0026amp; (!kv_self.v_l[il] || vl)) failed\n\nAnd with `--k-cache-hadamard` it generated gibberish. I tried with the latest ik\\_llama.cpp from git. But it sounds like you managed to get these options working?\n\nAs of your bottleneck, for token generation I think it is both your CPU and RAM. During token generation, all 64 cores on my EPYC 7763 get saturated a little bit sooner than memory bandwidth of 8-channel 3200MHz RAM, so any slower CPU would reduce the performance. This also means your rig is well balanced, faster memory would probably not make much difference, so your current memory is well suited for your CPU. 4 tokens/s is very good given your CPU and RAM speeds, probably GPUs help a lot.\n\nBut your prompt processing speed is unusually slow, you should be getting about the same speed as me (around 200 tokens/s) since prompt processing is done on GPUs and my CPU is almost idle while it happens. I see you have `--n-cpu-moe 68` \\- I did not try it myself, but very likely it is not equivalent to the proper `-ot` options. Calibrating `--tensor-split` and `-ot` lines to decide how many layers to put and where can be time consuming but I think it is necessary if you want to get the best performance. Notice how I have `exps=CPU` at the end - order is important. And you can manually write layers number from 3 and up in each `-ot` line, to define which layers go to which CUDA device - in my case I put 4 on my first GPU (CUDA0) and 5 layers on the rest, but depending on your VRAM usage by the system and other factors, you may need to change my example.",
          "created_at": "2026-01-08T23:15:41Z",
          "was_summarised": false
        },
        {
          "id": "nyhwmta",
          "author": "tenebreoscure",
          "content": "You can get definitely better speeds. I'm currently running IQ4\\_XS on plain llama.cpp, using something like this:\n\n    CUDA_DEVICE_ORDER='PCI_BUS_ID' LLAMA_SET_ROWS=1 ./llama.cpp/build/bin/llama-server \\ \n    --model zai-org_GLM-4.7-IQ4_XS-00001-of-00005.gguf \\ \n    --alias bartowski/GLM-4.7-IQ4_XS \\ \n    --ctx-size 65536 \\ \n    --flash-attn on \\ \n    -ngl 99 \\ \n    -ot \"blk\\.[0-9]\\.ffn.*=CUDA0,blk\\.1[0-2]\\.ffn.*=CUDA0,blk\\.1[3-9]\\.ffn.*=CUDA1,blk\\.20\\.ffn.*=CUDA1,blk\\.2[1-8]\\.ffn.*=CUDA2,blk\\.29\\.ffn.*=CUDA3,blk\\.3[0-6]\\.ffn.*=CUDA3,blk\\.3[7-9]\\.ffn.*=CUDA4,blk\\.4[0-1]\\.ffn.*=CUDA4,blk\\\\..*_exps\\\\.=CPU\" \\ \n    --no-mmap \\ \n    --threads 11 \\ \n    --parallel 1 \\ \n    --host 127.0.0.1 \\ \n    --port 8080\n\nOn an AM5 platform with 70 GB/S Ram bandwdith, 192GB RAM and a combination of 120GB of VRAM.\n\nI get ~100pp/8TG @8k and ~100pp/7.5TG @16k, and run at 65K. The trick is to load with the override tensor flag as many layers as you can on the video cards without compromising too much with context. \n\nI'd suggest against quantizing the cache to even 8 bit as it brings spelling errors and slight hallucinations, it's tolerable for general conversation, not for coding or anything income related. \n\nAlso for ik_llama.cpp use Ubergarm's quants https://huggingface.co/ubergarm/GLM-4.7-GGUF or Thireus's, they are optimized for ik_llama.cpp and should give faster results. Ubergarm has an excellent readme with every trick to squeeze out performances.\n\nHow many ram sticks do you have? That epyc has an 8ch memory controller, if you have all the dimm slots occupied the ram bandwidth should be around double mine. I'd suggest measuring it with Intel MLC or a similar tool. Nevermind, read the comment about the processor having 2 ccds. So yeah, It's probably bottlenecking. I'd still try to squeeze out some more numbers.\n\nPersonally --n-cpu-moe never worked for me and gave sluggish speed, the -ot approach worked better, so I'd try that one. I'd also choose the smallest Q4 quant over the K_M one. Power limiting should not be an issue, you can monitor it with nvtop anyway to check if the gpus cap it.",
          "created_at": "2026-01-08T23:35:56Z",
          "was_summarised": false
        },
        {
          "id": "nyhdw1m",
          "author": "FullstackSensei",
          "content": "You can sell those four 3090s, and buy an 8 CCD Epyc (256MB L3 cache). You'll save a lot of power, and end up with better performance.\n\nJokes aside, two things are killing your performance: that CPU and your RAM speed (you don't say how many sticks you have, so there's also a chance your memory configuration is bad if you have 4x64GB sticks).\n\nEpyc Rome and Milan have ~26GB/s bandwidth between each CCD and the IO die. To get maximum memory bandwidth, you need a CPU with 8 CCDs. Given the compute, 32 cores might not cut it, so you need either 48 or 64 cores to crunch those numbers. The IO die has 8 memory channels, each running at a maximum speed of 3200MT/s. You're running at 2133, which would be bad, if it wasn't for your CPU having 2 CCDs only cutting it's effective bandwidth by four.\n\nKnow thy hardware before buying parts and putting a system together. It's a bit of a moot point now that you have it, and with current RAM prices you'll probably need a kidney to upgrade to 3200 memory. Though you should try to overclock it to 2400 or even 2666 if 2400 works.1",
          "created_at": "2026-01-08T22:05:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhgj3c",
          "author": "MikeRoz",
          "content": "Can you do any better on the RAM speed? I can beat this using 2 GPUs and all the experts in system memory (also using ik_llama.cpp, IQ5_K quant). Getting ~10 tps generation, ~83 tps prompt processing, 4k or so into context out of 64k. Main difference between us is I'm running DDR5-6000 and you're running DDR4-2133. Another is that I'm not bothering with anything but the first layers on GPU - this allows me to allocate 64k cache yet use only 2 GPUs.",
          "created_at": "2026-01-08T22:17:37Z",
          "was_summarised": false
        },
        {
          "id": "nyhi65g",
          "author": "Egoz3ntrum",
          "content": "That dockerfile will be useful, thanks.",
          "created_at": "2026-01-08T22:25:09Z",
          "was_summarised": false
        },
        {
          "id": "nyhm5j5",
          "author": "FullOf_Bad_Ideas",
          "content": "try 2.10bpw exl3 quant\n\nhttps://huggingface.co/mratsim/GLM-4.7-EXL3/tree/2.10bpw-tuned\n\nIt probably won't be smart, but it will be stupid much faster since it will be all be in VRAM\n\nwith llama cpp and IQ3_XSS GLM 4.6/4.7 I had about 3.5 t/s generation speed. 3200 DDR4 128GB and 2x rtx 3090 ti 24gb.",
          "created_at": "2026-01-08T22:43:41Z",
          "was_summarised": false
        },
        {
          "id": "nyhmjkn",
          "author": "Leflakk",
          "content": "CPU \u0026amp; RAM bottlebeck and not enough VRAM. Why donâ€™t you rather consider minimax m2.1 with lower quantz?",
          "created_at": "2026-01-08T22:45:32Z",
          "was_summarised": false
        },
        {
          "id": "nyhqi8u",
          "author": "segmond",
          "content": "I'm getting 6tk/sec on quad 3090 with regular llama.cpp",
          "created_at": "2026-01-08T23:04:48Z",
          "was_summarised": false
        },
        {
          "id": "nyhrh0k",
          "author": "ScoreUnique",
          "content": "Quick tip- explore -ot flag, that thing shows big numbers on ik llama CPP.",
          "created_at": "2026-01-08T23:09:39Z",
          "was_summarised": false
        },
        {
          "id": "nyhwbzl",
          "author": "chub0ka",
          "content": "Havent yet tried graph but 360gb quant on kimi runs 10t/s generation and 2 3090 is enough. Using 8 gpus doesnt help",
          "created_at": "2026-01-08T23:34:21Z",
          "was_summarised": false
        },
        {
          "id": "nyigyvn",
          "author": "ortegaalfredo",
          "content": "Prompt processing is super slow. As PP is mostly CPU-bound, I believe you might be hitting some kind of power-limitation, or power saving.",
          "created_at": "2026-01-09T01:21:30Z",
          "was_summarised": false
        },
        {
          "id": "nyih1e6",
          "author": "ortegaalfredo",
          "content": "Prompt processing is super slow. As PP is mostly CPU-bound, I believe you might be hitting some kind of power-limitation, or power saving is activating.",
          "created_at": "2026-01-09T01:21:53Z",
          "was_summarised": false
        },
        {
          "id": "nyip2mi",
          "author": "southern_gio",
          "content": "Dude thanks for shearing",
          "created_at": "2026-01-09T02:04:47Z",
          "was_summarised": false
        },
        {
          "id": "nyhdlk6",
          "author": "cantgetthistowork",
          "content": "Very very slow. My Q4 K2-Thinking on 2x3090 runs faster than this",
          "created_at": "2026-01-08T22:04:31Z",
          "was_summarised": false
        },
        {
          "id": "nyhoo4p",
          "author": "leonbollerup",
          "content": "Why not run gpt-oss-120b .. should rub nicely on that setup ?",
          "created_at": "2026-01-08T22:55:43Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://localhost:8080/v1/chat/completions",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://github.com/ikawrakow/ik_llama.cpp.git",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "Detailed technical benchmarks, configuration, and optimization insights for running GLM-4.7 on RTX 3090s with ik_llama.cpp, including performance metrics, hardware specs, and actionable questions for improvement.",
        "processed_at": "2026-01-12T10:56:09.046348106Z"
      },
      "processed_at": "2026-01-09T03:44:48.920871222Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://localhost:8080/v1/chat/completions: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.92074068Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/ikawrakow/ik_llama.cpp.git: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920870651Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q71sbe",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "title": "Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)",
      "content": "Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything\n\nhttps://preview.redd.it/shr3e0liv1cg1.png?width=2560\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079\n\nGithub: [https://github.com/MVPandey/DTS](https://github.com/MVPandey/DTS)\n\nMotivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:\n\n(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)\n\n1. Generates N diverse strategies\n2. Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)\n3. Rolls out full multi-turn conversations down each branch\n4. Has 3 independent LLM judges score each trajectory, takes the median\n5. Prunes branches below threshold, backpropagates scores\n6. Repeats for however many rounds you configure\n\nhttps://preview.redd.it/zkii0idvv1cg1.png?width=762\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4\n\nThree judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.\n\nMain additions over CAE:\n\n* user intent forking (strategies get stress-tested against different personas)\n* deep research integration via GPT-Researcher for domain context\n* proper visualization with conversation playback\n\nOnly supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls\n\nIt's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.\n\n\\--\n\nBTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.",
      "author": "ManavTheWorld",
      "created_at": "2026-01-08T04:08:39Z",
      "comments": [
        {
          "id": "nycvkhb",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T07:05:10Z",
          "was_summarised": false
        },
        {
          "id": "nyc6x5o",
          "author": "TheGrossVolcano",
          "content": "This is actually pretty clever - using beam search instead of pure MCTS makes way more sense for dialogue since you don't want the exploration to go completely off the rails\n\n  \nThe user intent forking is a nice touch, most people forget that the same strategy can totally bomb depending on who you're talking to",
          "created_at": "2026-01-08T04:11:18Z",
          "was_summarised": false
        },
        {
          "id": "nyc8h0p",
          "author": "charlesrwest0",
          "content": "Weird thought... Could you use this to optimize an rp response?",
          "created_at": "2026-01-08T04:20:41Z",
          "was_summarised": false
        },
        {
          "id": "nycytz1",
          "author": "harlekinrains",
          "content": "firecrawls pricing guides you into a monthly subscription and is prohibitively costly for what it provides (140 USD/year). without knowing the intricacies, if you also could implement alternatives, that would be swell.\n\nThis github already collected a bunch of search providers, maybe it helps: https://github.com/rikkahub/rikkahub/tree/ffa2a0c4796d835454c7a9a0469f897ff1ffdb63/search/src/main/java/me/rerere/search",
          "created_at": "2026-01-08T07:33:08Z",
          "was_summarised": false
        },
        {
          "id": "nycqqjr",
          "author": "ItilityMSP",
          "content": "Nice work, lots of potential here to improve many ai chat agents. â­",
          "created_at": "2026-01-08T06:26:17Z",
          "was_summarised": false
        },
        {
          "id": "nycz02m",
          "author": "Nyghtbynger",
          "content": "That's cool, I was looking to optimize my prompts I think it has a good future in prompt fitting (for a specific dataset or customer)",
          "created_at": "2026-01-08T07:34:38Z",
          "was_summarised": false
        },
        {
          "id": "nydbiy1",
          "author": "Much-Researcher6135",
          "content": "This is new to me, pretty interesting. Anyone here used tech like this for interview prep before? Or is it all just chatbot strategy discovery for you guys?",
          "created_at": "2026-01-08T09:27:49Z",
          "was_summarised": false
        },
        {
          "id": "nydgto6",
          "author": "IrisColt",
          "content": "Thanks!",
          "created_at": "2026-01-08T10:15:52Z",
          "was_summarised": false
        },
        {
          "id": "nyizliz",
          "author": "hiepxanh",
          "content": "thank you",
          "created_at": "2026-01-09T03:00:43Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/MVPandey/DTS",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.8,
        "reason": "The post describes a novel MCTS-style tree search for optimizing dialogue paths using LLMs, detailing technical architecture, parallel beam search, user intent variants, and LLM judges, with specific implementation details and open-source availability.",
        "processed_at": "2026-01-12T10:56:10.474611152Z"
      },
      "processed_at": "2026-01-09T03:44:48.920961523Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/MVPandey/DTS: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.920961142Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7c0pd",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/",
      "title": "AI21 releases Jamba2 3B and Jamba2 Mini, built for grounding and instruction following",
      "content": "*Disclaimer: I work for AI21, creator of the Jamba model family.*\n\nWeâ€™re excited to announce the public release of Jamba2 3B and Jamba2 Mini.\n\nThe Jamba2 family aims to give enterprises cost-effective models that will integrate well into production agent stacks.\n\nThese models are designed for reliable instruction following and grounded outputs, working well over long documents and avoiding drifting once context becomes large.\n\nThey perform best for precise question answering over internal policies, technical manuals and knowledge bases, without the overhead of thinking tokens which can become costly.\n\n**Key performance data**\n\nJamba2 3B and Jamba2 Mini outperform peers due to their hybrid SSM-Transformer architecture and KV cache innovations:\n\n* Outpaces Ministral3 14B and Qwen3 30B A3B across FACTS, IFBench and IFEval.Â \n* Beats Ministral3 3B and Qwen3 4B on IFEval and IFBench, tying with Qwen3 4B as category leader on FACTS.\n* At context lengths of 100K, Jamba2 Mini delivers 2.7X greater throughput than Ministral3 14B and 1.4X greater throughout than Qwen3 30B A3B.\n* At context lengths of 100K, Jamba2 3B delivers 1.7X greater throughout than Ministral3 3B and 2.7X greater throughput than Qwen 3 14B.\n\nItâ€™s available today in AI21â€™s SaaS and from Hugging Face.\n\nHappy to answer questions or dig into benchmarks if people want more detail.\n\nBlog: [http://www.ai21.com/blog/introducing-jamba2](http://www.ai21.com/blog/introducing-jamba2)  \nHugging Face: [https://huggingface.co/collections/ai21labs/jamba2](https://huggingface.co/collections/ai21labs/jamba2)",
      "author": "zennaxxarion",
      "created_at": "2026-01-08T13:38:34Z",
      "comments": [
        {
          "id": "nye9nvl",
          "author": "StillResult4344",
          "content": "Nice to see more hybrid architectures hitting the scene, those throughput numbers at 100K context are pretty solid. How's the memory usage compared to pure transformer models at those longer contexts?",
          "created_at": "2026-01-08T13:40:39Z",
          "was_summarised": false
        },
        {
          "id": "nyfxuey",
          "author": "gofiend",
          "content": "Whatâ€™s the llama.cpp (infer) and unsloth (training) situation?",
          "created_at": "2026-01-08T18:17:46Z",
          "was_summarised": false
        },
        {
          "id": "nyf8u9c",
          "author": "lacerating_aura",
          "content": "Hi, thank you for the release. How well does it compare to other hybrid architecture models, like qwen3 next?",
          "created_at": "2026-01-08T16:29:13Z",
          "was_summarised": false
        },
        {
          "id": "nygqyrp",
          "author": "crantob",
          "content": "This appears to me to be important iterative engineering progress in linear attention implementations.\n\nSkÃ¥l",
          "created_at": "2026-01-08T20:25:01Z",
          "was_summarised": false
        },
        {
          "id": "nyhn4nu",
          "author": "danigoncalves",
          "content": "What would be the max context before degrading in quality?",
          "created_at": "2026-01-08T22:48:18Z",
          "was_summarised": false
        },
        {
          "id": "nyhzj0z",
          "author": "casual_butte_play",
          "content": "Cool models! Heads up, thereâ€™s a typo in your Tiny Models chart, where it refers to Qwen3 4B A3B. Probably just a typo in chart generation while changing 30B A3B -\u0026gt; 4B",
          "created_at": "2026-01-08T23:50:57Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://www.ai21.com/blog/introducing-jamba2",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://huggingface.co/collections/ai21labs/jamba2",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 1,
        "reason": "Announces Jamba2 3B and Mini models with specific performance metrics, architectural innovations, and enterprise use cases, representing a significant AI model release.",
        "processed_at": "2026-01-12T10:56:06.457122282Z"
      },
      "processed_at": "2026-01-09T03:44:48.921023958Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://www.ai21.com/blog/introducing-jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921005501Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://huggingface.co/collections/ai21labs/jamba2: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921023757Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q7hywi",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/",
      "title": "How do you manage quality when AI agents write code faster than humans can review it?",
      "content": "We are shifting to an agentic workflow. My thesis is \"Code at Inference Speed.\" My CTO's counter-argument is that **reviewing code is harder than writing it**.\n\nHis concern is simple: If AI increases code volume by 10x, human review becomes a fatal bottleneck. He predicts technical debt will explode because humans canâ€™t mentally verify that much logic that quickly.\n\n  \nHow do handle this? I know one option is to slow down releases but is there any other approaches people are taking.",
      "author": "lostsoul8282",
      "created_at": "2026-01-08T17:28:30Z",
      "comments": [
        {
          "id": "nyfrj2m",
          "author": "Thick-Protection-458",
          "content": "\\\u0026gt; when AI agents write code faster than humans can review it?\n\nEasily. Just the bottleneck moves from me producing code (which is already lesser part of my job comparing to thinking about high-level structures. So it is kinda not a bottleneck anyway, just a nice spot to optimize) to me reviewing code.\n\nBefore that it was problematic too. Just we did not achieve the stage when this become bottleneck (means earlier bottlenecks is partially solved).\n\nAnd no, no way that electronic fucker (or human. My own, lol - better to at least review your own code later, when your stream of thoughts changed enough so you have a chance to see things from different angle) output get past me before I am sure I understand what this thing is doing.\n\n\\\u0026gt; **reviewing code is harder than writing it**\n\nHe is exactly right.\n\nIf you don't do it in digestable chunks.\n\nAnd for chunks to be digestable you have to know what to expect. So you have to take part in planning structural stuff. Either all by yourself or combo of you + LLM agent (it may give boost here too, by reviewing your ideas for missing corner cases or even noticing utterly wrong understanding of some stuff; also by suggesting tweaks). So this way you kinda know what to expect here.\n\nSo if you want to vibecode the whole thing and only review in the end - no, probably not the way unless coding agents get not only good quality, but actually superhuman quality. And even than - they would not be perfect decision mechanisms, so stacking them and human devs would still make sense. Because as soon as we make and notice different kind of errors - stacking different weak mechanisms would still work.\n\nIf you think about it like about pair programming, on the other hand - just a \"pair\" being not a human, but machine - it may start making sense,",
          "created_at": "2026-01-08T17:50:39Z",
          "was_summarised": false
        },
        {
          "id": "nyfv2lp",
          "author": "Abject-Kitchen3198",
          "content": "Start by accepting CTOs argument. \n\nSlow down. Accept that LLM induced productivity factor will be between 0.5 and 2x on a case-by-case basis.\n\nIterate with AI until you get a solution with minimal amount of code with acceptable quality that you feel comfortable reviewing. \n\nDo this for a quarter or two until you realize that either LLMs are not helpful for your case or that they provide some improvement on average and you can keep using them.",
          "created_at": "2026-01-08T18:05:52Z",
          "was_summarised": false
        },
        {
          "id": "nyfr4kz",
          "author": "AndThenFlashlights",
          "content": "I work in a field that has some pretty severe safety and liability consequences if something goes wrong. Qualified and competent humans eye need to review and comprehend every line of code that goes into the codebase, full stop. Reliability is more important than adding features. And weâ€™re usually working with devices or APIs that arenâ€™t documented publicly, so LLMs currently arenâ€™t super helpful at writing things unattended - theyâ€™re more useful in my workflow for writing API / class boilerplate or small contained methods, not vibe-coding whole things independently. \n\nTreat the AI like a flock of interns you need to watch and manage. You ever had too many interns to keep track of, and experienced that unfocused chaos? This is why I donâ€™t take on more than 1 intern at a time anymore.",
          "created_at": "2026-01-08T17:48:55Z",
          "was_summarised": false
        },
        {
          "id": "nyg6air",
          "author": "FullstackSensei",
          "content": "I find it funny how many here think LLMs will be able to review code and fix slop. Sounds like a chicken and egg problem to me. If you can train a model to detect and fix slop, then why wasn't the coding model trained to not generate said slop in the first place?\n\nIf we were anywhere near what some here seem to be predicting, why would anthropic spend a cool billion buying a Javascript runtime (Bun) rather than tuning a version of Claude to write something similar themselves?",
          "created_at": "2026-01-08T18:53:48Z",
          "was_summarised": false
        },
        {
          "id": "nyfrv55",
          "author": "Capaj",
          "content": "tests. Lots of them",
          "created_at": "2026-01-08T17:52:06Z",
          "was_summarised": false
        },
        {
          "id": "nyft98g",
          "author": "bigh-aus",
          "content": "It's a valid concern. But it's the same concern that larger enterprises are dealing with their current code stacks.  You need to increase the ecosystem around the code. Much like human written code that you outsourced to XYZ small company from ABC country.  \n\n\\#1: use a safe language: (rust, zig, safe c++, java, go etc). The compiler / runtime errors will help improve quality and catch bugs, vs interpreted languages where it's only runtime..  (It's one of the reasons I'm learning rust)\n\n\\#2: Full test suite imo is the main thing - unit tests, external API tests, integration tests, defensive tests, behavior tests, chaos tests, security tests, DR tests. Start simple, and scale up.  EG: extract any s3 buckets and check that they have encryption + auth turned on is a  classic example for low hanging fruit.  TLDR: how do you validate that the code is right? validate it by testing.\n\n\\#3: Have the code checked in in small steps, so if there is a problem rollback is easy.  Also look into having agents do code review.\n\n\\#4 CI/CD run as much static and dynamic analysis as you can on the code as part of the build / deployment pipeline.  Build agents to analyze the code, improvements, code smells.  Manage by exception.\n\n\\#5: Full red / blue team to test the security operation of the system, and build up automated security tests. \n\n\\#6: If required - compliance testing - is it HIPPA / PCI / Fedramp etc. How can you have continual testing to prove that the systems adhere to the standards. \n\n\\#7: run tests ON your staff - eg if there's a bug, how long does it take to find it, etc etc.  Break a non prod environment and have your staff try to fix it.\n\nAlso look at ways you can improve / reduce / optimize / etc the code using profiling and manual analysis.\n\nAlso do the dev, stage, prod environments at a minimum (more if needed).  Never have agents code in prod. ever.",
          "created_at": "2026-01-08T17:58:05Z",
          "was_summarised": false
        },
        {
          "id": "nyg0c0e",
          "author": "Zulfiqaar",
          "content": "\u0026gt;Â My thesis is \"Code at Inference Speed.\"\n\nJust cause someone can type at 100WPM doesn't mean they should\n\n\nThe alternatives all centre around increasing code quality or increasing review capacity",
          "created_at": "2026-01-08T18:28:23Z",
          "was_summarised": false
        },
        {
          "id": "nyfot0f",
          "author": "seanpuppy",
          "content": "I think this just highlights the importance of hiring highly skilled senior devs over jr's\n\nAny Senior dev today will have spent a TON of time reading and reviewing code, and will be both faster and better and finding issues.",
          "created_at": "2026-01-08T17:38:48Z",
          "was_summarised": false
        },
        {
          "id": "nyg6yqu",
          "author": "FastDecode1",
          "content": "Use AIs to review. Duh.\n\nWhat kind of \"agentic workflow\" are you using if the only thing that's automated is code generation? If you paid money for that, you need a refund.",
          "created_at": "2026-01-08T18:56:40Z",
          "was_summarised": false
        },
        {
          "id": "nyfo1wx",
          "author": "sabergeek",
          "content": "We'll probably have models for code review at some point, so that AI cleans it own slop.",
          "created_at": "2026-01-08T17:35:29Z",
          "was_summarised": false
        },
        {
          "id": "nygw7oq",
          "author": "ttkciar",
          "content": "Your CTO is totally right, and the problem he describes predates LLM codegen.  The advent of codegen has exacerbated problem tremendously, is all.\n\nPart of the problem in places I've worked is that management controls how much of developers' time is spend writing new code vs paying off technical debt, and management does not allocate enough time to paying off that debt.\n\nIn that sense, it is a people-problem, not a technical problem.  Fix management and the problem becomes a lot more tractable.\n\nOn the other hand, there are some things you can do to make LLM-inferred projects faster/easier to validate and review:\n\n**Write comprehensive unit tests**\n\nPreferably have the humans do this before codegen, because ideally unit tests will describe how code is expected to behave, which will help LLMs infer the expected code.  Not many devs like to write unit tests, though, so having your LLM generate unit tests after the fact is a second-best solution.  Note that you will need to instruct the LLM to write \"testable\" code, because sometimes the most natural-seeming implementations are not easily unit-tested.\n\nUnit tests with mocked dependencies are beneficial because they exercise the different parts of your project in isolation and verify that their outputs/side-effects comply with expectations.  This means you can find many bugs simply by running your unit tests, and which unit tests fail point you precisely at the code which needs to be fixed (if your tests are high-enough granularity, which requires that your functions/methods are decomposed into subroutines.  This is an important aspect of writing code to be testable).\n\nIt also makes adjusting the behavior of the project to comply with expectations easier, if you find that code does not do what you want it to do.  You can tweak the appropriate unit test(s), or write new tests, and have the dev or LLM fix the code so that the test passes.\n\nIt is good industry practice to make sure a development branch passes all unit tests before merging it into the main branch, and then making sure the merge passes all unit tests before pushing it to the remote repo.\n\n**Write good documentation**\n\nOne of the best uses I've found for codegen LLMs is to have them explain my coworkers' code to me.  Most codegen models (and some non-codegen models!) are good at writing code documentation.  This helps me come up to speed not just for code reviews but also for contributing to legacy projects with which I am familiar.\n\nIdeally you should have at least two layers of documentation, preferably three:\n\n* A high-level view, which is short and easy to read, explaining the purpose of the project, who is expected to use it, and for what, and the general flow of data through the project -- its inputs, its outputs, its side-effects, and the components it passes through in-between.\n\n* A component-level view, which describes the main subsystems involved in the project and their interfaces.  These can be libraries, external dependencies like databases or service APIs, frameworks, or any other reasonable-seeming partitioning of the project into a small handful of parts.  If you omit any documentation, it would be this one, not the high- or low-level views.\n\n* A low-level view, usually by source code file, which describes what the code in the file is for, what its classes and any global state are, the methods used by those classes, and what other files use those classes and/or call those methods.\n\nGood documentation will get the human reviewers up to speed quickly and let them start and finish their reviews more quickly.\n\n**Generate a list of possible bugs/problems**\n\nYou don't want to totally automate the code review process, but there's nothing wrong with asking the LLM to infer a list of what might be bugs or weaknesses in the project, for the human reviewers to assess.  When I ask GLM-4.5-Air to enumerate problems in my code, usually only about a third of the problems it identifies are actual problems which need fixing, but it's still better to have it than not.\n\nThis can help focus code reviewers' attention and at least give them something to consider, regarding whether the project should be better than it is.\n\n**Use a structured log**\n\nA lot of problems only become visible once you've been using a project for a while for real-world tasks.  A structured log will not only help you spot problems, but also expose the program's internal state in the steps leading up to the problem.  This is invaluable for rapid troubleshooting.\n\nWhen a problem crops up, you can look back through the log to identify exactly where things went awry, and use the conditions represented in the log to inform bugfixes and (especially!) new unit tests which would have identified the problem before it was put into production.\n\nStrictly speaking this is slightly out of scope for your problem, as the structured log only becomes useful *after* the code passes review and is put into production, but the simple fact is that not all problems get caught in code review.  Realistically new code needs to be vetted both before and after deployment.\n\n\n**These measures will accelerate code review, but the underlying problem persists.**\n\nIncorporating all of these measures can shorten the time it takes to review a project, but human reviewers still have to put in the work to verify that the code is good.  Depending on how many reviewers you have and how much code you are generating, they might or might not be able to keep up.\n\nWhether to bottleneck deployment of new code on code review, and how much, is and always has been a trade-off determined by the development team's management.  It is their job to assess the tradeoffs between releasing thoroughly-vetted code versus releasing possibly-buggy code and adding to the employer's technical debt.\n\nGenerating new code via LLM inference doesn't change that, but you should be able to demonstrate mathematically that given fixed human dev resources (say, programmer-hours per month, allocated to developing new code vs code reviews vs paying down technical debt), and given a fixed management tolerance for accumulating technical debt, the total useful code deployed per unit time is increased when LLMs generate at least some of the new code.",
          "created_at": "2026-01-08T20:48:32Z",
          "was_summarised": false
        },
        {
          "id": "nyfq6dy",
          "author": "1ncehost",
          "content": "I've been dealing with this for a year, and this is predominantly a solved issue with project management risk mitigation. Essentially executives have struggled with this issue since forever: how do you maintain quality when you don't know or interact with everyone in your company? Tests and process are the ultimate answer.\n\nYou must adopt the mindset of an executive and trust the employees, but ensure there are thoroughly enforced safegaurds, audits, and so on to maintain quality. The code you care about becomes the \"operating system\" that derives the systems, not the system design itself.",
          "created_at": "2026-01-08T17:44:47Z",
          "was_summarised": false
        },
        {
          "id": "nyfnzaq",
          "author": "notAllBits",
          "content": "Remit-driven development",
          "created_at": "2026-01-08T17:35:10Z",
          "was_summarised": false
        },
        {
          "id": "nyfsbbt",
          "author": "geoffwolf98",
          "content": "You have to balance the risks - whether it is better to get it out the door but potentially loses you millions due to a price error (or what ever) or have reliable working code that wont bankrupt you.",
          "created_at": "2026-01-08T17:54:02Z",
          "was_summarised": false
        },
        {
          "id": "nyg9e9z",
          "author": "adityaguru149",
          "content": "Yeah it gets difficult reviewing a lot of the slop by AI. My way is writing lots of tests and using AI for quick summarisations for code blacks so that I don't have to read through every line. I also get more involved in the architecting phase so that AI has better guidance before writing code.",
          "created_at": "2026-01-08T19:07:15Z",
          "was_summarised": false
        },
        {
          "id": "nyge9ds",
          "author": "CV514",
          "content": "AI agents have gained some innate right to merge PR or something? No? That's how.",
          "created_at": "2026-01-08T19:28:33Z",
          "was_summarised": false
        },
        {
          "id": "nygfde1",
          "author": "Jmc_da_boss",
          "content": "Oh wow, your telling me the historical bottle neck of human review and alignment in programming is STILL the bottleneck in programming?\n\nThat's crazy, however will we handle this thing that's been true for decades.",
          "created_at": "2026-01-08T19:33:27Z",
          "was_summarised": false
        },
        {
          "id": "nygjke0",
          "author": "synn89",
          "content": "I  expect we'll probably develop new code design methodologies that work best with AI. Languages may also end up being preferred for the same reason. This is why agile and MVC exists today, to optimize for human meat brains.",
          "created_at": "2026-01-08T19:51:57Z",
          "was_summarised": false
        },
        {
          "id": "nygoeq8",
          "author": "CallinCthulhu",
          "content": "Preliminary review by AI catches a lot of shit early. Still needs human review, but that review is faster",
          "created_at": "2026-01-08T20:13:32Z",
          "was_summarised": false
        },
        {
          "id": "nygps00",
          "author": "blackkettle",
          "content": "Thatâ€™s not an appropriate way to use AI for coding. Agentic workflows with high expertise can definitely make you much faster.  Blindly committing AI code based on prompts and no experience?  See you at the next post mortem!\n\nYour CTO is right.\n\nAnd AI shouldnâ€™t really be â€œincreasing code volumeâ€.  It should be used again with expertise to speed up well defined, low risk, relative tasks and gradually iterate to more complex ones.",
          "created_at": "2026-01-08T20:19:41Z",
          "was_summarised": false
        },
        {
          "id": "nygxd2i",
          "author": "a-wiseman-speaketh",
          "content": "I think this is like a corollary of Brandolini's Law - and we've seen how that's played out with the degeneration of shared reality and objective truth over the last decade, particularly.\n\nI will point out that one of the skills a senior dev should be great at and every LLM I have tried is absolutely awful at is DELETING code, or never writing it to begin with.",
          "created_at": "2026-01-08T20:53:34Z",
          "was_summarised": false
        },
        {
          "id": "nygzcnm",
          "author": "Psychological_Ear393",
          "content": "\u0026gt;How do you manage quality when AI agents write code faster than humans can review it?\n\nIf you are pumping out code faster than a human can understand and review it, then you literally can't.  It's a matter of doing the maths of which side you manage for what the product goals and roadmap is.  A pipe can only hold so much volume.  Right now you have pressure on the input side and it's more like a storm water drain than a filtered water outlet.  To strain the storm water you need a bigger pipe and grate which lets more things through.\n\nIt's up to the dev to ensure they are submitting quality pull requests.  If a PR comes in a human doesn't understand it then they have failed at their job.  If a reviewer finds a problem, it doesn't matter where it came from, that dev put in the PR - PRs have problems that's why we have them but to put one in that had no attempt to find the problems and submit understandable quality is egregious and if agents are writing code faster than the gates can handle then that's what's happening.\n\nPerformance objectives need to be updated to include appropriate use of AI. Everyone needs to be on the same page about what matters to your product, if some members of your team want to move faster than humans can understand and others want more thorough reviews then you have a culture problem that needs to be addressed.\n\nI mostly use AI for weird problems where I don't know where to start, like chunks of code I haven't touched before, then I take over and try to solve it myself where I can.  I use it to check the work I did for anything I missed, and you need to be careful with that too it can dream things so you need to know what you know to assess it. I also use it for bulk changes where it has a sample to go off for style and patterns.\n\nThe other day I had to put in a change that I didn't understand.  It was a legacy product in a framework I don't know and from top down they said they urgently need it and they are OK with AI writing it.  I reviewed it as best I could but I had no idea why it worked and in the PR I clearly stated that it was mostly AI written and I didn't fully understand how it works.  I'm a consultant and told them it's a bad idea, the owners said they wanted it, ok sure the people paying the bills get what they want.",
          "created_at": "2026-01-08T21:02:22Z",
          "was_summarised": false
        },
        {
          "id": "nyh2g45",
          "author": "DHasselhoff77",
          "content": "Look up \"The Goal\" by Eliyahu M. Goldratt. Your CTO is right.",
          "created_at": "2026-01-08T21:16:09Z",
          "was_summarised": false
        },
        {
          "id": "nyh8zks",
          "author": "Foreign_Risk_2031",
          "content": "Itâ€™s true. Itâ€™s difficult to accept but true. This is your CTOs job to solve. You can outsource testing. Or make more agentic workflows to review.",
          "created_at": "2026-01-08T21:44:35Z",
          "was_summarised": false
        },
        {
          "id": "nyhe0ju",
          "author": "rosstafarien",
          "content": "It's not tech debt that's your problem. That's literally the least of your worries. It's that nobody understands your codebase and nobody can say that it's correctly solving the problem.\n\nHow are you managing requirements? How are you testing the system to be sure that the requirements are being met? How are you going to confirm that a future change doesn't break existing functionality?\n\nAnd I have yet to see an AI produce sane code at 10x the rate of a human developer. An AI can produce boilerplate at 10x the rate, but that isn't the code you care about.",
          "created_at": "2026-01-08T22:06:21Z",
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.8,
        "reason": "Discusses technical challenges of AI-generated code at scale, highlighting bottlenecks in human review and exploring solutions for maintaining quality in agentic workflows.",
        "processed_at": "2026-01-12T10:56:06.714508628Z"
      },
      "processed_at": "2026-01-09T03:44:48.921283839Z"
    },
    {
      "flow_id": "",
      "id": "1q7m2eh",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/",
      "title": "Built a blind benchmark for coding models - which local models should I add?",
      "content": "3 AI judges score each output blind. Early results from 10 coding tasks - Deepseek V3.2 at #9. GLM 4.7 at #6, beating Claude Opus 4.5.\n\nSome open-source models are free to evaluate. Which local models should I evaluate and add to the leaderboard?\n\n[codelens.ai/leaderboard](http://codelens.ai/leaderboard)\n\nEDIT: Tested community suggestions! Results now live on the leaderboard:  \n  \n\\- GPT-OSS-120B, Qwen3 Next 80B, Devstral 2, Nemotron Nano 30B, and more  \n  \nKeep the suggestions coming - we'll keep adding models.",
      "author": "Equivalent-Yak2407",
      "created_at": "2026-01-08T19:54:08Z",
      "comments": [
        {
          "id": "nygvlfu",
          "author": "ciprianveg",
          "content": "minimax m2.1, qwen 235b",
          "created_at": "2026-01-08T20:45:48Z",
          "was_summarised": false
        },
        {
          "id": "nygtwck",
          "author": "Aggressive-Bother470",
          "content": "gpt-oss-120b, Seed-OSS-36B, Qwen3-30B-A3B-Thinking-2507-BF16, GLM-4.6-UD-IQ2\\_M",
          "created_at": "2026-01-08T20:38:14Z",
          "was_summarised": false
        },
        {
          "id": "nyhe3dy",
          "author": "MrBIMC",
          "content": "Devstral-2512 is goated. I know it's free only temporarily, but as far as free models go - it most often delivers exactly to spec. So I'd like it benchmarked.",
          "created_at": "2026-01-08T22:06:42Z",
          "was_summarised": false
        },
        {
          "id": "nyh5uym",
          "author": "-InformalBanana-",
          "content": "Qwen3 2507 30b a3b instruct, qwen3 next 80b, gpt oss 20b/120b, Devstral small 2 24b, Nemotron nano 3 e0b a3b, Nemotron Cascade 14b.\nI tried Nemotron models and I think they are bad and benchmaxed so if you cound check that. For example Nemotron Cascade 14b has better LCBv6 score than qwen next 80b a3b. But in my one shot try it even had syntax errors so complete failure.",
          "created_at": "2026-01-08T21:31:00Z",
          "was_summarised": false
        },
        {
          "id": "nyiql7p",
          "author": "pmttyji",
          "content": "* Kimi K2 Instruct 0905\n* Kimi-K2-Thinking\n* Devstral-2-123B-Instruct-2512\n* Devstral-Small-2-24B-Instruct-2512\n* Mistral-Large-3-675B-Instruct-2512\n* Ling-1T\n* Olmo-3.1-32B-Instruct\n* Qwen3-32B\n* Llama-3\\_3-Nemotron-Super-49B-v1\\_5\n* dots.llm1.inst",
          "created_at": "2026-01-09T02:12:42Z",
          "was_summarised": false
        },
        {
          "id": "nyizqfy",
          "author": "celsowm",
          "content": "IQuest-Coder-V1",
          "created_at": "2026-01-09T03:01:27Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "http://codelens.ai/leaderboard",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/6ocf1gbxj6cg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "Posts a blind benchmark for coding models with specific results, compares models, and invites community input for evaluation, providing technical details and a leaderboard link.",
        "processed_at": "2026-01-12T10:56:09.400607227Z"
      },
      "processed_at": "2026-01-09T03:44:48.921417296Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch http://codelens.ai/leaderboard: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921416515Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q6n5vl",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/",
      "title": "16x AMD MI50 32GB at 10 t/s (tg) \u0026amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)",
      "content": "Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)\n\non vllm-gfx906-deepseek with 69000 context length\n\n**Power draw**: 550W (idle) / 2400W (peak inference)\n\n**Goal**: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation \u0026amp; prompt processing)\n\n**Coming next**: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking\n\n**Credits**: BIG thanks to the Global Open source Community!\n\nAll setup details here:\n\n[https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32](https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32)\n\n\n**Feel free to ask any questions and/or share any comments.**\n\nps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! \n\nps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... ",
      "author": "ai-infos",
      "created_at": "2026-01-07T18:22:05Z",
      "comments": [
        {
          "id": "nyb7g9w",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-08T01:00:14Z",
          "was_summarised": false
        },
        {
          "id": "ny8yjc2",
          "author": "fallingdowndizzyvr",
          "content": "\u0026gt; Power draw: 550W (idle) / 2400W (peak inference)\n\nSweet. It's winter. Might as well have your heater do work instead of making empty BTUs.",
          "created_at": "2026-01-07T18:49:27Z",
          "was_summarised": false
        },
        {
          "id": "ny8uc81",
          "author": "Soft_Possible1862",
          "content": "Holy shitâ€¦.",
          "created_at": "2026-01-07T18:31:17Z",
          "was_summarised": false
        },
        {
          "id": "ny8sxj3",
          "author": "kevin_1994",
          "content": "How loud is it? How are you able to run 2400W from home?",
          "created_at": "2026-01-07T18:25:10Z",
          "was_summarised": false
        },
        {
          "id": "ny9q1kn",
          "author": "SourceCodeplz",
          "content": "Tbh if you are coding professionally, this really isn't that much of a spend for having a basically offline programmer working with you on just electricity.",
          "created_at": "2026-01-07T20:49:32Z",
          "was_summarised": false
        },
        {
          "id": "ny8yh6u",
          "author": "OnlineParacosm",
          "content": "Can you give us a rough all in cost so I can figure what the tokens per second cost basis is for this? Thanks for such a great write up.",
          "created_at": "2026-01-07T18:49:12Z",
          "was_summarised": false
        },
        {
          "id": "ny9pr28",
          "author": "ThatCrankyGuy",
          "content": "Thousands of dollars of equipment hang by garden twist-tie wire. Reminds me of grad days.",
          "created_at": "2026-01-07T20:48:19Z",
          "was_summarised": false
        },
        {
          "id": "ny8vpf6",
          "author": "Dorkits",
          "content": "Me with my rumble 3060ti : Dream build ðŸ˜²",
          "created_at": "2026-01-07T18:37:14Z",
          "was_summarised": false
        },
        {
          "id": "ny8zw75",
          "author": "FullstackSensei",
          "content": "Is there a meaningful difference between something like DS AWQ and something like Minimax 2.1 at Q8?",
          "created_at": "2026-01-07T18:55:16Z",
          "was_summarised": false
        },
        {
          "id": "ny986ve",
          "author": "ForsookComparison",
          "content": "How is your prompt processing so good? Does Tensor parallelism come into play?",
          "created_at": "2026-01-07T19:31:38Z",
          "was_summarised": false
        },
        {
          "id": "nybo6v7",
          "author": "vulcan4d",
          "content": "Ditch the furnace, best home heater!",
          "created_at": "2026-01-08T02:28:27Z",
          "was_summarised": false
        },
        {
          "id": "ny9ewd9",
          "author": "organicmanipulation",
          "content": "Nice setup! I notice that you're splitting your PCIe lanes into two 8x. Can you please share the exact PCIe bifurcation card you're using?",
          "created_at": "2026-01-07T20:00:41Z",
          "was_summarised": false
        },
        {
          "id": "nya9bqr",
          "author": "cashmillionair",
          "content": "Thank you for sharing, appreciate it!",
          "created_at": "2026-01-07T22:12:32Z",
          "was_summarised": false
        },
        {
          "id": "nyb7t5l",
          "author": "noiserr",
          "content": "That's nuts! And I love it.",
          "created_at": "2026-01-08T01:02:04Z",
          "was_summarised": false
        },
        {
          "id": "nybm752",
          "author": "MaximKiselev",
          "content": "10 t/s Carl....",
          "created_at": "2026-01-08T02:18:00Z",
          "was_summarised": false
        },
        {
          "id": "nybvmx1",
          "author": "ryfromoz",
          "content": "I love massive frankenstein rigs like this!",
          "created_at": "2026-01-08T03:07:25Z",
          "was_summarised": false
        },
        {
          "id": "nydbb4o",
          "author": "PreparationLow6188",
          "content": "A Wow for this supreme project. It is the time should consider reenable MI50 on the shelf.",
          "created_at": "2026-01-08T09:25:48Z",
          "was_summarised": false
        },
        {
          "id": "ny8yc7v",
          "author": "egomarker",
          "content": "10 tks? Sigh",
          "created_at": "2026-01-07T18:48:36Z",
          "was_summarised": false
        },
        {
          "id": "ny943r0",
          "author": "exaknight21",
          "content": "Can you share your build please. Like what are those fans, how did you hook them up, what motherboard youâ€™re using. I am a little new to this aspect.",
          "created_at": "2026-01-07T19:13:39Z",
          "was_summarised": false
        },
        {
          "id": "ny9zj2e",
          "author": "Different-Toe-955",
          "content": "Epic setup I love the zip ties holding the GPUs up. How do those oculink extenders work out? Looks like each GPU is running at x8.",
          "created_at": "2026-01-07T21:29:36Z",
          "was_summarised": false
        },
        {
          "id": "nycaq4e",
          "author": "badgerbadgerbadgerWI",
          "content": "This is the kind of setup that makes enterprise local deployment actually viable. MI50s at those prices vs NVIDIA is a completely different ROI calculation. Are you seeing any stability issues with vllm on the older gfx906 arch over longer inference runs?",
          "created_at": "2026-01-08T04:34:52Z",
          "was_summarised": false
        },
        {
          "id": "nygckst",
          "author": "qcodec",
          "content": "500W/2400W. Oh my, even my solar system can't handle this. I guess I'll have to install a standalone one.",
          "created_at": "2026-01-08T19:21:10Z",
          "was_summarised": false
        },
        {
          "id": "nyieqz7",
          "author": "el3mancee",
          "content": "Nice setup.",
          "created_at": "2026-01-09T01:09:44Z",
          "was_summarised": false
        },
        {
          "id": "nyihpr7",
          "author": "el3mancee",
          "content": "https://preview.redd.it/vr6jf0i178cg1.jpeg?width=4284\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=2e4036b31d87b2e9d7d75dcdbebdae2f7e07cecf\n\nMy setup can run Deepseek 3.1 IQ4\\_XS at 7.5 t/s. 200W total when running.",
          "created_at": "2026-01-09T01:25:31Z",
          "was_summarised": false
        },
        {
          "id": "ny8ycbp",
          "author": "Far-Low-4705",
          "content": "pretty sure a mac would it run faster, and at far less power consumption.\n\nStill super cool, but not sure how practical it would be",
          "created_at": "2026-01-07T18:48:37Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "image_blocks": [
        {
          "url": "https://i.redd.it/lor8ccu2xybg1.png",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "The post details a high-performance AI setup using 16 AMD MI50 GPUs to run Deepseek V3.2 at 10 tok/s output and 2000 tok/s input, with specific technical specs, power draw, and context length, showcasing an innovative open-source approach to cost-effective local AI deployment.",
        "processed_at": "2026-01-12T10:56:10.045781849Z"
      },
      "processed_at": "2026-01-09T03:44:48.92183308Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921832338Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q6sp4b",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/",
      "title": "Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning",
      "content": "As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:\n\n* 169M parameters\n* Streaming support\n* Zero-shot voice cloning\n* 0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds\n* Requires 3-12 seconds of reference audio for voice cloning\n* Apache 2.0 license\n\nYes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.\n\nItâ€™s not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!\n\nGitHub repo: [https://github.com/samuel-vitorino/sopro](https://github.com/samuel-vitorino/sopro)",
      "author": "SammyDaBeast",
      "created_at": "2026-01-07T21:46:19Z",
      "comments": [
        {
          "id": "nya3x2v",
          "author": "Accurate-Tea8319",
          "content": "Pretty impressive for a solo project on a single GPU tbh. The streaming support is clutch - most TTS models make you wait forever for the full generation\n\n  \nHow's the quality compared to something like Coqui or Tortoise? The zero-shot cloning sounds tempting but I've been burned by models that promise it and deliver robot voices lol",
          "created_at": "2026-01-07T21:48:39Z",
          "was_summarised": false
        },
        {
          "id": "nyatrmo",
          "author": "TheRealMasonMac",
          "content": "How much did it cost to train?",
          "created_at": "2026-01-07T23:51:24Z",
          "was_summarised": false
        },
        {
          "id": "nya6icb",
          "author": "HungryMachines",
          "content": "The voice sounds a bit hoarse on the sample, is that something that can be improved with more training?",
          "created_at": "2026-01-07T21:59:57Z",
          "was_summarised": false
        },
        {
          "id": "nybzmea",
          "author": "lastrosade",
          "content": "My God, you gave us a model, a clear usage, an architecture, datasets, ~~training scripts.~~\n\n~~All we need now is a brave soul with money.\nHonestly, I'd love to see tomorrow if I can improve on this.\nMaybe even put some money down for training.\nI'd love to do it with a smaller parameter count though.~~\n\n~~If someone managed to make Kokoro that fucking good and bilingual and have multiple voices, I think we can make a kick ass single language, single voice, 60 million or less parameters Model.~~\n\nSomething I would really like is for someone to manage to pin down the exact recipe for a good TTS model and have that recipe be completely open source so that other people may concentrate on finding data sets for other languages and make multiple high quality, very small TTS models.\n\n~~And you gave me so much fucking hype with this.~~\n\nNever mind, false hopes, I just realized you did not give the training scripts, I'm fucking stupid.",
          "created_at": "2026-01-08T03:29:08Z",
          "was_summarised": false
        },
        {
          "id": "nybd9sa",
          "author": "RIP26770",
          "content": "We need a ComfyUI node ASAP ! Thanks for sharing this ðŸ™",
          "created_at": "2026-01-08T01:30:52Z",
          "was_summarised": false
        },
        {
          "id": "nyaex4o",
          "author": "SlavaSobov",
          "content": "Great work! I'll give it a try later. It looks very nice for small edge devices!",
          "created_at": "2026-01-07T22:38:03Z",
          "was_summarised": false
        },
        {
          "id": "nyaes62",
          "author": "PsychologicalFactor1",
          "content": "It will support Portuguese, right? â€¦right?",
          "created_at": "2026-01-07T22:37:24Z",
          "was_summarised": false
        },
        {
          "id": "nydjwmr",
          "author": "danigoncalves",
          "content": "Congrats mate! Very nice job you did here with such lower capacity. Maybe you can try to apply to some european fund in order to take this further because I guess Amalia is only TTT :)",
          "created_at": "2026-01-08T10:43:01Z",
          "was_summarised": false
        },
        {
          "id": "nye7roq",
          "author": "Fickle_Performer9630",
          "content": "Whatâ€™s the relation to Soprano TTS model?",
          "created_at": "2026-01-08T13:30:15Z",
          "was_summarised": false
        },
        {
          "id": "nycwctl",
          "author": "rm-rf-rm",
          "content": "The examples in the README are truly bad. There are so so many such \"I made a TTS\" projects - genuinely curious what your aim is? Just learn? Have fun?\n\nIt would be so much better for you and the community to contribute to one of the existing open source TTS projects. What the ecosystem lacks is genuinely good model that can handle long generations without going haywire. Its sad that we dont have aggressive competition from open source in TTS like we do in STT, LLMs, Image gen etc.",
          "created_at": "2026-01-08T07:11:47Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://github.com/samuel-vitorino/sopro",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "Announces a new 169M parameter real-time TTS model with zero-shot voice cloning, including performance metrics and technical details, representing a novel AI application.",
        "processed_at": "2026-01-12T10:56:06.88116606Z"
      },
      "processed_at": "2026-01-09T03:44:48.921946587Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://github.com/samuel-vitorino/sopro: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.921946096Z"
        }
      ]
    },
    {
      "flow_id": "",
      "id": "1q6c9wc",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/",
      "title": "DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.",
      "content": "arXiv:2501.12948 \\[cs.CL\\]: https://arxiv.org/abs/2501.12948",
      "author": "Nunki08",
      "created_at": "2026-01-07T10:49:12Z",
      "comments": [
        {
          "id": "ny6w58o",
          "author": "WithoutReason1729",
          "content": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "created_at": "2026-01-07T12:50:10Z",
          "was_summarised": false
        },
        {
          "id": "ny6ls96",
          "author": "qtvivies",
          "content": "https://preview.redd.it/t6ic0x3nywbg1.png?width=1965\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=ae5ef60128b8a0cf89351e8673ea41eddafb037c\n\nSomething interesting towards the end. Looks like someone forgot about this",
          "created_at": "2026-01-07T11:36:50Z",
          "was_summarised": false
        },
        {
          "id": "ny7c3yc",
          "author": "Ok_Technology_5962",
          "content": "Current research is linear attention. DeepSeek 3.2 with the cache optimization and now they had a massive paper come out that puts the linear into the whole modelaking it possible to train more than 60 layers. So yes this one is done. The compute and thinking will now happen internally in 1000 layers",
          "created_at": "2026-01-07T14:21:09Z",
          "was_summarised": false
        },
        {
          "id": "ny8iz9u",
          "author": "warnerbell",
          "content": "The original paper was light on implementation specifics. If they've added more on how they got the reasoning behavior to emerge, that's valuable.",
          "created_at": "2026-01-07T17:41:52Z",
          "was_summarised": false
        },
        {
          "id": "ny6gjni",
          "author": "ResidentPositive4122",
          "content": "New arch about to drop? dsv4 + r2? Packing all the goodies learned from last year. Hopefully they try smaller sizes as well. Would be interesting to see how the arch improvements work at several sizes.",
          "created_at": "2026-01-07T10:53:31Z",
          "was_summarised": false
        },
        {
          "id": "ny9ne3n",
          "author": "CryptoUsher",
          "content": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "created_at": "2026-01-07T20:38:03Z",
          "was_summarised": false
        },
        {
          "id": "nyae2ug",
          "author": "timfduffy",
          "content": "I think all this info was previously released as a [supplment to their R1 paper in Nature](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf).",
          "created_at": "2026-01-07T22:34:09Z",
          "was_summarised": false
        },
        {
          "id": "ny74o1r",
          "author": "jeffwadsworth",
          "content": "I was using the online chat version last night to improve a large Java class (40K tokens) with multiple methods.  It did so beautifully with zero issues in one shot.  The same task in my sub of Gemini 3 Pro chat interface failed in a few shots due to hallucinations.  They have really improved that model a lot from a year ago.",
          "created_at": "2026-01-07T13:40:40Z",
          "was_summarised": false
        },
        {
          "id": "nycauta",
          "author": "badgerbadgerbadgerWI",
          "content": "The level of detail they're releasing is remarkable. This kind of transparency is what pushes the whole field forward. Really interested in their distillation approach - getting smaller models to match larger ones' reasoning is key for edge deployment.",
          "created_at": "2026-01-08T04:35:42Z",
          "was_summarised": false
        },
        {
          "id": "ny9okeg",
          "author": "CryptoUsher",
          "content": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "created_at": "2026-01-07T20:43:14Z",
          "was_summarised": false
        },
        {
          "id": "nydlzki",
          "author": "Eyelbee",
          "content": "It's crazy it's only been one year, feels like ages",
          "created_at": "2026-01-08T11:00:27Z",
          "was_summarised": false
        },
        {
          "id": "ny6xeqo",
          "author": "Aggressive-Bother470",
          "content": "New grpo details perhaps? From reading the hf page it implied it was maybe light in that regard?",
          "created_at": "2026-01-07T12:58:03Z",
          "was_summarised": false
        },
        {
          "id": "ny76206",
          "author": "TelloLeEngineer",
          "content": "does arxiv have a diff UI?",
          "created_at": "2026-01-07T13:48:18Z",
          "was_summarised": false
        },
        {
          "id": "ny6y8mn",
          "author": "yoshiK",
          "content": "I did quickly throw the two papers into Gemini. It's really fun to live in the future. \n\n#Gemini summary: \n\nThe paper **\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\"** (arXiv:2501.12948) marks a significant milestone in open-source AI by demonstrating that advanced reasoning (similar to OpenAIâ€™s o1) can be achieved through large-scale Reinforcement Learning (RL) with minimal human-annotated data.\n\nThe two versions you provided represent the initial release (**v1**, Jan 22, 2025) and the latest updated version (which has been significantly expanded to **86+ pages** as of January 2026).\n\n### 1. Overall Paper Summary\nThe paper introduces two primary models:\n*   **DeepSeek-R1-Zero:** A model trained via \"pure RL\" (using the GRPO algorithm) starting directly from a base model without any Supervised Fine-Tuning (SFT). It demonstrates that reasoning behaviors like self-correction and reflection can emerge purely from reward signals.\n*   **DeepSeek-R1:** A more \"user-friendly\" version that uses a multi-stage pipeline (Cold-start SFT â†’ Reasoning RL â†’ Rejection Sampling/SFT â†’ General RL) to fix the \"readability\" and \"language mixing\" issues of R1-Zero while maintaining state-of-the-art reasoning performance.\n*   **Distillation:** The authors show that the reasoning patterns discovered by the 671B model can be distilled into smaller models (1.5B to 70B), allowing a 14B model to outperform much larger ones on math and coding benchmarks.\n\n---\n\n### 2. Comparison: Extensions in the New Version\nThe newer version is a massive technical expansion (growing from roughly 22 pages to over 85 pages). The key additions and extensions include:\n\n#### A. The \"Aha Moment\" Expansion (Section 2.2.1)\nThe new version provides a much deeper analysis of the **\"Aha Moment\"**â€”the point during RL training where the model unexpectedly learns to \"re-think\" its approach. The extension includes more qualitative examples and internal data showing the model's transition from linear solving to iterative self-correction without being prompted to do so.\n\n#### B. Detailed 4-Stage Training Pipeline\nWhile v1 outlined the stages, the new version details the specific composition of the **800k total training samples**:\n*   **Stage 1 (Cold Start):** Expanded details on the ~5,000-10,000 long CoT (Chain of Thought) samples used to \"prime\" the model.\n*   **Stage 3 (Rejection Sampling):** A deeper dive into how 600k reasoning-related and 200k non-reasoning samples were filtered and used to improve the model's general chat capabilities and prevent \"forgetting\" during the reasoning-heavy RL stages.\n\n#### C. Comprehensive Ablation Studies\nThe new version adds extensive \"What if?\" scenarios that were absent or brief in v1:\n*   **Distillation vs. RL:** New evidence explaining *why* distilling a large model's reasoning traces into a small model is more effective than training that small model directly with its own RL.\n*   **Base Model Impact:** Analysis of how different base models (DeepSeek-V3 vs. Qwen vs. Llama) respond to the R1 training recipe.\n\n#### D. Expanded \"Unsuccessful Attempts\" (Section 4.2)\nOne of the most valuable additions for researchers is the expanded section on what **did not work**. The new version elaborates on their failures with:\n*   **Process Reward Models (PRM):** Detailed reasons why step-level rewards were difficult to scale or prone to \"reward hacking\" compared to the outcome-based rewards used in R1.\n*   **Monte Carlo Tree Search (MCTS):** Technical explanation of why MCTS didn't provide the expected gains over simple RL in the context of LLM reasoning.\n\n#### E. New Benchmarks \u0026amp; Technical Specs\n*   **Updated Results:** Includes more recent evaluations on benchmarks like **AIME 2025**, **LiveCodeBench**, and specialized medical/legal reasoning tests.\n*   **Hyperparameters:** The new version includes exhaustive tables of training hyperparameters (learning rates, GRPO group sizes, KL divergence coefficients) which were previously withheld or summarized.\n\n### Summary Table\n| Feature | v1 (Original) | Latest Version (Extension) |\n| :--- | :--- | :--- |\n| **Page Count** | ~22 Pages | **86+ Pages** |\n| **Methodology** | High-level 4-stage overview | Granular detail on each stage (SFT, RL, Rejection Sampling) |\n| **Behaviors** | Mentions \"self-correction\" | Deep dive into \"Aha Moment\" with case studies |\n| **Failed Paths** | Brief mention of PRM/MCTS | Exhaustive analysis of why PRM and MCTS underperformed |\n| **Distillation** | Introduced 1.5B to 70B models | Added deep ablation on distillation efficiency and data filtering |\n| **Hyperparameters** | Partial/Summary | **Complete Technical Specs** for reproducibility |",
          "created_at": "2026-01-07T13:03:12Z",
          "was_summarised": false
        }
      ],
      "web_blocks": [
        {
          "url": "https://www.reddit.com/gallery/1q6c9wc",
          "was_fetched": false,
          "was_summarised": false
        },
        {
          "url": "https://arxiv.org/abs/2501.12948",
          "was_fetched": false,
          "was_summarised": false
        }
      ],
      "quality": {
        "processor_name": "is_relevant",
        "result": "pass",
        "score": 0.9,
        "reason": "The post announces a significant update to DeepSeek-R1's paper, expanding from 22 to 86 pages with added technical details, indicating a major release with substantial new information on AI model development.",
        "processed_at": "2026-01-12T10:56:08.613947407Z"
      },
      "processed_at": "2026-01-09T03:44:48.92329457Z",
      "errors": [
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://www.reddit.com/gallery/1q6c9wc: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923280221Z"
        },
        {
          "processor_name": "reddit",
          "stage": "source",
          "error": "jina fetch https://arxiv.org/abs/2501.12948: jina: context canceled",
          "occurred_at": "2026-01-09T03:44:48.923294319Z"
        }
      ]
    }
  ]
}