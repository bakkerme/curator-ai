server:
  port: "8080"
  host: "0.0.0.0"

database:
  path: "./data/badger"

llm:
  provider: "openai"
  endpoint: "http://localhost:11434"
  model: "qwen3-30b-a3b-mlx"
  options:
    temperature: "0.7"
    top_p: "0.9"

pipeline:
  config_path: "./configs/pipeline.yaml"
  data_path: "./data/pipeline"
  
  # Default processing settings
  batch_size: 100
  max_retries: 3
  timeout: "30s"